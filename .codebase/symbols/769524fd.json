{
  "file_path": "/work/context-engine/scripts/ingest/pipeline.py",
  "file_hash": "412e0e18f2293cf0c46777bfafc2d27664158b5b",
  "updated_at": "2025-12-26T17:34:21.482665",
  "symbols": {
    "function__detect_repo_name_from_path_85": {
      "name": "_detect_repo_name_from_path",
      "type": "function",
      "start_line": 85,
      "end_line": 91,
      "content_hash": "dd73a04200e8078fd1be58d9380c60a2d0dfaaa8",
      "content": "def _detect_repo_name_from_path(path: Path) -> str:\n    \"\"\"Wrapper function to use workspace_state repository detection.\"\"\"\n    try:\n        from scripts.workspace_state import _extract_repo_name_from_path as _ws_detect\n        return _ws_detect(str(path))\n    except ImportError:\n        return path.name if path.is_dir() else path.parent.name",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_detect_language_94": {
      "name": "detect_language",
      "type": "function",
      "start_line": 94,
      "end_line": 104,
      "content_hash": "bbb81d9ee6e6ac5fdf943a809c2d0cdaffcfc757",
      "content": "def detect_language(path: Path) -> str:\n    \"\"\"Detect language from file extension or name pattern.\"\"\"\n    ext_lang = CODE_EXTS.get(path.suffix.lower())\n    if ext_lang:\n        return ext_lang\n    fname_lower = path.name.lower()\n    if fname_lower in EXTENSIONLESS_FILES:\n        return EXTENSIONLESS_FILES[fname_lower]\n    if fname_lower.startswith(\"dockerfile\"):\n        return \"dockerfile\"\n    return \"unknown\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_build_information_107": {
      "name": "build_information",
      "type": "function",
      "start_line": 107,
      "end_line": 114,
      "content_hash": "290ed15d5e3eedb537f3da5c491d4bdb65e42528",
      "content": "def build_information(\n    language: str, path: Path, start: int, end: int, first_line: str\n) -> str:\n    \"\"\"Build the information/document string for a chunk.\"\"\"\n    first_line = (first_line or \"\").strip()\n    if len(first_line) > 160:\n        first_line = first_line[:160] + \"\u2026\"\n    return f\"{language} code from {path} lines {start}-{end}. {first_line}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_index_single_file_117": {
      "name": "index_single_file",
      "type": "function",
      "start_line": 117,
      "end_line": 164,
      "content_hash": "65018cf7f50fa6383634e54f4e7533541c43f3b6",
      "content": "def index_single_file(\n    client: QdrantClient,\n    model: \"TextEmbedding\",\n    collection: str,\n    vector_name: str,\n    file_path: Path,\n    *,\n    dedupe: bool = True,\n    skip_unchanged: bool = True,\n    pseudo_mode: str = \"full\",\n    trust_cache: bool | None = None,\n    repo_name_for_cache: str | None = None,\n) -> bool:\n    \"\"\"Index a single file path. Returns True if indexed, False if skipped.\"\"\"\n    try:\n        if _should_skip_explicit_file_by_excluder(file_path):\n            try:\n                delete_points_by_path(client, collection, str(file_path))\n            except Exception:\n                pass\n            print(f\"Skipping excluded file: {file_path}\")\n            return False\n    except Exception:\n        return False\n\n    _file_lock_ctx = None\n    if file_indexing_lock is not None:\n        try:\n            _file_lock_ctx = file_indexing_lock(str(file_path))\n            _file_lock_ctx.__enter__()\n        except FileExistsError:\n            print(f\"[FILE_LOCKED] Skipping {file_path} - another process is indexing it\")\n            return False\n        except Exception:\n            pass\n\n    try:\n        return _index_single_file_inner(\n            client, model, collection, vector_name, file_path,\n            dedupe=dedupe, skip_unchanged=skip_unchanged, pseudo_mode=pseudo_mode,\n            trust_cache=trust_cache, repo_name_for_cache=repo_name_for_cache,\n        )\n    finally:\n        if _file_lock_ctx is not None:\n            try:\n                _file_lock_ctx.__exit__(None, None, None)\n            except Exception:\n                pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__index_single_file_inner_167": {
      "name": "_index_single_file_inner",
      "type": "function",
      "start_line": 167,
      "end_line": 492,
      "content_hash": "1928a52ba2bbc675a0906f779836de6abaa8335b",
      "content": "def _index_single_file_inner(\n    client: QdrantClient,\n    model: \"TextEmbedding\",\n    collection: str,\n    vector_name: str,\n    file_path: Path,\n    *,\n    dedupe: bool = True,\n    skip_unchanged: bool = True,\n    pseudo_mode: str = \"full\",\n    trust_cache: bool | None = None,\n    repo_name_for_cache: str | None = None,\n) -> bool:\n    \"\"\"Inner implementation of index_single_file (after lock is acquired).\"\"\"\n    if trust_cache is None:\n        try:\n            trust_cache = os.environ.get(\"INDEX_TRUST_CACHE\", \"\").strip().lower() in {\n                \"1\", \"true\", \"yes\", \"on\",\n            }\n        except Exception:\n            trust_cache = False\n\n    fast_fs = _env_truthy(os.environ.get(\"INDEX_FS_FASTPATH\"), False)\n    if skip_unchanged and fast_fs and get_cached_file_meta is not None:\n        try:\n            repo_for_cache = repo_name_for_cache or _detect_repo_name_from_path(file_path)\n            meta = get_cached_file_meta(str(file_path), repo_for_cache) or {}\n            size = meta.get(\"size\")\n            mtime = meta.get(\"mtime\")\n            if size is not None and mtime is not None:\n                st = file_path.stat()\n                if int(getattr(st, \"st_size\", 0)) == int(size) and int(\n                    getattr(st, \"st_mtime\", 0)\n                ) == int(mtime):\n                    print(f\"Skipping unchanged file (fs-meta): {file_path}\")\n                    return False\n        except Exception:\n            pass\n\n    try:\n        text = file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n    except Exception as e:\n        print(f\"Skipping {file_path}: {e}\")\n        return False\n\n    language = detect_language(file_path)\n    file_hash = hashlib.sha1(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n\n    repo_tag = repo_name_for_cache or _detect_repo_name_from_path(file_path)\n\n    repo_id: str | None = None\n    repo_rel_path: str | None = None\n    if logical_repo_reuse_enabled() and get_workspace_state is not None:\n        try:\n            ws_root = os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"/work\"\n            state = get_workspace_state(ws_root, repo_tag)\n            lrid = state.get(\"logical_repo_id\") if isinstance(state, dict) else None\n            if isinstance(lrid, str) and lrid:\n                repo_id = lrid\n            try:\n                fp = file_path.resolve()\n            except Exception:\n                fp = file_path\n            try:\n                ws_base = Path(os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"/work\").resolve()\n                repo_root = ws_base\n                if repo_tag:\n                    candidate = ws_base / repo_tag\n                    if candidate.exists():\n                        repo_root = candidate\n                rel = fp.relative_to(repo_root)\n                repo_rel_path = rel.as_posix()\n            except Exception:\n                repo_rel_path = None\n        except Exception as e:\n            print(f\"[logical_repo] Failed to derive logical identity for {file_path}: {e}\")\n\n    changed_symbols = set()\n    if get_cached_symbols and set_cached_symbols:\n        cached_symbols = get_cached_symbols(str(file_path))\n        if cached_symbols:\n            current_symbols = extract_symbols_with_tree_sitter(str(file_path))\n            _, changed = compare_symbol_changes(cached_symbols, current_symbols)\n            for symbol_data in current_symbols.values():\n                symbol_id = f\"{symbol_data['type']}_{symbol_data['name']}_{symbol_data['start_line']}\"\n                if symbol_id in changed:\n                    changed_symbols.add(symbol_id)\n\n    if skip_unchanged:\n        ws_path = os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"/work\"\n        try:\n            if get_cached_file_hash:\n                prev_local = get_cached_file_hash(str(file_path), repo_tag)\n                if prev_local and file_hash and prev_local == file_hash:\n                    if fast_fs and set_cached_file_hash:\n                        try:\n                            set_cached_file_hash(str(file_path), file_hash, repo_tag)\n                        except Exception:\n                            pass\n                    print(f\"Skipping unchanged file (cache): {file_path}\")\n                    return False\n        except Exception:\n            pass\n\n        if not trust_cache:\n            prev = get_indexed_file_hash(\n                client, collection, str(file_path),\n                repo_id=repo_id, repo_rel_path=repo_rel_path,\n            )\n            if prev and prev == file_hash:\n                if fast_fs and set_cached_file_hash:\n                    try:\n                        set_cached_file_hash(str(file_path), file_hash, repo_tag)\n                    except Exception:\n                        pass\n                print(f\"Skipping unchanged file: {file_path}\")\n                return False\n\n    if dedupe:\n        delete_points_by_path(client, collection, str(file_path))\n\n    symbols = _extract_symbols(language, text)\n    imports, calls = _get_imports_calls(language, text)\n    last_mod, churn_count, author_count = _git_metadata(file_path)\n\n    CHUNK_LINES = int(os.environ.get(\"INDEX_CHUNK_LINES\", \"120\") or 120)\n    CHUNK_OVERLAP = int(os.environ.get(\"INDEX_CHUNK_OVERLAP\", \"20\") or 20)\n    use_micro = os.environ.get(\"INDEX_MICRO_CHUNKS\", \"0\").lower() in {\"1\", \"true\", \"yes\", \"on\"}\n    use_semantic = os.environ.get(\"INDEX_SEMANTIC_CHUNKS\", \"1\").lower() in {\"1\", \"true\", \"yes\", \"on\"}\n\n    if use_micro:\n        try:\n            _cap = int(os.environ.get(\"MAX_MICRO_CHUNKS_PER_FILE\", \"200\") or 200)\n            _base_tokens = int(os.environ.get(\"MICRO_CHUNK_TOKENS\", \"128\") or 128)\n            _base_stride = int(os.environ.get(\"MICRO_CHUNK_STRIDE\", \"64\") or 64)\n            chunks = chunk_by_tokens(text, k_tokens=_base_tokens, stride_tokens=_base_stride)\n            if _cap > 0 and len(chunks) > _cap:\n                _before = len(chunks)\n                _scale = (len(chunks) / _cap) * 1.1\n                _new_tokens = max(_base_tokens, int(_base_tokens * _scale))\n                _new_stride = max(_base_stride, int(_base_stride * _scale))\n                chunks = chunk_by_tokens(text, k_tokens=_new_tokens, stride_tokens=_new_stride)\n                try:\n                    print(\n                        f\"[ingest] micro-chunks resized path={file_path} count={_before}->{len(chunks)} \"\n                        f\"tokens={_base_tokens}->{_new_tokens} stride={_base_stride}->{_new_stride}\"\n                    )\n                except Exception:\n                    pass\n        except Exception:\n            chunks = chunk_by_tokens(text)\n    elif use_semantic:\n        chunks = chunk_semantic(text, language, CHUNK_LINES, CHUNK_OVERLAP)\n    else:\n        chunks = chunk_lines(text, CHUNK_LINES, CHUNK_OVERLAP)\n\n    batch_texts: List[str] = []\n    batch_meta: List[Dict] = []\n    batch_ids: List[int] = []\n    batch_lex: List[list[float]] = []\n    batch_lex_text: List[str] = []\n\n    def make_point(pid, dense_vec, lex_vec, payload, lex_text: str = \"\"):\n        if vector_name:\n            vecs = {vector_name: dense_vec, LEX_VECTOR_NAME: lex_vec}\n            try:\n                if os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}:\n                    vecs[MINI_VECTOR_NAME] = project_mini(list(dense_vec), MINI_VEC_DIM)\n            except Exception:\n                pass\n            if LEX_SPARSE_MODE and lex_text:\n                sparse_vec = _lex_sparse_vector_text(lex_text)\n                if sparse_vec.get(\"indices\"):\n                    vecs[LEX_SPARSE_NAME] = models.SparseVector(**sparse_vec)\n            return models.PointStruct(id=pid, vector=vecs, payload=payload)\n        else:\n            return models.PointStruct(id=pid, vector=dense_vec, payload=payload)\n\n    pseudo_batch_concurrency = int(os.environ.get(\"PSEUDO_BATCH_CONCURRENCY\", \"1\") or 1)\n    use_batch_pseudo = pseudo_batch_concurrency > 1 and pseudo_mode == \"full\"\n\n    chunk_data: list[dict] = []\n    for ch in chunks:\n        info = build_information(\n            language, file_path, ch[\"start\"], ch[\"end\"],\n            ch[\"text\"].splitlines()[0] if ch[\"text\"] else \"\",\n        )\n        kind, sym, sym_path = _choose_symbol_for_chunk(ch[\"start\"], ch[\"end\"], symbols)\n        if \"kind\" in ch and ch.get(\"kind\"):\n            kind = ch.get(\"kind\") or kind\n        if \"symbol\" in ch and ch.get(\"symbol\"):\n            sym = ch.get(\"symbol\") or sym\n        if \"symbol_path\" in ch and ch.get(\"symbol_path\"):\n            sym_path = ch.get(\"symbol_path\") or sym_path\n        if not ch.get(\"kind\") and kind:\n            ch[\"kind\"] = kind\n        if not ch.get(\"symbol\") and sym:\n            ch[\"symbol\"] = sym\n        if not ch.get(\"symbol_path\") and sym_path:\n            ch[\"symbol_path\"] = sym_path\n\n        _cur_path = str(file_path)\n        _host_path, _container_path = _compute_host_and_container_paths(_cur_path)\n\n        payload = {\n            \"document\": info,\n            \"information\": info,\n            \"metadata\": {\n                \"path\": str(file_path),\n                \"path_prefix\": str(file_path.parent),\n                \"ext\": str(file_path.suffix).lstrip(\".\").lower(),\n                \"language\": language,\n                \"kind\": kind,\n                \"symbol\": sym,\n                \"symbol_path\": sym_path,\n                \"repo\": repo_tag,\n                \"start_line\": ch[\"start\"],\n                \"end_line\": ch[\"end\"],\n                \"code\": ch[\"text\"],\n                \"file_hash\": file_hash,\n                \"imports\": imports,\n                \"calls\": calls,\n                \"ingested_at\": int(time.time()),\n                \"last_modified_at\": int(last_mod),\n                \"churn_count\": int(churn_count),\n                \"author_count\": int(author_count),\n                \"repo_id\": repo_id,\n                \"repo_rel_path\": repo_rel_path,\n                \"host_path\": _host_path,\n                \"container_path\": _container_path,\n            },\n        }\n\n        needs_pseudo_gen = False\n        cached_pseudo, cached_tags = \"\", []\n        if pseudo_mode != \"off\":\n            needs_pseudo_gen, cached_pseudo, cached_tags = should_process_pseudo_for_chunk(\n                str(file_path), ch, changed_symbols\n            )\n\n        chunk_data.append({\n            \"chunk\": ch,\n            \"info\": info,\n            \"payload\": payload,\n            \"kind\": kind,\n            \"needs_pseudo\": needs_pseudo_gen and pseudo_mode == \"full\",\n            \"cached_pseudo\": cached_pseudo,\n            \"cached_tags\": cached_tags,\n        })\n\n    if use_batch_pseudo:\n        pending_indices = [i for i, cd in enumerate(chunk_data) if cd[\"needs_pseudo\"]]\n        pending_texts = [chunk_data[i][\"chunk\"].get(\"text\") or \"\" for i in pending_indices]\n\n        if pending_texts:\n            try:\n                from scripts.refrag_glm import generate_pseudo_tags_batch\n                batch_results = generate_pseudo_tags_batch(pending_texts, concurrency=pseudo_batch_concurrency)\n                for idx, (pseudo, tags) in zip(pending_indices, batch_results):\n                    chunk_data[idx][\"cached_pseudo\"] = pseudo\n                    chunk_data[idx][\"cached_tags\"] = tags\n                    chunk_data[idx][\"needs_pseudo\"] = False\n                    if pseudo or tags:\n                        ch = chunk_data[idx][\"chunk\"]\n                        symbol_name = ch.get(\"symbol\", \"\")\n                        if symbol_name and set_cached_pseudo:\n                            k = ch.get(\"kind\", \"unknown\")\n                            start_line = ch.get(\"start\", 0)\n                            symbol_id = f\"{k}_{symbol_name}_{start_line}\"\n                            set_cached_pseudo(str(file_path), symbol_id, pseudo, tags, file_hash)\n            except Exception as e:\n                print(f\"[PSEUDO_BATCH] Batch failed, falling back to sequential: {e}\")\n                use_batch_pseudo = False\n\n    for cd in chunk_data:\n        ch = cd[\"chunk\"]\n        payload = cd[\"payload\"]\n        pseudo = cd[\"cached_pseudo\"]\n        tags = cd[\"cached_tags\"]\n\n        if not use_batch_pseudo and cd[\"needs_pseudo\"]:\n            try:\n                pseudo, tags = generate_pseudo_tags(ch.get(\"text\") or \"\")\n                if pseudo or tags:\n                    symbol_name = ch.get(\"symbol\", \"\")\n                    if symbol_name:\n                        kind = ch.get(\"kind\", \"unknown\")\n                        start_line = ch.get(\"start\", 0)\n                        symbol_id = f\"{kind}_{symbol_name}_{start_line}\"\n                        if set_cached_pseudo:\n                            set_cached_pseudo(str(file_path), symbol_id, pseudo, tags, file_hash)\n            except Exception:\n                pass\n\n        if pseudo:\n            payload[\"pseudo\"] = pseudo\n        if tags:\n            payload[\"tags\"] = tags\n        batch_texts.append(cd[\"info\"])\n        batch_meta.append(payload)\n        batch_ids.append(hash_id(ch[\"text\"], str(file_path), ch[\"start\"], ch[\"end\"]))\n        aug_lex_text = (ch.get(\"text\") or \"\") + (\" \" + pseudo if pseudo else \"\") + (\" \" + \" \".join(tags) if tags else \"\")\n        batch_lex.append(_lex_hash_vector_text(aug_lex_text))\n        batch_lex_text.append(aug_lex_text)\n\n    if batch_texts:\n        vectors = embed_batch(model, batch_texts)\n        for _idx, _m in enumerate(batch_meta):\n            try:\n                _m[\"pid_str\"] = str(batch_ids[_idx])\n            except Exception:\n                pass\n        points = [\n            make_point(i, v, lx, m, lt)\n            for i, v, lx, m, lt in zip(batch_ids, vectors, batch_lex, batch_meta, batch_lex_text)\n        ]\n        upsert_points(client, collection, points)\n        try:\n            ws = os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"/work\"\n            if set_cached_file_hash:\n                file_repo_tag = repo_tag\n                set_cached_file_hash(str(file_path), file_hash, file_repo_tag)\n        except Exception:\n            pass\n        return True\n    return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_make_point_329": {
      "name": "make_point",
      "type": "function",
      "start_line": 329,
      "end_line": 343,
      "content_hash": "04145f5f9bba0e31e5abb845156c83d02d6432ef",
      "content": "    def make_point(pid, dense_vec, lex_vec, payload, lex_text: str = \"\"):\n        if vector_name:\n            vecs = {vector_name: dense_vec, LEX_VECTOR_NAME: lex_vec}\n            try:\n                if os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}:\n                    vecs[MINI_VECTOR_NAME] = project_mini(list(dense_vec), MINI_VEC_DIM)\n            except Exception:\n                pass\n            if LEX_SPARSE_MODE and lex_text:\n                sparse_vec = _lex_sparse_vector_text(lex_text)\n                if sparse_vec.get(\"indices\"):\n                    vecs[LEX_SPARSE_NAME] = models.SparseVector(**sparse_vec)\n            return models.PointStruct(id=pid, vector=vecs, payload=payload)\n        else:\n            return models.PointStruct(id=pid, vector=dense_vec, payload=payload)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_index_repo_495": {
      "name": "index_repo",
      "type": "function",
      "start_line": 495,
      "end_line": 627,
      "content_hash": "86c1e9bd397974437d8505ec4026dffeaa9fd962",
      "content": "def index_repo(\n    root: Path,\n    qdrant_url: str,\n    api_key: str,\n    collection: str,\n    model_name: str,\n    recreate: bool,\n    *,\n    dedupe: bool = True,\n    skip_unchanged: bool = True,\n    pseudo_mode: str = \"full\",\n):\n    \"\"\"Index a repository into Qdrant.\"\"\"\n    fast_fs = _env_truthy(os.environ.get(\"INDEX_FS_FASTPATH\"), False)\n    if skip_unchanged and not recreate and fast_fs and get_cached_file_meta is not None:\n        try:\n            is_multi_repo = bool(is_multi_repo_mode and is_multi_repo_mode())\n            root_repo_for_cache = (\n                _detect_repo_name_from_path(root)\n                if (not is_multi_repo and _detect_repo_name_from_path)\n                else None\n            )\n            all_unchanged = True\n            for file_path in iter_files(root):\n                per_file_repo_for_cache = (\n                    root_repo_for_cache\n                    if root_repo_for_cache is not None\n                    else (\n                        _detect_repo_name_from_path(file_path)\n                        if _detect_repo_name_from_path\n                        else None\n                    )\n                )\n                meta = get_cached_file_meta(str(file_path), per_file_repo_for_cache) or {}\n                size = meta.get(\"size\")\n                mtime = meta.get(\"mtime\")\n                if size is None or mtime is None:\n                    all_unchanged = False\n                    break\n                st = file_path.stat()\n                if int(getattr(st, \"st_size\", 0)) != int(size) or int(getattr(st, \"st_mtime\", 0)) != int(mtime):\n                    all_unchanged = False\n                    break\n            if all_unchanged:\n                try:\n                    print(\"[fast_index] No changes detected via fs metadata; skipping model and Qdrant setup\")\n                except Exception:\n                    pass\n                return\n        except Exception:\n            pass\n\n    try:\n        from scripts.embedder import get_embedding_model, get_model_dimension\n        model = get_embedding_model(model_name)\n        dim = get_model_dimension(model_name)\n    except ImportError:\n        from fastembed import TextEmbedding\n        model = TextEmbedding(model_name=model_name)\n        dim = len(next(model.embed([\"dimension probe\"])))\n\n    client = QdrantClient(\n        url=qdrant_url,\n        api_key=api_key or None,\n        timeout=int(os.environ.get(\"QDRANT_TIMEOUT\", \"20\") or 20),\n    )\n\n    if recreate:\n        vector_name = _sanitize_vector_name(model_name)\n    else:\n        vector_name = None\n        try:\n            info = client.get_collection(collection)\n            cfg = info.config.params.vectors\n            if isinstance(cfg, dict) and cfg:\n                for name, params in cfg.items():\n                    psize = getattr(params, \"size\", None) or getattr(params, \"dim\", None)\n                    if psize and int(psize) == int(dim):\n                        vector_name = name\n                        break\n                if vector_name is None and LEX_VECTOR_NAME in cfg:\n                    for name in cfg.keys():\n                        if name != LEX_VECTOR_NAME:\n                            vector_name = name\n                            break\n        except Exception:\n            pass\n        if vector_name is None:\n            vector_name = _sanitize_vector_name(model_name)\n\n    if recreate:\n        recreate_collection(client, collection, dim, vector_name)\n\n    try:\n        ensure_collection_and_indexes_once(client, collection, dim, vector_name)\n    except Exception:\n        ensure_collection(client, collection, dim, vector_name)\n        ensure_payload_indexes(client, collection)\n\n    is_multi_repo = bool(is_multi_repo_mode and is_multi_repo_mode())\n    root_repo_for_cache = (\n        _detect_repo_name_from_path(root)\n        if (not is_multi_repo and _detect_repo_name_from_path)\n        else None\n    )\n\n    try:\n        from tqdm import tqdm\n        files = list(iter_files(root))\n        iterator = tqdm(files, desc=\"Indexing files\")\n    except ImportError:\n        files = list(iter_files(root))\n        iterator = files\n\n    for file_path in iterator:\n        per_file_repo_for_cache = (\n            root_repo_for_cache\n            if root_repo_for_cache is not None\n            else (\n                _detect_repo_name_from_path(file_path)\n                if _detect_repo_name_from_path\n                else None\n            )\n        )\n        try:\n            index_single_file(\n                client, model, collection, vector_name, file_path,\n                dedupe=dedupe, skip_unchanged=skip_unchanged,\n                pseudo_mode=pseudo_mode,\n                repo_name_for_cache=per_file_repo_for_cache,\n            )\n        except Exception as e:\n            print(f\"Error indexing {file_path}: {e}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_process_file_with_smart_reindexing_630": {
      "name": "process_file_with_smart_reindexing",
      "type": "function",
      "start_line": 630,
      "end_line": 1060,
      "content_hash": "09bec6a31c2f9b452c0e74530edd545112886ae5",
      "content": "def process_file_with_smart_reindexing(\n    file_path,\n    text: str,\n    language: str,\n    client: QdrantClient,\n    current_collection: str,\n    per_file_repo,\n    model,\n    vector_name: str | None,\n) -> str:\n    \"\"\"Smart, chunk-level reindexing for a single file.\n\n    Rebuilds all points for the file with *accurate* line numbers while:\n    - Reusing existing embeddings/lexical vectors for unchanged chunks (by code content), and\n    - Re-embedding only for changed chunks.\n    \"\"\"\n    # Allow test monkeypatching on ingest_code.* to be honored here.\n    # Must be done FIRST before any helper calls.\n    _ingest_mod = None\n    try:\n        import importlib\n        _ingest_mod = importlib.import_module(\"scripts.ingest_code\")\n    except Exception:\n        _ingest_mod = None\n    _embed_batch = getattr(_ingest_mod, \"embed_batch\", embed_batch) if _ingest_mod else embed_batch\n    _upsert_points_fn = getattr(_ingest_mod, \"upsert_points\", upsert_points) if _ingest_mod else upsert_points\n    _delete_points_fn = getattr(_ingest_mod, \"delete_points_by_path\", delete_points_by_path) if _ingest_mod else delete_points_by_path\n    _should_process_pseudo = getattr(_ingest_mod, \"should_process_pseudo_for_chunk\", should_process_pseudo_for_chunk) if _ingest_mod else should_process_pseudo_for_chunk\n\n    try:\n        p = Path(str(file_path))\n        if _should_skip_explicit_file_by_excluder(p):\n            try:\n                _delete_points_fn(client, current_collection, str(p))\n            except Exception:\n                pass\n            print(f\"[SMART_REINDEX] Skipping excluded file: {file_path}\")\n            return \"skipped\"\n    except Exception:\n        return \"skipped\"\n\n    print(f\"[SMART_REINDEX] Processing {file_path} with chunk-level reindexing\")\n\n    try:\n        fp = str(file_path)\n    except Exception:\n        fp = str(file_path)\n    try:\n        if not isinstance(file_path, Path):\n            file_path = Path(fp)\n    except Exception:\n        file_path = Path(fp)\n\n    file_hash = hashlib.sha1(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n\n    repo_id: str | None = None\n    repo_rel_path: str | None = None\n    try:\n        repo_tag = per_file_repo\n        if logical_repo_reuse_enabled() and get_workspace_state is not None:\n            ws_root = os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"/work\"\n            state = get_workspace_state(ws_root, repo_tag)\n            lrid = state.get(\"logical_repo_id\") if isinstance(state, dict) else None\n            if isinstance(lrid, str) and lrid.strip():\n                repo_id = lrid.strip()\n            try:\n                ws_base = Path(ws_root).resolve()\n                repo_root = ws_base\n                if repo_tag:\n                    candidate = ws_base / str(repo_tag)\n                    if candidate.exists():\n                        repo_root = candidate\n                rel = file_path.resolve().relative_to(repo_root)\n                repo_rel_path = rel.as_posix()\n            except Exception:\n                repo_rel_path = None\n    except Exception as e:\n        print(f\"[SMART_REINDEX] Failed to derive logical repo identity for {file_path}: {e}\")\n\n    symbol_meta = extract_symbols_with_tree_sitter(fp)\n    if not symbol_meta:\n        print(f\"[SMART_REINDEX] No symbols found in {file_path}, falling back to full reindex\")\n        return \"failed\"\n\n    cached_symbols = get_cached_symbols(fp) if get_cached_symbols else {}\n    unchanged_symbols: list[str] = []\n    changed_symbols: list[str] = []\n    if cached_symbols and compare_symbol_changes:\n        try:\n            unchanged_symbols, changed_symbols = compare_symbol_changes(\n                cached_symbols, symbol_meta\n            )\n        except Exception:\n            unchanged_symbols = []\n            changed_symbols = list(symbol_meta.keys())\n    else:\n        changed_symbols = list(symbol_meta.keys())\n    changed_set = set(changed_symbols)\n\n    if len(changed_symbols) == 0 and cached_symbols:\n        print(f\"[SMART_REINDEX] {file_path}: 0 changes detected, skipping\")\n        return \"skipped\"\n\n    existing_points = []\n    try:\n        filt = models.Filter(\n            must=[\n                models.FieldCondition(\n                    key=\"metadata.path\", match=models.MatchValue(value=fp)\n                )\n            ]\n        )\n        next_offset = None\n        while True:\n            pts, next_offset = client.scroll(\n                collection_name=current_collection,\n                scroll_filter=filt,\n                with_payload=True,\n                with_vectors=True,\n                limit=256,\n                offset=next_offset,\n            )\n            if not pts:\n                break\n            existing_points.extend(pts)\n            if next_offset is None:\n                break\n    except Exception as e:\n        print(f\"[SMART_REINDEX] Failed to load existing points for {file_path}: {e}\")\n        existing_points = []\n\n    points_by_code: dict[tuple[str, str, str], list] = {}\n    try:\n        for rec in existing_points:\n            payload = rec.payload or {}\n            md = payload.get(\"metadata\") or {}\n            code_text = md.get(\"code\") or \"\"\n            embed_text = payload.get(\"information\") or payload.get(\"document\") or \"\"\n            kind = md.get(\"kind\") or \"\"\n            sym_name = md.get(\"symbol\") or \"\"\n            start_line = md.get(\"start_line\") or 0\n            symbol_id = (\n                f\"{kind}_{sym_name}_{start_line}\"\n                if kind and sym_name and start_line\n                else \"\"\n            )\n            key = (symbol_id, code_text, embed_text) if symbol_id else (\"\", code_text, embed_text)\n            points_by_code.setdefault(key, []).append(rec)\n    except Exception:\n        points_by_code = {}\n\n    CHUNK_LINES = int(os.environ.get(\"INDEX_CHUNK_LINES\", \"120\") or 120)\n    CHUNK_OVERLAP = int(os.environ.get(\"INDEX_CHUNK_OVERLAP\", \"20\") or 20)\n    use_micro = os.environ.get(\"INDEX_MICRO_CHUNKS\", \"0\").lower() in {\"1\", \"true\", \"yes\", \"on\"}\n    use_semantic = os.environ.get(\"INDEX_SEMANTIC_CHUNKS\", \"1\").lower() in {\"1\", \"true\", \"yes\", \"on\"}\n\n    if use_micro:\n        chunks = chunk_by_tokens(text)\n    elif use_semantic:\n        chunks = chunk_semantic(text, language, CHUNK_LINES, CHUNK_OVERLAP)\n    else:\n        chunks = chunk_lines(text, CHUNK_LINES, CHUNK_OVERLAP)\n    \n    symbol_spans = _extract_symbols(language, text)\n\n    reused_points: list[models.PointStruct] = []\n    embed_texts: list[str] = []\n    embed_payloads: list[dict] = []\n    embed_ids: list[int] = []\n    embed_lex: list[list[float]] = []\n    embed_lex_text: list[str] = []\n\n    imports, calls = _get_imports_calls(language, text)\n    last_mod, churn_count, author_count = _git_metadata(file_path)\n\n    pseudo_batch_concurrency = int(os.environ.get(\"PSEUDO_BATCH_CONCURRENCY\", \"1\") or 1)\n    use_batch_pseudo = pseudo_batch_concurrency > 1\n\n    chunk_data_sr: list[dict] = []\n    for ch in chunks:\n        info = build_information(\n            language, file_path, ch[\"start\"], ch[\"end\"],\n            ch[\"text\"].splitlines()[0] if ch[\"text\"] else \"\",\n        )\n        kind, sym, sym_path = _choose_symbol_for_chunk(ch[\"start\"], ch[\"end\"], symbol_spans)\n        if \"kind\" in ch and ch.get(\"kind\"):\n            kind = ch.get(\"kind\") or kind\n        if \"symbol\" in ch and ch.get(\"symbol\"):\n            sym = ch.get(\"symbol\") or sym\n        if \"symbol_path\" in ch and ch.get(\"symbol_path\"):\n            sym_path = ch.get(\"symbol_path\") or sym_path\n        if not ch.get(\"kind\") and kind:\n            ch[\"kind\"] = kind\n        if not ch.get(\"symbol\") and sym:\n            ch[\"symbol\"] = sym\n        if not ch.get(\"symbol_path\") and sym_path:\n            ch[\"symbol_path\"] = sym_path\n\n        _cur_path = str(file_path)\n        _host_path, _container_path = _compute_host_and_container_paths(_cur_path)\n\n        payload = {\n            \"document\": info,\n            \"information\": info,\n            \"metadata\": {\n                \"path\": str(file_path),\n                \"path_prefix\": str(file_path.parent),\n                \"ext\": str(file_path.suffix).lstrip(\".\").lower(),\n                \"language\": language,\n                \"kind\": kind,\n                \"symbol\": sym,\n                \"symbol_path\": sym_path or \"\",\n                \"repo\": per_file_repo,\n                \"start_line\": ch[\"start\"],\n                \"end_line\": ch[\"end\"],\n                \"code\": ch[\"text\"],\n                \"file_hash\": file_hash,\n                \"imports\": imports,\n                \"calls\": calls,\n                \"ingested_at\": int(time.time()),\n                \"last_modified_at\": int(last_mod),\n                \"churn_count\": int(churn_count),\n                \"author_count\": int(author_count),\n                \"host_path\": _host_path,\n                \"container_path\": _container_path,\n                \"repo_id\": repo_id,\n                \"repo_rel_path\": repo_rel_path,\n            },\n        }\n\n        needs_pseudo_gen, cached_pseudo, cached_tags = _should_process_pseudo(\n            fp, ch, changed_set\n        )\n\n        chunk_data_sr.append({\n            \"chunk\": ch,\n            \"info\": info,\n            \"payload\": payload,\n            \"kind\": kind,\n            \"sym\": sym,\n            \"sym_path\": sym_path,\n            \"needs_pseudo\": needs_pseudo_gen,\n            \"cached_pseudo\": cached_pseudo,\n            \"cached_tags\": cached_tags,\n        })\n\n    if use_batch_pseudo:\n        pending_indices = [i for i, cd in enumerate(chunk_data_sr) if cd[\"needs_pseudo\"]]\n        pending_texts = [chunk_data_sr[i][\"chunk\"].get(\"text\") or \"\" for i in pending_indices]\n\n        if pending_texts:\n            try:\n                from scripts.refrag_glm import generate_pseudo_tags_batch\n                batch_results = generate_pseudo_tags_batch(pending_texts, concurrency=pseudo_batch_concurrency)\n                for idx, (pseudo, tags) in zip(pending_indices, batch_results):\n                    chunk_data_sr[idx][\"cached_pseudo\"] = pseudo\n                    chunk_data_sr[idx][\"cached_tags\"] = tags\n                    chunk_data_sr[idx][\"needs_pseudo\"] = False\n                    if pseudo or tags:\n                        ch = chunk_data_sr[idx][\"chunk\"]\n                        symbol_name = ch.get(\"symbol\", \"\")\n                        if symbol_name and set_cached_pseudo:\n                            k = ch.get(\"kind\", \"unknown\")\n                            start_line = ch.get(\"start\", 0)\n                            sid = f\"{k}_{symbol_name}_{start_line}\"\n                            set_cached_pseudo(fp, sid, pseudo, tags, file_hash)\n            except Exception as e:\n                print(f\"[PSEUDO_BATCH] Smart reindex batch failed, falling back: {e}\")\n                use_batch_pseudo = False\n\n    for cd in chunk_data_sr:\n        ch = cd[\"chunk\"]\n        payload = cd[\"payload\"]\n        pseudo = cd[\"cached_pseudo\"]\n        tags = cd[\"cached_tags\"]\n\n        if not use_batch_pseudo and cd[\"needs_pseudo\"]:\n            try:\n                pseudo, tags = generate_pseudo_tags(ch.get(\"text\") or \"\")\n                if pseudo or tags:\n                    symbol_name = ch.get(\"symbol\", \"\")\n                    if symbol_name:\n                        k = ch.get(\"kind\", \"unknown\")\n                        start_line = ch.get(\"start\", 0)\n                        sid = f\"{k}_{symbol_name}_{start_line}\"\n                        if set_cached_pseudo:\n                            set_cached_pseudo(fp, sid, pseudo, tags, file_hash)\n            except Exception:\n                pass\n\n        if pseudo:\n            payload[\"pseudo\"] = pseudo\n        if tags:\n            payload[\"tags\"] = tags\n\n        info = cd[\"info\"]\n        kind = cd[\"kind\"]\n        sym = cd[\"sym\"]\n\n        code_text = ch.get(\"text\") or \"\"\n        chunk_symbol_id = \"\"\n        if sym and kind:\n            chunk_symbol_id = f\"{kind}_{sym}_{ch['start']}\"\n\n        reuse_key = (chunk_symbol_id, code_text, info)\n        fallback_key = (\"\", code_text, info)\n        reused_rec = None\n        used_key = None\n        bucket = points_by_code.get(reuse_key)\n        if bucket is not None:\n            used_key = reuse_key\n        else:\n            bucket = points_by_code.get(fallback_key)\n            if bucket is not None:\n                used_key = fallback_key\n        if bucket:\n            try:\n                reused_rec = bucket.pop()\n                if not bucket:\n                    if used_key is not None:\n                        points_by_code.pop(used_key, None)\n            except Exception:\n                reused_rec = None\n\n        if reused_rec is not None:\n            try:\n                vec = reused_rec.vector\n                if vector_name and isinstance(vec, dict) and vector_name not in vec:\n                    raise ValueError(\"reused vector missing dense key\")\n                aug_lex_text = (code_text or \"\") + (\" \" + pseudo if pseudo else \"\") + (\n                    \" \" + \" \".join(tags) if tags else \"\"\n                )\n                refreshed_lex = _lex_hash_vector_text(aug_lex_text)\n                if vector_name:\n                    if isinstance(vec, dict):\n                        vec = dict(vec)\n                        vec[LEX_VECTOR_NAME] = refreshed_lex\n                        if LEX_SPARSE_MODE and aug_lex_text:\n                            try:\n                                sparse_vec = _lex_sparse_vector_text(aug_lex_text)\n                                if sparse_vec.get(\"indices\"):\n                                    vec[LEX_SPARSE_NAME] = models.SparseVector(**sparse_vec)\n                            except Exception:\n                                pass\n                    else:\n                        vecs = {vector_name: vec, LEX_VECTOR_NAME: refreshed_lex}\n                        try:\n                            if os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}:\n                                vecs[MINI_VECTOR_NAME] = project_mini(list(vec), MINI_VEC_DIM)\n                        except Exception:\n                            pass\n                        if LEX_SPARSE_MODE and aug_lex_text:\n                            try:\n                                sparse_vec = _lex_sparse_vector_text(aug_lex_text)\n                                if sparse_vec.get(\"indices\"):\n                                    vecs[LEX_SPARSE_NAME] = models.SparseVector(**sparse_vec)\n                            except Exception:\n                                pass\n                        vec = vecs\n                else:\n                    if isinstance(vec, dict):\n                        dense = None\n                        try:\n                            for k, v in vec.items():\n                                if k not in {LEX_VECTOR_NAME, MINI_VECTOR_NAME}:\n                                    dense = v\n                                    break\n                        except Exception:\n                            dense = None\n                        if dense is None:\n                            raise ValueError(\"reused vector has no dense component\")\n                        vec = dense\n                pid = hash_id(code_text, fp, ch[\"start\"], ch[\"end\"])\n                reused_points.append(\n                    models.PointStruct(id=pid, vector=vec, payload=payload)\n                )\n                continue\n            except Exception:\n                pass\n\n        embed_texts.append(info)\n        embed_payloads.append(payload)\n        embed_ids.append(hash_id(code_text, fp, ch[\"start\"], ch[\"end\"]))\n        aug_lex_text = (code_text or \"\") + (\" \" + pseudo if pseudo else \"\") + (\" \" + \" \".join(tags) if tags else \"\")\n        embed_lex.append(_lex_hash_vector_text(aug_lex_text))\n        embed_lex_text.append(aug_lex_text)\n\n    new_points: list[models.PointStruct] = []\n    if embed_texts:\n        vectors = _embed_batch(model, embed_texts)\n        for pid, v, lx, pl, lt in zip(embed_ids, vectors, embed_lex, embed_payloads, embed_lex_text):\n            if vector_name:\n                vecs = {vector_name: v, LEX_VECTOR_NAME: lx}\n                try:\n                    if os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}:\n                        vecs[MINI_VECTOR_NAME] = project_mini(list(v), MINI_VEC_DIM)\n                except Exception:\n                    pass\n                if LEX_SPARSE_MODE and lt:\n                    sparse_vec = _lex_sparse_vector_text(lt)\n                    if sparse_vec.get(\"indices\"):\n                        vecs[LEX_SPARSE_NAME] = models.SparseVector(**sparse_vec)\n                new_points.append(models.PointStruct(id=pid, vector=vecs, payload=pl))\n            else:\n                new_points.append(models.PointStruct(id=pid, vector=v, payload=pl))\n\n    all_points = reused_points + new_points\n\n    try:\n        _delete_points_fn(client, current_collection, fp)\n    except Exception as e:\n        print(f\"[SMART_REINDEX] Failed to delete old points for {file_path}: {e}\")\n\n    if all_points:\n        _upsert_points_fn(client, current_collection, all_points)\n\n    try:\n        if set_cached_symbols:\n            set_cached_symbols(fp, symbol_meta, file_hash)\n    except Exception as e:\n        print(f\"[SMART_REINDEX] Failed to update symbol cache for {file_path}: {e}\")\n    try:\n        if set_cached_file_hash:\n            set_cached_file_hash(fp, file_hash, per_file_repo)\n    except Exception:\n        pass\n\n    print(\n        f\"[SMART_REINDEX] Completed {file_path}: chunks={len(chunks)}, reused_points={len(reused_points)}, embedded_points={len(new_points)}\"\n    )\n    return \"success\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_pseudo_backfill_tick_1064": {
      "name": "pseudo_backfill_tick",
      "type": "function",
      "start_line": 1064,
      "end_line": 1236,
      "content_hash": "f698f7f50065b8e7c7f14f402fbfdbddb6f8fb54",
      "content": "def pseudo_backfill_tick(\n    client: QdrantClient,\n    collection: str,\n    repo_name: str | None = None,\n    *,\n    max_points: int = 256,\n    dim: int | None = None,\n    vector_name: str | None = None,\n) -> int:\n    \"\"\"Best-effort pseudo/tag backfill for a collection.\"\"\"\n    from scripts.ingest.qdrant import ensure_collection_and_indexes_once\n    \n    if not collection or max_points <= 0:\n        return 0\n\n    try:\n        from qdrant_client import models as _models\n    except Exception:\n        return 0\n\n    must_conditions: list[Any] = []\n    if repo_name:\n        try:\n            must_conditions.append(\n                _models.FieldCondition(\n                    key=\"metadata.repo\",\n                    match=_models.MatchValue(value=repo_name),\n                )\n            )\n        except Exception:\n            pass\n\n    flt = None\n    try:\n        null_cond = getattr(_models, \"IsNullCondition\", None)\n        empty_cond = getattr(_models, \"IsEmptyCondition\", None)\n        if null_cond is not None:\n            should_conditions = []\n            try:\n                should_conditions.append(null_cond(is_null=\"pseudo\"))\n            except Exception:\n                pass\n            try:\n                should_conditions.append(null_cond(is_null=\"tags\"))\n            except Exception:\n                pass\n            if empty_cond is not None:\n                try:\n                    should_conditions.append(empty_cond(is_empty=\"tags\"))\n                except Exception:\n                    pass\n            flt = _models.Filter(\n                must=must_conditions or None,\n                should=should_conditions or None,\n            )\n        else:\n            flt = _models.Filter(must=must_conditions or None)\n    except Exception:\n        flt = None\n\n    processed = 0\n    debug_enabled = (os.environ.get(\"PSEUDO_BACKFILL_DEBUG\") or \"\").strip().lower() in {\n        \"1\", \"true\", \"yes\", \"on\",\n    }\n    next_offset = None\n\n    def _maybe_ensure_collection() -> bool:\n        if not dim or not vector_name:\n            return False\n        try:\n            ensure_collection_and_indexes_once(client, collection, int(dim), vector_name)\n            return True\n        except Exception:\n            return False\n\n    while processed < max_points:\n        batch_limit = max(1, min(64, max_points - processed))\n        try:\n            points, next_offset = client.scroll(\n                collection_name=collection,\n                scroll_filter=flt,\n                limit=batch_limit,\n                with_payload=True,\n                with_vectors=True,\n                offset=next_offset,\n            )\n        except Exception:\n            if _maybe_ensure_collection():\n                try:\n                    points, next_offset = client.scroll(\n                        collection_name=collection,\n                        scroll_filter=flt,\n                        limit=batch_limit,\n                        with_payload=True,\n                        with_vectors=True,\n                        offset=next_offset,\n                    )\n                except Exception:\n                    break\n            else:\n                break\n\n        if not points:\n            break\n\n        new_points: list[Any] = []\n        for rec in points:\n            try:\n                payload = rec.payload or {}\n                md = payload.get(\"metadata\") or {}\n                code = md.get(\"code\") or \"\"\n                if not code:\n                    continue\n\n                pseudo = payload.get(\"pseudo\") or \"\"\n                tags_val = payload.get(\"tags\") or []\n                tags: list[str] = list(tags_val) if isinstance(tags_val, list) else []\n\n                if not pseudo and not tags:\n                    try:\n                        pseudo, tags = generate_pseudo_tags(code)\n                    except Exception:\n                        pseudo, tags = \"\", []\n\n                if not pseudo and not tags:\n                    continue\n\n                payload[\"pseudo\"] = pseudo\n                payload[\"tags\"] = tags\n\n                aug_text = f\"{code} {pseudo} {' '.join(tags)}\".strip()\n                lex_vec = _lex_hash_vector_text(aug_text)\n\n                vec = rec.vector\n                if isinstance(vec, dict):\n                    vecs = dict(vec)\n                    vecs[LEX_VECTOR_NAME] = lex_vec\n                    new_vec = vecs\n                else:\n                    new_vec = vec\n\n                if LEX_SPARSE_MODE and aug_text and isinstance(new_vec, dict):\n                    sparse_vec = _lex_sparse_vector_text(aug_text)\n                    if sparse_vec.get(\"indices\"):\n                        new_vec[LEX_SPARSE_NAME] = models.SparseVector(**sparse_vec)\n\n                new_points.append(\n                    models.PointStruct(\n                        id=rec.id,\n                        vector=new_vec,\n                        payload=payload,\n                    )\n                )\n                processed += 1\n            except Exception:\n                continue\n\n        if new_points:\n            try:\n                upsert_points(client, collection, new_points)\n            except Exception:\n                if _maybe_ensure_collection():\n                    try:\n                        upsert_points(client, collection, new_points)\n                    except Exception:\n                        break\n                else:\n                    break\n\n        if next_offset is None:\n            break\n\n    return processed",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__maybe_ensure_collection_1130": {
      "name": "_maybe_ensure_collection",
      "type": "function",
      "start_line": 1130,
      "end_line": 1137,
      "content_hash": "5f3ec858b1082975d5977ec60e2678ef7a5b098d",
      "content": "    def _maybe_ensure_collection() -> bool:\n        if not dim or not vector_name:\n            return False\n        try:\n            ensure_collection_and_indexes_once(client, collection, int(dim), vector_name)\n            return True\n        except Exception:\n            return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}