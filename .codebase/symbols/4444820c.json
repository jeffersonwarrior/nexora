{
  "file_path": "/work/external-deps/Context-Engine/scripts/refrag_llamacpp.py",
  "file_hash": "a72fb618a729d749a7970eb5a7f2201532d6cad5",
  "updated_at": "2025-12-26T17:34:22.786629",
  "symbols": {
    "function__bool_env_19": {
      "name": "_bool_env",
      "type": "function",
      "start_line": 19,
      "end_line": 25,
      "content_hash": "6f469238ff958586a4bb94d4a458c09c306cdd07",
      "content": "def _bool_env(name: str, default: str = \"0\") -> bool:\n    return str(os.environ.get(name, default)).strip().lower() in {\n        \"1\",\n        \"true\",\n        \"yes\",\n        \"on\",\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_is_decoder_enabled_28": {
      "name": "is_decoder_enabled",
      "type": "function",
      "start_line": 28,
      "end_line": 29,
      "content_hash": "650763289d1ceb5124d639085a7d3860d21f05ea",
      "content": "def is_decoder_enabled() -> bool:\n    return _bool_env(\"REFRAG_DECODER\", \"0\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_runtime_kind_32": {
      "name": "get_runtime_kind",
      "type": "function",
      "start_line": 32,
      "end_line": 33,
      "content_hash": "ce6a10ed41cff2252e30007c5f02be088e008c48",
      "content": "def get_runtime_kind() -> str:\n    return str(os.environ.get(\"REFRAG_RUNTIME\", \"llamacpp\")).strip().lower()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_phi_path_36": {
      "name": "get_phi_path",
      "type": "function",
      "start_line": 36,
      "end_line": 37,
      "content_hash": "efc86526b4f0cb032336bacb1be7e2dd80362876",
      "content": "def get_phi_path() -> str:\n    return str(os.environ.get(\"REFRAG_PHI_PATH\", \"\")).strip()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_encoder_model_40": {
      "name": "get_encoder_model",
      "type": "function",
      "start_line": 40,
      "end_line": 41,
      "content_hash": "0785b930eeda3757699fe197fa1098ced3b9178a",
      "content": "def get_encoder_model() -> str:\n    return str(os.environ.get(\"REFRAG_ENCODER_MODEL\", \"BAAI/bge-base-en-v1.5\")).strip()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_sense_policy_44": {
      "name": "get_sense_policy",
      "type": "function",
      "start_line": 44,
      "end_line": 45,
      "content_hash": "197856925340aae03cf65f6405f1fc5947c5a266",
      "content": "def get_sense_policy() -> str:\n    return str(os.environ.get(\"REFRAG_SENSE\", \"heuristic\")).strip().lower()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__max_parallel_48": {
      "name": "_max_parallel",
      "type": "function",
      "start_line": 48,
      "end_line": 53,
      "content_hash": "fb938b115e54b4c659d89775c1cde2ecec3ffd1d",
      "content": "def _max_parallel() -> int:\n    try:\n        val = int(os.environ.get(\"LLAMACPP_MAX_PARALLEL\", \"\").strip() or \"2\")\n        return max(1, val)\n    except Exception:\n        return 2",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__refresh_parallel_semaphore_61": {
      "name": "_refresh_parallel_semaphore",
      "type": "function",
      "start_line": 61,
      "end_line": 72,
      "content_hash": "de89751adc4dcbe8459d552a4871515bde0abf68",
      "content": "def _refresh_parallel_semaphore() -> None:\n    \"\"\"Refresh semaphore when LLAMACPP_MAX_PARALLEL changes at runtime.\"\"\"\n    try:\n        desired = _max_parallel()\n    except Exception:\n        desired = 2\n    with _LLAMACPP_SLOT_LOCK:\n        global _LLAMACPP_SLOT, _LLAMACPP_PARALLEL\n        if desired == _LLAMACPP_PARALLEL:\n            return\n        _LLAMACPP_PARALLEL = desired\n        _LLAMACPP_SLOT = threading.Semaphore(desired)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__parallel_slot_76": {
      "name": "_parallel_slot",
      "type": "function",
      "start_line": 76,
      "end_line": 89,
      "content_hash": "695dde96359ad56d8d9e1066a901f542fca65d11",
      "content": "def _parallel_slot():\n    \"\"\"Context manager honoring LLAMACPP_MAX_PARALLEL.\"\"\"\n    _refresh_parallel_semaphore()\n    slot = globals().get(\"_LLAMACPP_SLOT\")\n    if not isinstance(slot, threading.Semaphore):\n        slot = threading.Semaphore(_max_parallel())\n        globals()[\"_LLAMACPP_SLOT\"] = slot\n    acquired = slot.acquire(timeout=float(os.environ.get(\"LLAMACPP_ACQUIRE_TIMEOUT\", \"30\") or 30))\n    if not acquired:\n        raise RuntimeError(\"llama.cpp saturated: parallel limit reached\")\n    try:\n        yield\n    finally:\n        slot.release()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__maybe_warm_95": {
      "name": "_maybe_warm",
      "type": "function",
      "start_line": 95,
      "end_line": 111,
      "content_hash": "f76ece7d3f0f846f59e6ccae6b2facf4f766b78d",
      "content": "def _maybe_warm(base_url: str) -> None:\n    global _WARM_CHECKED\n    if _WARM_CHECKED:\n        return\n    _WARM_CHECKED = True\n    if not _bool_env(\"LLAMACPP_AUTOWARM\", \"1\"):\n        return\n    try:\n        from urllib import request\n\n        req = request.Request(base_url.rstrip(\"/\") + \"/health\", method=\"GET\")\n        timeout = float(os.environ.get(\"LLAMACPP_WARM_TIMEOUT\", \"3\") or 3)\n        with request.urlopen(req, timeout=timeout):\n            pass\n    except Exception:\n        # Ignore warm failures; decoder calls will raise later if truly unavailable\n        return",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_LlamaCppRefragClient_114": {
      "name": "LlamaCppRefragClient",
      "type": "class",
      "start_line": 114,
      "end_line": 230,
      "content_hash": "99f821b9ee99f0dd5a2b53293d2ad60677eee2c8",
      "content": "class LlamaCppRefragClient:\n    \"\"\"Feature-flagged client for llama.cpp decoder.\n\n    Modes:\n      - prompt (default): builds a compressed textual prompt from merged spans.\n      - soft: reserved for patched server with soft-embedding support (NotImplemented).\n    \"\"\"\n\n    def __init__(self, base_url: Optional[str] = None) -> None:\n        if base_url:\n            self.base_url = base_url\n        else:\n            # Smart URL resolution: GPU vs Docker based on USE_GPU_DECODER flag\n            use_gpu = str(os.environ.get(\"USE_GPU_DECODER\", \"0\")).strip().lower()\n            if use_gpu in {\"1\", \"true\", \"yes\", \"on\"}:\n                # Use native GPU-accelerated server\n                # Use localhost when running on host, host.docker.internal when in container\n                if os.path.exists(\"/.dockerenv\"):\n                    self.base_url = \"http://host.docker.internal:8081\"\n                else:\n                    self.base_url = \"http://localhost:8081\"\n            else:\n                # Use configured LLAMACPP_URL (default: Docker CPU-only)\n                self.base_url = os.environ.get(\"LLAMACPP_URL\", \"http://localhost:8080\")\n        _maybe_warm(self.base_url)\n        if get_runtime_kind() != \"llamacpp\":\n            raise ValueError(\n                \"REFRAG_RUNTIME must be 'llamacpp' for LlamaCppRefragClient\"\n            )\n\n    def _post(self, path: str, json_payload: Dict[str, Any]) -> Dict[str, Any]:\n        import json as _json\n        from urllib import request\n\n        req = request.Request(self.base_url.rstrip(\"/\") + path, method=\"POST\")\n        req.add_header(\"Content-Type\", \"application/json\")\n        data = _json.dumps(json_payload).encode(\"utf-8\")\n        import os as _os\n        _timeout = float(_os.environ.get(\"LLAMACPP_TIMEOUT_SEC\", \"60\") or 60)\n        with request.urlopen(req, data=data, timeout=_timeout) as resp:\n            body = resp.read()\n        return _json.loads(body.decode(\"utf-8\"))\n\n    def generate_with_soft_embeddings(\n        self,\n        prompt: str,\n        soft_embeddings: Optional[list[list[float]]] = None,\n        max_tokens: int = 256,\n        **gen_kwargs: Any,\n    ) -> str:\n        if not is_decoder_enabled():\n            raise RuntimeError(\"Decoder path disabled: set REFRAG_DECODER=1 to enable\")\n        mode = os.environ.get(\"REFRAG_DECODER_MODE\", \"prompt\").strip().lower()\n        if mode == \"soft\":\n            if not soft_embeddings:\n                raise ValueError(\"soft mode requires soft_embeddings\")\n            _rp = float(os.environ.get(\"DECODER_REPEAT_PENALTY\", str(gen_kwargs.get(\"repeat_penalty\", 1.1))))\n            _rln = int(os.environ.get(\"DECODER_REPEAT_LAST_N\", str(gen_kwargs.get(\"repeat_last_n\", 128))))\n            _pp = float(os.environ.get(\"DECODER_PRESENCE_PENALTY\", str(gen_kwargs.get(\"presence_penalty\", 0.0))))\n            _fp = float(os.environ.get(\"DECODER_FREQUENCY_PENALTY\", str(gen_kwargs.get(\"frequency_penalty\", 0.0))))\n            payload = {\n                \"prompt\": prompt,\n                \"soft_embeddings\": soft_embeddings,\n                \"scale\": float(os.environ.get(\"REFRAG_SOFT_SCALE\", \"1.0\") or 1.0),\n                \"n_predict\": int(gen_kwargs.get(\"max_tokens\", max_tokens)),\n                \"temperature\": float(gen_kwargs.get(\"temperature\", 0.2)),\n                \"top_k\": int(gen_kwargs.get(\"top_k\", 40)),\n                \"top_p\": float(gen_kwargs.get(\"top_p\", 0.95)),\n                \"repeat_penalty\": _rp,\n                \"repeat_last_n\": _rln,\n                \"presence_penalty\": _pp,\n                \"frequency_penalty\": _fp,\n                \"stop\": gen_kwargs.get(\"stop\") or [],\n            }\n            try:\n                with _parallel_slot():\n                    _start = time.time()\n                    res = self._post(\"/soft_completion\", payload)\n                    elapsed = time.time() - _start\n                    os.environ.setdefault(\n                        \"LLAMACPP_LAST_LATENCY_SEC\", f\"{elapsed:.3f}\"\n                    )\n            except Exception as e:\n                raise RuntimeError(f\"llama.cpp soft_completion failed: {e}\")\n            return (res.get(\"content\") or res.get(\"generation\") or \"\").strip()\n        # Prompt mode: send a normal completion request using a compressed prompt\n        # Allow repetition controls; fall back to env if not passed\n        _rp = float(os.environ.get(\"DECODER_REPEAT_PENALTY\", str(gen_kwargs.get(\"repeat_penalty\", 1.1))))\n        _rln = int(os.environ.get(\"DECODER_REPEAT_LAST_N\", str(gen_kwargs.get(\"repeat_last_n\", 128))))\n        _pp = float(os.environ.get(\"DECODER_PRESENCE_PENALTY\", str(gen_kwargs.get(\"presence_penalty\", 0.0))))\n        _fp = float(os.environ.get(\"DECODER_FREQUENCY_PENALTY\", str(gen_kwargs.get(\"frequency_penalty\", 0.0))))\n        payload = {\n            \"prompt\": prompt,\n            \"n_predict\": int(gen_kwargs.get(\"max_tokens\", max_tokens)),\n            # fast, deterministic-ish defaults; callers can override via gen_kwargs\n            \"temperature\": float(gen_kwargs.get(\"temperature\", 0.2)),\n            \"top_k\": int(gen_kwargs.get(\"top_k\", 40)),\n            \"top_p\": float(gen_kwargs.get(\"top_p\", 0.95)),\n            \"repeat_penalty\": _rp,\n            \"repeat_last_n\": _rln,\n            \"presence_penalty\": _pp,\n            \"frequency_penalty\": _fp,\n            \"stop\": gen_kwargs.get(\"stop\") or [],\n        }\n        try:\n            with _parallel_slot():\n                _start = time.time()\n                res = self._post(\"/completion\", payload)\n                elapsed = time.time() - _start\n                os.environ.setdefault(\n                    \"LLAMACPP_LAST_LATENCY_SEC\", f\"{elapsed:.3f}\"\n                )\n        except Exception as e:\n            raise RuntimeError(f\"llama.cpp completion failed: {e}\")\n        # llama.cpp server returns { 'content': '...' } or { 'token': ... } streams; we expect non-stream\n        txt = (res.get(\"content\") or res.get(\"generation\") or \"\").strip()\n        return txt",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___122": {
      "name": "__init__",
      "type": "method",
      "start_line": 122,
      "end_line": 142,
      "content_hash": "3a1a232494ac568cbd3c1773543074584c78022b",
      "content": "    def __init__(self, base_url: Optional[str] = None) -> None:\n        if base_url:\n            self.base_url = base_url\n        else:\n            # Smart URL resolution: GPU vs Docker based on USE_GPU_DECODER flag\n            use_gpu = str(os.environ.get(\"USE_GPU_DECODER\", \"0\")).strip().lower()\n            if use_gpu in {\"1\", \"true\", \"yes\", \"on\"}:\n                # Use native GPU-accelerated server\n                # Use localhost when running on host, host.docker.internal when in container\n                if os.path.exists(\"/.dockerenv\"):\n                    self.base_url = \"http://host.docker.internal:8081\"\n                else:\n                    self.base_url = \"http://localhost:8081\"\n            else:\n                # Use configured LLAMACPP_URL (default: Docker CPU-only)\n                self.base_url = os.environ.get(\"LLAMACPP_URL\", \"http://localhost:8080\")\n        _maybe_warm(self.base_url)\n        if get_runtime_kind() != \"llamacpp\":\n            raise ValueError(\n                \"REFRAG_RUNTIME must be 'llamacpp' for LlamaCppRefragClient\"\n            )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__post_144": {
      "name": "_post",
      "type": "method",
      "start_line": 144,
      "end_line": 155,
      "content_hash": "78328492a6340a365378e2e9fe115a102fcafc3f",
      "content": "    def _post(self, path: str, json_payload: Dict[str, Any]) -> Dict[str, Any]:\n        import json as _json\n        from urllib import request\n\n        req = request.Request(self.base_url.rstrip(\"/\") + path, method=\"POST\")\n        req.add_header(\"Content-Type\", \"application/json\")\n        data = _json.dumps(json_payload).encode(\"utf-8\")\n        import os as _os\n        _timeout = float(_os.environ.get(\"LLAMACPP_TIMEOUT_SEC\", \"60\") or 60)\n        with request.urlopen(req, data=data, timeout=_timeout) as resp:\n            body = resp.read()\n        return _json.loads(body.decode(\"utf-8\"))",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_generate_with_soft_embeddings_157": {
      "name": "generate_with_soft_embeddings",
      "type": "method",
      "start_line": 157,
      "end_line": 230,
      "content_hash": "a66ad7ca01ee950697bcd14ad5d1f7569196bddf",
      "content": "    def generate_with_soft_embeddings(\n        self,\n        prompt: str,\n        soft_embeddings: Optional[list[list[float]]] = None,\n        max_tokens: int = 256,\n        **gen_kwargs: Any,\n    ) -> str:\n        if not is_decoder_enabled():\n            raise RuntimeError(\"Decoder path disabled: set REFRAG_DECODER=1 to enable\")\n        mode = os.environ.get(\"REFRAG_DECODER_MODE\", \"prompt\").strip().lower()\n        if mode == \"soft\":\n            if not soft_embeddings:\n                raise ValueError(\"soft mode requires soft_embeddings\")\n            _rp = float(os.environ.get(\"DECODER_REPEAT_PENALTY\", str(gen_kwargs.get(\"repeat_penalty\", 1.1))))\n            _rln = int(os.environ.get(\"DECODER_REPEAT_LAST_N\", str(gen_kwargs.get(\"repeat_last_n\", 128))))\n            _pp = float(os.environ.get(\"DECODER_PRESENCE_PENALTY\", str(gen_kwargs.get(\"presence_penalty\", 0.0))))\n            _fp = float(os.environ.get(\"DECODER_FREQUENCY_PENALTY\", str(gen_kwargs.get(\"frequency_penalty\", 0.0))))\n            payload = {\n                \"prompt\": prompt,\n                \"soft_embeddings\": soft_embeddings,\n                \"scale\": float(os.environ.get(\"REFRAG_SOFT_SCALE\", \"1.0\") or 1.0),\n                \"n_predict\": int(gen_kwargs.get(\"max_tokens\", max_tokens)),\n                \"temperature\": float(gen_kwargs.get(\"temperature\", 0.2)),\n                \"top_k\": int(gen_kwargs.get(\"top_k\", 40)),\n                \"top_p\": float(gen_kwargs.get(\"top_p\", 0.95)),\n                \"repeat_penalty\": _rp,\n                \"repeat_last_n\": _rln,\n                \"presence_penalty\": _pp,\n                \"frequency_penalty\": _fp,\n                \"stop\": gen_kwargs.get(\"stop\") or [],\n            }\n            try:\n                with _parallel_slot():\n                    _start = time.time()\n                    res = self._post(\"/soft_completion\", payload)\n                    elapsed = time.time() - _start\n                    os.environ.setdefault(\n                        \"LLAMACPP_LAST_LATENCY_SEC\", f\"{elapsed:.3f}\"\n                    )\n            except Exception as e:\n                raise RuntimeError(f\"llama.cpp soft_completion failed: {e}\")\n            return (res.get(\"content\") or res.get(\"generation\") or \"\").strip()\n        # Prompt mode: send a normal completion request using a compressed prompt\n        # Allow repetition controls; fall back to env if not passed\n        _rp = float(os.environ.get(\"DECODER_REPEAT_PENALTY\", str(gen_kwargs.get(\"repeat_penalty\", 1.1))))\n        _rln = int(os.environ.get(\"DECODER_REPEAT_LAST_N\", str(gen_kwargs.get(\"repeat_last_n\", 128))))\n        _pp = float(os.environ.get(\"DECODER_PRESENCE_PENALTY\", str(gen_kwargs.get(\"presence_penalty\", 0.0))))\n        _fp = float(os.environ.get(\"DECODER_FREQUENCY_PENALTY\", str(gen_kwargs.get(\"frequency_penalty\", 0.0))))\n        payload = {\n            \"prompt\": prompt,\n            \"n_predict\": int(gen_kwargs.get(\"max_tokens\", max_tokens)),\n            # fast, deterministic-ish defaults; callers can override via gen_kwargs\n            \"temperature\": float(gen_kwargs.get(\"temperature\", 0.2)),\n            \"top_k\": int(gen_kwargs.get(\"top_k\", 40)),\n            \"top_p\": float(gen_kwargs.get(\"top_p\", 0.95)),\n            \"repeat_penalty\": _rp,\n            \"repeat_last_n\": _rln,\n            \"presence_penalty\": _pp,\n            \"frequency_penalty\": _fp,\n            \"stop\": gen_kwargs.get(\"stop\") or [],\n        }\n        try:\n            with _parallel_slot():\n                _start = time.time()\n                res = self._post(\"/completion\", payload)\n                elapsed = time.time() - _start\n                os.environ.setdefault(\n                    \"LLAMACPP_LAST_LATENCY_SEC\", f\"{elapsed:.3f}\"\n                )\n        except Exception as e:\n            raise RuntimeError(f\"llama.cpp completion failed: {e}\")\n        # llama.cpp server returns { 'content': '...' } or { 'token': ... } streams; we expect non-stream\n        txt = (res.get(\"content\") or res.get(\"generation\") or \"\").strip()\n        return txt",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}