{
  "file_path": "/work/external-deps/Context-Engine/scripts/hybrid/embed.py",
  "file_hash": "97ac2dd9ebef25e13b34e6171c1084c03b5b9ba3",
  "updated_at": "2025-12-26T17:34:21.583041",
  "symbols": {
    "function__safe_int_65": {
      "name": "_safe_int",
      "type": "function",
      "start_line": 65,
      "end_line": 72,
      "content_hash": "57cfdbf41a32cf75c4331e80c7448c6e49184cb2",
      "content": "def _safe_int(val, default: int) -> int:\n    \"\"\"Safely parse an integer from a value, returning default on failure.\"\"\"\n    try:\n        if val is None or (isinstance(val, str) and val.strip() == \"\"):\n            return default\n        return int(val)\n    except (ValueError, TypeError):\n        return default",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_embed_cache_80": {
      "name": "_get_embed_cache",
      "type": "function",
      "start_line": 80,
      "end_line": 87,
      "content_hash": "d6ed4e323c8b51cc5bea6198887afde7c30d0a47",
      "content": "def _get_embed_cache() -> Any:\n    \"\"\"Get or initialize the embedding cache (unified or legacy).\"\"\"\n    global _EMBED_CACHE\n    if UNIFIED_CACHE_AVAILABLE:\n        if _EMBED_CACHE is None:\n            _EMBED_CACHE = get_embedding_cache()\n        return _EMBED_CACHE\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_embedding_model_93": {
      "name": "get_embedding_model",
      "type": "function",
      "start_line": 93,
      "end_line": 119,
      "content_hash": "91251b053d7edb7c089f7bd47a435102490bb176",
      "content": "def get_embedding_model(model_name: Optional[str] = None) -> EmbeddingModel:\n    \"\"\"\n    Get or create an embedding model instance.\n\n    Uses the embedder factory when available, otherwise falls back to\n    direct TextEmbedding instantiation.\n\n    Args:\n        model_name: Model name override. If None, uses EMBEDDING_MODEL env var.\n\n    Returns:\n        Embedding model instance.\n\n    Raises:\n        ImportError: If neither embedder factory nor fastembed is available.\n    \"\"\"\n    if _EMBEDDER_FACTORY and _get_embedding_model is not None:\n        return _get_embedding_model(model_name)\n\n    if TextEmbedding is None:\n        raise ImportError(\n            \"No embedding backend available. Install fastembed or ensure \"\n            \"scripts.embedder is importable.\"\n        )\n\n    name = model_name or MODEL_NAME\n    return TextEmbedding(model_name=name)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_embed_queries_cached_125": {
      "name": "embed_queries_cached",
      "type": "function",
      "start_line": 125,
      "end_line": 200,
      "content_hash": "382909a95876282be18c817e4eb01ce5a0ab0b9d",
      "content": "def embed_queries_cached(\n    model: Any,\n    queries: List[str],\n) -> List[List[float]]:\n    \"\"\"\n    Cache dense query embeddings to avoid repeated compute across expansions/retries.\n\n    Optimized: batch-embeds all missing queries in one model call (2-5x faster).\n    Thread-safe with bounded cache size.\n\n    When Qwen3 is enabled and QWEN3_QUERY_INSTRUCTION=1, applies instruction\n    prefix to queries before embedding for improved retrieval quality.\n\n    Args:\n        model: Embedding model instance (TextEmbedding or compatible).\n        queries: List of query strings to embed.\n\n    Returns:\n        List of embedding vectors (one per query, guaranteed 1:1 correspondence).\n\n    Raises:\n        ValueError: If queries is empty or contains only invalid entries.\n        RuntimeError: If embedding fails for any query.\n    \"\"\"\n    if not queries:\n        raise ValueError(\"queries must be a non-empty list\")\n\n    # Validate and sanitize queries - filter empty/whitespace but track positions\n    sanitized: List[str] = []\n    original_indices: List[int] = []\n    for i, q in enumerate(queries):\n        if q and isinstance(q, str) and q.strip():\n            sanitized.append(q.strip())\n            original_indices.append(i)\n\n    if not sanitized:\n        raise ValueError(\"queries contains no valid non-empty strings\")\n\n    try:\n        # Best-effort model name extraction; fall back to env\n        name = getattr(model, \"model_name\", None) or os.environ.get(\n            \"EMBEDDING_MODEL\", MODEL_NAME\n        )\n    except Exception:\n        name = os.environ.get(\"EMBEDDING_MODEL\", MODEL_NAME)\n\n    # Apply Qwen3 instruction prefix if enabled (queries only, not documents)\n    try:\n        from scripts.embedder import prefix_queries\n        sanitized = prefix_queries(sanitized, name)\n    except ImportError:\n        pass\n\n    cache = _get_embed_cache()\n\n    if UNIFIED_CACHE_AVAILABLE and cache is not None:\n        embeddings = _embed_with_unified_cache(model, sanitized, name, cache)\n    else:\n        embeddings = _embed_with_legacy_cache(model, sanitized, name)\n\n    # Reconstruct full result list with None for invalid queries\n    # (callers should not pass empty queries, but if they do, fail clearly)\n    if len(embeddings) != len(sanitized):\n        raise RuntimeError(\n            f\"Embedding count mismatch: got {len(embeddings)} for {len(sanitized)} queries\"\n        )\n\n    # If all queries were valid, return directly\n    if len(sanitized) == len(queries):\n        return embeddings\n\n    # Otherwise, we had some empty queries - raise error (callers must sanitize)\n    raise ValueError(\n        f\"queries contained {len(queries) - len(sanitized)} empty/invalid entries; \"\n        \"caller must filter empty queries before calling embed_queries_cached\"\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__embed_with_unified_cache_203": {
      "name": "_embed_with_unified_cache",
      "type": "function",
      "start_line": 203,
      "end_line": 243,
      "content_hash": "6746fdeb7da3ec95e3f1ffb45a7448974635429a",
      "content": "def _embed_with_unified_cache(\n    model: Any,\n    queries: List[str],\n    model_name: str,\n    cache: Any,\n) -> List[List[float]]:\n    \"\"\"Embed queries using the unified cache system.\"\"\"\n    missing_queries: List[str] = []\n    missing_indices: List[int] = []\n\n    # Find missing queries\n    for i, q in enumerate(queries):\n        key = (str(model_name), str(q))\n        if cache.get(key) is None:\n            missing_queries.append(str(q))\n            missing_indices.append(i)\n\n    # Batch-embed all missing queries in one call\n    if missing_queries:\n        try:\n            vecs = list(model.embed(missing_queries))\n            # Cache all new embeddings\n            for q, vec in zip(missing_queries, vecs):\n                key = (str(model_name), str(q))\n                cache.set(key, vec.tolist())\n        except Exception:\n            # Fallback to one-by-one if batch fails\n            for q in missing_queries:\n                key = (str(model_name), str(q))\n                vec = next(model.embed([q])).tolist()\n                cache.set(key, vec)\n\n    # Return embeddings in original order from cache\n    out: List[List[float]] = []\n    for q in queries:\n        key = (str(model_name), str(q))\n        v = cache.get(key)\n        if v is None:\n            raise RuntimeError(f\"Failed to embed query: {q!r}\")\n        out.append(v)\n    return out",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__embed_with_legacy_cache_246": {
      "name": "_embed_with_legacy_cache",
      "type": "function",
      "start_line": 246,
      "end_line": 297,
      "content_hash": "6fab3e195a273d672db3b5db9b9418346253b885",
      "content": "def _embed_with_legacy_cache(\n    model: Any,\n    queries: List[str],\n    model_name: str,\n) -> List[List[float]]:\n    \"\"\"Embed queries using the legacy OrderedDict cache.\"\"\"\n    missing_queries: List[str] = []\n    missing_indices: List[int] = []\n\n    with _EMBED_LOCK:\n        for i, q in enumerate(queries):\n            key = (str(model_name), str(q))\n            if key not in _EMBED_QUERY_CACHE:\n                missing_queries.append(str(q))\n                missing_indices.append(i)\n\n    # Batch-embed all missing queries in one call\n    if missing_queries:\n        try:\n            # Embed all missing queries at once\n            vecs = list(model.embed(missing_queries))\n            with _EMBED_LOCK:\n                # Cache all new embeddings\n                for q, vec in zip(missing_queries, vecs):\n                    key = (str(model_name), str(q))\n                    if key not in _EMBED_QUERY_CACHE:\n                        _EMBED_QUERY_CACHE[key] = vec.tolist()\n                        # Evict oldest entries if cache exceeds limit\n                        while len(_EMBED_QUERY_CACHE) > MAX_EMBED_CACHE:\n                            _EMBED_QUERY_CACHE.popitem(last=False)\n        except Exception:\n            # Fallback to one-by-one if batch fails\n            for q in missing_queries:\n                key = (str(model_name), str(q))\n                vec = next(model.embed([q])).tolist()\n                with _EMBED_LOCK:\n                    if key not in _EMBED_QUERY_CACHE:\n                        _EMBED_QUERY_CACHE[key] = vec\n                        # Evict oldest entries if cache exceeds limit\n                        while len(_EMBED_QUERY_CACHE) > MAX_EMBED_CACHE:\n                            _EMBED_QUERY_CACHE.popitem(last=False)\n\n    # Return embeddings in original order from cache (thread-safe read)\n    out: List[List[float]] = []\n    with _EMBED_LOCK:\n        for q in queries:\n            key = (str(model_name), str(q))\n            v = _EMBED_QUERY_CACHE.get(key)\n            if v is None:\n                raise RuntimeError(f\"Failed to embed query: {q!r}\")\n            out.append(v)\n    return out",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_clear_embedding_cache_307": {
      "name": "clear_embedding_cache",
      "type": "function",
      "start_line": 307,
      "end_line": 319,
      "content_hash": "5c48454a9b7e605cdd5b312bc6f2ad6b45a62992",
      "content": "def clear_embedding_cache() -> None:\n    \"\"\"Clear the embedding cache (both unified and legacy).\"\"\"\n    global _EMBED_CACHE\n\n    cache = _get_embed_cache()\n    if cache is not None:\n        try:\n            cache.clear()\n        except Exception:\n            pass\n\n    with _EMBED_LOCK:\n        _EMBED_QUERY_CACHE.clear()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_embedding_cache_stats_322": {
      "name": "get_embedding_cache_stats",
      "type": "function",
      "start_line": 322,
      "end_line": 336,
      "content_hash": "fe041e20df009e2c5f7a0b752b42669fa8e1c152",
      "content": "def get_embedding_cache_stats() -> dict:\n    \"\"\"Get statistics about the embedding cache.\"\"\"\n    cache = _get_embed_cache()\n    if UNIFIED_CACHE_AVAILABLE and cache is not None:\n        try:\n            return cache.get_stats()\n        except Exception:\n            pass\n\n    with _EMBED_LOCK:\n        return {\n            \"cache_type\": \"legacy\",\n            \"size\": len(_EMBED_QUERY_CACHE),\n            \"max_size\": MAX_EMBED_CACHE,\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}