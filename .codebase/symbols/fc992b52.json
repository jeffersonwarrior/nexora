{
  "file_path": "/work/context-engine/scripts/upload_delta_bundle.py",
  "file_hash": "1a442cf3a56e343dc521f2f1d023845077a5a01c",
  "updated_at": "2025-12-26T17:34:25.102082",
  "symbols": {
    "function_get_workspace_key_30": {
      "name": "get_workspace_key",
      "type": "function",
      "start_line": 30,
      "end_line": 42,
      "content_hash": "f5e23e53c67be91d81bfdb3cdfd35a96508af755",
      "content": "def get_workspace_key(workspace_path: str) -> str:\n    \"\"\"Generate 16-char hash for collision avoidance in remote uploads.\n\n    Remote uploads may have identical folder names from different users,\n    so uses longer hash than local indexing (8-chars) to ensure uniqueness.\n\n    Both host paths (/home/user/project/repo) and container paths (/work/repo)\n    should generate the same key for the same repository.\n    \"\"\"\n    repo_name = Path(workspace_path).name\n    if _SLUGGED_REPO_RE.match(repo_name):\n        return repo_name[-16:]\n    return hashlib.sha256(repo_name.encode(\"utf-8\")).hexdigest()[:16]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cleanup_empty_dirs_45": {
      "name": "_cleanup_empty_dirs",
      "type": "function",
      "start_line": 45,
      "end_line": 61,
      "content_hash": "54cccb0ae771bb459031c952d75d8fc8fa50ce83",
      "content": "def _cleanup_empty_dirs(path: Path, stop_at: Path) -> None:\n    \"\"\"Recursively remove empty directories up to stop_at (exclusive).\"\"\"\n    try:\n        path = path.resolve()\n        stop_at = stop_at.resolve()\n    except Exception:\n        pass\n    while True:\n        try:\n            if path == stop_at or not path.exists() or not path.is_dir():\n                break\n            if any(path.iterdir()):\n                break\n            path.rmdir()\n            path = path.parent\n        except Exception:\n            break",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_process_delta_bundle_64": {
      "name": "process_delta_bundle",
      "type": "function",
      "start_line": 64,
      "end_line": 404,
      "content_hash": "64f5a00e87b2f76dc705cc7a9442d4c7a2b8e152",
      "content": "def process_delta_bundle(workspace_path: str, bundle_path: Path, manifest: Dict[str, Any]) -> Dict[str, int]:\n    \"\"\"Process delta bundle and return operation counts.\"\"\"\n    operations_count = {\n        \"created\": 0,\n        \"updated\": 0,\n        \"deleted\": 0,\n        \"moved\": 0,\n        \"skipped\": 0,\n        \"failed\": 0,\n    }\n\n    try:\n        # CRITICAL: Always materialize writes under WORK_DIR using a slugged repo directory.\n        # Do NOT write directly into the client-supplied workspace_path, since that may be a host\n        # path (e.g. /home/user/repo) that is not mounted/visible to the watcher/indexer.\n        workspace_leaf = Path(workspace_path).name\n\n        repo_name_for_state: Optional[str] = None\n\n        serving_slug: Optional[str] = None\n        active_slug: Optional[str] = None\n        if _extract_repo_name_from_path and get_collection_state_snapshot:\n            try:\n                repo_name_for_state = _extract_repo_name_from_path(workspace_path)\n                if repo_name_for_state:\n                    snapshot = get_collection_state_snapshot(workspace_path=None, repo_name=repo_name_for_state)  # type: ignore[arg-type]\n                    serving_slug = snapshot.get(\"serving_repo_slug\")\n                    active_slug = snapshot.get(\"active_repo_slug\")\n            except Exception:\n                serving_slug = None\n                active_slug = None\n\n        slug_order: list[str] = []\n        serving_candidate: Optional[str] = None\n        if serving_slug and _SLUGGED_REPO_RE.match(serving_slug):\n            serving_candidate = serving_slug\n        if active_slug and _SLUGGED_REPO_RE.match(active_slug) and active_slug not in slug_order:\n            slug_order.append(active_slug)\n\n        # If staging is active, we must mirror uploads into BOTH the canonical slug and\n        # the \"*_old\" slug. Relying purely on snapshot detection is brittle (e.g. when\n        # the client workspace_path is a host path). When we can infer a canonical slug,\n        # force both targets.\n        staging_active = False\n        staging_gate = bool(is_staging_enabled() if callable(is_staging_enabled) else False)\n        try:\n            if serving_slug and str(serving_slug).endswith(\"_old\"):\n                staging_active = True\n        except Exception:\n            staging_active = False\n\n        if not staging_gate:\n            staging_active = False\n\n        def _append_slug(slug: Optional[str]) -> None:\n            if slug and _SLUGGED_REPO_RE.match(slug) and slug not in slug_order:\n                slug_order.append(slug)\n\n        if repo_name_for_state and _SLUGGED_REPO_RE.match(repo_name_for_state):\n            canonical_slug = repo_name_for_state[:-4] if repo_name_for_state.endswith(\"_old\") else repo_name_for_state\n            old_slug_candidate = (\n                repo_name_for_state if repo_name_for_state.endswith(\"_old\") else f\"{canonical_slug}_old\"\n            )\n            if staging_active:\n                slug_order = []\n                _append_slug(canonical_slug)\n                _append_slug(old_slug_candidate)\n            elif not slug_order:\n                _append_slug(canonical_slug)\n                old_slug_path = Path(WORK_DIR) / old_slug_candidate\n                if old_slug_path.exists():\n                    _append_slug(old_slug_candidate)\n\n        if not slug_order:\n            if _SLUGGED_REPO_RE.match(workspace_leaf):\n                slug_order.append(workspace_leaf)\n            else:\n                if _extract_repo_name_from_path:\n                    repo_name = _extract_repo_name_from_path(workspace_path) or workspace_leaf\n                else:\n                    repo_name = workspace_leaf\n                workspace_key = get_workspace_key(workspace_path)\n                slug_order.append(f\"{repo_name}-{workspace_key}\")\n\n        # Best-effort: if staging is active according to workspace_state, ensure we mirror to\n        # both the canonical slug and its *_old slug.\n        if staging_gate and (not staging_active) and get_staging_targets and _extract_repo_name_from_path:\n            try:\n                repo_name_for_staging = _extract_repo_name_from_path(workspace_path) or slug_order[0]\n                targets = get_staging_targets(workspace_path=workspace_path, repo_name=repo_name_for_staging)\n                if isinstance(targets, dict) and targets.get(\"staging\"):\n                    staging_active = True\n            except Exception as staging_err:\n                logger.debug(f\"[upload_service] Failed to detect staging: {staging_err}\")\n\n        def _slug_exists(slug: str) -> bool:\n            try:\n                return (\n                    (Path(WORK_DIR) / slug).exists()\n                    or (Path(WORK_DIR) / \".codebase\" / \"repos\" / slug).exists()\n                )\n            except Exception:\n                return False\n\n        if staging_gate and (not staging_active) and slug_order:\n            primary = slug_order[0]\n            if _SLUGGED_REPO_RE.match(primary):\n                canonical = primary[:-4] if primary.endswith(\"_old\") else primary\n                inferred_old = primary if primary.endswith(\"_old\") else f\"{canonical}_old\"\n                if _slug_exists(inferred_old):\n                    staging_active = True\n\n        if staging_gate and staging_active and slug_order:\n            primary = slug_order[0]\n            if _SLUGGED_REPO_RE.match(primary):\n                canonical = primary[:-4] if primary.endswith(\"_old\") else primary\n                old_slug = primary if primary.endswith(\"_old\") else f\"{canonical}_old\"\n                desired = [canonical, old_slug]\n                slug_order = [s for s in desired if _SLUGGED_REPO_RE.match(s)]\n        elif staging_gate and not staging_active and serving_candidate:\n            # Ignore serving slugs when staging is disabled; keep deterministic non-staging writes.\n            if serving_candidate in slug_order:\n                slug_order = [s for s in slug_order if s != serving_candidate]\n\n        if staging_gate:\n            try:\n                logger.info(f\"[upload_service] Delta bundle targets (staging={staging_active}): {slug_order}\")\n            except Exception:\n                pass\n\n        replica_roots: Dict[str, Path] = {}\n        for slug in slug_order:\n            path = Path(WORK_DIR) / slug\n            path.mkdir(parents=True, exist_ok=True)\n            try:\n                marker_dir = Path(WORK_DIR) / \".codebase\" / \"repos\" / slug\n                marker_dir.mkdir(parents=True, exist_ok=True)\n                (marker_dir / \".ctxce_managed_upload\").write_text(\"1\\n\")\n            except Exception:\n                pass\n            replica_roots[slug] = path.resolve()\n\n        primary_slug = slug_order[0]\n        workspace_root = replica_roots[primary_slug]\n\n        def _safe_join(base: Path, rel: str) -> Path:\n            # SECURITY: Prevent path traversal / absolute-path writes by ensuring the resolved\n            # candidate path stays within the intended workspace root.\n            rp = Path(str(rel))\n            if str(rp) in {\".\", \"\"}:\n                raise ValueError(\"Invalid operation path\")\n            if rp.is_absolute():\n                raise ValueError(f\"Absolute paths are not allowed: {rel}\")\n            base_resolved = base.resolve()\n            candidate = (base_resolved / rp).resolve()\n            try:\n                ok = candidate.is_relative_to(base_resolved)\n            except Exception:\n                ok = os.path.commonpath([str(base_resolved), str(candidate)]) == str(base_resolved)\n            if not ok:\n                raise ValueError(f\"Path escapes workspace: {rel}\")\n            return candidate\n\n        with tarfile.open(bundle_path, \"r:gz\") as tar:\n            ops_member = None\n            for member in tar.getnames():\n                if member.endswith(\"metadata/operations.json\"):\n                    ops_member = member\n                    break\n\n            if not ops_member:\n                raise ValueError(\"operations.json not found in bundle\")\n\n            ops_file = tar.extractfile(ops_member)\n            if not ops_file:\n                raise ValueError(\"Cannot extract operations.json\")\n\n            operations_data = json.loads(ops_file.read().decode(\"utf-8\"))\n            operations = operations_data.get(\"operations\", [])\n\n            # Best-effort: extract git history metadata for watcher to ingest\n            try:\n                git_member = None\n                for member in tar.getnames():\n                    if member.endswith(\"metadata/git_history.json\"):\n                        git_member = member\n                        break\n                if git_member:\n                    git_file = tar.extractfile(git_member)\n                    if git_file:\n                        history_bytes = git_file.read()\n                        bundle_id = manifest.get(\"bundle_id\") or \"unknown\"\n                        for root in replica_roots.values():\n                            try:\n                                history_dir = root / \".remote-git\"\n                                history_dir.mkdir(parents=True, exist_ok=True)\n                                history_path = history_dir / f\"git_history_{bundle_id}.json\"\n                                history_path.write_bytes(history_bytes)\n                            except Exception as write_err:\n                                logger.debug(\n                                    f\"[upload_service] Failed to write git history manifest for {root}: {write_err}\",\n                                )\n            except Exception as git_err:\n                logger.debug(f\"[upload_service] Error extracting git history metadata: {git_err}\")\n\n            def _apply_operation_to_workspace(workspace_root: Path) -> bool:\n                \"\"\"Apply a single file operation to a workspace. Returns True on success.\"\"\"\n                nonlocal operations_count, op_type, rel_path, tar\n                \n                target_path = _safe_join(workspace_root, rel_path)\n\n                safe_source_path = None\n                source_rel_path = None\n                if op_type == \"moved\":\n                    source_rel_path = operation.get(\"source_path\") or operation.get(\"source_relative_path\")\n                    if source_rel_path:\n                        safe_source_path = _safe_join(workspace_root, source_rel_path)\n\n                try:\n                    if op_type == \"created\":\n                        file_member = None\n                        for member in tar.getnames():\n                            if member.endswith(f\"files/created/{rel_path}\"):\n                                file_member = member\n                                break\n\n                        if file_member:\n                            file_content = tar.extractfile(file_member)\n                            if file_content:\n                                target_path.parent.mkdir(parents=True, exist_ok=True)\n                                target_path.write_bytes(file_content.read())\n                                return True\n                            else:\n                                return False\n                        else:\n                            return False\n\n                    elif op_type == \"updated\":\n                        file_member = None\n                        for member in tar.getnames():\n                            if member.endswith(f\"files/updated/{rel_path}\"):\n                                file_member = member\n                                break\n\n                        if file_member:\n                            file_content = tar.extractfile(file_member)\n                            if file_content:\n                                target_path.parent.mkdir(parents=True, exist_ok=True)\n                                target_path.write_bytes(file_content.read())\n                                return True\n                            else:\n                                return False\n                        else:\n                            return False\n\n                    elif op_type == \"deleted\":\n                        if target_path.exists():\n                            target_path.unlink(missing_ok=True)\n                            return True\n                        else:\n                            return True  # Already deleted\n\n                    elif op_type == \"moved\":\n                        if safe_source_path and safe_source_path.exists():\n                            target_path.parent.mkdir(parents=True, exist_ok=True)\n                            safe_source_path.rename(target_path)\n                            return True\n                        # Remote uploads may not have the source file on the server (e.g. staging\n                        # mirrors). In that case, clients can embed the destination content under\n                        # files/moved/<dest>.\n                        file_member = None\n                        for member in tar.getnames():\n                            if member.endswith(f\"files/moved/{rel_path}\"):\n                                file_member = member\n                                break\n                        if file_member:\n                            file_content = tar.extractfile(file_member)\n                            if file_content:\n                                target_path.parent.mkdir(parents=True, exist_ok=True)\n                                target_path.write_bytes(file_content.read())\n                                return True\n                            return False\n                        return False\n\n                    else:\n                        logger.warning(f\"[upload_service] Unknown operation type: {op_type}\")\n                        return False\n                except Exception as e:\n                    logger.debug(f\"[upload_service] Failed to apply {op_type} to {rel_path} in {workspace_root}: {e}\")\n                    return False\n\n            for operation in operations:\n                op_type = operation.get(\"operation\")\n                rel_path = operation.get(\"path\")\n\n                if not rel_path:\n                    operations_count[\"skipped\"] += 1\n                    continue\n\n                sanitized_path = rel_path\n                skipped_due_to_exact_slug = False\n                for slug in replica_roots.keys():\n                    if sanitized_path == slug:\n                        skipped_due_to_exact_slug = True\n                        break\n                    prefix = f\"{slug}/\"\n                    if sanitized_path.startswith(prefix):\n                        sanitized_path = sanitized_path[len(prefix):]\n                        break\n\n                if skipped_due_to_exact_slug or not sanitized_path:\n                    logger.debug(\n                        f\"[upload_service] Skipping operation {op_type} for path {rel_path}: \"\n                        \"appears to reference slug root directly.\",\n                    )\n                    operations_count[\"skipped\"] += 1\n                    continue\n\n                rel_path = sanitized_path\n\n                replica_results: Dict[str, bool] = {}\n                for slug, root in replica_roots.items():\n                    replica_results[slug] = _apply_operation_to_workspace(root)\n\n                success_any = any(replica_results.values())\n                success_all = all(replica_results.values())\n                if success_any:\n                    operations_count.setdefault(op_type, 0)\n                    operations_count[op_type] = operations_count.get(op_type, 0) + 1\n                    if not success_all:\n                        logger.debug(\n                            f\"[upload_service] Partial success for {op_type} {rel_path}: {replica_results}\"\n                        )\n                else:\n                    operations_count[\"failed\"] += 1\n\n        return operations_count\n\n    except Exception as e:\n        logger.error(f\"Error processing delta bundle: {e}\")\n        raise",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__append_slug_118": {
      "name": "_append_slug",
      "type": "function",
      "start_line": 118,
      "end_line": 120,
      "content_hash": "f2b0cde6e8920137b55c15ffe17f19a52520e462",
      "content": "        def _append_slug(slug: Optional[str]) -> None:\n            if slug and _SLUGGED_REPO_RE.match(slug) and slug not in slug_order:\n                slug_order.append(slug)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__slug_exists_159": {
      "name": "_slug_exists",
      "type": "function",
      "start_line": 159,
      "end_line": 166,
      "content_hash": "2b74e125d377105e1ee5625a68c33366b809adb0",
      "content": "        def _slug_exists(slug: str) -> bool:\n            try:\n                return (\n                    (Path(WORK_DIR) / slug).exists()\n                    or (Path(WORK_DIR) / \".codebase\" / \"repos\" / slug).exists()\n                )\n            except Exception:\n                return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__safe_join_209": {
      "name": "_safe_join",
      "type": "function",
      "start_line": 209,
      "end_line": 225,
      "content_hash": "31b8d6a54d1a35484871528aebb3517d559f2a0d",
      "content": "        def _safe_join(base: Path, rel: str) -> Path:\n            # SECURITY: Prevent path traversal / absolute-path writes by ensuring the resolved\n            # candidate path stays within the intended workspace root.\n            rp = Path(str(rel))\n            if str(rp) in {\".\", \"\"}:\n                raise ValueError(\"Invalid operation path\")\n            if rp.is_absolute():\n                raise ValueError(f\"Absolute paths are not allowed: {rel}\")\n            base_resolved = base.resolve()\n            candidate = (base_resolved / rp).resolve()\n            try:\n                ok = candidate.is_relative_to(base_resolved)\n            except Exception:\n                ok = os.path.commonpath([str(base_resolved), str(candidate)]) == str(base_resolved)\n            if not ok:\n                raise ValueError(f\"Path escapes workspace: {rel}\")\n            return candidate",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__apply_operation_to_workspace_269": {
      "name": "_apply_operation_to_workspace",
      "type": "function",
      "start_line": 269,
      "end_line": 353,
      "content_hash": "bfaa06b3b66c15c053702dda8244dcee479e4340",
      "content": "            def _apply_operation_to_workspace(workspace_root: Path) -> bool:\n                \"\"\"Apply a single file operation to a workspace. Returns True on success.\"\"\"\n                nonlocal operations_count, op_type, rel_path, tar\n                \n                target_path = _safe_join(workspace_root, rel_path)\n\n                safe_source_path = None\n                source_rel_path = None\n                if op_type == \"moved\":\n                    source_rel_path = operation.get(\"source_path\") or operation.get(\"source_relative_path\")\n                    if source_rel_path:\n                        safe_source_path = _safe_join(workspace_root, source_rel_path)\n\n                try:\n                    if op_type == \"created\":\n                        file_member = None\n                        for member in tar.getnames():\n                            if member.endswith(f\"files/created/{rel_path}\"):\n                                file_member = member\n                                break\n\n                        if file_member:\n                            file_content = tar.extractfile(file_member)\n                            if file_content:\n                                target_path.parent.mkdir(parents=True, exist_ok=True)\n                                target_path.write_bytes(file_content.read())\n                                return True\n                            else:\n                                return False\n                        else:\n                            return False\n\n                    elif op_type == \"updated\":\n                        file_member = None\n                        for member in tar.getnames():\n                            if member.endswith(f\"files/updated/{rel_path}\"):\n                                file_member = member\n                                break\n\n                        if file_member:\n                            file_content = tar.extractfile(file_member)\n                            if file_content:\n                                target_path.parent.mkdir(parents=True, exist_ok=True)\n                                target_path.write_bytes(file_content.read())\n                                return True\n                            else:\n                                return False\n                        else:\n                            return False\n\n                    elif op_type == \"deleted\":\n                        if target_path.exists():\n                            target_path.unlink(missing_ok=True)\n                            return True\n                        else:\n                            return True  # Already deleted\n\n                    elif op_type == \"moved\":\n                        if safe_source_path and safe_source_path.exists():\n                            target_path.parent.mkdir(parents=True, exist_ok=True)\n                            safe_source_path.rename(target_path)\n                            return True\n                        # Remote uploads may not have the source file on the server (e.g. staging\n                        # mirrors). In that case, clients can embed the destination content under\n                        # files/moved/<dest>.\n                        file_member = None\n                        for member in tar.getnames():\n                            if member.endswith(f\"files/moved/{rel_path}\"):\n                                file_member = member\n                                break\n                        if file_member:\n                            file_content = tar.extractfile(file_member)\n                            if file_content:\n                                target_path.parent.mkdir(parents=True, exist_ok=True)\n                                target_path.write_bytes(file_content.read())\n                                return True\n                            return False\n                        return False\n\n                    else:\n                        logger.warning(f\"[upload_service] Unknown operation type: {op_type}\")\n                        return False\n                except Exception as e:\n                    logger.debug(f\"[upload_service] Failed to apply {op_type} to {rel_path} in {workspace_root}: {e}\")\n                    return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}