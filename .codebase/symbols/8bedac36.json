{
  "file_path": "/work/context-engine/scripts/learning_reranker_worker.py",
  "file_hash": "c36f916415c44212f890bd833b5920481d1f8f7c",
  "updated_at": "2025-12-26T17:34:23.034146",
  "symbols": {
    "function_get_logger_72": {
      "name": "get_logger",
      "type": "function",
      "start_line": 72,
      "end_line": 80,
      "content_hash": "56912c7a8ac38afe826da65f1a105c65a8bdc47d",
      "content": "def get_logger():\n    \"\"\"Get logger for worker.\"\"\"\n    try:\n        from scripts.logger import get_logger as _get_logger\n        return _get_logger(__name__)\n    except Exception:\n        import logging\n        logging.basicConfig(level=logging.INFO)\n        return logging.getLogger(__name__)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_CollectionLearner_86": {
      "name": "CollectionLearner",
      "type": "class",
      "start_line": 86,
      "end_line": 586,
      "content_hash": "45da705d585e45a31e9e76430e84f4c8e2029f9c",
      "content": "class CollectionLearner:\n    \"\"\"Handles learning for a single collection.\"\"\"\n\n    def __init__(self, collection: str):\n        self.collection = collection\n        self._lock_file = None\n\n        # Acquire exclusive lock to prevent multiple workers on same collection\n        self._acquire_lock()\n\n        self.scorer = TinyScorer(lr=LEARNING_RATE)\n        self.scorer.set_collection(collection)\n        self.refiner = LatentRefiner(dim=self.scorer.dim, lr=LEARNING_RATE)\n        self.refiner.set_collection(collection)\n\n        # Learned projection: raw embedding dim \u2192 working dim (256)\n        # Lower LR than scorer/refiner - projection is more sensitive\n        from scripts.embedder import get_model_dimension\n        embed_dim = get_model_dimension()  # Respects EMBEDDING_MODEL env\n        self.projection = LearnedProjection(\n            input_dim=embed_dim,\n            output_dim=self.scorer.dim,\n            lr=LEARNING_RATE * 0.5,  # Half the learning rate\n        )\n        self.projection.set_collection(collection)\n\n        self._last_processed_ts = self._load_checkpoint()\n\n        # Reuse the serving reranker's embed + project code path 1:1.\n        # We only use its private feature helpers, not its scoring loop.\n        self._feature_reranker = RecursiveReranker(\n            n_iterations=1,\n            dim=self.scorer.dim,\n            early_stop=False,\n            blend_with_initial=0.0,\n        )\n\n        # VICReg for residual regularization (prevents collapse, decorrelates)\n        self.vicreg = VICReg(\n            lambda_var=1.0,\n            lambda_cov=0.04,\n            lambda_inv=0.1,\n        ) if VICREG_WEIGHT > 0 else None\n\n        # Learned hybrid weights: dense vs. lexical balance\n        self.hybrid_weights = LearnedHybridWeights(lr=0.01)\n        self.hybrid_weights.set_collection(collection)\n\n        # Query expander: learns synonyms/related terms from usage\n        self.query_expander = QueryExpander(lr=0.1)\n        self.query_expander.set_collection(collection)\n\n        # LLM teacher for higher-quality supervision (optional)\n        # Supports llama.cpp or GLM API - auto-detects based on env\n        self._llm_client = None\n        self._llm_runtime = None\n        self._llm_calls = 0\n        if LLM_TEACHER_ENABLED:\n            try:\n                # Auto-detect runtime: GLM_API_KEY -> glm, else -> llamacpp\n                runtime = os.environ.get(\"REFRAG_RUNTIME\", \"\").strip().lower()\n                if not runtime:\n                    if os.environ.get(\"GLM_API_KEY\", \"\").strip():\n                        runtime = \"glm\"\n                    else:\n                        runtime = \"llamacpp\"\n\n                if runtime == \"glm\":\n                    from scripts.refrag_glm import GLMRefragClient\n                    self._llm_client = GLMRefragClient()\n                else:\n                    from scripts.refrag_llamacpp import LlamaCppRefragClient, is_decoder_enabled\n                    if is_decoder_enabled():\n                        self._llm_client = LlamaCppRefragClient()\n                    else:\n                        logger.info(f\"[{collection}] LLM teacher skipped (decoder disabled)\")\n\n                if self._llm_client:\n                    self._llm_runtime = runtime\n                    logger.info(f\"[{collection}] LLM teacher enabled ({runtime}, sample_rate={LLM_TEACHER_SAMPLE_RATE})\")\n            except Exception as e:\n                logger.warning(f\"[{collection}] LLM teacher unavailable: {e}\")\n\n        # Metrics tracking for logging\n        self._vicreg_loss_sum = 0.0\n        self._vicreg_count = 0\n        self._proj_grad_norm_sum = 0.0\n        self._proj_grad_count = 0\n        self._hybrid_updates = 0\n        self._expander_updates = 0\n\n    @staticmethod\n    def _sanitize_collection(collection: str) -> str:\n        \"\"\"Sanitize collection name to prevent path traversal.\"\"\"\n        return \"\".join(c if c.isalnum() or c in \"-_\" else \"_\" for c in collection)\n\n    def _acquire_lock(self):\n        \"\"\"Acquire exclusive lock to prevent multiple workers on same collection.\"\"\"\n        import fcntl\n        safe_name = self._sanitize_collection(self.collection)\n        lock_path = Path(TinyScorer.WEIGHTS_DIR) / f\"{safe_name}.lock\"\n        lock_path.parent.mkdir(parents=True, exist_ok=True)\n        self._lock_file = open(lock_path, \"w\")\n        try:\n            fcntl.flock(self._lock_file, fcntl.LOCK_EX | fcntl.LOCK_NB)\n            logger.debug(f\"[{self.collection}] Acquired exclusive lock\")\n        except OSError:\n            logger.error(f\"[{self.collection}] Another worker already running. Exiting.\")\n            sys.exit(1)\n\n    def _release_lock(self):\n        \"\"\"Release the exclusive lock.\"\"\"\n        if self._lock_file:\n            import fcntl\n            try:\n                fcntl.flock(self._lock_file, fcntl.LOCK_UN)\n                self._lock_file.close()\n            except Exception:\n                pass\n            self._lock_file = None\n\n    def _load_checkpoint(self) -> float:\n        \"\"\"Load last processed timestamp from checkpoint file.\"\"\"\n        safe_name = self._sanitize_collection(self.collection)\n        checkpoint_path = Path(TinyScorer.WEIGHTS_DIR) / f\"checkpoint_{safe_name}.json\"\n        try:\n            if checkpoint_path.exists():\n                with open(checkpoint_path) as f:\n                    return json.load(f).get(\"last_ts\", 0)\n        except Exception as e:\n            logger.warning(f\"[{self.collection}] Failed to load checkpoint: {e}\")\n        return 0\n\n    def _save_checkpoint(self, ts: float):\n        \"\"\"Save last processed timestamp to checkpoint file atomically.\"\"\"\n        safe_name = self._sanitize_collection(self.collection)\n        checkpoint_path = Path(TinyScorer.WEIGHTS_DIR) / f\"checkpoint_{safe_name}.json\"\n        tmp_path = checkpoint_path.with_suffix(\".json.tmp\")\n        try:\n            checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n            with open(tmp_path, \"w\") as f:\n                json.dump({\"last_ts\": ts, \"collection\": self.collection}, f)\n            os.replace(tmp_path, checkpoint_path)\n        except Exception as e:\n            logger.warning(f\"[{self.collection}] Failed to save checkpoint: {e}\")\n            if tmp_path.exists():\n                try:\n                    tmp_path.unlink()\n                except Exception:\n                    pass\n\n    def _encode(self, texts: List[str]) -> np.ndarray:\n        \"\"\"Encode texts to BGE embeddings (768-dim, raw without projection).\"\"\"\n        return self._feature_reranker._encode_raw(texts)\n\n    def _encode_project(self, texts: List[str]) -> np.ndarray:\n        \"\"\"Encode + project via learned projection (for inference/serving compat).\"\"\"\n        embs = self._encode(texts)\n        return self.projection.forward(embs)\n\n    def _encode_project_with_cache(\n        self, texts: List[str]\n    ) -> Tuple[np.ndarray, Dict[str, Any]]:\n        \"\"\"Encode + project with cache for gradient backprop through projection.\n\n        Returns:\n            projected: (batch, output_dim) projected embeddings\n            cache: dict with raw_embs and projection cache for backward pass\n        \"\"\"\n        raw_embs = self._encode(texts)  # (batch, 768)\n        projected, proj_cache = self.projection.forward_with_cache(raw_embs)\n        cache = {\"raw_embs\": raw_embs, \"proj_cache\": proj_cache}\n        return projected, cache\n\n    @staticmethod\n    def _pack_doc(candidate: Dict[str, Any], max_chars: int = 500) -> str:\n        \"\"\"Pack candidate into doc text (shared by learning and teacher scoring).\"\"\"\n        parts = []\n        if candidate.get(\"symbol\"):\n            parts.append(str(candidate[\"symbol\"]))\n        if candidate.get(\"path\"):\n            parts.append(str(candidate[\"path\"]))\n        code = candidate.get(\"code\") or candidate.get(\"snippet\") or candidate.get(\"text\") or \"\"\n        if code:\n            parts.append(str(code)[:max_chars])\n        return \" \".join(parts) if parts else \"empty\"\n\n    def _llm_judge(self, query: str, candidates: List[Dict[str, Any]], top_k: int = 5) -> Optional[np.ndarray]:\n        \"\"\"Get LLM relevance judgments for top candidates.\n\n        Works with llama.cpp or GLM API (auto-detected at init).\n\n        Returns:\n            scores: (n_candidates,) array with LLM-derived scores, or None if failed\n        \"\"\"\n        if not self._llm_client or not candidates:\n            return None\n\n        # Only judge top-k to save API cost\n        n = len(candidates)\n        top_k = min(top_k, n)\n\n        # Build prompt for GLM to rate relevance\n        prompt_parts = [f\"Rate code search relevance 0-10 for query: \\\"{query}\\\"\\n\"]\n        for i in range(top_k):\n            c = candidates[i]\n            doc = self._pack_doc(c, max_chars=800)\n            prompt_parts.append(f\"[{i}] {doc}\")\n\n        prompt_parts.append(\n            \"\\nRespond with JSON: {\\\"scores\\\": [N, N, ...]} where each N is 0-10 relevance.\"\n        )\n        prompt = \"\\n\".join(prompt_parts)\n\n        try:\n            import json\n            response = self._llm_client.generate_with_soft_embeddings(\n                prompt=prompt,\n                max_tokens=64,\n                temperature=0.1,\n                force_json=True,\n                disable_thinking=True,  # Fast mode\n            )\n            data = json.loads(response)\n            llm_scores = data.get(\"scores\", [])\n\n            if len(llm_scores) >= top_k:\n                # Normalize to 0-1 and extend to all candidates\n                scores = np.zeros(n, dtype=np.float32)\n                for i in range(top_k):\n                    scores[i] = float(llm_scores[i]) / 10.0\n                # Lower candidates get decaying scores\n                for i in range(top_k, n):\n                    scores[i] = max(0, scores[top_k - 1] * 0.5 ** (i - top_k + 1))\n\n                self._llm_calls += 1\n                return scores\n        except Exception as e:\n            logger.debug(f\"[{self.collection}] LLM judge failed: {e}\")\n\n        return None\n\n    def process_events(self, limit: int = 1000) -> int:\n        \"\"\"Process pending events and return count processed.\"\"\"\n        events = read_events(self.collection, since_ts=self._last_processed_ts, limit=limit)\n        if not events:\n            return 0\n\n        processed = 0\n        batch_events = []\n\n        for event in events:\n            batch_events.append(event)\n            if len(batch_events) >= BATCH_SIZE:\n                self._learn_from_batch(batch_events)\n                processed += len(batch_events)\n                batch_events = []\n\n        # Process remaining\n        if batch_events:\n            self._learn_from_batch(batch_events)\n            processed += len(batch_events)\n\n        # Update checkpoint\n        if events:\n            self._last_processed_ts = max(e.get(\"ts\", 0) for e in events)\n            self._save_checkpoint(self._last_processed_ts)\n\n        # Save weights after processing batch (both scorer and refiner)\n        if processed > 0:\n            self.scorer._save_weights(checkpoint=True)\n            self.refiner._save_weights(checkpoint=True)\n            metrics = self.scorer.get_metrics()\n\n            # VICReg + projection info\n            extra_info = \"\"\n            if self._vicreg_count > 0:\n                avg_vicreg = self._vicreg_loss_sum / self._vicreg_count\n                extra_info += f\" | vicreg={avg_vicreg:.4f}\"\n                self._vicreg_loss_sum = 0.0\n                self._vicreg_count = 0\n\n            if self._proj_grad_count > 0:\n                avg_proj_grad = self._proj_grad_norm_sum / self._proj_grad_count\n                extra_info += f\" | proj_grad={avg_proj_grad:.4f}\"\n                self._proj_grad_norm_sum = 0.0\n                self._proj_grad_count = 0\n\n            # Hybrid weight, expander, and GLM stats\n            hw_info = f\" dense_w={self.hybrid_weights.dense_weight:.2f}\"\n            exp_stats = self.query_expander.get_stats()\n            exp_info = f\" terms={exp_stats['terms']}\"\n            llm_info = f\" llm={self._llm_calls}({self._llm_runtime})\" if self._llm_client else \"\"\n\n            logger.info(\n                f\"[{self.collection}] Processed {processed} events | \"\n                f\"scorer_v{metrics['version']} refiner_v{self.refiner._version} proj_v{self.projection._version}{hw_info}{exp_info}{llm_info} | \"\n                f\"lr={metrics['learning_rate']:.6f} | \"\n                f\"avg_loss={metrics['avg_loss']:.4f}{extra_info} | converged={metrics['converged']}\"\n            )\n\n        return processed\n\n    def _learn_from_batch(self, events: List[Dict[str, Any]]):\n        \"\"\"Learn from batch with 3-pass deep supervision + VICReg + projection learning.\n\n        Full end-to-end learning:\n        - Projection: BGE (768) \u2192 working dim (256), learns domain-specific subspace\n        - Scorer: learns to rank with current z\n        - Refiner: learns to improve z toward teacher-optimal state\n        - VICReg: regularizes residuals to prevent collapse\n\n        Deep Supervision (TRM-style):\n        - Each refinement pass gets a loss signal toward teacher_z\n        - Later passes have decaying weight (pass 1: 1.0, pass 2: 0.7, pass 3: 0.5)\n        \"\"\"\n        # Fill missing teacher scores in batch (amortizes ONNX overhead)\n        self._maybe_fill_teacher_scores(events)\n\n        # Accumulate data for end-of-batch updates\n        vicreg_data: List[tuple] = []  # [(z, z_refined, refiner_cache), ...]\n        projection_grads: List[Tuple[np.ndarray, Dict[str, Any]]] = []  # [(grad, proj_cache), ...]\n\n        # Deep supervision weights: earlier passes contribute more\n        pass_weights = [1.0, 0.7, 0.5]\n        n_passes = 3\n\n        for event in events:\n            try:\n                query = event.get(\"query\", \"\")\n                candidates = event.get(\"candidates\", [])\n                teacher_scores = event.get(\"teacher_scores\")\n\n                if not query or not candidates or not teacher_scores:\n                    continue\n\n                # Validate alignment between candidates and teacher scores\n                if len(teacher_scores) != len(candidates):\n                    logger.warning(f\"[{self.collection}] Skipping event: mismatched lengths\")\n                    continue\n\n                # Build doc texts from candidates\n                doc_texts = [self._pack_doc(c) for c in candidates]\n                teacher_arr = np.array(teacher_scores, dtype=np.float32)\n\n                # ===== LLM TEACHER: Higher-quality supervision (sampled) =====\n                if self._llm_client and np.random.random() < LLM_TEACHER_SAMPLE_RATE:\n                    llm_scores = self._llm_judge(query, candidates, top_k=5)\n                    if llm_scores is not None:\n                        # Blend LLM with ONNX: LLM is more reliable, weight it higher\n                        teacher_arr = 0.3 * teacher_arr + 0.7 * llm_scores\n\n                # ===== ENCODE WITH CACHE FOR PROJECTION LEARNING =====\n                # Query embedding with cache for gradient backprop\n                query_proj, query_proj_cache = self._encode_project_with_cache([query])\n                query_emb = query_proj[0]\n\n                # Doc embeddings with cache for gradient backprop\n                doc_embs, doc_proj_cache = self._encode_project_with_cache(doc_texts)\n\n                # Compute teacher-weighted document summary as target z\n                teacher_weights = np.exp(teacher_arr - teacher_arr.max())\n                teacher_weights = teacher_weights / (teacher_weights.sum() + 1e-8)\n                teacher_z = (teacher_weights[:, None] * doc_embs).sum(axis=0)\n                teacher_z = teacher_z / (np.linalg.norm(teacher_z) + 1e-8)\n\n                # Initialize latent state from query\n                z = query_emb.copy()\n\n                # ===== PROJECTION GRADIENT: Contrastive alignment loss =====\n                # Goal: projection should produce embeddings where query is close to\n                # high-scoring docs and far from low-scoring docs\n                # Gradient w.r.t. query: weighted by teacher scores\n                # Push query toward high-teacher-score docs\n                query_grad = (teacher_weights[:, None] * doc_embs).sum(axis=0) - query_emb\n                query_grad = query_grad / (np.linalg.norm(query_grad) + 1e-8)\n\n                # Gradient w.r.t. docs: each doc pulled/pushed based on teacher score\n                # High score docs: pull toward query, Low score docs: push away\n                centered_weights = teacher_weights - teacher_weights.mean()\n                doc_grad = centered_weights[:, None] * (query_emb - doc_embs)\n\n                projection_grads.append((query_grad.reshape(1, -1), query_proj_cache))\n                projection_grads.append((doc_grad, doc_proj_cache))\n\n                # ===== LEARN HYBRID WEIGHTS (dense vs. lexical) =====\n                # Extract dense/lexical scores if available in candidates\n                if candidates and \"dense_score\" in candidates[0] and \"lexical_score\" in candidates[0]:\n                    dense_scores = np.array([c.get(\"dense_score\", 0) for c in candidates], dtype=np.float32)\n                    lexical_scores = np.array([c.get(\"lexical_score\", 0) for c in candidates], dtype=np.float32)\n                    self.hybrid_weights.learn_from_teacher(dense_scores, lexical_scores, teacher_arr)\n                    self._hybrid_updates += 1\n\n                # ===== LEARN QUERY EXPANSIONS =====\n                # Learn term associations from high-scoring docs\n                self.query_expander.learn_from_teacher(query, doc_texts, teacher_arr)\n                self._expander_updates += 1\n\n                # ===== 3-PASS DEEP SUPERVISION =====\n                for pass_idx in range(n_passes):\n                    # Score with current z\n                    scores = self.scorer.forward(query_emb, doc_embs, z)\n\n                    # Train scorer at this pass (learns to rank with current z)\n                    self.scorer.learn_from_teacher(query_emb, doc_embs, z, teacher_arr)\n\n                    # Train refiner: z \u2192 z' toward teacher_z\n                    if self.vicreg is not None:\n                        # Get cache for VICReg backprop\n                        _, z_orig, z_refined, cache = self.refiner.learn_from_teacher_with_cache(\n                            z, query_emb, doc_embs, scores, teacher_z\n                        )\n                        cache[\"pass_weight\"] = pass_weights[pass_idx]\n                        vicreg_data.append((z_orig, z_refined, cache))\n                    else:\n                        self.refiner.learn_from_teacher(z, query_emb, doc_embs, scores, teacher_z)\n\n                    # Update z for next pass (refinement chain)\n                    z = self.refiner.refine(z, query_emb, doc_embs, scores)\n\n            except Exception as e:\n                logger.warning(f\"[{self.collection}] Error processing event: {e}\")\n                continue\n\n        # ===== PROJECTION LEARNING: batch update =====\n        if projection_grads:\n            try:\n                for grad, cache in projection_grads:\n                    self.projection.backward(grad, cache[\"proj_cache\"], weight=0.1)\n                    self._proj_grad_norm_sum += np.linalg.norm(grad)\n                    self._proj_grad_count += 1\n            except Exception as e:\n                logger.warning(f\"[{self.collection}] Projection update failed: {e}\")\n\n        # ===== VICReg: batch-level residual regularization =====\n        if self.vicreg is not None and len(vicreg_data) >= VICREG_MIN_BATCH:\n            try:\n                z_batch = np.vstack([item[0] for item in vicreg_data])\n                z_refined_batch = np.vstack([item[1] for item in vicreg_data])\n\n                vicreg_loss, vicreg_grad, _ = self.vicreg.forward(z_batch, z_refined_batch)\n\n                self._vicreg_loss_sum += vicreg_loss\n                self._vicreg_count += 1\n\n                for i, (_, _, cache) in enumerate(vicreg_data):\n                    pass_weight = cache.get(\"pass_weight\", 1.0)\n                    self.refiner.apply_vicreg_gradient(\n                        vicreg_grad[i], cache, weight=VICREG_WEIGHT * pass_weight\n                    )\n\n            except Exception as e:\n                logger.warning(f\"[{self.collection}] VICReg failed: {e}\")\n\n    def _maybe_fill_teacher_scores(self, events: List[Dict[str, Any]]):\n        \"\"\"Compute teacher scores for events that don't already have them.\"\"\"\n        try:\n            from scripts.rerank_local import rerank_local\n        except Exception:\n            rerank_local = None\n\n        if rerank_local is None:\n            return\n\n        # Gather pairs across events so we can call rerank_local once.\n        all_pairs: List[tuple] = []\n        slices: List[tuple] = []  # (event_index, start, end)\n\n        for event_index, event in enumerate(events):\n            if event.get(\"teacher_scores\"):\n                continue\n\n            query = event.get(\"query\", \"\")\n            candidates = event.get(\"candidates\", [])\n            if not query or not candidates:\n                continue\n\n            start = len(all_pairs)\n            for c in candidates:\n                doc_text = self._pack_doc(c)  # Same packing as learning path\n                all_pairs.append((query, doc_text))\n            end = len(all_pairs)\n            if end > start:\n                slices.append((event_index, start, end))\n\n        if not slices:\n            return\n\n        try:\n            scores = rerank_local(all_pairs)\n        except Exception as e:\n            logger.warning(f\"[{self.collection}] Teacher scoring failed for {len(all_pairs)} pairs: {e}\")\n            return\n\n        # Map scores back to each event.\n        for event_index, start, end in slices:\n            try:\n                events[event_index][\"teacher_scores\"] = list(scores[start:end])\n            except Exception as e:\n                logger.warning(f\"[{self.collection}] Failed to map teacher scores for event {event_index}: {e}\")\n                continue",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___89": {
      "name": "__init__",
      "type": "method",
      "start_line": 89,
      "end_line": 175,
      "content_hash": "cddd3dc88b096ad607a93353b7e1903e9c4614a5",
      "content": "    def __init__(self, collection: str):\n        self.collection = collection\n        self._lock_file = None\n\n        # Acquire exclusive lock to prevent multiple workers on same collection\n        self._acquire_lock()\n\n        self.scorer = TinyScorer(lr=LEARNING_RATE)\n        self.scorer.set_collection(collection)\n        self.refiner = LatentRefiner(dim=self.scorer.dim, lr=LEARNING_RATE)\n        self.refiner.set_collection(collection)\n\n        # Learned projection: raw embedding dim \u2192 working dim (256)\n        # Lower LR than scorer/refiner - projection is more sensitive\n        from scripts.embedder import get_model_dimension\n        embed_dim = get_model_dimension()  # Respects EMBEDDING_MODEL env\n        self.projection = LearnedProjection(\n            input_dim=embed_dim,\n            output_dim=self.scorer.dim,\n            lr=LEARNING_RATE * 0.5,  # Half the learning rate\n        )\n        self.projection.set_collection(collection)\n\n        self._last_processed_ts = self._load_checkpoint()\n\n        # Reuse the serving reranker's embed + project code path 1:1.\n        # We only use its private feature helpers, not its scoring loop.\n        self._feature_reranker = RecursiveReranker(\n            n_iterations=1,\n            dim=self.scorer.dim,\n            early_stop=False,\n            blend_with_initial=0.0,\n        )\n\n        # VICReg for residual regularization (prevents collapse, decorrelates)\n        self.vicreg = VICReg(\n            lambda_var=1.0,\n            lambda_cov=0.04,\n            lambda_inv=0.1,\n        ) if VICREG_WEIGHT > 0 else None\n\n        # Learned hybrid weights: dense vs. lexical balance\n        self.hybrid_weights = LearnedHybridWeights(lr=0.01)\n        self.hybrid_weights.set_collection(collection)\n\n        # Query expander: learns synonyms/related terms from usage\n        self.query_expander = QueryExpander(lr=0.1)\n        self.query_expander.set_collection(collection)\n\n        # LLM teacher for higher-quality supervision (optional)\n        # Supports llama.cpp or GLM API - auto-detects based on env\n        self._llm_client = None\n        self._llm_runtime = None\n        self._llm_calls = 0\n        if LLM_TEACHER_ENABLED:\n            try:\n                # Auto-detect runtime: GLM_API_KEY -> glm, else -> llamacpp\n                runtime = os.environ.get(\"REFRAG_RUNTIME\", \"\").strip().lower()\n                if not runtime:\n                    if os.environ.get(\"GLM_API_KEY\", \"\").strip():\n                        runtime = \"glm\"\n                    else:\n                        runtime = \"llamacpp\"\n\n                if runtime == \"glm\":\n                    from scripts.refrag_glm import GLMRefragClient\n                    self._llm_client = GLMRefragClient()\n                else:\n                    from scripts.refrag_llamacpp import LlamaCppRefragClient, is_decoder_enabled\n                    if is_decoder_enabled():\n                        self._llm_client = LlamaCppRefragClient()\n                    else:\n                        logger.info(f\"[{collection}] LLM teacher skipped (decoder disabled)\")\n\n                if self._llm_client:\n                    self._llm_runtime = runtime\n                    logger.info(f\"[{collection}] LLM teacher enabled ({runtime}, sample_rate={LLM_TEACHER_SAMPLE_RATE})\")\n            except Exception as e:\n                logger.warning(f\"[{collection}] LLM teacher unavailable: {e}\")\n\n        # Metrics tracking for logging\n        self._vicreg_loss_sum = 0.0\n        self._vicreg_count = 0\n        self._proj_grad_norm_sum = 0.0\n        self._proj_grad_count = 0\n        self._hybrid_updates = 0\n        self._expander_updates = 0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__sanitize_collection_178": {
      "name": "_sanitize_collection",
      "type": "method",
      "start_line": 178,
      "end_line": 180,
      "content_hash": "e359c3b5022e7b75309996e45f7d7fa4e7a96710",
      "content": "    def _sanitize_collection(collection: str) -> str:\n        \"\"\"Sanitize collection name to prevent path traversal.\"\"\"\n        return \"\".join(c if c.isalnum() or c in \"-_\" else \"_\" for c in collection)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__acquire_lock_182": {
      "name": "_acquire_lock",
      "type": "method",
      "start_line": 182,
      "end_line": 194,
      "content_hash": "2aaa41b944ec6a4a095c6ec5db0afb19538400a3",
      "content": "    def _acquire_lock(self):\n        \"\"\"Acquire exclusive lock to prevent multiple workers on same collection.\"\"\"\n        import fcntl\n        safe_name = self._sanitize_collection(self.collection)\n        lock_path = Path(TinyScorer.WEIGHTS_DIR) / f\"{safe_name}.lock\"\n        lock_path.parent.mkdir(parents=True, exist_ok=True)\n        self._lock_file = open(lock_path, \"w\")\n        try:\n            fcntl.flock(self._lock_file, fcntl.LOCK_EX | fcntl.LOCK_NB)\n            logger.debug(f\"[{self.collection}] Acquired exclusive lock\")\n        except OSError:\n            logger.error(f\"[{self.collection}] Another worker already running. Exiting.\")\n            sys.exit(1)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__release_lock_196": {
      "name": "_release_lock",
      "type": "method",
      "start_line": 196,
      "end_line": 205,
      "content_hash": "1ab59b303fbdd68e64632b46a02a0b235e47688e",
      "content": "    def _release_lock(self):\n        \"\"\"Release the exclusive lock.\"\"\"\n        if self._lock_file:\n            import fcntl\n            try:\n                fcntl.flock(self._lock_file, fcntl.LOCK_UN)\n                self._lock_file.close()\n            except Exception:\n                pass\n            self._lock_file = None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__load_checkpoint_207": {
      "name": "_load_checkpoint",
      "type": "method",
      "start_line": 207,
      "end_line": 217,
      "content_hash": "1bc3cca5e40829523268d2dbfbd4bca197608d3f",
      "content": "    def _load_checkpoint(self) -> float:\n        \"\"\"Load last processed timestamp from checkpoint file.\"\"\"\n        safe_name = self._sanitize_collection(self.collection)\n        checkpoint_path = Path(TinyScorer.WEIGHTS_DIR) / f\"checkpoint_{safe_name}.json\"\n        try:\n            if checkpoint_path.exists():\n                with open(checkpoint_path) as f:\n                    return json.load(f).get(\"last_ts\", 0)\n        except Exception as e:\n            logger.warning(f\"[{self.collection}] Failed to load checkpoint: {e}\")\n        return 0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__save_checkpoint_219": {
      "name": "_save_checkpoint",
      "type": "method",
      "start_line": 219,
      "end_line": 235,
      "content_hash": "ec75aa592fb96046bdfd3061c449022343986943",
      "content": "    def _save_checkpoint(self, ts: float):\n        \"\"\"Save last processed timestamp to checkpoint file atomically.\"\"\"\n        safe_name = self._sanitize_collection(self.collection)\n        checkpoint_path = Path(TinyScorer.WEIGHTS_DIR) / f\"checkpoint_{safe_name}.json\"\n        tmp_path = checkpoint_path.with_suffix(\".json.tmp\")\n        try:\n            checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n            with open(tmp_path, \"w\") as f:\n                json.dump({\"last_ts\": ts, \"collection\": self.collection}, f)\n            os.replace(tmp_path, checkpoint_path)\n        except Exception as e:\n            logger.warning(f\"[{self.collection}] Failed to save checkpoint: {e}\")\n            if tmp_path.exists():\n                try:\n                    tmp_path.unlink()\n                except Exception:\n                    pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__encode_237": {
      "name": "_encode",
      "type": "method",
      "start_line": 237,
      "end_line": 239,
      "content_hash": "a4431d384e3ff8f4b8208874e3e503668f6f22dd",
      "content": "    def _encode(self, texts: List[str]) -> np.ndarray:\n        \"\"\"Encode texts to BGE embeddings (768-dim, raw without projection).\"\"\"\n        return self._feature_reranker._encode_raw(texts)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__encode_project_241": {
      "name": "_encode_project",
      "type": "method",
      "start_line": 241,
      "end_line": 244,
      "content_hash": "a5d174e2a3f4c8dc8e3625320da50963785dce61",
      "content": "    def _encode_project(self, texts: List[str]) -> np.ndarray:\n        \"\"\"Encode + project via learned projection (for inference/serving compat).\"\"\"\n        embs = self._encode(texts)\n        return self.projection.forward(embs)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__encode_project_with_cache_246": {
      "name": "_encode_project_with_cache",
      "type": "method",
      "start_line": 246,
      "end_line": 258,
      "content_hash": "318f04ed99bca307376de41070edea4ab225818c",
      "content": "    def _encode_project_with_cache(\n        self, texts: List[str]\n    ) -> Tuple[np.ndarray, Dict[str, Any]]:\n        \"\"\"Encode + project with cache for gradient backprop through projection.\n\n        Returns:\n            projected: (batch, output_dim) projected embeddings\n            cache: dict with raw_embs and projection cache for backward pass\n        \"\"\"\n        raw_embs = self._encode(texts)  # (batch, 768)\n        projected, proj_cache = self.projection.forward_with_cache(raw_embs)\n        cache = {\"raw_embs\": raw_embs, \"proj_cache\": proj_cache}\n        return projected, cache",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__pack_doc_261": {
      "name": "_pack_doc",
      "type": "method",
      "start_line": 261,
      "end_line": 271,
      "content_hash": "ecbf92ea5590a996a9ee7ac090e995a6b3a49f67",
      "content": "    def _pack_doc(candidate: Dict[str, Any], max_chars: int = 500) -> str:\n        \"\"\"Pack candidate into doc text (shared by learning and teacher scoring).\"\"\"\n        parts = []\n        if candidate.get(\"symbol\"):\n            parts.append(str(candidate[\"symbol\"]))\n        if candidate.get(\"path\"):\n            parts.append(str(candidate[\"path\"]))\n        code = candidate.get(\"code\") or candidate.get(\"snippet\") or candidate.get(\"text\") or \"\"\n        if code:\n            parts.append(str(code)[:max_chars])\n        return \" \".join(parts) if parts else \"empty\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__llm_judge_273": {
      "name": "_llm_judge",
      "type": "method",
      "start_line": 273,
      "end_line": 326,
      "content_hash": "66e7ec9a4457bf10ee0e99a8cee41bb70cd60bed",
      "content": "    def _llm_judge(self, query: str, candidates: List[Dict[str, Any]], top_k: int = 5) -> Optional[np.ndarray]:\n        \"\"\"Get LLM relevance judgments for top candidates.\n\n        Works with llama.cpp or GLM API (auto-detected at init).\n\n        Returns:\n            scores: (n_candidates,) array with LLM-derived scores, or None if failed\n        \"\"\"\n        if not self._llm_client or not candidates:\n            return None\n\n        # Only judge top-k to save API cost\n        n = len(candidates)\n        top_k = min(top_k, n)\n\n        # Build prompt for GLM to rate relevance\n        prompt_parts = [f\"Rate code search relevance 0-10 for query: \\\"{query}\\\"\\n\"]\n        for i in range(top_k):\n            c = candidates[i]\n            doc = self._pack_doc(c, max_chars=800)\n            prompt_parts.append(f\"[{i}] {doc}\")\n\n        prompt_parts.append(\n            \"\\nRespond with JSON: {\\\"scores\\\": [N, N, ...]} where each N is 0-10 relevance.\"\n        )\n        prompt = \"\\n\".join(prompt_parts)\n\n        try:\n            import json\n            response = self._llm_client.generate_with_soft_embeddings(\n                prompt=prompt,\n                max_tokens=64,\n                temperature=0.1,\n                force_json=True,\n                disable_thinking=True,  # Fast mode\n            )\n            data = json.loads(response)\n            llm_scores = data.get(\"scores\", [])\n\n            if len(llm_scores) >= top_k:\n                # Normalize to 0-1 and extend to all candidates\n                scores = np.zeros(n, dtype=np.float32)\n                for i in range(top_k):\n                    scores[i] = float(llm_scores[i]) / 10.0\n                # Lower candidates get decaying scores\n                for i in range(top_k, n):\n                    scores[i] = max(0, scores[top_k - 1] * 0.5 ** (i - top_k + 1))\n\n                self._llm_calls += 1\n                return scores\n        except Exception as e:\n            logger.debug(f\"[{self.collection}] LLM judge failed: {e}\")\n\n        return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_process_events_328": {
      "name": "process_events",
      "type": "method",
      "start_line": 328,
      "end_line": 387,
      "content_hash": "4a6b0dd70546bae13fa15868c1e5c531ab9a1fec",
      "content": "    def process_events(self, limit: int = 1000) -> int:\n        \"\"\"Process pending events and return count processed.\"\"\"\n        events = read_events(self.collection, since_ts=self._last_processed_ts, limit=limit)\n        if not events:\n            return 0\n\n        processed = 0\n        batch_events = []\n\n        for event in events:\n            batch_events.append(event)\n            if len(batch_events) >= BATCH_SIZE:\n                self._learn_from_batch(batch_events)\n                processed += len(batch_events)\n                batch_events = []\n\n        # Process remaining\n        if batch_events:\n            self._learn_from_batch(batch_events)\n            processed += len(batch_events)\n\n        # Update checkpoint\n        if events:\n            self._last_processed_ts = max(e.get(\"ts\", 0) for e in events)\n            self._save_checkpoint(self._last_processed_ts)\n\n        # Save weights after processing batch (both scorer and refiner)\n        if processed > 0:\n            self.scorer._save_weights(checkpoint=True)\n            self.refiner._save_weights(checkpoint=True)\n            metrics = self.scorer.get_metrics()\n\n            # VICReg + projection info\n            extra_info = \"\"\n            if self._vicreg_count > 0:\n                avg_vicreg = self._vicreg_loss_sum / self._vicreg_count\n                extra_info += f\" | vicreg={avg_vicreg:.4f}\"\n                self._vicreg_loss_sum = 0.0\n                self._vicreg_count = 0\n\n            if self._proj_grad_count > 0:\n                avg_proj_grad = self._proj_grad_norm_sum / self._proj_grad_count\n                extra_info += f\" | proj_grad={avg_proj_grad:.4f}\"\n                self._proj_grad_norm_sum = 0.0\n                self._proj_grad_count = 0\n\n            # Hybrid weight, expander, and GLM stats\n            hw_info = f\" dense_w={self.hybrid_weights.dense_weight:.2f}\"\n            exp_stats = self.query_expander.get_stats()\n            exp_info = f\" terms={exp_stats['terms']}\"\n            llm_info = f\" llm={self._llm_calls}({self._llm_runtime})\" if self._llm_client else \"\"\n\n            logger.info(\n                f\"[{self.collection}] Processed {processed} events | \"\n                f\"scorer_v{metrics['version']} refiner_v{self.refiner._version} proj_v{self.projection._version}{hw_info}{exp_info}{llm_info} | \"\n                f\"lr={metrics['learning_rate']:.6f} | \"\n                f\"avg_loss={metrics['avg_loss']:.4f}{extra_info} | converged={metrics['converged']}\"\n            )\n\n        return processed",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__learn_from_batch_389": {
      "name": "_learn_from_batch",
      "type": "method",
      "start_line": 389,
      "end_line": 538,
      "content_hash": "94eb818a1e9ad931d684a8cacad51aa1c0221c21",
      "content": "    def _learn_from_batch(self, events: List[Dict[str, Any]]):\n        \"\"\"Learn from batch with 3-pass deep supervision + VICReg + projection learning.\n\n        Full end-to-end learning:\n        - Projection: BGE (768) \u2192 working dim (256), learns domain-specific subspace\n        - Scorer: learns to rank with current z\n        - Refiner: learns to improve z toward teacher-optimal state\n        - VICReg: regularizes residuals to prevent collapse\n\n        Deep Supervision (TRM-style):\n        - Each refinement pass gets a loss signal toward teacher_z\n        - Later passes have decaying weight (pass 1: 1.0, pass 2: 0.7, pass 3: 0.5)\n        \"\"\"\n        # Fill missing teacher scores in batch (amortizes ONNX overhead)\n        self._maybe_fill_teacher_scores(events)\n\n        # Accumulate data for end-of-batch updates\n        vicreg_data: List[tuple] = []  # [(z, z_refined, refiner_cache), ...]\n        projection_grads: List[Tuple[np.ndarray, Dict[str, Any]]] = []  # [(grad, proj_cache), ...]\n\n        # Deep supervision weights: earlier passes contribute more\n        pass_weights = [1.0, 0.7, 0.5]\n        n_passes = 3\n\n        for event in events:\n            try:\n                query = event.get(\"query\", \"\")\n                candidates = event.get(\"candidates\", [])\n                teacher_scores = event.get(\"teacher_scores\")\n\n                if not query or not candidates or not teacher_scores:\n                    continue\n\n                # Validate alignment between candidates and teacher scores\n                if len(teacher_scores) != len(candidates):\n                    logger.warning(f\"[{self.collection}] Skipping event: mismatched lengths\")\n                    continue\n\n                # Build doc texts from candidates\n                doc_texts = [self._pack_doc(c) for c in candidates]\n                teacher_arr = np.array(teacher_scores, dtype=np.float32)\n\n                # ===== LLM TEACHER: Higher-quality supervision (sampled) =====\n                if self._llm_client and np.random.random() < LLM_TEACHER_SAMPLE_RATE:\n                    llm_scores = self._llm_judge(query, candidates, top_k=5)\n                    if llm_scores is not None:\n                        # Blend LLM with ONNX: LLM is more reliable, weight it higher\n                        teacher_arr = 0.3 * teacher_arr + 0.7 * llm_scores\n\n                # ===== ENCODE WITH CACHE FOR PROJECTION LEARNING =====\n                # Query embedding with cache for gradient backprop\n                query_proj, query_proj_cache = self._encode_project_with_cache([query])\n                query_emb = query_proj[0]\n\n                # Doc embeddings with cache for gradient backprop\n                doc_embs, doc_proj_cache = self._encode_project_with_cache(doc_texts)\n\n                # Compute teacher-weighted document summary as target z\n                teacher_weights = np.exp(teacher_arr - teacher_arr.max())\n                teacher_weights = teacher_weights / (teacher_weights.sum() + 1e-8)\n                teacher_z = (teacher_weights[:, None] * doc_embs).sum(axis=0)\n                teacher_z = teacher_z / (np.linalg.norm(teacher_z) + 1e-8)\n\n                # Initialize latent state from query\n                z = query_emb.copy()\n\n                # ===== PROJECTION GRADIENT: Contrastive alignment loss =====\n                # Goal: projection should produce embeddings where query is close to\n                # high-scoring docs and far from low-scoring docs\n                # Gradient w.r.t. query: weighted by teacher scores\n                # Push query toward high-teacher-score docs\n                query_grad = (teacher_weights[:, None] * doc_embs).sum(axis=0) - query_emb\n                query_grad = query_grad / (np.linalg.norm(query_grad) + 1e-8)\n\n                # Gradient w.r.t. docs: each doc pulled/pushed based on teacher score\n                # High score docs: pull toward query, Low score docs: push away\n                centered_weights = teacher_weights - teacher_weights.mean()\n                doc_grad = centered_weights[:, None] * (query_emb - doc_embs)\n\n                projection_grads.append((query_grad.reshape(1, -1), query_proj_cache))\n                projection_grads.append((doc_grad, doc_proj_cache))\n\n                # ===== LEARN HYBRID WEIGHTS (dense vs. lexical) =====\n                # Extract dense/lexical scores if available in candidates\n                if candidates and \"dense_score\" in candidates[0] and \"lexical_score\" in candidates[0]:\n                    dense_scores = np.array([c.get(\"dense_score\", 0) for c in candidates], dtype=np.float32)\n                    lexical_scores = np.array([c.get(\"lexical_score\", 0) for c in candidates], dtype=np.float32)\n                    self.hybrid_weights.learn_from_teacher(dense_scores, lexical_scores, teacher_arr)\n                    self._hybrid_updates += 1\n\n                # ===== LEARN QUERY EXPANSIONS =====\n                # Learn term associations from high-scoring docs\n                self.query_expander.learn_from_teacher(query, doc_texts, teacher_arr)\n                self._expander_updates += 1\n\n                # ===== 3-PASS DEEP SUPERVISION =====\n                for pass_idx in range(n_passes):\n                    # Score with current z\n                    scores = self.scorer.forward(query_emb, doc_embs, z)\n\n                    # Train scorer at this pass (learns to rank with current z)\n                    self.scorer.learn_from_teacher(query_emb, doc_embs, z, teacher_arr)\n\n                    # Train refiner: z \u2192 z' toward teacher_z\n                    if self.vicreg is not None:\n                        # Get cache for VICReg backprop\n                        _, z_orig, z_refined, cache = self.refiner.learn_from_teacher_with_cache(\n                            z, query_emb, doc_embs, scores, teacher_z\n                        )\n                        cache[\"pass_weight\"] = pass_weights[pass_idx]\n                        vicreg_data.append((z_orig, z_refined, cache))\n                    else:\n                        self.refiner.learn_from_teacher(z, query_emb, doc_embs, scores, teacher_z)\n\n                    # Update z for next pass (refinement chain)\n                    z = self.refiner.refine(z, query_emb, doc_embs, scores)\n\n            except Exception as e:\n                logger.warning(f\"[{self.collection}] Error processing event: {e}\")\n                continue\n\n        # ===== PROJECTION LEARNING: batch update =====\n        if projection_grads:\n            try:\n                for grad, cache in projection_grads:\n                    self.projection.backward(grad, cache[\"proj_cache\"], weight=0.1)\n                    self._proj_grad_norm_sum += np.linalg.norm(grad)\n                    self._proj_grad_count += 1\n            except Exception as e:\n                logger.warning(f\"[{self.collection}] Projection update failed: {e}\")\n\n        # ===== VICReg: batch-level residual regularization =====\n        if self.vicreg is not None and len(vicreg_data) >= VICREG_MIN_BATCH:\n            try:\n                z_batch = np.vstack([item[0] for item in vicreg_data])\n                z_refined_batch = np.vstack([item[1] for item in vicreg_data])\n\n                vicreg_loss, vicreg_grad, _ = self.vicreg.forward(z_batch, z_refined_batch)\n\n                self._vicreg_loss_sum += vicreg_loss\n                self._vicreg_count += 1\n\n                for i, (_, _, cache) in enumerate(vicreg_data):\n                    pass_weight = cache.get(\"pass_weight\", 1.0)\n                    self.refiner.apply_vicreg_gradient(\n                        vicreg_grad[i], cache, weight=VICREG_WEIGHT * pass_weight\n                    )\n\n            except Exception as e:\n                logger.warning(f\"[{self.collection}] VICReg failed: {e}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__maybe_fill_teacher_scores_540": {
      "name": "_maybe_fill_teacher_scores",
      "type": "method",
      "start_line": 540,
      "end_line": 586,
      "content_hash": "3d96a6b5c40ea3255ad70953eb5167d94d49063d",
      "content": "    def _maybe_fill_teacher_scores(self, events: List[Dict[str, Any]]):\n        \"\"\"Compute teacher scores for events that don't already have them.\"\"\"\n        try:\n            from scripts.rerank_local import rerank_local\n        except Exception:\n            rerank_local = None\n\n        if rerank_local is None:\n            return\n\n        # Gather pairs across events so we can call rerank_local once.\n        all_pairs: List[tuple] = []\n        slices: List[tuple] = []  # (event_index, start, end)\n\n        for event_index, event in enumerate(events):\n            if event.get(\"teacher_scores\"):\n                continue\n\n            query = event.get(\"query\", \"\")\n            candidates = event.get(\"candidates\", [])\n            if not query or not candidates:\n                continue\n\n            start = len(all_pairs)\n            for c in candidates:\n                doc_text = self._pack_doc(c)  # Same packing as learning path\n                all_pairs.append((query, doc_text))\n            end = len(all_pairs)\n            if end > start:\n                slices.append((event_index, start, end))\n\n        if not slices:\n            return\n\n        try:\n            scores = rerank_local(all_pairs)\n        except Exception as e:\n            logger.warning(f\"[{self.collection}] Teacher scoring failed for {len(all_pairs)} pairs: {e}\")\n            return\n\n        # Map scores back to each event.\n        for event_index, start, end in slices:\n            try:\n                events[event_index][\"teacher_scores\"] = list(scores[start:end])\n            except Exception as e:\n                logger.warning(f\"[{self.collection}] Failed to map teacher scores for event {event_index}: {e}\")\n                continue",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_discover_collections_589": {
      "name": "discover_collections",
      "type": "function",
      "start_line": 589,
      "end_line": 613,
      "content_hash": "08f5bcf6892760dd56946e99601aa23f97d8718b",
      "content": "def discover_collections() -> List[str]:\n    \"\"\"Discover collections with pending events.\"\"\"\n    events_dir = Path(RERANK_EVENTS_DIR)\n    if not events_dir.exists():\n        return []\n\n    import re\n\n    collections: List[str] = []\n    seen = set()\n    for f in events_dir.glob(\"events_*.ndjson\"):\n        stem = f.stem  # events_<collection>_<YYYYMMDDHH>\n        if not stem.startswith(\"events_\"):\n            continue\n\n        rest = stem[len(\"events_\") :]\n        # Strip the hour suffix if present (10 digits)\n        m = re.match(r\"^(?P<name>.+)_(?P<hour>\\d{10})$\", rest)\n        name = m.group(\"name\") if m else rest\n\n        if name and name not in seen:\n            seen.add(name)\n            collections.append(name)\n\n    return collections",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_run_once_616": {
      "name": "run_once",
      "type": "function",
      "start_line": 616,
      "end_line": 629,
      "content_hash": "92082b855bbe1a0d238ad58a1eca7348626498dc",
      "content": "def run_once():\n    \"\"\"Process all pending events once and exit.\"\"\"\n    collections = discover_collections()\n    if not collections:\n        logger.info(\"No collections with pending events\")\n        return\n\n    total = 0\n    for coll in collections:\n        learner = CollectionLearner(coll)\n        processed = learner.process_events()\n        total += processed\n\n    logger.info(f\"Processed {total} events across {len(collections)} collections\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_run_daemon_632": {
      "name": "run_daemon",
      "type": "function",
      "start_line": 632,
      "end_line": 662,
      "content_hash": "eec6a0235be8caa7d38d59c3e6839ff2ba65cc2a",
      "content": "def run_daemon():\n    \"\"\"Run continuously, polling for new events.\"\"\"\n    logger.info(f\"Starting learning worker daemon (poll interval: {POLL_INTERVAL}s)\")\n    learners: Dict[str, CollectionLearner] = {}\n    last_cleanup = 0\n    cleanup_interval = 3600  # Cleanup old events hourly\n\n    while True:\n        try:\n            collections = discover_collections()\n            for coll in collections:\n                if coll not in learners:\n                    learners[coll] = CollectionLearner(coll)\n                learners[coll].process_events()\n\n            # Periodic cleanup of old event files\n            now = time.time()\n            if RERANK_EVENTS_RETENTION_DAYS > 0 and now - last_cleanup > cleanup_interval:\n                for coll in collections:\n                    deleted = cleanup_old_events(coll, RERANK_EVENTS_RETENTION_DAYS)\n                    if deleted > 0:\n                        logger.info(f\"[{coll}] Cleaned up {deleted} old event files\")\n                last_cleanup = now\n\n        except KeyboardInterrupt:\n            logger.info(\"Shutting down\")\n            break\n        except Exception as e:\n            logger.error(f\"Error in daemon loop: {e}\")\n\n        time.sleep(POLL_INTERVAL)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_main_665": {
      "name": "main",
      "type": "function",
      "start_line": 665,
      "end_line": 679,
      "content_hash": "ed281ffedac6e771c2f71bf3e7f0c5b8c134dd54",
      "content": "def main():\n    parser = argparse.ArgumentParser(description=\"Background learning reranker worker\")\n    parser.add_argument(\"--daemon\", action=\"store_true\", help=\"Run continuously\")\n    parser.add_argument(\"--once\", action=\"store_true\", help=\"Process once and exit\")\n    parser.add_argument(\"--collection\", type=str, help=\"Process specific collection only\")\n    args = parser.parse_args()\n\n    if args.collection:\n        learner = CollectionLearner(args.collection)\n        processed = learner.process_events()\n        logger.info(f\"Processed {processed} events for {args.collection}\")\n    elif args.daemon:\n        run_daemon()\n    else:\n        run_once()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}