{
  "file_path": "/work/context-engine/scripts/collection_admin.py",
  "file_hash": "e4a3148b30123a261c2ce793fa52df3960deedcd",
  "updated_at": "2025-12-26T17:34:23.293235",
  "symbols": {
    "function__resolve_work_root_34": {
      "name": "_resolve_work_root",
      "type": "function",
      "start_line": 34,
      "end_line": 40,
      "content_hash": "ea3966675913ab33e4c113652e6a38f67fae03e1",
      "content": "def _resolve_work_root(work_dir: Optional[str] = None) -> Path:\n    return Path(\n        work_dir\n        or os.environ.get(\"WORK_DIR\")\n        or os.environ.get(\"WORKDIR\")\n        or \"/work\"\n    ).resolve()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__resolve_codebase_root_43": {
      "name": "_resolve_codebase_root",
      "type": "function",
      "start_line": 43,
      "end_line": 67,
      "content_hash": "c03098dae69a182d1f84ec4001be2db639783e9b",
      "content": "def _resolve_codebase_root(work_root: Path) -> Path:\n    env_root = (\n        os.environ.get(\"CTXCE_CODEBASE_ROOT\")\n        or os.environ.get(\"CODEBASE_ROOT\")\n        or \"\"\n    ).strip()\n\n    candidates = []\n    if env_root:\n        candidates.append(Path(env_root))\n    candidates.append(work_root)\n    candidates.append(work_root.parent)\n\n    for candidate in candidates:\n        try:\n            base = candidate.resolve()\n        except Exception:\n            base = candidate\n        try:\n            if (base / \".codebase\" / \"repos\").exists():\n                return base\n        except Exception:\n            continue\n\n    return work_root",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__delete_path_tree_70": {
      "name": "_delete_path_tree",
      "type": "function",
      "start_line": 70,
      "end_line": 80,
      "content_hash": "b217abbf36aa50fdbaffd3811465d9c6daa0c12d",
      "content": "def _delete_path_tree(p: Path) -> bool:\n    try:\n        if not p.exists():\n            return False\n        if p.is_dir():\n            shutil.rmtree(p)\n            return True\n        p.unlink()\n        return True\n    except Exception:\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__read_state_collection_83": {
      "name": "_read_state_collection",
      "type": "function",
      "start_line": 83,
      "end_line": 91,
      "content_hash": "f9dcbf3d85d8f984ff613b1d9448c57a7654e2c3",
      "content": "def _read_state_collection(state_path: Path) -> str:\n    try:\n        with open(state_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        if isinstance(data, dict):\n            return str(data.get(\"qdrant_collection\") or \"\").strip()\n    except Exception:\n        pass\n    return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__managed_upload_marker_path_94": {
      "name": "_managed_upload_marker_path",
      "type": "function",
      "start_line": 94,
      "end_line": 102,
      "content_hash": "01e1cb4efabb108de06a6c5705ad7a530f83ad33",
      "content": "def _managed_upload_marker_path(\n    *,\n    work_root: Path,\n    slug_name: str,\n    marker_root: Optional[Path] = None,\n) -> Path:\n    # Marker is stored with per-repo metadata, not inside the repo workspace tree.\n    base = marker_root or work_root\n    return base / \".codebase\" / \"repos\" / slug_name / _MARKER_NAME",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__is_managed_upload_workspace_dir_105": {
      "name": "_is_managed_upload_workspace_dir",
      "type": "function",
      "start_line": 105,
      "end_line": 124,
      "content_hash": "dca2673517c6ee1d9e2ae656599f157b62533c43",
      "content": "def _is_managed_upload_workspace_dir(\n    p: Path,\n    *,\n    work_root: Path,\n    marker_root: Optional[Path] = None,\n) -> bool:\n    try:\n        if not p.is_dir():\n            return False\n        if p.parent.resolve() != work_root:\n            return False\n        if not _SLUGGED_REPO_RE.match(p.name or \"\"):\n            return False\n        return _managed_upload_marker_path(\n            work_root=work_root,\n            marker_root=marker_root,\n            slug_name=p.name,\n        ).exists()\n    except Exception:\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cleanup_state_files_for_mapping_127": {
      "name": "_cleanup_state_files_for_mapping",
      "type": "function",
      "start_line": 127,
      "end_line": 169,
      "content_hash": "304649dc488d9b945b4832b7841da6426d168df7",
      "content": "def _cleanup_state_files_for_mapping(\n    mapping: Dict[str, Any],\n    *,\n    work_root: Optional[Path] = None,\n    codebase_root: Optional[Path] = None,\n) -> int:\n    removed = 0\n    state_file = mapping.get(\"state_file\")\n    if isinstance(state_file, str) and state_file.strip():\n        p = Path(state_file)\n        state_dir = p.parent\n        try:\n            base = (\n                codebase_root\n                or work_root\n                or Path(os.environ.get(\"WORK_DIR\") or os.environ.get(\"WORKDIR\") or \"/work\")\n            ).resolve()\n        except Exception:\n            base = codebase_root or work_root or Path(\n                os.environ.get(\"WORK_DIR\") or os.environ.get(\"WORKDIR\") or \"/work\"\n            )\n\n        # Multi-repo mode stores per-repo metadata under /work/.codebase/repos/<repo>/\n        try:\n            repos_root = (base / \".codebase\" / \"repos\").resolve()\n            sd = state_dir.resolve()\n            if sd != repos_root and str(sd).startswith(str(repos_root) + os.sep):\n                if _delete_path_tree(sd):\n                    removed += 1\n                return removed\n        except Exception:\n            pass\n\n        # Single-repo mode stores metadata under <workspace>/.codebase/\n        if _delete_path_tree(p):\n            removed += 1\n        _delete_path_tree(state_dir / \"cache.json\")\n        try:\n            for sym in state_dir.glob(\"symbols_*.json\"):\n                _delete_path_tree(sym)\n        except Exception:\n            pass\n    return removed",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_delete_collection_everywhere_172": {
      "name": "delete_collection_everywhere",
      "type": "function",
      "start_line": 172,
      "end_line": 283,
      "content_hash": "6e30c5cd06eaa4ca7f1770b2f16f27934d344e5e",
      "content": "def delete_collection_everywhere(\n    *,\n    collection: str,\n    work_dir: Optional[str] = None,\n    qdrant_url: Optional[str] = None,\n    cleanup_fs: bool = True,\n) -> Dict[str, Any]:\n    enabled = (\n        str(os.environ.get(\"CTXCE_ADMIN_COLLECTION_DELETE_ENABLED\", \"0\")).strip().lower()\n        in {\"1\", \"true\", \"yes\", \"on\"}\n    )\n    if not enabled:\n        raise PermissionError(\"Collection deletion is disabled by server configuration\")\n\n    name = (collection or \"\").strip()\n    if not name:\n        raise ValueError(\"collection is required\")\n\n    work_root = _resolve_work_root(work_dir)\n    codebase_root = _resolve_codebase_root(work_root)\n\n    out: Dict[str, Any] = {\n        \"collection\": name,\n        \"qdrant_deleted\": False,\n        \"registry_marked_deleted\": False,\n        \"deleted_state_files\": 0,\n        \"deleted_managed_workspaces\": 0,\n    }\n\n    target_is_old = name.endswith(\"_old\")\n\n    # 1) Delete Qdrant collection\n    try:\n        if pooled_qdrant_client is not None:\n            with pooled_qdrant_client(url=qdrant_url, api_key=os.environ.get(\"QDRANT_API_KEY\")) as cli:\n                try:\n                    cli.delete_collection(collection_name=name)\n                    out[\"qdrant_deleted\"] = True\n                except Exception:\n                    out[\"qdrant_deleted\"] = False\n    except Exception:\n        out[\"qdrant_deleted\"] = False\n\n    # 2) Mark deleted in registry DB\n    try:\n        mark_collection_deleted(name)\n        out[\"registry_marked_deleted\"] = True\n    except Exception:\n        out[\"registry_marked_deleted\"] = False\n\n    # 3) Cleanup workspace state metadata + managed upload workspaces\n    if not cleanup_fs:\n        return out\n\n    mappings = []\n    try:\n        if get_collection_mappings is not None:\n            mappings = get_collection_mappings(search_root=str(codebase_root)) or []\n    except Exception:\n        mappings = []\n\n    # NOTE: logically linked worktrees still share the\n    # primary repo's qdrant_collection in their state.json files. When the UI targets the lineage\n    # collection directly, no mapping below matches, so filesystem cleanup is a no-op and we keep\n    # both the metadata (`.codebase/repos/<slug>`) and the workspace on disk. This conservative\n    # behavior is intentional until we have branch-aware deletion semantics\u2014we do not want to\n    # cascade-delete shared worktrees that may host future branch/version state.\n    for m in mappings:\n        try:\n            if str(m.get(\"collection_name\") or \"\").strip() != name:\n                continue\n\n            # Safety: when targeting a staging clone collection (\"*_old\"), never delete\n            # a non-\"*_old\" workspace on disk even if its state temporarily points at the clone.\n            if target_is_old:\n                repo = str(m.get(\"repo_name\") or \"\").strip()\n                if not repo.endswith(\"_old\"):\n                    continue\n\n            container_path = m.get(\"container_path\")\n            if isinstance(container_path, str) and container_path.strip():\n                p = Path(container_path)\n                try:\n                    p = p.resolve()\n                except Exception:\n                    pass\n\n                if target_is_old:\n                    # For staging clone workspaces, delete the workspace dir directly when it\n                    # is under the expected work_root and ends with \"_old\".\n                    try:\n                        if p.parent.resolve() == work_root and (p.name or \"\").endswith(\"_old\"):\n                            if _delete_path_tree(p):\n                                out[\"deleted_managed_workspaces\"] += 1\n                    except Exception:\n                        pass\n                else:\n                    if _is_managed_upload_workspace_dir(p, work_root=work_root, marker_root=codebase_root):\n                        if _delete_path_tree(p):\n                            out[\"deleted_managed_workspaces\"] += 1\n\n            # Cleanup state metadata after workspace deletion so the marker still exists\n            # when authorizing the filesystem delete.\n            out[\"deleted_state_files\"] += _cleanup_state_files_for_mapping(\n                m,\n                work_root=work_root,\n                codebase_root=codebase_root,\n            )\n        except Exception:\n            continue\n\n    return out",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__normalize_qdrant_url_286": {
      "name": "_normalize_qdrant_url",
      "type": "function",
      "start_line": 286,
      "end_line": 290,
      "content_hash": "104752920c0716c3ba7a37cb0a84a0a6b641f931",
      "content": "def _normalize_qdrant_url(qdrant_url: Optional[str]) -> str:\n    url = (qdrant_url or os.environ.get(\"QDRANT_URL\") or \"http://qdrant:6333\").strip()\n    if not url:\n        url = \"http://qdrant:6333\"\n    return url.rstrip(\"/\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_copy_collection_qdrant_293": {
      "name": "copy_collection_qdrant",
      "type": "function",
      "start_line": 293,
      "end_line": 480,
      "content_hash": "73c2cbc80c7756ddff1e9a05a975f9ae459975ae",
      "content": "def copy_collection_qdrant(\n    *,\n    source: str,\n    target: Optional[str] = None,\n    qdrant_url: Optional[str] = None,\n    overwrite: bool = False,\n) -> str:\n    \"\"\"Copy a Qdrant collection using the pooled client.\n\n    Returns the target collection name.\n    \"\"\"\n    src = (source or \"\").strip()\n    if not src:\n        raise ValueError(\"source collection is required\")\n\n    dest = (target or f\"{src}__copy__{datetime.utcnow().strftime('%Y%m%d%H%M%S')}\").strip()\n    if not dest:\n        raise ValueError(\"target collection is required\")\n\n    base_url = _normalize_qdrant_url(qdrant_url)\n    api_key = os.environ.get(\"QDRANT_API_KEY\") or \"\"\n    headers = {\"api-key\": api_key} if api_key else {}\n\n    def _copy_client_timeout_seconds() -> Optional[float]:\n        try:\n            raw = (\n                os.environ.get(\"CTXCE_COPY_COLLECTION_TIMEOUT\")\n                or os.environ.get(\"QDRANT_TIMEOUT\")\n                or \"\"\n            )\n            raw = str(raw).strip()\n            if not raw:\n                # Default: no timeout. Staging clone is background and may take a long time.\n                return None\n            if raw.lower() in {\"0\", \"none\", \"null\", \"false\", \"off\", \"disabled\"}:\n                return None\n            return float(raw)\n        except Exception:\n            return None\n\n    copied = False\n\n    def _manual_copy_points() -> None:\n        if QdrantClient is None or qmodels is None:\n            raise RuntimeError(\"QdrantClient unavailable for manual collection copy\")\n        cli = QdrantClient(url=base_url, api_key=api_key or None, timeout=_copy_client_timeout_seconds())\n        try:\n            if overwrite:\n                try:\n                    cli.delete_collection(collection_name=dest)\n                except Exception:\n                    pass\n\n            try:\n                src_info = cli.get_collection(collection_name=src)\n            except Exception as exc:\n                raise RuntimeError(f\"Failed to fetch source collection config for {src}: {exc}\") from exc\n\n            vectors_config = None\n            sparse_vectors_config = None\n            try:\n                params = getattr(getattr(src_info, \"config\", None), \"params\", None)\n                if params is not None:\n                    vectors_config = getattr(params, \"vectors\", None)\n                    sparse_vectors_config = getattr(params, \"sparse_vectors\", None)\n            except Exception:\n                vectors_config = None\n                sparse_vectors_config = None\n\n            if vectors_config is None:\n                raise RuntimeError(f\"Cannot determine vectors config for source collection {src}\")\n\n            try:\n                cli.create_collection(\n                    collection_name=dest,\n                    vectors_config=vectors_config,\n                    sparse_vectors_config=sparse_vectors_config,\n                )\n            except Exception as exc:\n                # Allow clone to proceed if collection already exists.\n                if \"already exists\" not in str(exc).lower():\n                    raise RuntimeError(\n                        f\"Failed to create destination collection {dest}: {exc}\"\n                    ) from exc\n\n            # Allow transient network hiccups when verifying the destination collection.\n            verify_attempts = max(1, int(os.environ.get(\"CTXCE_COPY_VERIFY_RETRIES\", \"3\") or \"3\"))\n            verify_delay = float(os.environ.get(\"CTXCE_COPY_VERIFY_DELAY\", \"2\") or \"2\")\n            last_err: Optional[Exception] = None\n            for _ in range(verify_attempts):\n                try:\n                    cli.get_collection(collection_name=dest)\n                    last_err = None\n                    break\n                except Exception as exc:\n                    last_err = exc\n                    time.sleep(max(0.1, verify_delay))\n            if last_err is not None:\n                raise RuntimeError(\n                    f\"Destination collection {dest} unavailable after creation: {last_err}\"\n                ) from last_err\n\n            offset = None\n            batch_limit = int(os.environ.get(\"CTXCE_COPY_COLLECTION_BATCH\", \"512\") or \"512\")\n            while True:\n                try:\n                    points, next_offset = cli.scroll(\n                        collection_name=src,\n                        limit=batch_limit,\n                        offset=offset,\n                        with_payload=True,\n                        with_vectors=True,\n                    )\n                except Exception as exc:\n                    raise RuntimeError(f\"Failed to scroll points from {src}: {exc}\") from exc\n\n                if points:\n                    structured: List[qmodels.PointStruct] = []\n                    for record in points:\n                        if record is None:\n                            continue\n                        point_id = getattr(record, \"id\", None)\n                        payload = getattr(record, \"payload\", None)\n                        vector = None\n                        if hasattr(record, \"vector\") and getattr(record, \"vector\") is not None:\n                            vector = getattr(record, \"vector\")\n                        elif hasattr(record, \"vectors\") and getattr(record, \"vectors\") is not None:\n                            vector = getattr(record, \"vectors\")\n                        structured.append(\n                            qmodels.PointStruct(id=point_id, vector=vector, payload=payload)\n                        )\n                    if structured:\n                        try:\n                            cli.upsert(collection_name=dest, points=structured)\n                        except Exception as exc:\n                            raise RuntimeError(f\"Failed to upsert points into {dest}: {exc}\") from exc\n\n                if next_offset is None:\n                    break\n                offset = next_offset\n        finally:\n            try:\n                cli.close()\n            except Exception:\n                pass\n\n    def _count_points(name: str) -> Optional[int]:\n        if QdrantClient is None:\n            return None\n        cli = QdrantClient(url=base_url, api_key=api_key or None, timeout=_copy_client_timeout_seconds())\n        try:\n            res = cli.count(collection_name=name, exact=True)\n            return int(getattr(res, \"count\", 0))\n        except Exception:\n            return None\n        finally:\n            try:\n                cli.close()\n            except Exception:\n                pass\n\n    source_count = _count_points(src)\n\n    if pooled_qdrant_client is not None:\n        with pooled_qdrant_client(url=base_url, api_key=api_key or None) as cli:\n            if overwrite:\n                try:\n                    cli.delete_collection(collection_name=dest)\n                except Exception:\n                    # Fall back to HTTP delete below if needed\n                    pass\n            try:\n                copy_method = getattr(cli, \"copy_collection\", None)\n                if callable(copy_method):\n                    copy_method(collection_name=src, new_collection_name=dest)\n                    copied = True\n            except AttributeError:\n                copied = False\n            except Exception as exc:\n                raise RuntimeError(f\"Failed to copy collection {src} -> {dest}: {exc}\") from exc\n\n    if not copied:\n        # Always run the manual scroll+upsert copy. Many Qdrant deployments (including ours)\n        # either lack /clone entirely or return success while creating an empty collection.\n        # The manual path guarantees the destination gets the exact same points/payloads/vectors.\n        _manual_copy_points()\n\n    return dest",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__copy_client_timeout_seconds_316": {
      "name": "_copy_client_timeout_seconds",
      "type": "function",
      "start_line": 316,
      "end_line": 331,
      "content_hash": "cb9de0705434271e5b4717ad19a386942858e513",
      "content": "    def _copy_client_timeout_seconds() -> Optional[float]:\n        try:\n            raw = (\n                os.environ.get(\"CTXCE_COPY_COLLECTION_TIMEOUT\")\n                or os.environ.get(\"QDRANT_TIMEOUT\")\n                or \"\"\n            )\n            raw = str(raw).strip()\n            if not raw:\n                # Default: no timeout. Staging clone is background and may take a long time.\n                return None\n            if raw.lower() in {\"0\", \"none\", \"null\", \"false\", \"off\", \"disabled\"}:\n                return None\n            return float(raw)\n        except Exception:\n            return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__manual_copy_points_335": {
      "name": "_manual_copy_points",
      "type": "function",
      "start_line": 335,
      "end_line": 437,
      "content_hash": "c17d120e9f097c1bbcb63a835b252be6f20076a1",
      "content": "    def _manual_copy_points() -> None:\n        if QdrantClient is None or qmodels is None:\n            raise RuntimeError(\"QdrantClient unavailable for manual collection copy\")\n        cli = QdrantClient(url=base_url, api_key=api_key or None, timeout=_copy_client_timeout_seconds())\n        try:\n            if overwrite:\n                try:\n                    cli.delete_collection(collection_name=dest)\n                except Exception:\n                    pass\n\n            try:\n                src_info = cli.get_collection(collection_name=src)\n            except Exception as exc:\n                raise RuntimeError(f\"Failed to fetch source collection config for {src}: {exc}\") from exc\n\n            vectors_config = None\n            sparse_vectors_config = None\n            try:\n                params = getattr(getattr(src_info, \"config\", None), \"params\", None)\n                if params is not None:\n                    vectors_config = getattr(params, \"vectors\", None)\n                    sparse_vectors_config = getattr(params, \"sparse_vectors\", None)\n            except Exception:\n                vectors_config = None\n                sparse_vectors_config = None\n\n            if vectors_config is None:\n                raise RuntimeError(f\"Cannot determine vectors config for source collection {src}\")\n\n            try:\n                cli.create_collection(\n                    collection_name=dest,\n                    vectors_config=vectors_config,\n                    sparse_vectors_config=sparse_vectors_config,\n                )\n            except Exception as exc:\n                # Allow clone to proceed if collection already exists.\n                if \"already exists\" not in str(exc).lower():\n                    raise RuntimeError(\n                        f\"Failed to create destination collection {dest}: {exc}\"\n                    ) from exc\n\n            # Allow transient network hiccups when verifying the destination collection.\n            verify_attempts = max(1, int(os.environ.get(\"CTXCE_COPY_VERIFY_RETRIES\", \"3\") or \"3\"))\n            verify_delay = float(os.environ.get(\"CTXCE_COPY_VERIFY_DELAY\", \"2\") or \"2\")\n            last_err: Optional[Exception] = None\n            for _ in range(verify_attempts):\n                try:\n                    cli.get_collection(collection_name=dest)\n                    last_err = None\n                    break\n                except Exception as exc:\n                    last_err = exc\n                    time.sleep(max(0.1, verify_delay))\n            if last_err is not None:\n                raise RuntimeError(\n                    f\"Destination collection {dest} unavailable after creation: {last_err}\"\n                ) from last_err\n\n            offset = None\n            batch_limit = int(os.environ.get(\"CTXCE_COPY_COLLECTION_BATCH\", \"512\") or \"512\")\n            while True:\n                try:\n                    points, next_offset = cli.scroll(\n                        collection_name=src,\n                        limit=batch_limit,\n                        offset=offset,\n                        with_payload=True,\n                        with_vectors=True,\n                    )\n                except Exception as exc:\n                    raise RuntimeError(f\"Failed to scroll points from {src}: {exc}\") from exc\n\n                if points:\n                    structured: List[qmodels.PointStruct] = []\n                    for record in points:\n                        if record is None:\n                            continue\n                        point_id = getattr(record, \"id\", None)\n                        payload = getattr(record, \"payload\", None)\n                        vector = None\n                        if hasattr(record, \"vector\") and getattr(record, \"vector\") is not None:\n                            vector = getattr(record, \"vector\")\n                        elif hasattr(record, \"vectors\") and getattr(record, \"vectors\") is not None:\n                            vector = getattr(record, \"vectors\")\n                        structured.append(\n                            qmodels.PointStruct(id=point_id, vector=vector, payload=payload)\n                        )\n                    if structured:\n                        try:\n                            cli.upsert(collection_name=dest, points=structured)\n                        except Exception as exc:\n                            raise RuntimeError(f\"Failed to upsert points into {dest}: {exc}\") from exc\n\n                if next_offset is None:\n                    break\n                offset = next_offset\n        finally:\n            try:\n                cli.close()\n            except Exception:\n                pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__count_points_439": {
      "name": "_count_points",
      "type": "function",
      "start_line": 439,
      "end_line": 452,
      "content_hash": "4196c3078e2aebbad3bdb5405dfc5927b9396bf6",
      "content": "    def _count_points(name: str) -> Optional[int]:\n        if QdrantClient is None:\n            return None\n        cli = QdrantClient(url=base_url, api_key=api_key or None, timeout=_copy_client_timeout_seconds())\n        try:\n            res = cli.count(collection_name=name, exact=True)\n            return int(getattr(res, \"count\", 0))\n        except Exception:\n            return None\n        finally:\n            try:\n                cli.close()\n            except Exception:\n                pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}