{
  "file_path": "/work/external-deps/Context-Engine/scripts/collection_health.py",
  "file_hash": "ab4ead261c4395be563811ab6c1a039bb9429494",
  "updated_at": "2025-12-26T17:34:21.576555",
  "symbols": {
    "function_get_cached_files_count_31": {
      "name": "get_cached_files_count",
      "type": "function",
      "start_line": 31,
      "end_line": 39,
      "content_hash": "dc95ec8723ae6875ff706229d75816ba25b6cafe",
      "content": "def get_cached_files_count(workspace_path: str) -> int:\n    \"\"\"Return the number of files tracked in the local cache.\"\"\"\n    try:\n        cache = _read_cache(workspace_path)\n        file_hashes = cache.get(\"file_hashes\", {})\n        return len(file_hashes)\n    except Exception as e:\n        logger.warning(f\"Failed to read cache: {e}\")\n        return 0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_collection_points_count_42": {
      "name": "get_collection_points_count",
      "type": "function",
      "start_line": 42,
      "end_line": 60,
      "content_hash": "4019e2b620473bfbdf6541a868903af26446be2f",
      "content": "def get_collection_points_count(collection_name: str, qdrant_url: Optional[str] = None) -> int:\n    \"\"\"Return the number of points in the Qdrant collection.\"\"\"\n    try:\n        from qdrant_client import QdrantClient\n        \n        url = qdrant_url or os.environ.get(\"QDRANT_URL\", \"http://localhost:6333\")\n        api_key = os.environ.get(\"QDRANT_API_KEY\")\n        \n        client = QdrantClient(\n            url=url,\n            api_key=api_key or None,\n            timeout=int(os.environ.get(\"QDRANT_TIMEOUT\", \"20\") or 20),\n        )\n        \n        result = client.count(collection_name=collection_name, exact=True)\n        return int(getattr(result, \"count\", 0))\n    except Exception as e:\n        logger.warning(f\"Failed to get collection count: {e}\")\n        return -1",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_unique_files_in_collection_63": {
      "name": "get_unique_files_in_collection",
      "type": "function",
      "start_line": 63,
      "end_line": 110,
      "content_hash": "836aaed8f12a649e5b4441970b47aaed26358e8b",
      "content": "def get_unique_files_in_collection(collection_name: str, qdrant_url: Optional[str] = None) -> int:\n    \"\"\"Return the number of unique files (distinct paths) in the collection.\"\"\"\n    try:\n        from qdrant_client import QdrantClient\n        from qdrant_client import models\n        \n        url = qdrant_url or os.environ.get(\"QDRANT_URL\", \"http://localhost:6333\")\n        api_key = os.environ.get(\"QDRANT_API_KEY\")\n        \n        client = QdrantClient(\n            url=url,\n            api_key=api_key or None,\n            timeout=int(os.environ.get(\"QDRANT_TIMEOUT\", \"20\") or 20),\n        )\n        \n        # Scroll through all points and collect unique paths\n        unique_paths = set()\n        offset = None\n        batch_size = 100\n        \n        while True:\n            points, offset = client.scroll(\n                collection_name=collection_name,\n                limit=batch_size,\n                offset=offset,\n                with_payload=True,\n            )\n            \n            if not points:\n                break\n                \n            for point in points:\n                try:\n                    payload = point.payload or {}\n                    metadata = payload.get(\"metadata\", {})\n                    path = metadata.get(\"path\")\n                    if path:\n                        unique_paths.add(str(path))\n                except Exception:\n                    continue\n            \n            if offset is None:\n                break\n        \n        return len(unique_paths)\n    except Exception as e:\n        logger.warning(f\"Failed to count unique files: {e}\")\n        return -1",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_clear_cache_113": {
      "name": "clear_cache",
      "type": "function",
      "start_line": 113,
      "end_line": 122,
      "content_hash": "d22cbef4459ca0e8e0897dd3d3cdca03d02b14c3",
      "content": "def clear_cache(workspace_path: str) -> bool:\n    \"\"\"Clear the local file hash cache.\"\"\"\n    try:\n        cache = {\"file_hashes\": {}, \"updated_at\": \"\"}\n        _write_cache(workspace_path, cache)\n        logger.info(f\"Cleared cache for workspace: {workspace_path}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Failed to clear cache: {e}\")\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_clear_repo_cache_125": {
      "name": "clear_repo_cache",
      "type": "function",
      "start_line": 125,
      "end_line": 144,
      "content_hash": "4705490763bc9d4d7dfabe58c42fd84f686916f4",
      "content": "def clear_repo_cache(repo_name: str) -> bool:\n    \"\"\"Clear the local file hash cache for a specific repo (multi-repo mode).\"\"\"\n    try:\n        from scripts.workspace_state import _get_repo_state_dir, CACHE_FILENAME\n        from datetime import datetime\n\n        state_dir = _get_repo_state_dir(repo_name)\n        cache_path = state_dir / CACHE_FILENAME\n\n        if cache_path.exists():\n            cache = {\"file_hashes\": {}, \"updated_at\": datetime.now().isoformat()}\n            with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n                import json\n                json.dump(cache, f, indent=2)\n            logger.info(f\"Cleared cache for repo: {repo_name}\")\n            return True\n        return False\n    except Exception as e:\n        logger.error(f\"Failed to clear repo cache for {repo_name}: {e}\")\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_clear_indexing_caches_147": {
      "name": "clear_indexing_caches",
      "type": "function",
      "start_line": 147,
      "end_line": 177,
      "content_hash": "1b84184fa93106b87d4f83e9c4f57805489713ba",
      "content": "def clear_indexing_caches(workspace_path: str, repo_name: Optional[str] = None) -> Dict[str, Any]:\n    \"\"\"Clear file-hash and symbol caches for the workspace/repo.\"\"\"\n    result: Dict[str, Any] = {\n        \"file_hash_cache\": False,\n        \"symbol_cache_dirs\": 0,\n    }\n\n    multi_repo = False\n    try:\n        multi_repo = is_multi_repo_mode()\n    except Exception:\n        multi_repo = False\n\n    try:\n        if multi_repo and repo_name:\n            result[\"file_hash_cache\"] = clear_repo_cache(repo_name)\n        else:\n            result[\"file_hash_cache\"] = clear_cache(workspace_path)\n    except Exception as e:\n        result[\"file_hash_error\"] = str(e)\n\n    if clear_symbol_cache:\n        try:\n            cleared_dirs = clear_symbol_cache(workspace_path, repo_name)\n            result[\"symbol_cache_dirs\"] = cleared_dirs\n        except Exception as e:\n            result[\"symbol_cache_error\"] = str(e)\n    else:\n        result[\"symbol_cache_error\"] = \"clear_symbol_cache unavailable\"\n\n    return result",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_repo_cached_files_count_180": {
      "name": "get_repo_cached_files_count",
      "type": "function",
      "start_line": 180,
      "end_line": 197,
      "content_hash": "4325ec8126a57c98d237d2cf26f7bd373a824717",
      "content": "def get_repo_cached_files_count(repo_name: str) -> int:\n    \"\"\"Return the number of files tracked in a repo's cache (multi-repo mode).\"\"\"\n    try:\n        from scripts.workspace_state import _get_repo_state_dir, CACHE_FILENAME\n        import json\n\n        state_dir = _get_repo_state_dir(repo_name)\n        cache_path = state_dir / CACHE_FILENAME\n\n        if not cache_path.exists():\n            return 0\n\n        with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n            cache = json.load(f)\n        return len(cache.get(\"file_hashes\", {}))\n    except Exception as e:\n        logger.warning(f\"Failed to read repo cache for {repo_name}: {e}\")\n        return 0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_auto_heal_multi_repo_200": {
      "name": "auto_heal_multi_repo",
      "type": "function",
      "start_line": 200,
      "end_line": 269,
      "content_hash": "0f1968482ea7bb4a24fa46a549366908d08d12de",
      "content": "def auto_heal_multi_repo(\n    workspace_path: str,\n    qdrant_url: Optional[str] = None,\n    dry_run: bool = False,\n) -> Dict[str, Any]:\n    \"\"\"\n    Health check for multi-repo mode. Checks each repo's collection.\n\n    Only clears cache when collection is EMPTY but cache has entries.\n    This is the safest possible check - avoids clearing cache unnecessarily.\n\n    Returns dict with per-repo results.\n    \"\"\"\n    from scripts.workspace_state import is_multi_repo_mode, get_collection_name\n\n    results = {\"repos_checked\": 0, \"repos_healed\": 0, \"details\": {}}\n\n    if not is_multi_repo_mode():\n        results[\"skipped\"] = \"not in multi-repo mode\"\n        return results\n\n    # Find all repos under workspace\n    ws = Path(workspace_path)\n    repos = []\n    for entry in ws.iterdir():\n        if entry.is_dir() and not entry.name.startswith(\".\"):\n            # Check if it looks like a repo (has .git or files)\n            if (entry / \".git\").exists() or any(entry.iterdir()):\n                repos.append(entry.name)\n\n    for repo_name in repos:\n        try:\n            # Get collection name for this repo\n            repo_path = ws / repo_name\n            collection = get_collection_name(str(repo_path))\n            if not collection:\n                continue\n\n            cached_count = get_repo_cached_files_count(repo_name)\n            if cached_count == 0:\n                # No cache entries, nothing to validate\n                continue\n\n            points_count = get_collection_points_count(collection, qdrant_url)\n            results[\"repos_checked\"] += 1\n\n            repo_result = {\n                \"collection\": collection,\n                \"cached_files\": cached_count,\n                \"collection_points\": points_count,\n                \"action\": \"none\",\n            }\n\n            # ONLY clear if collection is completely empty\n            # This is ultra-conservative - won't clear for partial issues\n            if points_count == 0 and cached_count > 0:\n                repo_result[\"issue\"] = f\"Collection empty but cache has {cached_count} files\"\n                if not dry_run:\n                    if clear_repo_cache(repo_name):\n                        repo_result[\"action\"] = \"cleared_cache\"\n                        results[\"repos_healed\"] += 1\n                else:\n                    repo_result[\"action\"] = \"would_clear_cache\"\n\n            results[\"details\"][repo_name] = repo_result\n\n        except Exception as e:\n            results[\"details\"][repo_name] = {\"error\": str(e)}\n\n    return results",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_detect_collection_health_272": {
      "name": "detect_collection_health",
      "type": "function",
      "start_line": 272,
      "end_line": 328,
      "content_hash": "9e4a49d9bcd319e631f13d6c8a87a401c827f104",
      "content": "def detect_collection_health(\n    workspace_path: str,\n    collection_name: str,\n    qdrant_url: Optional[str] = None,\n    threshold: float = 0.1,\n) -> Dict[str, Any]:\n    \"\"\"\n    Detect cache/collection sync issues.\n    \n    Returns a dict with:\n    - healthy: bool\n    - cached_files: int\n    - collection_points: int\n    - unique_files_in_collection: int\n    - issue: Optional[str] - description of the problem\n    - recommendation: Optional[str] - suggested fix\n    \"\"\"\n    cached_count = get_cached_files_count(workspace_path)\n    points_count = get_collection_points_count(collection_name, qdrant_url)\n    unique_files = get_unique_files_in_collection(collection_name, qdrant_url)\n    \n    result = {\n        \"healthy\": True,\n        \"cached_files\": cached_count,\n        \"collection_points\": points_count,\n        \"unique_files_in_collection\": unique_files,\n        \"issue\": None,\n        \"recommendation\": None,\n    }\n    \n    # Check 1: Collection is empty but cache has entries\n    if points_count == 0 and cached_count > 0:\n        result[\"healthy\"] = False\n        result[\"issue\"] = f\"Collection is empty but cache has {cached_count} files\"\n        result[\"recommendation\"] = \"Clear cache and force reindex\"\n        return result\n    \n    # Check 2: Unique files in collection is way less than cached files\n    if unique_files >= 0 and cached_count > 0:\n        ratio = unique_files / cached_count if cached_count > 0 else 0\n        if ratio < threshold:\n            result[\"healthy\"] = False\n            result[\"issue\"] = (\n                f\"Cache has {cached_count} files but collection only has {unique_files} \"\n                f\"unique files ({ratio:.1%} < {threshold:.0%} threshold)\"\n            )\n            result[\"recommendation\"] = \"Clear cache and force reindex\"\n            return result\n    \n    # Check 3: Collection has points but no unique files detected (metadata issue)\n    if points_count > 0 and unique_files == 0:\n        result[\"healthy\"] = False\n        result[\"issue\"] = f\"Collection has {points_count} points but no valid file paths in metadata\"\n        result[\"recommendation\"] = \"Recreate collection with proper metadata\"\n        return result\n    \n    return result",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_auto_heal_if_needed_331": {
      "name": "auto_heal_if_needed",
      "type": "function",
      "start_line": 331,
      "end_line": 368,
      "content_hash": "ce1da91e4cdea95853fe86683c07bf214346c3b7",
      "content": "def auto_heal_if_needed(\n    workspace_path: str,\n    collection_name: str,\n    qdrant_url: Optional[str] = None,\n    dry_run: bool = False,\n) -> Dict[str, Any]:\n    \"\"\"\n    Detect and automatically fix cache/collection sync issues.\n    \n    Returns a dict with:\n    - action_taken: str\n    - health_check: Dict (from detect_collection_health)\n    \"\"\"\n    health = detect_collection_health(workspace_path, collection_name, qdrant_url)\n    \n    result = {\n        \"action_taken\": \"none\",\n        \"health_check\": health,\n    }\n    \n    if not health[\"healthy\"]:\n        logger.warning(f\"Collection health issue detected: {health['issue']}\")\n        logger.info(f\"Recommendation: {health['recommendation']}\")\n        \n        if not dry_run:\n            if \"Clear cache\" in health[\"recommendation\"]:\n                if clear_cache(workspace_path):\n                    result[\"action_taken\"] = \"cleared_cache\"\n                    logger.info(\"Cache cleared. Reindex required.\")\n                else:\n                    result[\"action_taken\"] = \"clear_cache_failed\"\n        else:\n            result[\"action_taken\"] = \"dry_run\"\n            logger.info(\"Dry run mode - no action taken\")\n    else:\n        logger.info(\"Collection health check passed\")\n    \n    return result",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_main_371": {
      "name": "main",
      "type": "function",
      "start_line": 371,
      "end_line": 430,
      "content_hash": "cbc9febb554a056d23d79c9b69de9c8e199f614e",
      "content": "def main():\n    \"\"\"CLI for health checking and healing.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Check and heal collection health\")\n    parser.add_argument(\n        \"--workspace\",\n        default=os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"/work\",\n        help=\"Workspace path (default: WATCH_ROOT or /work)\",\n    )\n    parser.add_argument(\n        \"--collection\",\n        default=os.environ.get(\"COLLECTION_NAME\", \"codebase\"),\n        help=\"Collection name (default: COLLECTION_NAME env or codebase)\",\n    )\n    parser.add_argument(\n        \"--qdrant-url\",\n        default=os.environ.get(\"QDRANT_URL\", \"http://localhost:6333\"),\n        help=\"Qdrant URL (default: QDRANT_URL env or http://localhost:6333)\",\n    )\n    parser.add_argument(\n        \"--auto-heal\",\n        action=\"store_true\",\n        help=\"Automatically fix issues (clear cache if needed)\",\n    )\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=\"Check health but don't take action\",\n    )\n    \n    args = parser.parse_args()\n    \n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n    )\n    \n    if args.auto_heal:\n        result = auto_heal_if_needed(\n            args.workspace,\n            args.collection,\n            args.qdrant_url,\n            dry_run=args.dry_run,\n        )\n        print(f\"\\nAction taken: {result['action_taken']}\")\n    else:\n        health = detect_collection_health(\n            args.workspace,\n            args.collection,\n            args.qdrant_url,\n        )\n        print(f\"\\nHealth check results:\")\n        print(f\"  Healthy: {health['healthy']}\")\n        print(f\"  Cached files: {health['cached_files']}\")\n        print(f\"  Collection points: {health['collection_points']}\")\n        print(f\"  Unique files in collection: {health['unique_files_in_collection']}\")\n        if not health['healthy']:\n            print(f\"  Issue: {health['issue']}\")\n            print(f\"  Recommendation: {health['recommendation']}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}