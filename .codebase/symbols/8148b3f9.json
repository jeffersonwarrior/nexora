{
  "file_path": "/work/context-engine/scripts/remote_upload_client.py",
  "file_hash": "02bc38d2585b3e8f853bc4d927e0e23e6492354e",
  "updated_at": "2025-12-26T17:34:23.538877",
  "symbols": {
    "function__cache_missing_stats_51": {
      "name": "_cache_missing_stats",
      "type": "function",
      "start_line": 51,
      "end_line": 67,
      "content_hash": "025eac01545c497fdf80ec51cd92f7800ae5dff8",
      "content": "def _cache_missing_stats(file_hashes: Dict[str, Any]) -> Tuple[bool, int, int]:\n    \"\"\"Return (is_stale, missing_count, checked_count) for cached paths.\"\"\"\n    if not file_hashes:\n        return (False, 0, 0)\n    missing = 0\n    checked = 0\n    for path_str in file_hashes.keys():\n        try:\n            if not Path(path_str).exists():\n                missing += 1\n        except Exception:\n            missing += 1\n        checked += 1\n    if checked == 0:\n        return (False, 0, 0)\n    missing_ratio = missing / checked\n    return (missing_ratio >= 0.25, missing, checked)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__find_git_root_70": {
      "name": "_find_git_root",
      "type": "function",
      "start_line": 70,
      "end_line": 89,
      "content_hash": "66cfd8f7748f85044bc03a22a7b4b1270b47eb0e",
      "content": "def _find_git_root(start: Path) -> Optional[Path]:\n    \"\"\"Best-effort detection of the git repository root for a workspace.\n\n    Walks up from the given path looking for a .git directory. Returns None if\n    no repo is found or git metadata is unavailable.\n    \"\"\"\n    try:\n        cur = start.resolve()\n    except Exception:\n        cur = start\n    try:\n        for p in [cur] + list(cur.parents):\n            try:\n                if (p / \".git\").exists():\n                    return p\n            except Exception:\n                continue\n    except Exception:\n        return None\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__compute_logical_repo_id_92": {
      "name": "_compute_logical_repo_id",
      "type": "function",
      "start_line": 92,
      "end_line": 119,
      "content_hash": "f2d72f56edc79aebaf24b62298dc6c75ca63b0f1",
      "content": "def _compute_logical_repo_id(workspace_path: str) -> str:\n    try:\n        p = Path(workspace_path).resolve()\n    except Exception:\n        p = Path(workspace_path)\n\n    try:\n        r = subprocess.run(\n            [\"git\", \"-C\", str(p), \"rev-parse\", \"--git-common-dir\"],\n            capture_output=True,\n            text=True,\n        )\n        raw = (r.stdout or \"\").strip()\n        if r.returncode == 0 and raw:\n            common = Path(raw)\n            if not common.is_absolute():\n                base = p if p.is_dir() else p.parent\n                common = base / common\n            key = str(common.resolve())\n            prefix = \"git:\"\n        else:\n            raise RuntimeError\n    except Exception:\n        key = str(p)\n        prefix = \"fs:\"\n\n    h = hashlib.sha1(key.encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:16]\n    return f\"{prefix}{h}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__redact_emails_122": {
      "name": "_redact_emails",
      "type": "function",
      "start_line": 122,
      "end_line": 129,
      "content_hash": "1f113388bb1ba7d500a082b24e2b2ec271b8bc3c",
      "content": "def _redact_emails(text: str) -> str:\n    \"\"\"Redact email addresses from commit messages for privacy.\"\"\"\n    try:\n        return re.sub(\n            r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", \"<redacted>\", text or \"\",\n        )\n    except Exception:\n        return text",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__collect_git_history_for_workspace_132": {
      "name": "_collect_git_history_for_workspace",
      "type": "function",
      "start_line": 132,
      "end_line": 355,
      "content_hash": "e4ad3c2943e14cd11bbc1383379db5296536e629",
      "content": "def _collect_git_history_for_workspace(workspace_path: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Best-effort collection of recent git history for a workspace.\n\n    Uses REMOTE_UPLOAD_GIT_MAX_COMMITS (0/empty disables) and\n    REMOTE_UPLOAD_GIT_SINCE (optional) to bound history. Returns a\n    serializable dict suitable for writing as metadata/git_history.json, or\n    None when git metadata is unavailable.\n    \"\"\"\n    # Read configuration from environment\n    try:\n        raw_max = (os.environ.get(\"REMOTE_UPLOAD_GIT_MAX_COMMITS\", \"\") or \"\").strip()\n        max_commits = int(raw_max) if raw_max else 0\n    except Exception:\n        max_commits = 0\n    since = (os.environ.get(\"REMOTE_UPLOAD_GIT_SINCE\", \"\") or \"\").strip()\n    force_full = str(os.environ.get(\"REMOTE_UPLOAD_GIT_FORCE\", \"\") or \"\").strip().lower() in {\n        \"1\",\n        \"true\",\n        \"yes\",\n        \"on\",\n    }\n\n    if max_commits <= 0:\n        return None\n\n    root = _find_git_root(Path(workspace_path))\n    if not root:\n        return None\n\n    # Git history cache: avoid emitting identical manifests when HEAD/settings are unchanged\n    base = Path(os.environ.get(\"WORKSPACE_PATH\") or workspace_path).resolve()\n    git_cache_path = base / \".context-engine\" / \"git_history_cache.json\"\n    current_head = \"\"\n    try:\n        head_proc = subprocess.run(\n            [\"git\", \"rev-parse\", \"HEAD\"],\n            cwd=str(root),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            encoding=\"utf-8\",\n            errors=\"replace\",\n        )\n        if head_proc.returncode == 0 and head_proc.stdout.strip():\n            current_head = head_proc.stdout.strip()\n    except Exception:\n        current_head = \"\"\n\n    cache: Dict[str, Any] = {}\n    if not force_full:\n        try:\n            if git_cache_path.exists():\n                with git_cache_path.open(\"r\", encoding=\"utf-8\") as f:\n                    obj = json.load(f)\n                    if isinstance(obj, dict):\n                        cache = obj\n        except Exception:\n            cache = {}\n\n        if current_head and cache.get(\"last_head\") == current_head and cache.get(\"max_commits\") == max_commits and str(cache.get(\"since\") or \"\") == since:\n            return None\n\n    base_head = \"\"\n    prev_head = \"\"\n    if not force_full:\n        try:\n            prev_head = str(cache.get(\"last_head\") or \"\").strip()\n            if current_head and prev_head and prev_head != current_head:\n                base_head = prev_head\n        except Exception:\n            base_head = \"\"\n\n    snapshot_mode = bool(force_full)\n    if not snapshot_mode and current_head and prev_head and prev_head != current_head:\n        try:\n            anc = subprocess.run(\n                [\"git\", \"merge-base\", \"--is-ancestor\", prev_head, current_head],\n                cwd=str(root),\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                encoding=\"utf-8\",\n                errors=\"replace\",\n            )\n            if anc.returncode != 0:\n                snapshot_mode = True\n                base_head = \"\"\n        except Exception:\n            pass\n\n    # Build git rev-list command (simple HEAD-based history)\n    cmd: List[str] = [\"git\", \"rev-list\", \"--no-merges\"]\n    if since:\n        cmd.append(f\"--since={since}\")\n    if base_head and current_head:\n        cmd.append(f\"{base_head}..{current_head}\")\n    else:\n        cmd.append(\"HEAD\")\n\n    try:\n        proc = subprocess.run(\n            cmd,\n            cwd=str(root),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            encoding=\"utf-8\",\n            errors=\"replace\",\n        )\n        if proc.returncode != 0 or not proc.stdout.strip():\n            return None\n        commits = [l.strip() for l in proc.stdout.splitlines() if l.strip()]\n    except Exception:\n        return None\n\n    if not commits:\n        return None\n    if len(commits) > max_commits:\n        commits = commits[:max_commits]\n\n    records: List[Dict[str, Any]] = []\n    for sha in commits:\n        try:\n            fmt = \"%H%x1f%an%x1f%ae%x1f%ad%x1f%s%x1f%b\"\n            show_proc = subprocess.run(\n                [\"git\", \"show\", \"-s\", f\"--format={fmt}\", sha],\n                cwd=str(root),\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                encoding=\"utf-8\",\n                errors=\"replace\",\n            )\n            if show_proc.returncode != 0 or not show_proc.stdout.strip():\n                continue\n            parts = show_proc.stdout.strip().split(\"\\x1f\")\n            c_sha, an, _ae, ad, subj, body = (parts + [\"\"] * 6)[:6]\n\n            files_proc = subprocess.run(\n                [\"git\", \"diff-tree\", \"--no-commit-id\", \"--name-only\", \"-r\", sha],\n                cwd=str(root),\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                encoding=\"utf-8\",\n                errors=\"replace\",\n            )\n            files: List[str] = []\n            if files_proc.returncode == 0 and files_proc.stdout:\n                files = [f for f in files_proc.stdout.splitlines() if f]\n\n            diff_text = \"\"\n            try:\n                diff_proc = subprocess.run(\n                    [\"git\", \"show\", \"--stat\", \"--patch\", \"--unified=3\", sha],\n                    cwd=str(root),\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                    text=True,\n                    encoding=\"utf-8\",\n                    errors=\"replace\",\n                )\n                if diff_proc.returncode == 0 and diff_proc.stdout:\n                    try:\n                        max_chars = int(os.environ.get(\"COMMIT_SUMMARY_DIFF_CHARS\", \"6000\") or 6000)\n                    except Exception:\n                        max_chars = 6000\n                    diff_text = diff_proc.stdout[:max_chars]\n            except Exception:\n                diff_text = \"\"\n\n            msg = _redact_emails((subj + (\"\\n\" + body if body else \"\")).strip())\n            if len(msg) > 2000:\n                msg = msg[:2000] + \"\\u2026\"\n\n            records.append(\n                {\n                    \"commit_id\": c_sha or sha,\n                    \"author_name\": an,\n                    \"authored_date\": ad,\n                    \"message\": msg,\n                    \"files\": files,\n                    \"diff\": diff_text,\n                }\n            )\n        except Exception:\n            continue\n\n    if not records:\n        return None\n\n    try:\n        repo_name = root.name\n    except Exception:\n        repo_name = \"workspace\"\n\n    manifest = {\n        \"version\": 1,\n        \"repo_name\": repo_name,\n        \"generated_at\": datetime.now().isoformat(),\n        \"head\": current_head,\n        \"prev_head\": prev_head,\n        \"base_head\": base_head,\n        \"mode\": \"snapshot\" if snapshot_mode else \"delta\",\n        \"max_commits\": max_commits,\n        \"since\": since,\n        \"commits\": records,\n    }\n\n    # Update git history cache with the HEAD and settings used for this manifest\n    try:\n        git_cache_path.parent.mkdir(parents=True, exist_ok=True)\n        cache_out = {\n            \"last_head\": current_head or (commits[0] if commits else \"\"),\n            \"max_commits\": max_commits,\n            \"since\": since,\n            \"updated_at\": datetime.now().isoformat(),\n        }\n        with git_cache_path.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(cache_out, f, indent=2)\n    except Exception:\n        pass\n\n    return manifest",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__load_local_cache_file_hashes_358": {
      "name": "_load_local_cache_file_hashes",
      "type": "function",
      "start_line": 358,
      "end_line": 403,
      "content_hash": "1985170782a8690b1878bb3b587520dbfaecda7b",
      "content": "def _load_local_cache_file_hashes(workspace_path: str, repo_name: Optional[str]) -> Dict[str, str]:\n    \"\"\"Best-effort read of the local cache.json file_hashes map.\n\n    This mirrors the layout used by workspace_state without introducing new\n    dependencies. It is used only to enumerate candidate paths; normal hash\n    lookups still go through get_cached_file_hash.\n    \"\"\"\n    try:\n        base = Path(os.environ.get(\"WORKSPACE_PATH\") or workspace_path).resolve()\n        multi_repo = os.environ.get(\"MULTI_REPO_MODE\", \"0\").strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n        if multi_repo and repo_name:\n            cache_path = base / \".codebase\" / \"repos\" / repo_name / \"cache.json\"\n        else:\n            cache_path = base / \".codebase\" / \"cache.json\"\n\n        if not cache_path.exists():\n            return {}\n\n        with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        if not isinstance(data, dict):\n            return {}\n        file_hashes = data.get(\"file_hashes\", {})\n        if not isinstance(file_hashes, dict):\n            return {}\n        is_stale, missing, checked = _cache_missing_stats(file_hashes)\n        if is_stale:\n            logger.warning(\n                \"[remote_upload] Detected stale local cache (%d/%d missing); clearing %s\",\n                missing,\n                checked,\n                cache_path,\n            )\n            try:\n                cache_path.unlink(missing_ok=True)  # type: ignore[arg-type]\n            except TypeError:\n                try:\n                    cache_path.unlink()\n                except Exception:\n                    pass\n            except Exception:\n                pass\n            return {}\n        return file_hashes\n    except Exception:\n        return {}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_RemoteUploadClient_406": {
      "name": "RemoteUploadClient",
      "type": "class",
      "start_line": 406,
      "end_line": 1498,
      "content_hash": "9b18a20151cb52f76e0057fabe3b8f3abbd94426",
      "content": "class RemoteUploadClient:\n    \"\"\"Client for uploading delta bundles to remote server.\"\"\"\n\n    def _translate_to_container_path(self, host_path: str) -> str:\n        \"\"\"Translate host path to container path for API communication.\"\"\"\n        host_root = (os.environ.get(\"HOST_ROOT\", \"\") or \"/home/coder/project/Context-Engine/dev-workspace\").strip()\n        container_root = (os.environ.get(\"CONTAINER_ROOT\", \"/work\") or \"/work\").strip()\n\n        host_path_obj = Path(host_path)\n        if host_root:\n            try:\n                host_root_obj = Path(host_root)\n                relative = host_path_obj.relative_to(host_root_obj)\n                container = PurePosixPath(container_root)\n                if relative.parts:\n                    container = container.joinpath(*relative.parts)\n                return str(container)\n            except ValueError:\n                pass\n            except Exception:\n                pass\n\n        try:\n            container = PurePosixPath(container_root)\n            usable_parts = [part for part in host_path_obj.parts if part not in (host_path_obj.anchor, host_path_obj.drive)]\n            if usable_parts:\n                repo_name = usable_parts[-1]\n                return str(container.joinpath(repo_name))\n        except Exception:\n            pass\n\n        return host_path.replace('\\\\', '/').replace(':', '')\n\n    def __init__(self, upload_endpoint: str, workspace_path: str, collection_name: str,\n                 max_retries: int = 3, timeout: int = 30, metadata_path: Optional[str] = None,\n                 logical_repo_id: Optional[str] = None):\n        \"\"\"Initialize remote upload client.\"\"\"\n        self.upload_endpoint = upload_endpoint.rstrip('/')\n        self.workspace_path = workspace_path\n        self.collection_name = collection_name\n        self.max_retries = max_retries\n        self.timeout = timeout\n        self.temp_dir = None\n        self.logical_repo_id = logical_repo_id\n\n        # Set environment variables for cache functions\n        os.environ[\"WORKSPACE_PATH\"] = workspace_path\n\n        # Get repo name for cache operations\n        try:\n            from scripts.workspace_state import _extract_repo_name_from_path\n            self.repo_name = _extract_repo_name_from_path(workspace_path)\n            # Fallback to directory name if repo detection fails (for non-git repos)\n            if not self.repo_name:\n                self.repo_name = Path(workspace_path).name\n        except ImportError:\n            self.repo_name = Path(workspace_path).name\n\n        # In-memory stat cache to avoid rehashing unchanged files on every watch iteration\n        self._stat_cache: Dict[str, Tuple[int, int]] = {}\n\n        # Setup HTTP session with simple retry\n        self.session = requests.Session()\n        retry_strategy = Retry(total=max_retries, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        self.session.mount(\"http://\", adapter)\n        self.session.mount(\"https://\", adapter)\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit with cleanup.\"\"\"\n        self.cleanup()\n\n    def cleanup(self):\n        \"\"\"Clean up temporary directories.\"\"\"\n        if self.temp_dir and os.path.exists(self.temp_dir):\n            try:\n                import shutil\n                shutil.rmtree(self.temp_dir)\n                logger.debug(f\"[remote_upload] Cleaned up temporary directory: {self.temp_dir}\")\n            except Exception as e:\n                logger.warning(f\"[remote_upload] Failed to cleanup temp directory {self.temp_dir}: {e}\")\n            finally:\n                self.temp_dir = None\n\n    def get_mapping_summary(self) -> Dict[str, Any]:\n        \"\"\"Return derived collection mapping details.\"\"\"\n        container_path = self._translate_to_container_path(self.workspace_path)\n        return {\n            \"repo_name\": self.repo_name,\n            \"collection_name\": self.collection_name,\n            \"source_path\": self.workspace_path,\n            \"container_path\": container_path,\n            \"upload_endpoint\": self.upload_endpoint,\n        }\n\n    def log_mapping_summary(self) -> None:\n        \"\"\"Log mapping summary for user visibility.\"\"\"\n        info = self.get_mapping_summary()\n        logger.info(\"[remote_upload] Collection mapping:\")\n        logger.info(f\"  repo_name: {info['repo_name']}\")\n        logger.info(f\"  collection_name: {info['collection_name']}\")\n        logger.info(f\"  source_path: {info['source_path']}\")\n        logger.info(f\"  container_path: {info['container_path']}\")\n\n    def _get_temp_bundle_dir(self) -> Path:\n        \"\"\"Get or create temporary directory for bundle creation.\"\"\"\n        if not self.temp_dir:\n            self.temp_dir = tempfile.mkdtemp(prefix=\"delta_bundle_\")\n        return Path(self.temp_dir)\n\n    # CLI is stateless - sequence tracking is handled by server\n\n    def detect_file_changes(self, changed_paths: List[Path]) -> Dict[str, List]:\n        \"\"\"\n        Detect what type of changes occurred for each file path.\n\n        Args:\n            changed_paths: List of changed file paths\n\n        Returns:\n            Dictionary with change types: created, updated, deleted, moved, unchanged\n        \"\"\"\n        changes = {\n            \"created\": [],\n            \"updated\": [],\n            \"deleted\": [],\n            \"moved\": [],\n            \"unchanged\": []\n        }\n\n        for path in changed_paths:\n            # Resolve to an absolute path for stable cache keys\n            try:\n                abs_path = str(path.resolve())\n            except Exception:\n                # Skip paths that cannot be resolved\n                continue\n\n            cached_hash = get_cached_file_hash(abs_path, self.repo_name)\n\n            if not path.exists():\n                # File was deleted\n                if cached_hash:\n                    changes[\"deleted\"].append(path)\n                # Remove from in-memory stat cache if present\n                try:\n                    if abs_path in self._stat_cache:\n                        self._stat_cache.pop(abs_path, None)\n                except Exception:\n                    pass\n                continue\n\n            # File exists - use stat to avoid unnecessary re-hashing when possible\n            try:\n                stat = path.stat()\n            except Exception:\n                # Skip files we can't stat\n                continue\n\n            prev_mtime_ns = prev_size = None\n            try:\n                prev_mtime_ns, prev_size = self._stat_cache.get(abs_path, (None, None))\n            except Exception:\n                prev_mtime_ns, prev_size = None, None\n\n            # If mtime and size are unchanged and we have a cached hash, treat as unchanged\n            if prev_mtime_ns == getattr(stat, \"st_mtime_ns\", None) and prev_size == stat.st_size and cached_hash:\n                changes[\"unchanged\"].append(path)\n                continue\n\n            # Stat changed or no prior entry \u2013 hash content to classify change\n            try:\n                with open(path, 'rb') as f:\n                    content = f.read()\n                current_hash = hashlib.sha1(content).hexdigest()\n            except Exception:\n                # Skip files that can't be read\n                continue\n\n            if not cached_hash:\n                # New file\n                changes[\"created\"].append(path)\n            elif cached_hash != current_hash:\n                # Modified file\n                changes[\"updated\"].append(path)\n            else:\n                # Unchanged (content same despite stat change)\n                changes[\"unchanged\"].append(path)\n\n            # Update caches\n            try:\n                self._stat_cache[abs_path] = (getattr(stat, \"st_mtime_ns\", int(stat.st_mtime * 1e9)), stat.st_size)\n            except Exception:\n                pass\n            set_cached_file_hash(abs_path, current_hash, self.repo_name)\n\n        # Detect moves by looking for files with same content hash\n        # but different paths (requires additional tracking)\n        changes[\"moved\"] = self._detect_moves(changes[\"created\"], changes[\"deleted\"])\n\n        return changes\n\n    def _detect_moves(self, created_files: List[Path], deleted_files: List[Path]) -> List[Tuple[Path, Path]]:\n        \"\"\"\n        Detect file moves by matching content hashes between created and deleted files.\n\n        Args:\n            created_files: List of newly created files\n            deleted_files: List of deleted files\n\n        Returns:\n            List of (source, destination) path tuples for detected moves\n        \"\"\"\n        moves = []\n        deleted_hashes = {}\n\n        # Build hash map for deleted files\n        for deleted_path in deleted_files:\n            try:\n                # Try to get cached hash first, fallback to file content\n                cached_hash = get_cached_file_hash(str(deleted_path), self.repo_name)\n                if cached_hash:\n                    deleted_hashes[cached_hash] = deleted_path\n                    continue\n\n                # If no cached hash, try to read from file if it still exists\n                if deleted_path.exists():\n                    with open(deleted_path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    deleted_hashes[file_hash] = deleted_path\n            except Exception:\n                continue\n\n        # Match created files with deleted files by hash\n        for created_path in created_files:\n            try:\n                with open(created_path, 'rb') as f:\n                    content = f.read()\n                file_hash = hashlib.sha1(content).hexdigest()\n\n                if file_hash in deleted_hashes:\n                    source_path = deleted_hashes[file_hash]\n                    moves.append((source_path, created_path))\n                    # Remove from consideration\n                    del deleted_hashes[file_hash]\n            except Exception:\n                continue\n\n        return moves\n\n    def create_delta_bundle(\n        self,\n        changes: Dict[str, List],\n        git_history: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"\n        Create a delta bundle from detected changes.\n\n        Args:\n            changes: Dictionary of file changes by type\n\n        Returns:\n            Tuple of (bundle_path, manifest_metadata)\n        \"\"\"\n        bundle_id = str(uuid.uuid4())\n        # CLI is stateless - server handles sequence numbers\n        created_at = datetime.now().isoformat()\n\n        # Create temporary directory for bundle\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Create directory structure\n            files_dir = temp_path / \"files\"\n            metadata_dir = temp_path / \"metadata\"\n            files_dir.mkdir()\n            metadata_dir.mkdir()\n\n            # Create subdirectories\n            (files_dir / \"created\").mkdir()\n            (files_dir / \"updated\").mkdir()\n            (files_dir / \"moved\").mkdir()\n\n            operations = []\n            total_size = 0\n            file_hashes = {}\n\n            # Process created files\n            for path in changes[\"created\"]:\n                rel_path = path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    with open(path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    content_hash = f\"sha1:{file_hash}\"\n\n                    # Write file to bundle\n                    bundle_file_path = files_dir / \"created\" / rel_path\n                    bundle_file_path.parent.mkdir(parents=True, exist_ok=True)\n                    bundle_file_path.write_bytes(content)\n\n                    # Get file info\n                    stat = path.stat()\n                    language = idx.CODE_EXTS.get(path.suffix.lower(), \"unknown\")\n\n                    operation = {\n                        \"operation\": \"created\",\n                        \"path\": rel_path,\n                        \"relative_path\": rel_path,\n                        \"absolute_path\": str(path.resolve()),\n                        \"size_bytes\": stat.st_size,\n                        \"content_hash\": content_hash,\n                        \"file_hash\": f\"sha1:{idx.hash_id(content.decode('utf-8', errors='ignore'), rel_path, 1, len(content.splitlines()))}\",\n                        \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                        \"language\": language\n                    }\n                    operations.append(operation)\n                    file_hashes[rel_path] = f\"sha1:{file_hash}\"\n                    total_size += stat.st_size\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing created file {path}: {e}\")\n                    continue\n\n            # Process updated files\n            for path in changes[\"updated\"]:\n                rel_path = path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    with open(path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    content_hash = f\"sha1:{file_hash}\"\n                    previous_hash = get_cached_file_hash(str(path.resolve()), self.repo_name)\n\n                    # Write file to bundle\n                    bundle_file_path = files_dir / \"updated\" / rel_path\n                    bundle_file_path.parent.mkdir(parents=True, exist_ok=True)\n                    bundle_file_path.write_bytes(content)\n\n                    # Get file info\n                    stat = path.stat()\n                    language = idx.CODE_EXTS.get(path.suffix.lower(), \"unknown\")\n\n                    operation = {\n                        \"operation\": \"updated\",\n                        \"path\": rel_path,\n                        \"relative_path\": rel_path,\n                        \"absolute_path\": str(path.resolve()),\n                        \"size_bytes\": stat.st_size,\n                        \"content_hash\": content_hash,\n                        \"previous_hash\": f\"sha1:{previous_hash}\" if previous_hash else None,\n                        \"file_hash\": f\"sha1:{idx.hash_id(content.decode('utf-8', errors='ignore'), rel_path, 1, len(content.splitlines()))}\",\n                        \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                        \"language\": language\n                    }\n                    operations.append(operation)\n                    file_hashes[rel_path] = f\"sha1:{file_hash}\"\n                    total_size += stat.st_size\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing updated file {path}: {e}\")\n                    continue\n\n            # Process moved files\n            for source_path, dest_path in changes[\"moved\"]:\n                dest_rel_path = dest_path.relative_to(Path(self.workspace_path)).as_posix()\n                source_rel_path = source_path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    with open(dest_path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    content_hash = f\"sha1:{file_hash}\"\n\n                    # Write file to bundle\n                    bundle_file_path = files_dir / \"moved\" / dest_rel_path\n                    bundle_file_path.parent.mkdir(parents=True, exist_ok=True)\n                    bundle_file_path.write_bytes(content)\n\n                    # Get file info\n                    stat = dest_path.stat()\n                    language = idx.CODE_EXTS.get(dest_path.suffix.lower(), \"unknown\")\n\n                    operation = {\n                        \"operation\": \"moved\",\n                        \"path\": dest_rel_path,\n                        \"relative_path\": dest_rel_path,\n                        \"absolute_path\": str(dest_path.resolve()),\n                        \"source_path\": source_rel_path,\n                        \"source_relative_path\": source_rel_path,\n                        \"source_absolute_path\": str(source_path.resolve()),\n                        \"size_bytes\": stat.st_size,\n                        \"content_hash\": content_hash,\n                        \"file_hash\": f\"sha1:{idx.hash_id(content.decode('utf-8', errors='ignore'), dest_rel_path, 1, len(content.splitlines()))}\",\n                        \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                        \"language\": language\n                    }\n                    operations.append(operation)\n                    file_hashes[dest_rel_path] = f\"sha1:{file_hash}\"\n                    total_size += stat.st_size\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing moved file {source_path} -> {dest_path}: {e}\")\n                    continue\n\n            # Process deleted files\n            for path in changes[\"deleted\"]:\n                rel_path = path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    previous_hash = get_cached_file_hash(str(path.resolve()), self.repo_name)\n\n                    operation = {\n                        \"operation\": \"deleted\",\n                        \"path\": rel_path,\n                        \"relative_path\": rel_path,\n                        \"absolute_path\": str(path.resolve()),\n                        \"previous_hash\": f\"sha1:{previous_hash}\" if previous_hash else None,\n                        \"file_hash\": None,\n                        \"modified_time\": datetime.now().isoformat(),\n                        \"language\": idx.CODE_EXTS.get(path.suffix.lower(), \"unknown\")\n                    }\n                    operations.append(operation)\n\n                    # Once a delete operation has been recorded, drop the cache entry\n                    # so subsequent scans do not keep re-reporting the same deletion.\n                    try:\n                        remove_cached_file(str(path.resolve()), self.repo_name)\n                    except Exception:\n                        pass\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing deleted file {path}: {e}\")\n                    continue\n\n            # Create manifest\n            manifest = {\n                \"version\": \"1.0\",\n                \"bundle_id\": bundle_id,\n                \"workspace_path\": self.workspace_path,\n                \"collection_name\": self.collection_name,\n                \"created_at\": created_at,\n                # CLI is stateless - server handles sequence numbers\n                \"sequence_number\": None,  # Server will assign\n                \"parent_sequence\": None,   # Server will determine\n                \"operations\": {\n                    \"created\": len(changes[\"created\"]),\n                    \"updated\": len(changes[\"updated\"]),\n                    \"deleted\": len(changes[\"deleted\"]),\n                    \"moved\": len(changes[\"moved\"])\n                },\n                \"total_files\": len(operations),\n                \"total_size_bytes\": total_size,\n                \"compression\": \"gzip\",\n                \"encoding\": \"utf-8\"\n            }\n\n            # Write manifest\n            (temp_path / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n\n            # Write operations metadata\n            operations_metadata = {\n                \"operations\": operations\n            }\n            (metadata_dir / \"operations.json\").write_text(json.dumps(operations_metadata, indent=2))\n\n            # Write hashes\n            hashes_metadata = {\n                \"workspace_path\": self.workspace_path,\n                \"updated_at\": created_at,\n                \"file_hashes\": file_hashes\n            }\n            (metadata_dir / \"hashes.json\").write_text(json.dumps(hashes_metadata, indent=2))\n\n            # Optional: attach recent git history for this workspace\n            try:\n                if git_history is None:\n                    git_history = _collect_git_history_for_workspace(self.workspace_path)\n                if git_history:\n                    (metadata_dir / \"git_history.json\").write_text(\n                        json.dumps(git_history, indent=2)\n                    )\n            except Exception:\n                # Best-effort only; never fail bundle creation on git history issues\n                pass\n\n            # Create tarball in temporary directory\n            temp_bundle_dir = self._get_temp_bundle_dir()\n            bundle_path = temp_bundle_dir / f\"{bundle_id}.tar.gz\"\n            with tarfile.open(bundle_path, \"w:gz\") as tar:\n                tar.add(temp_path, arcname=f\"{bundle_id}\")\n\n            return str(bundle_path), manifest\n\n    def upload_bundle(self, bundle_path: str, manifest: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Upload delta bundle to remote server with exponential backoff retry.\n\n        Args:\n            bundle_path: Path to the bundle tarball\n            manifest: Bundle manifest metadata\n\n        Returns:\n            Server response dictionary\n        \"\"\"\n        last_error = None\n\n        for attempt in range(self.max_retries + 1):\n            try:\n                # Simple exponential backoff\n                if attempt > 0:\n                    delay = min(2 ** (attempt - 1), 30)  # 1, 2, 4, 8... capped at 30s\n                    logger.info(f\"[remote_upload] Retry attempt {attempt + 1}/{self.max_retries + 1} after {delay}s delay\")\n                    time.sleep(delay)\n\n                # Verify bundle exists\n                if not os.path.exists(bundle_path):\n                    return {\"success\": False, \"error\": {\"code\": \"BUNDLE_NOT_FOUND\", \"message\": f\"Bundle not found: {bundle_path}\"}}\n\n                # Check bundle size (server-side enforcement)\n                bundle_size = os.path.getsize(bundle_path)\n\n                files = {\n                    \"bundle\": open(bundle_path, \"rb\"),\n                }\n                data = {\n                    \"workspace_path\": self._translate_to_container_path(self.workspace_path),\n                    \"collection_name\": self.collection_name,\n                    \"sequence_number\": manifest.get(\"sequence_number\"),\n                    \"force\": False,\n                    \"source_path\": self.workspace_path,\n                    \"logical_repo_id\": _compute_logical_repo_id(self.workspace_path),\n                }\n\n                sess = get_auth_session(self.upload_endpoint)\n                if sess:\n                    data[\"session\"] = sess\n\n                if getattr(self, \"logical_repo_id\", None):\n                    data['logical_repo_id'] = self.logical_repo_id\n\n                logger.info(f\"[remote_upload] Uploading bundle {manifest['bundle_id']} (size: {bundle_size} bytes)\")\n\n                response = self.session.post(\n                    f\"{self.upload_endpoint}/api/v1/delta/upload\",\n                    files=files,\n                    data=data,\n                    timeout=(10, self.timeout)\n                )\n\n                result = None\n                try:\n                    result = response.json()\n                except Exception:\n                    result = None\n\n                if response.status_code == 200 and isinstance(result, dict) and result.get(\"success\", False):\n                    logger.info(f\"[remote_upload] Successfully uploaded bundle {manifest['bundle_id']}\")\n                    seq = result.get(\"sequence_number\")\n                    if seq is not None:\n                        try:\n                            manifest[\"sequence\"] = seq\n                        except Exception:\n                            pass\n                    return result\n\n                # Handle error\n                error_msg = f\"Upload failed with status {response.status_code}\"\n                try:\n                    error_detail = result if isinstance(result, dict) else response.json()\n                    error_detail_msg = error_detail.get('error', {}).get('message', 'Unknown error')\n                    error_msg += f\": {error_detail_msg}\"\n                    error_code = error_detail.get('error', {}).get('code', 'HTTP_ERROR')\n                except Exception:\n                    error_msg += f\": {response.text[:200]}\"\n                    error_code = \"HTTP_ERROR\"\n\n                # Special-case 401 to make auth issues obvious to users\n                if response.status_code == 401:\n                    if error_code in {None, \"HTTP_ERROR\"}:\n                        error_code = \"UNAUTHORIZED\"\n                    # Always append a clear hint for auth failures\n                    error_msg += \" (unauthorized; please log in with `ctxce auth login` and retry)\"\n\n                last_error = {\"success\": False, \"error\": {\"code\": error_code, \"message\": error_msg, \"status_code\": response.status_code}}\n\n                # Don't retry on client errors (except 429)\n                if 400 <= response.status_code < 500 and response.status_code != 429:\n                    return last_error\n\n                logger.warning(f\"[remote_upload] Upload attempt {attempt + 1} failed: {error_msg}\")\n\n            except requests.exceptions.ConnectTimeout as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"TIMEOUT_ERROR\", \"message\": f\"Upload timeout: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Upload timeout on attempt {attempt + 1}: {e}\")\n\n            except requests.exceptions.ReadTimeout as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"TIMEOUT_ERROR\", \"message\": f\"Upload timeout: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Upload read timeout on attempt {attempt + 1}: {e}\")\n                \n                # After read timeout, poll to check if server processed the bundle\n                logger.info(f\"[remote_upload] Read timeout occurred, polling server to check if bundle was processed...\")\n                poll_result = self._poll_after_timeout(manifest)\n                if poll_result.get(\"success\"):\n                    logger.info(f\"[remote_upload] Server confirmed processing of bundle {manifest['bundle_id']} after timeout\")\n                    return poll_result\n                \n                logger.warning(f\"[remote_upload] Server did not process bundle after timeout, proceeding with failure\")\n                break\n\n            except requests.exceptions.Timeout as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"TIMEOUT_ERROR\", \"message\": f\"Upload timeout: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Upload timeout on attempt {attempt + 1}: {e}\")\n                \n                # For generic timeout, also try polling\n                logger.info(f\"[remote_upload] Timeout occurred, polling server to check if bundle was processed...\")\n                poll_result = self._poll_after_timeout(manifest)\n                if poll_result.get(\"success\"):\n                    logger.info(f\"[remote_upload] Server confirmed processing of bundle {manifest['bundle_id']} after timeout\")\n                    return poll_result\n                \n                logger.warning(f\"[remote_upload] Server did not process bundle after timeout, proceeding with failure\")\n                break\n\n            except requests.exceptions.ConnectionError as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"CONNECTION_ERROR\", \"message\": f\"Connection error: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Connection error on attempt {attempt + 1}: {e}\")\n\n            except requests.exceptions.RequestException as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"NETWORK_ERROR\", \"message\": f\"Network error: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Network error on attempt {attempt + 1}: {e}\")\n\n            except Exception as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"UPLOAD_ERROR\", \"message\": f\"Upload error: {str(e)}\"}}\n                logger.error(f\"[remote_upload] Unexpected error on attempt {attempt + 1}: {e}\")\n\n        # All retries exhausted\n        logger.error(f\"[remote_upload] All {self.max_retries + 1} upload attempts failed for bundle {manifest.get('bundle_id', 'unknown')}\")\n        return last_error or {\n            \"success\": False,\n            \"error\": {\n                \"code\": \"MAX_RETRIES_EXCEEDED\",\n                \"message\": f\"Upload failed after {self.max_retries + 1} attempts\"\n            }\n        }\n\n    def _poll_after_timeout(self, manifest: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Poll server status after a timeout to check if bundle was processed.\n        \n        Args:\n            manifest: Bundle manifest containing sequence information\n            \n        Returns:\n            Dictionary indicating success if bundle was processed\n        \"\"\"\n        try:\n            # Get current server status to know the expected sequence\n            status = self.get_server_status()\n            if not status.get(\"success\"):\n                return {\"success\": False, \"error\": status.get(\"error\", {\"code\": \"UNKNOWN\", \"message\": \"Failed to get status\"})}\n\n            current_sequence = status.get(\"last_sequence\", 0)\n            expected_sequence = manifest.get(\"sequence\", current_sequence + 1)\n\n            logger.info(f\"[remote_upload] Current server sequence: {current_sequence}, expected: {expected_sequence}\")\n\n            # If server is already at expected sequence, bundle was processed\n            if current_sequence >= expected_sequence:\n                return {\n                    \"success\": True,\n                    \"message\": f\"Bundle processed (server at sequence {current_sequence})\",\n                    \"sequence\": current_sequence,\n                }\n\n            # Poll window is configurable via REMOTE_UPLOAD_POLL_MAX_SECS (seconds).\n            # Values <= 0 mean \"no timeout\" (poll until success or process exit).\n            try:\n                max_poll_time = int(os.environ.get(\"REMOTE_UPLOAD_POLL_MAX_SECS\", \"300\"))\n            except Exception:\n                max_poll_time = 300\n            poll_interval = 5\n            start_time = time.time()\n\n            while True:\n                elapsed = time.time() - start_time\n                if max_poll_time > 0 and elapsed >= max_poll_time:\n                    logger.warning(\n                        f\"[remote_upload] Polling timed out after {int(elapsed)}s (limit={max_poll_time}s), bundle was not confirmed as processed\"\n                    )\n                    return {\n                        \"success\": False,\n                        \"error\": {\n                            \"code\": \"POLL_TIMEOUT\",\n                            \"message\": f\"Bundle not confirmed processed after polling for {int(elapsed)}s (limit={max_poll_time}s)\",\n                        },\n                    }\n\n                logger.info(\n                    f\"[remote_upload] Polling server status... (elapsed: {int(elapsed)}s, limit={'no-limit' if max_poll_time <= 0 else max_poll_time}s)\"\n                )\n                time.sleep(poll_interval)\n\n                status = self.get_server_status()\n                if status.get(\"success\"):\n                    new_sequence = status.get(\"last_sequence\", 0)\n                    if new_sequence >= expected_sequence:\n                        logger.info(\n                            f\"[remote_upload] Server sequence advanced to {new_sequence}, bundle was processed!\"\n                        )\n                        return {\n                            \"success\": True,\n                            \"message\": f\"Bundle processed after timeout (server at sequence {new_sequence})\",\n                            \"sequence\": new_sequence,\n                        }\n                    logger.debug(\n                        f\"[remote_upload] Server sequence still at {new_sequence}, continuing to poll...\"\n                    )\n                else:\n                    logger.warning(\n                        f\"[remote_upload] Failed to get server status during poll: {status.get('error', {}).get('message', 'Unknown')}\"\n                    )\n            \n        except Exception as e:\n            logger.error(f\"[remote_upload] Error during post-timeout polling: {e}\")\n            return {\"success\": False, \"error\": {\"code\": \"POLL_ERROR\", \"message\": f\"Polling error: {str(e)}\"}}\n\n    def get_server_status(self) -> Dict[str, Any]:\n        \"\"\"Get server status with simplified error handling.\"\"\"\n        try:\n            container_workspace_path = self._translate_to_container_path(self.workspace_path)\n            connect_timeout = min(self.timeout, 10)\n            # Allow slower responses (e.g., cold starts/large collections) before bailing\n            read_timeout = max(self.timeout, 30)\n            response = self.session.get(\n                f\"{self.upload_endpoint}/api/v1/delta/status\",\n                params={'workspace_path': container_workspace_path},\n                timeout=(connect_timeout, read_timeout)\n            )\n\n            if response.status_code == 200:\n                return response.json()\n\n            # Handle error response\n            error_msg = f\"Status check failed with HTTP {response.status_code}\"\n            try:\n                error_detail = response.json()\n                error_msg += f\": {error_detail.get('error', {}).get('message', 'Unknown error')}\"\n            except Exception:\n                error_msg += f\": {response.text[:100]}\"\n\n            return {\"success\": False, \"error\": {\"code\": \"STATUS_ERROR\", \"message\": error_msg}}\n\n        except requests.exceptions.Timeout:\n            return {\"success\": False, \"error\": {\"code\": \"STATUS_TIMEOUT\", \"message\": \"Status check timeout\"}}\n        except requests.exceptions.ConnectionError:\n            return {\"success\": False, \"error\": {\"code\": \"CONNECTION_ERROR\", \"message\": f\"Cannot connect to server\"}}\n        except Exception as e:\n            return {\"success\": False, \"error\": {\"code\": \"STATUS_CHECK_ERROR\", \"message\": f\"Status check error: {str(e)}\"}}\n\n    def has_meaningful_changes(self, changes: Dict[str, List]) -> bool:\n        \"\"\"Check if changes warrant a delta upload.\"\"\"\n        total_changes = sum(len(files) for op, files in changes.items() if op != \"unchanged\")\n        return total_changes > 0\n\n    def upload_git_history_only(self, git_history: Dict[str, Any]) -> bool:\n        try:\n            empty_changes = {\n                \"created\": [],\n                \"updated\": [],\n                \"deleted\": [],\n                \"moved\": [],\n                \"unchanged\": [],\n            }\n            bundle_path, manifest = self.create_delta_bundle(\n                empty_changes,\n                git_history=git_history,\n            )\n            response = self.upload_bundle(bundle_path, manifest)\n            if response.get(\"success\", False):\n                try:\n                    if os.path.exists(bundle_path):\n                        os.remove(bundle_path)\n                    self.cleanup()\n                except Exception:\n                    pass\n                return True\n            return False\n        except Exception as e:\n            logger.error(f\"[remote_upload] Error uploading git history metadata: {e}\")\n            return False\n\n    def process_changes_and_upload(self, changes: Dict[str, List]) -> bool:\n        \"\"\"\n        Process pre-computed changes and upload delta bundle.\n        Includes comprehensive error handling and graceful fallback.\n\n        Args:\n            changes: Dictionary of file changes by type\n\n        Returns:\n            True if upload was successful, False otherwise\n        \"\"\"\n        try:\n            logger.info(f\"[remote_upload] Processing pre-computed changes\")\n\n            # Validate input\n            if not changes:\n                logger.info(\"[remote_upload] No changes provided\")\n                return True\n\n            if not self.has_meaningful_changes(changes):\n                logger.info(\"[remote_upload] No meaningful changes detected, skipping upload\")\n                return True\n\n            # Log change summary\n            total_changes = sum(len(files) for op, files in changes.items() if op != \"unchanged\")\n            logger.info(f\"[remote_upload] Detected {total_changes} meaningful changes: \"\n                       f\"{len(changes['created'])} created, {len(changes['updated'])} updated, \"\n                       f\"{len(changes['deleted'])} deleted, {len(changes['moved'])} moved\")\n\n            # Create delta bundle\n            bundle_path = None\n            try:\n                bundle_path, manifest = self.create_delta_bundle(changes)\n                logger.info(f\"[remote_upload] Created delta bundle: {manifest['bundle_id']} \"\n                           f\"(size: {manifest['total_size_bytes']} bytes)\")\n\n                # Validate bundle was created successfully\n                if not bundle_path or not os.path.exists(bundle_path):\n                    raise RuntimeError(f\"Failed to create bundle at {bundle_path}\")\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error creating delta bundle: {e}\")\n                # Clean up any temporary files on failure\n                self.cleanup()\n                return False\n\n            # Upload bundle with retry logic\n            try:\n                response = self.upload_bundle(bundle_path, manifest)\n\n                if response.get(\"success\", False):\n                    processed_ops = response.get('processed_operations', {})\n                    logger.info(f\"[remote_upload] Successfully uploaded bundle {manifest['bundle_id']}\")\n                    logger.info(f\"[remote_upload] Processed operations: {processed_ops}\")\n\n                    # Clean up temporary bundle after successful upload\n                    try:\n                        if os.path.exists(bundle_path):\n                            os.remove(bundle_path)\n                            logger.debug(f\"[remote_upload] Cleaned up temporary bundle: {bundle_path}\")\n                        # Also clean up the entire temp directory if this is the last bundle\n                        self.cleanup()\n                    except Exception as cleanup_error:\n                        logger.warning(f\"[remote_upload] Failed to cleanup bundle {bundle_path}: {cleanup_error}\")\n\n                    return True\n                else:\n                    error_msg = response.get('error', {}).get('message', 'Unknown upload error')\n                    logger.error(f\"[remote_upload] Upload failed: {error_msg}\")\n                    return False\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error uploading bundle: {e}\")\n                return False\n\n        except Exception as e:\n            logger.error(f\"[remote_upload] Unexpected error in process_changes_and_upload: {e}\")\n            return False\n\n    def get_all_code_files(self) -> List[Path]:\n        \"\"\"Get all code files in the workspace.\"\"\"\n        files: List[Path] = []\n        try:\n            workspace_path = Path(self.workspace_path)\n            if not workspace_path.exists():\n                return files\n\n            # Single walk with early pruning similar to standalone client\n            ext_suffixes = {str(ext).lower() for ext in idx.CODE_EXTS if str(ext).startswith('.')}\n            name_matches = {str(ext) for ext in idx.CODE_EXTS if not str(ext).startswith('.')}\n            dev_remote = os.environ.get(\"DEV_REMOTE_MODE\") == \"1\" or os.environ.get(\"REMOTE_UPLOAD_MODE\") == \"development\"\n            excluded = {\n                \"node_modules\", \"vendor\", \"dist\", \"build\", \"target\", \"out\",\n                \".git\", \".hg\", \".svn\", \".vscode\", \".idea\", \".venv\", \"venv\",\n                \"__pycache__\", \".pytest_cache\", \".mypy_cache\", \".cache\",\n                \".context-engine\", \".context-engine-uploader\", \".codebase\"\n            }\n            if dev_remote:\n                excluded.add(\"dev-workspace\")\n\n            seen = set()\n            for root, dirnames, filenames in os.walk(workspace_path):\n                dirnames[:] = [d for d in dirnames if d not in excluded and not d.startswith('.')]\n\n                for filename in filenames:\n                    if filename.startswith('.'):\n                        continue\n                    candidate = Path(root) / filename\n                    suffix = candidate.suffix.lower()\n                    if filename in name_matches or suffix in ext_suffixes:\n                        resolved = candidate.resolve()\n                        if resolved not in seen:\n                            seen.add(resolved)\n                            files.append(candidate)\n        except Exception as e:\n            logger.error(f\"[watch] Error scanning files: {e}\")\n\n        return files\n\n    def watch_loop(self, interval: int = 5):\n        \"\"\"Main file watching loop using existing detection and upload methods.\"\"\"\n        logger.info(f\"[watch] Starting file monitoring (interval: {interval}s)\")\n        logger.info(f\"[watch] Monitoring: {self.workspace_path}\")\n        logger.info(f\"[watch] Press Ctrl+C to stop\")\n\n        try:\n            while True:\n                try:\n                    # Use existing change detection over both filesystem and cached registry\n                    fs_files = self.get_all_code_files()\n                    path_map = {}\n                    for p in fs_files:\n                        try:\n                            resolved = p.resolve()\n                        except Exception:\n                            continue\n                        path_map[resolved] = p\n\n                    # Include any paths that are only present in the local cache (deleted files)\n                    cached_file_hashes = _load_local_cache_file_hashes(self.workspace_path, self.repo_name)\n                    for cached_abs in cached_file_hashes.keys():\n                        try:\n                            cached_path = Path(cached_abs)\n                            resolved = cached_path.resolve()\n                        except Exception:\n                            continue\n                        if resolved not in path_map:\n                            path_map[resolved] = cached_path\n\n                    all_paths = list(path_map.values())\n                    changes = self.detect_file_changes(all_paths)\n\n                    # Count only meaningful changes (exclude unchanged)\n                    meaningful_changes = len(changes.get(\"created\", [])) + len(changes.get(\"updated\", [])) + len(changes.get(\"deleted\", [])) + len(changes.get(\"moved\", []))\n\n                    if meaningful_changes > 0:\n                        logger.info(f\"[watch] Detected {meaningful_changes} changes: { {k: len(v) for k, v in changes.items() if k != 'unchanged'} }\")\n\n                        success = self.process_changes_and_upload(changes)\n\n                        if success:\n                            logger.info(f\"[watch] Successfully uploaded changes\")\n                        else:\n                            logger.error(f\"[watch] Failed to upload changes\")\n                    else:\n                        git_history = None\n                        try:\n                            git_history = _collect_git_history_for_workspace(self.workspace_path)\n                        except Exception:\n                            git_history = None\n\n                        if git_history:\n                            logger.info(\"[watch] Detected git history update; uploading git history metadata\")\n                            success = self.upload_git_history_only(git_history)\n                            if success:\n                                logger.info(\"[watch] Successfully uploaded git history metadata\")\n                            else:\n                                logger.error(\"[watch] Failed to upload git history metadata\")\n                        else:\n                            logger.debug(f\"[watch] No changes detected\")  # Debug level to avoid spam\n\n                    # Sleep until next check\n                    time.sleep(interval)\n\n                except KeyboardInterrupt:\n                    logger.info(f\"[watch] Received interrupt signal, stopping...\")\n                    break\n                except Exception as e:\n                    logger.error(f\"[watch] Error in watch loop: {e}\")\n                    time.sleep(interval)  # Continue even after errors\n\n        except KeyboardInterrupt:\n            logger.info(f\"[watch] File monitoring stopped by user\")\n\n    def process_and_upload_changes(self, changed_paths: List[Path]) -> bool:\n        \"\"\"\n        Process changed paths and upload delta bundle if meaningful changes exist.\n        Includes comprehensive error handling and graceful fallback.\n\n        Args:\n            changed_paths: List of changed file paths\n\n        Returns:\n            True if upload was successful, False otherwise\n        \"\"\"\n        try:\n            logger.info(f\"[remote_upload] Processing {len(changed_paths)} changed paths\")\n\n            # Validate input\n            if not changed_paths:\n                logger.info(\"[remote_upload] No changed paths provided\")\n                return True\n\n            # Detect changes\n            try:\n                changes = self.detect_file_changes(changed_paths)\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error detecting file changes: {e}\")\n                return False\n\n            if not self.has_meaningful_changes(changes):\n                logger.info(\"[remote_upload] No meaningful changes detected, skipping upload\")\n                return True\n\n            # Log change summary\n            total_changes = sum(len(files) for op, files in changes.items() if op != \"unchanged\")\n            logger.info(f\"[remote_upload] Detected {total_changes} meaningful changes: \"\n                       f\"{len(changes['created'])} created, {len(changes['updated'])} updated, \"\n                       f\"{len(changes['deleted'])} deleted, {len(changes['moved'])} moved\")\n\n            # Create delta bundle\n            bundle_path = None\n            try:\n                bundle_path, manifest = self.create_delta_bundle(changes)\n                logger.info(f\"[remote_upload] Created delta bundle: {manifest['bundle_id']} \"\n                           f\"(size: {manifest['total_size_bytes']} bytes)\")\n\n                # Validate bundle was created successfully\n                if not bundle_path or not os.path.exists(bundle_path):\n                    raise RuntimeError(f\"Failed to create bundle at {bundle_path}\")\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error creating delta bundle: {e}\")\n                # Clean up any temporary files on failure\n                self.cleanup()\n                return False\n\n            # Upload bundle with retry logic\n            try:\n                response = self.upload_bundle(bundle_path, manifest)\n\n                if response.get(\"success\", False):\n                    processed_ops = response.get('processed_operations', {})\n                    logger.info(f\"[remote_upload] Successfully uploaded bundle {manifest['bundle_id']}\")\n                    logger.info(f\"[remote_upload] Processed operations: {processed_ops}\")\n\n                    # Clean up temporary bundle after successful upload\n                    try:\n                        if os.path.exists(bundle_path):\n                            os.remove(bundle_path)\n                            logger.debug(f\"[remote_upload] Cleaned up temporary bundle: {bundle_path}\")\n                        # Also clean up the entire temp directory if this is the last bundle\n                        self.cleanup()\n                    except Exception as cleanup_error:\n                        logger.warning(f\"[remote_upload] Failed to cleanup bundle {bundle_path}: {cleanup_error}\")\n\n                    return True\n                else:\n                    error = response.get(\"error\", {})\n                    error_code = error.get(\"code\", \"UNKNOWN\")\n                    error_msg = error.get(\"message\", \"Unknown error\")\n\n                    logger.error(f\"[remote_upload] Upload failed: {error_msg}\")\n\n                    # Handle specific error types\n                    # CLI is stateless - server handles sequence management\n                    if error_code in [\"BUNDLE_TOO_LARGE\", \"BUNDLE_NOT_FOUND\"]:\n                        # These are unrecoverable errors\n                        logger.error(f\"[remote_upload] Unrecoverable error ({error_code}): {error_msg}\")\n                        return False\n                    elif error_code in [\"TIMEOUT_ERROR\", \"CONNECTION_ERROR\", \"NETWORK_ERROR\"]:\n                        # These might be temporary, suggest fallback\n                        logger.warning(f\"[remote_upload] Network-related error ({error_code}): {error_msg}\")\n                        logger.warning(\"[remote_upload] Consider falling back to local mode if this persists\")\n                        return False\n                    else:\n                        # Other errors\n                        logger.error(f\"[remote_upload] Upload error ({error_code}): {error_msg}\")\n                        return False\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Unexpected error during upload: {e}\")\n                return False\n\n        except Exception as e:\n            logger.error(f\"[remote_upload] Critical error in process_and_upload_changes: {e}\")\n            logger.exception(\"[remote_upload] Full traceback:\")\n            return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__translate_to_container_path_409": {
      "name": "_translate_to_container_path",
      "type": "method",
      "start_line": 409,
      "end_line": 437,
      "content_hash": "556ed38afa4c607e02d14713d4800a70f5904eeb",
      "content": "    def _translate_to_container_path(self, host_path: str) -> str:\n        \"\"\"Translate host path to container path for API communication.\"\"\"\n        host_root = (os.environ.get(\"HOST_ROOT\", \"\") or \"/home/coder/project/Context-Engine/dev-workspace\").strip()\n        container_root = (os.environ.get(\"CONTAINER_ROOT\", \"/work\") or \"/work\").strip()\n\n        host_path_obj = Path(host_path)\n        if host_root:\n            try:\n                host_root_obj = Path(host_root)\n                relative = host_path_obj.relative_to(host_root_obj)\n                container = PurePosixPath(container_root)\n                if relative.parts:\n                    container = container.joinpath(*relative.parts)\n                return str(container)\n            except ValueError:\n                pass\n            except Exception:\n                pass\n\n        try:\n            container = PurePosixPath(container_root)\n            usable_parts = [part for part in host_path_obj.parts if part not in (host_path_obj.anchor, host_path_obj.drive)]\n            if usable_parts:\n                repo_name = usable_parts[-1]\n                return str(container.joinpath(repo_name))\n        except Exception:\n            pass\n\n        return host_path.replace('\\\\', '/').replace(':', '')",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___439": {
      "name": "__init__",
      "type": "method",
      "start_line": 439,
      "end_line": 472,
      "content_hash": "882409cf2b36c298b9d662078167d016a7385854",
      "content": "    def __init__(self, upload_endpoint: str, workspace_path: str, collection_name: str,\n                 max_retries: int = 3, timeout: int = 30, metadata_path: Optional[str] = None,\n                 logical_repo_id: Optional[str] = None):\n        \"\"\"Initialize remote upload client.\"\"\"\n        self.upload_endpoint = upload_endpoint.rstrip('/')\n        self.workspace_path = workspace_path\n        self.collection_name = collection_name\n        self.max_retries = max_retries\n        self.timeout = timeout\n        self.temp_dir = None\n        self.logical_repo_id = logical_repo_id\n\n        # Set environment variables for cache functions\n        os.environ[\"WORKSPACE_PATH\"] = workspace_path\n\n        # Get repo name for cache operations\n        try:\n            from scripts.workspace_state import _extract_repo_name_from_path\n            self.repo_name = _extract_repo_name_from_path(workspace_path)\n            # Fallback to directory name if repo detection fails (for non-git repos)\n            if not self.repo_name:\n                self.repo_name = Path(workspace_path).name\n        except ImportError:\n            self.repo_name = Path(workspace_path).name\n\n        # In-memory stat cache to avoid rehashing unchanged files on every watch iteration\n        self._stat_cache: Dict[str, Tuple[int, int]] = {}\n\n        # Setup HTTP session with simple retry\n        self.session = requests.Session()\n        retry_strategy = Retry(total=max_retries, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        self.session.mount(\"http://\", adapter)\n        self.session.mount(\"https://\", adapter)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___enter___474": {
      "name": "__enter__",
      "type": "method",
      "start_line": 474,
      "end_line": 476,
      "content_hash": "96b998e00a1972043cbb74bb1fc33857195294eb",
      "content": "    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___exit___478": {
      "name": "__exit__",
      "type": "method",
      "start_line": 478,
      "end_line": 480,
      "content_hash": "639ce50895d8ab935a06836f17785c850314ed6f",
      "content": "    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit with cleanup.\"\"\"\n        self.cleanup()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_cleanup_482": {
      "name": "cleanup",
      "type": "method",
      "start_line": 482,
      "end_line": 492,
      "content_hash": "b1ed05095aafada5d596ff64fcd44c2985d51af4",
      "content": "    def cleanup(self):\n        \"\"\"Clean up temporary directories.\"\"\"\n        if self.temp_dir and os.path.exists(self.temp_dir):\n            try:\n                import shutil\n                shutil.rmtree(self.temp_dir)\n                logger.debug(f\"[remote_upload] Cleaned up temporary directory: {self.temp_dir}\")\n            except Exception as e:\n                logger.warning(f\"[remote_upload] Failed to cleanup temp directory {self.temp_dir}: {e}\")\n            finally:\n                self.temp_dir = None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_mapping_summary_494": {
      "name": "get_mapping_summary",
      "type": "method",
      "start_line": 494,
      "end_line": 503,
      "content_hash": "f92e3ff31dda930efc3d4740721f28af619bec84",
      "content": "    def get_mapping_summary(self) -> Dict[str, Any]:\n        \"\"\"Return derived collection mapping details.\"\"\"\n        container_path = self._translate_to_container_path(self.workspace_path)\n        return {\n            \"repo_name\": self.repo_name,\n            \"collection_name\": self.collection_name,\n            \"source_path\": self.workspace_path,\n            \"container_path\": container_path,\n            \"upload_endpoint\": self.upload_endpoint,\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_log_mapping_summary_505": {
      "name": "log_mapping_summary",
      "type": "method",
      "start_line": 505,
      "end_line": 512,
      "content_hash": "b9d32cf6a2fdfe8b722ea13116ab633047b95689",
      "content": "    def log_mapping_summary(self) -> None:\n        \"\"\"Log mapping summary for user visibility.\"\"\"\n        info = self.get_mapping_summary()\n        logger.info(\"[remote_upload] Collection mapping:\")\n        logger.info(f\"  repo_name: {info['repo_name']}\")\n        logger.info(f\"  collection_name: {info['collection_name']}\")\n        logger.info(f\"  source_path: {info['source_path']}\")\n        logger.info(f\"  container_path: {info['container_path']}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__get_temp_bundle_dir_514": {
      "name": "_get_temp_bundle_dir",
      "type": "method",
      "start_line": 514,
      "end_line": 518,
      "content_hash": "a68050d291a9f459916f07bd90da8d7d834ca82a",
      "content": "    def _get_temp_bundle_dir(self) -> Path:\n        \"\"\"Get or create temporary directory for bundle creation.\"\"\"\n        if not self.temp_dir:\n            self.temp_dir = tempfile.mkdtemp(prefix=\"delta_bundle_\")\n        return Path(self.temp_dir)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_detect_file_changes_522": {
      "name": "detect_file_changes",
      "type": "method",
      "start_line": 522,
      "end_line": 610,
      "content_hash": "0d4de2abe83b439f74503eefb35b4f8c4d105480",
      "content": "    def detect_file_changes(self, changed_paths: List[Path]) -> Dict[str, List]:\n        \"\"\"\n        Detect what type of changes occurred for each file path.\n\n        Args:\n            changed_paths: List of changed file paths\n\n        Returns:\n            Dictionary with change types: created, updated, deleted, moved, unchanged\n        \"\"\"\n        changes = {\n            \"created\": [],\n            \"updated\": [],\n            \"deleted\": [],\n            \"moved\": [],\n            \"unchanged\": []\n        }\n\n        for path in changed_paths:\n            # Resolve to an absolute path for stable cache keys\n            try:\n                abs_path = str(path.resolve())\n            except Exception:\n                # Skip paths that cannot be resolved\n                continue\n\n            cached_hash = get_cached_file_hash(abs_path, self.repo_name)\n\n            if not path.exists():\n                # File was deleted\n                if cached_hash:\n                    changes[\"deleted\"].append(path)\n                # Remove from in-memory stat cache if present\n                try:\n                    if abs_path in self._stat_cache:\n                        self._stat_cache.pop(abs_path, None)\n                except Exception:\n                    pass\n                continue\n\n            # File exists - use stat to avoid unnecessary re-hashing when possible\n            try:\n                stat = path.stat()\n            except Exception:\n                # Skip files we can't stat\n                continue\n\n            prev_mtime_ns = prev_size = None\n            try:\n                prev_mtime_ns, prev_size = self._stat_cache.get(abs_path, (None, None))\n            except Exception:\n                prev_mtime_ns, prev_size = None, None\n\n            # If mtime and size are unchanged and we have a cached hash, treat as unchanged\n            if prev_mtime_ns == getattr(stat, \"st_mtime_ns\", None) and prev_size == stat.st_size and cached_hash:\n                changes[\"unchanged\"].append(path)\n                continue\n\n            # Stat changed or no prior entry \u2013 hash content to classify change\n            try:\n                with open(path, 'rb') as f:\n                    content = f.read()\n                current_hash = hashlib.sha1(content).hexdigest()\n            except Exception:\n                # Skip files that can't be read\n                continue\n\n            if not cached_hash:\n                # New file\n                changes[\"created\"].append(path)\n            elif cached_hash != current_hash:\n                # Modified file\n                changes[\"updated\"].append(path)\n            else:\n                # Unchanged (content same despite stat change)\n                changes[\"unchanged\"].append(path)\n\n            # Update caches\n            try:\n                self._stat_cache[abs_path] = (getattr(stat, \"st_mtime_ns\", int(stat.st_mtime * 1e9)), stat.st_size)\n            except Exception:\n                pass\n            set_cached_file_hash(abs_path, current_hash, self.repo_name)\n\n        # Detect moves by looking for files with same content hash\n        # but different paths (requires additional tracking)\n        changes[\"moved\"] = self._detect_moves(changes[\"created\"], changes[\"deleted\"])\n\n        return changes",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__detect_moves_612": {
      "name": "_detect_moves",
      "type": "method",
      "start_line": 612,
      "end_line": 659,
      "content_hash": "8d8677570a24d3960edf8cfac0062bc9d9794be0",
      "content": "    def _detect_moves(self, created_files: List[Path], deleted_files: List[Path]) -> List[Tuple[Path, Path]]:\n        \"\"\"\n        Detect file moves by matching content hashes between created and deleted files.\n\n        Args:\n            created_files: List of newly created files\n            deleted_files: List of deleted files\n\n        Returns:\n            List of (source, destination) path tuples for detected moves\n        \"\"\"\n        moves = []\n        deleted_hashes = {}\n\n        # Build hash map for deleted files\n        for deleted_path in deleted_files:\n            try:\n                # Try to get cached hash first, fallback to file content\n                cached_hash = get_cached_file_hash(str(deleted_path), self.repo_name)\n                if cached_hash:\n                    deleted_hashes[cached_hash] = deleted_path\n                    continue\n\n                # If no cached hash, try to read from file if it still exists\n                if deleted_path.exists():\n                    with open(deleted_path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    deleted_hashes[file_hash] = deleted_path\n            except Exception:\n                continue\n\n        # Match created files with deleted files by hash\n        for created_path in created_files:\n            try:\n                with open(created_path, 'rb') as f:\n                    content = f.read()\n                file_hash = hashlib.sha1(content).hexdigest()\n\n                if file_hash in deleted_hashes:\n                    source_path = deleted_hashes[file_hash]\n                    moves.append((source_path, created_path))\n                    # Remove from consideration\n                    del deleted_hashes[file_hash]\n            except Exception:\n                continue\n\n        return moves",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_create_delta_bundle_661": {
      "name": "create_delta_bundle",
      "type": "method",
      "start_line": 661,
      "end_line": 901,
      "content_hash": "a06b6d37582906720f95351606949ca244e6e6c7",
      "content": "    def create_delta_bundle(\n        self,\n        changes: Dict[str, List],\n        git_history: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"\n        Create a delta bundle from detected changes.\n\n        Args:\n            changes: Dictionary of file changes by type\n\n        Returns:\n            Tuple of (bundle_path, manifest_metadata)\n        \"\"\"\n        bundle_id = str(uuid.uuid4())\n        # CLI is stateless - server handles sequence numbers\n        created_at = datetime.now().isoformat()\n\n        # Create temporary directory for bundle\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Create directory structure\n            files_dir = temp_path / \"files\"\n            metadata_dir = temp_path / \"metadata\"\n            files_dir.mkdir()\n            metadata_dir.mkdir()\n\n            # Create subdirectories\n            (files_dir / \"created\").mkdir()\n            (files_dir / \"updated\").mkdir()\n            (files_dir / \"moved\").mkdir()\n\n            operations = []\n            total_size = 0\n            file_hashes = {}\n\n            # Process created files\n            for path in changes[\"created\"]:\n                rel_path = path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    with open(path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    content_hash = f\"sha1:{file_hash}\"\n\n                    # Write file to bundle\n                    bundle_file_path = files_dir / \"created\" / rel_path\n                    bundle_file_path.parent.mkdir(parents=True, exist_ok=True)\n                    bundle_file_path.write_bytes(content)\n\n                    # Get file info\n                    stat = path.stat()\n                    language = idx.CODE_EXTS.get(path.suffix.lower(), \"unknown\")\n\n                    operation = {\n                        \"operation\": \"created\",\n                        \"path\": rel_path,\n                        \"relative_path\": rel_path,\n                        \"absolute_path\": str(path.resolve()),\n                        \"size_bytes\": stat.st_size,\n                        \"content_hash\": content_hash,\n                        \"file_hash\": f\"sha1:{idx.hash_id(content.decode('utf-8', errors='ignore'), rel_path, 1, len(content.splitlines()))}\",\n                        \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                        \"language\": language\n                    }\n                    operations.append(operation)\n                    file_hashes[rel_path] = f\"sha1:{file_hash}\"\n                    total_size += stat.st_size\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing created file {path}: {e}\")\n                    continue\n\n            # Process updated files\n            for path in changes[\"updated\"]:\n                rel_path = path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    with open(path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    content_hash = f\"sha1:{file_hash}\"\n                    previous_hash = get_cached_file_hash(str(path.resolve()), self.repo_name)\n\n                    # Write file to bundle\n                    bundle_file_path = files_dir / \"updated\" / rel_path\n                    bundle_file_path.parent.mkdir(parents=True, exist_ok=True)\n                    bundle_file_path.write_bytes(content)\n\n                    # Get file info\n                    stat = path.stat()\n                    language = idx.CODE_EXTS.get(path.suffix.lower(), \"unknown\")\n\n                    operation = {\n                        \"operation\": \"updated\",\n                        \"path\": rel_path,\n                        \"relative_path\": rel_path,\n                        \"absolute_path\": str(path.resolve()),\n                        \"size_bytes\": stat.st_size,\n                        \"content_hash\": content_hash,\n                        \"previous_hash\": f\"sha1:{previous_hash}\" if previous_hash else None,\n                        \"file_hash\": f\"sha1:{idx.hash_id(content.decode('utf-8', errors='ignore'), rel_path, 1, len(content.splitlines()))}\",\n                        \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                        \"language\": language\n                    }\n                    operations.append(operation)\n                    file_hashes[rel_path] = f\"sha1:{file_hash}\"\n                    total_size += stat.st_size\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing updated file {path}: {e}\")\n                    continue\n\n            # Process moved files\n            for source_path, dest_path in changes[\"moved\"]:\n                dest_rel_path = dest_path.relative_to(Path(self.workspace_path)).as_posix()\n                source_rel_path = source_path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    with open(dest_path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    content_hash = f\"sha1:{file_hash}\"\n\n                    # Write file to bundle\n                    bundle_file_path = files_dir / \"moved\" / dest_rel_path\n                    bundle_file_path.parent.mkdir(parents=True, exist_ok=True)\n                    bundle_file_path.write_bytes(content)\n\n                    # Get file info\n                    stat = dest_path.stat()\n                    language = idx.CODE_EXTS.get(dest_path.suffix.lower(), \"unknown\")\n\n                    operation = {\n                        \"operation\": \"moved\",\n                        \"path\": dest_rel_path,\n                        \"relative_path\": dest_rel_path,\n                        \"absolute_path\": str(dest_path.resolve()),\n                        \"source_path\": source_rel_path,\n                        \"source_relative_path\": source_rel_path,\n                        \"source_absolute_path\": str(source_path.resolve()),\n                        \"size_bytes\": stat.st_size,\n                        \"content_hash\": content_hash,\n                        \"file_hash\": f\"sha1:{idx.hash_id(content.decode('utf-8', errors='ignore'), dest_rel_path, 1, len(content.splitlines()))}\",\n                        \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                        \"language\": language\n                    }\n                    operations.append(operation)\n                    file_hashes[dest_rel_path] = f\"sha1:{file_hash}\"\n                    total_size += stat.st_size\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing moved file {source_path} -> {dest_path}: {e}\")\n                    continue\n\n            # Process deleted files\n            for path in changes[\"deleted\"]:\n                rel_path = path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    previous_hash = get_cached_file_hash(str(path.resolve()), self.repo_name)\n\n                    operation = {\n                        \"operation\": \"deleted\",\n                        \"path\": rel_path,\n                        \"relative_path\": rel_path,\n                        \"absolute_path\": str(path.resolve()),\n                        \"previous_hash\": f\"sha1:{previous_hash}\" if previous_hash else None,\n                        \"file_hash\": None,\n                        \"modified_time\": datetime.now().isoformat(),\n                        \"language\": idx.CODE_EXTS.get(path.suffix.lower(), \"unknown\")\n                    }\n                    operations.append(operation)\n\n                    # Once a delete operation has been recorded, drop the cache entry\n                    # so subsequent scans do not keep re-reporting the same deletion.\n                    try:\n                        remove_cached_file(str(path.resolve()), self.repo_name)\n                    except Exception:\n                        pass\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing deleted file {path}: {e}\")\n                    continue\n\n            # Create manifest\n            manifest = {\n                \"version\": \"1.0\",\n                \"bundle_id\": bundle_id,\n                \"workspace_path\": self.workspace_path,\n                \"collection_name\": self.collection_name,\n                \"created_at\": created_at,\n                # CLI is stateless - server handles sequence numbers\n                \"sequence_number\": None,  # Server will assign\n                \"parent_sequence\": None,   # Server will determine\n                \"operations\": {\n                    \"created\": len(changes[\"created\"]),\n                    \"updated\": len(changes[\"updated\"]),\n                    \"deleted\": len(changes[\"deleted\"]),\n                    \"moved\": len(changes[\"moved\"])\n                },\n                \"total_files\": len(operations),\n                \"total_size_bytes\": total_size,\n                \"compression\": \"gzip\",\n                \"encoding\": \"utf-8\"\n            }\n\n            # Write manifest\n            (temp_path / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n\n            # Write operations metadata\n            operations_metadata = {\n                \"operations\": operations\n            }\n            (metadata_dir / \"operations.json\").write_text(json.dumps(operations_metadata, indent=2))\n\n            # Write hashes\n            hashes_metadata = {\n                \"workspace_path\": self.workspace_path,\n                \"updated_at\": created_at,\n                \"file_hashes\": file_hashes\n            }\n            (metadata_dir / \"hashes.json\").write_text(json.dumps(hashes_metadata, indent=2))\n\n            # Optional: attach recent git history for this workspace\n            try:\n                if git_history is None:\n                    git_history = _collect_git_history_for_workspace(self.workspace_path)\n                if git_history:\n                    (metadata_dir / \"git_history.json\").write_text(\n                        json.dumps(git_history, indent=2)\n                    )\n            except Exception:\n                # Best-effort only; never fail bundle creation on git history issues\n                pass\n\n            # Create tarball in temporary directory\n            temp_bundle_dir = self._get_temp_bundle_dir()\n            bundle_path = temp_bundle_dir / f\"{bundle_id}.tar.gz\"\n            with tarfile.open(bundle_path, \"w:gz\") as tar:\n                tar.add(temp_path, arcname=f\"{bundle_id}\")\n\n            return str(bundle_path), manifest",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_upload_bundle_903": {
      "name": "upload_bundle",
      "type": "method",
      "start_line": 903,
      "end_line": 1053,
      "content_hash": "9499939fffafd8ab94bb850b0684c6668b682adb",
      "content": "    def upload_bundle(self, bundle_path: str, manifest: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Upload delta bundle to remote server with exponential backoff retry.\n\n        Args:\n            bundle_path: Path to the bundle tarball\n            manifest: Bundle manifest metadata\n\n        Returns:\n            Server response dictionary\n        \"\"\"\n        last_error = None\n\n        for attempt in range(self.max_retries + 1):\n            try:\n                # Simple exponential backoff\n                if attempt > 0:\n                    delay = min(2 ** (attempt - 1), 30)  # 1, 2, 4, 8... capped at 30s\n                    logger.info(f\"[remote_upload] Retry attempt {attempt + 1}/{self.max_retries + 1} after {delay}s delay\")\n                    time.sleep(delay)\n\n                # Verify bundle exists\n                if not os.path.exists(bundle_path):\n                    return {\"success\": False, \"error\": {\"code\": \"BUNDLE_NOT_FOUND\", \"message\": f\"Bundle not found: {bundle_path}\"}}\n\n                # Check bundle size (server-side enforcement)\n                bundle_size = os.path.getsize(bundle_path)\n\n                files = {\n                    \"bundle\": open(bundle_path, \"rb\"),\n                }\n                data = {\n                    \"workspace_path\": self._translate_to_container_path(self.workspace_path),\n                    \"collection_name\": self.collection_name,\n                    \"sequence_number\": manifest.get(\"sequence_number\"),\n                    \"force\": False,\n                    \"source_path\": self.workspace_path,\n                    \"logical_repo_id\": _compute_logical_repo_id(self.workspace_path),\n                }\n\n                sess = get_auth_session(self.upload_endpoint)\n                if sess:\n                    data[\"session\"] = sess\n\n                if getattr(self, \"logical_repo_id\", None):\n                    data['logical_repo_id'] = self.logical_repo_id\n\n                logger.info(f\"[remote_upload] Uploading bundle {manifest['bundle_id']} (size: {bundle_size} bytes)\")\n\n                response = self.session.post(\n                    f\"{self.upload_endpoint}/api/v1/delta/upload\",\n                    files=files,\n                    data=data,\n                    timeout=(10, self.timeout)\n                )\n\n                result = None\n                try:\n                    result = response.json()\n                except Exception:\n                    result = None\n\n                if response.status_code == 200 and isinstance(result, dict) and result.get(\"success\", False):\n                    logger.info(f\"[remote_upload] Successfully uploaded bundle {manifest['bundle_id']}\")\n                    seq = result.get(\"sequence_number\")\n                    if seq is not None:\n                        try:\n                            manifest[\"sequence\"] = seq\n                        except Exception:\n                            pass\n                    return result\n\n                # Handle error\n                error_msg = f\"Upload failed with status {response.status_code}\"\n                try:\n                    error_detail = result if isinstance(result, dict) else response.json()\n                    error_detail_msg = error_detail.get('error', {}).get('message', 'Unknown error')\n                    error_msg += f\": {error_detail_msg}\"\n                    error_code = error_detail.get('error', {}).get('code', 'HTTP_ERROR')\n                except Exception:\n                    error_msg += f\": {response.text[:200]}\"\n                    error_code = \"HTTP_ERROR\"\n\n                # Special-case 401 to make auth issues obvious to users\n                if response.status_code == 401:\n                    if error_code in {None, \"HTTP_ERROR\"}:\n                        error_code = \"UNAUTHORIZED\"\n                    # Always append a clear hint for auth failures\n                    error_msg += \" (unauthorized; please log in with `ctxce auth login` and retry)\"\n\n                last_error = {\"success\": False, \"error\": {\"code\": error_code, \"message\": error_msg, \"status_code\": response.status_code}}\n\n                # Don't retry on client errors (except 429)\n                if 400 <= response.status_code < 500 and response.status_code != 429:\n                    return last_error\n\n                logger.warning(f\"[remote_upload] Upload attempt {attempt + 1} failed: {error_msg}\")\n\n            except requests.exceptions.ConnectTimeout as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"TIMEOUT_ERROR\", \"message\": f\"Upload timeout: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Upload timeout on attempt {attempt + 1}: {e}\")\n\n            except requests.exceptions.ReadTimeout as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"TIMEOUT_ERROR\", \"message\": f\"Upload timeout: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Upload read timeout on attempt {attempt + 1}: {e}\")\n                \n                # After read timeout, poll to check if server processed the bundle\n                logger.info(f\"[remote_upload] Read timeout occurred, polling server to check if bundle was processed...\")\n                poll_result = self._poll_after_timeout(manifest)\n                if poll_result.get(\"success\"):\n                    logger.info(f\"[remote_upload] Server confirmed processing of bundle {manifest['bundle_id']} after timeout\")\n                    return poll_result\n                \n                logger.warning(f\"[remote_upload] Server did not process bundle after timeout, proceeding with failure\")\n                break\n\n            except requests.exceptions.Timeout as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"TIMEOUT_ERROR\", \"message\": f\"Upload timeout: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Upload timeout on attempt {attempt + 1}: {e}\")\n                \n                # For generic timeout, also try polling\n                logger.info(f\"[remote_upload] Timeout occurred, polling server to check if bundle was processed...\")\n                poll_result = self._poll_after_timeout(manifest)\n                if poll_result.get(\"success\"):\n                    logger.info(f\"[remote_upload] Server confirmed processing of bundle {manifest['bundle_id']} after timeout\")\n                    return poll_result\n                \n                logger.warning(f\"[remote_upload] Server did not process bundle after timeout, proceeding with failure\")\n                break\n\n            except requests.exceptions.ConnectionError as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"CONNECTION_ERROR\", \"message\": f\"Connection error: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Connection error on attempt {attempt + 1}: {e}\")\n\n            except requests.exceptions.RequestException as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"NETWORK_ERROR\", \"message\": f\"Network error: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Network error on attempt {attempt + 1}: {e}\")\n\n            except Exception as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"UPLOAD_ERROR\", \"message\": f\"Upload error: {str(e)}\"}}\n                logger.error(f\"[remote_upload] Unexpected error on attempt {attempt + 1}: {e}\")\n\n        # All retries exhausted\n        logger.error(f\"[remote_upload] All {self.max_retries + 1} upload attempts failed for bundle {manifest.get('bundle_id', 'unknown')}\")\n        return last_error or {\n            \"success\": False,\n            \"error\": {\n                \"code\": \"MAX_RETRIES_EXCEEDED\",\n                \"message\": f\"Upload failed after {self.max_retries + 1} attempts\"\n            }\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__poll_after_timeout_1055": {
      "name": "_poll_after_timeout",
      "type": "method",
      "start_line": 1055,
      "end_line": 1134,
      "content_hash": "136a965db8852f986b02a235ee25377349da9537",
      "content": "    def _poll_after_timeout(self, manifest: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Poll server status after a timeout to check if bundle was processed.\n        \n        Args:\n            manifest: Bundle manifest containing sequence information\n            \n        Returns:\n            Dictionary indicating success if bundle was processed\n        \"\"\"\n        try:\n            # Get current server status to know the expected sequence\n            status = self.get_server_status()\n            if not status.get(\"success\"):\n                return {\"success\": False, \"error\": status.get(\"error\", {\"code\": \"UNKNOWN\", \"message\": \"Failed to get status\"})}\n\n            current_sequence = status.get(\"last_sequence\", 0)\n            expected_sequence = manifest.get(\"sequence\", current_sequence + 1)\n\n            logger.info(f\"[remote_upload] Current server sequence: {current_sequence}, expected: {expected_sequence}\")\n\n            # If server is already at expected sequence, bundle was processed\n            if current_sequence >= expected_sequence:\n                return {\n                    \"success\": True,\n                    \"message\": f\"Bundle processed (server at sequence {current_sequence})\",\n                    \"sequence\": current_sequence,\n                }\n\n            # Poll window is configurable via REMOTE_UPLOAD_POLL_MAX_SECS (seconds).\n            # Values <= 0 mean \"no timeout\" (poll until success or process exit).\n            try:\n                max_poll_time = int(os.environ.get(\"REMOTE_UPLOAD_POLL_MAX_SECS\", \"300\"))\n            except Exception:\n                max_poll_time = 300\n            poll_interval = 5\n            start_time = time.time()\n\n            while True:\n                elapsed = time.time() - start_time\n                if max_poll_time > 0 and elapsed >= max_poll_time:\n                    logger.warning(\n                        f\"[remote_upload] Polling timed out after {int(elapsed)}s (limit={max_poll_time}s), bundle was not confirmed as processed\"\n                    )\n                    return {\n                        \"success\": False,\n                        \"error\": {\n                            \"code\": \"POLL_TIMEOUT\",\n                            \"message\": f\"Bundle not confirmed processed after polling for {int(elapsed)}s (limit={max_poll_time}s)\",\n                        },\n                    }\n\n                logger.info(\n                    f\"[remote_upload] Polling server status... (elapsed: {int(elapsed)}s, limit={'no-limit' if max_poll_time <= 0 else max_poll_time}s)\"\n                )\n                time.sleep(poll_interval)\n\n                status = self.get_server_status()\n                if status.get(\"success\"):\n                    new_sequence = status.get(\"last_sequence\", 0)\n                    if new_sequence >= expected_sequence:\n                        logger.info(\n                            f\"[remote_upload] Server sequence advanced to {new_sequence}, bundle was processed!\"\n                        )\n                        return {\n                            \"success\": True,\n                            \"message\": f\"Bundle processed after timeout (server at sequence {new_sequence})\",\n                            \"sequence\": new_sequence,\n                        }\n                    logger.debug(\n                        f\"[remote_upload] Server sequence still at {new_sequence}, continuing to poll...\"\n                    )\n                else:\n                    logger.warning(\n                        f\"[remote_upload] Failed to get server status during poll: {status.get('error', {}).get('message', 'Unknown')}\"\n                    )\n            \n        except Exception as e:\n            logger.error(f\"[remote_upload] Error during post-timeout polling: {e}\")\n            return {\"success\": False, \"error\": {\"code\": \"POLL_ERROR\", \"message\": f\"Polling error: {str(e)}\"}}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_server_status_1136": {
      "name": "get_server_status",
      "type": "method",
      "start_line": 1136,
      "end_line": 1167,
      "content_hash": "439d8e20bd6c63ac1d7ee0975cb195bf508cf82a",
      "content": "    def get_server_status(self) -> Dict[str, Any]:\n        \"\"\"Get server status with simplified error handling.\"\"\"\n        try:\n            container_workspace_path = self._translate_to_container_path(self.workspace_path)\n            connect_timeout = min(self.timeout, 10)\n            # Allow slower responses (e.g., cold starts/large collections) before bailing\n            read_timeout = max(self.timeout, 30)\n            response = self.session.get(\n                f\"{self.upload_endpoint}/api/v1/delta/status\",\n                params={'workspace_path': container_workspace_path},\n                timeout=(connect_timeout, read_timeout)\n            )\n\n            if response.status_code == 200:\n                return response.json()\n\n            # Handle error response\n            error_msg = f\"Status check failed with HTTP {response.status_code}\"\n            try:\n                error_detail = response.json()\n                error_msg += f\": {error_detail.get('error', {}).get('message', 'Unknown error')}\"\n            except Exception:\n                error_msg += f\": {response.text[:100]}\"\n\n            return {\"success\": False, \"error\": {\"code\": \"STATUS_ERROR\", \"message\": error_msg}}\n\n        except requests.exceptions.Timeout:\n            return {\"success\": False, \"error\": {\"code\": \"STATUS_TIMEOUT\", \"message\": \"Status check timeout\"}}\n        except requests.exceptions.ConnectionError:\n            return {\"success\": False, \"error\": {\"code\": \"CONNECTION_ERROR\", \"message\": f\"Cannot connect to server\"}}\n        except Exception as e:\n            return {\"success\": False, \"error\": {\"code\": \"STATUS_CHECK_ERROR\", \"message\": f\"Status check error: {str(e)}\"}}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_has_meaningful_changes_1169": {
      "name": "has_meaningful_changes",
      "type": "method",
      "start_line": 1169,
      "end_line": 1172,
      "content_hash": "4cd126933d7477e493544d2dc509141505fd7f6f",
      "content": "    def has_meaningful_changes(self, changes: Dict[str, List]) -> bool:\n        \"\"\"Check if changes warrant a delta upload.\"\"\"\n        total_changes = sum(len(files) for op, files in changes.items() if op != \"unchanged\")\n        return total_changes > 0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_upload_git_history_only_1174": {
      "name": "upload_git_history_only",
      "type": "method",
      "start_line": 1174,
      "end_line": 1199,
      "content_hash": "8079198395bbad97ec354e3818987977298675b0",
      "content": "    def upload_git_history_only(self, git_history: Dict[str, Any]) -> bool:\n        try:\n            empty_changes = {\n                \"created\": [],\n                \"updated\": [],\n                \"deleted\": [],\n                \"moved\": [],\n                \"unchanged\": [],\n            }\n            bundle_path, manifest = self.create_delta_bundle(\n                empty_changes,\n                git_history=git_history,\n            )\n            response = self.upload_bundle(bundle_path, manifest)\n            if response.get(\"success\", False):\n                try:\n                    if os.path.exists(bundle_path):\n                        os.remove(bundle_path)\n                    self.cleanup()\n                except Exception:\n                    pass\n                return True\n            return False\n        except Exception as e:\n            logger.error(f\"[remote_upload] Error uploading git history metadata: {e}\")\n            return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_process_changes_and_upload_1201": {
      "name": "process_changes_and_upload",
      "type": "method",
      "start_line": 1201,
      "end_line": 1278,
      "content_hash": "71f2916625e30a271759111b1b5fe4edb8d30b29",
      "content": "    def process_changes_and_upload(self, changes: Dict[str, List]) -> bool:\n        \"\"\"\n        Process pre-computed changes and upload delta bundle.\n        Includes comprehensive error handling and graceful fallback.\n\n        Args:\n            changes: Dictionary of file changes by type\n\n        Returns:\n            True if upload was successful, False otherwise\n        \"\"\"\n        try:\n            logger.info(f\"[remote_upload] Processing pre-computed changes\")\n\n            # Validate input\n            if not changes:\n                logger.info(\"[remote_upload] No changes provided\")\n                return True\n\n            if not self.has_meaningful_changes(changes):\n                logger.info(\"[remote_upload] No meaningful changes detected, skipping upload\")\n                return True\n\n            # Log change summary\n            total_changes = sum(len(files) for op, files in changes.items() if op != \"unchanged\")\n            logger.info(f\"[remote_upload] Detected {total_changes} meaningful changes: \"\n                       f\"{len(changes['created'])} created, {len(changes['updated'])} updated, \"\n                       f\"{len(changes['deleted'])} deleted, {len(changes['moved'])} moved\")\n\n            # Create delta bundle\n            bundle_path = None\n            try:\n                bundle_path, manifest = self.create_delta_bundle(changes)\n                logger.info(f\"[remote_upload] Created delta bundle: {manifest['bundle_id']} \"\n                           f\"(size: {manifest['total_size_bytes']} bytes)\")\n\n                # Validate bundle was created successfully\n                if not bundle_path or not os.path.exists(bundle_path):\n                    raise RuntimeError(f\"Failed to create bundle at {bundle_path}\")\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error creating delta bundle: {e}\")\n                # Clean up any temporary files on failure\n                self.cleanup()\n                return False\n\n            # Upload bundle with retry logic\n            try:\n                response = self.upload_bundle(bundle_path, manifest)\n\n                if response.get(\"success\", False):\n                    processed_ops = response.get('processed_operations', {})\n                    logger.info(f\"[remote_upload] Successfully uploaded bundle {manifest['bundle_id']}\")\n                    logger.info(f\"[remote_upload] Processed operations: {processed_ops}\")\n\n                    # Clean up temporary bundle after successful upload\n                    try:\n                        if os.path.exists(bundle_path):\n                            os.remove(bundle_path)\n                            logger.debug(f\"[remote_upload] Cleaned up temporary bundle: {bundle_path}\")\n                        # Also clean up the entire temp directory if this is the last bundle\n                        self.cleanup()\n                    except Exception as cleanup_error:\n                        logger.warning(f\"[remote_upload] Failed to cleanup bundle {bundle_path}: {cleanup_error}\")\n\n                    return True\n                else:\n                    error_msg = response.get('error', {}).get('message', 'Unknown upload error')\n                    logger.error(f\"[remote_upload] Upload failed: {error_msg}\")\n                    return False\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error uploading bundle: {e}\")\n                return False\n\n        except Exception as e:\n            logger.error(f\"[remote_upload] Unexpected error in process_changes_and_upload: {e}\")\n            return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_all_code_files_1280": {
      "name": "get_all_code_files",
      "type": "method",
      "start_line": 1280,
      "end_line": 1318,
      "content_hash": "aee20cc0e939ec7fcb865527a938122c11049e85",
      "content": "    def get_all_code_files(self) -> List[Path]:\n        \"\"\"Get all code files in the workspace.\"\"\"\n        files: List[Path] = []\n        try:\n            workspace_path = Path(self.workspace_path)\n            if not workspace_path.exists():\n                return files\n\n            # Single walk with early pruning similar to standalone client\n            ext_suffixes = {str(ext).lower() for ext in idx.CODE_EXTS if str(ext).startswith('.')}\n            name_matches = {str(ext) for ext in idx.CODE_EXTS if not str(ext).startswith('.')}\n            dev_remote = os.environ.get(\"DEV_REMOTE_MODE\") == \"1\" or os.environ.get(\"REMOTE_UPLOAD_MODE\") == \"development\"\n            excluded = {\n                \"node_modules\", \"vendor\", \"dist\", \"build\", \"target\", \"out\",\n                \".git\", \".hg\", \".svn\", \".vscode\", \".idea\", \".venv\", \"venv\",\n                \"__pycache__\", \".pytest_cache\", \".mypy_cache\", \".cache\",\n                \".context-engine\", \".context-engine-uploader\", \".codebase\"\n            }\n            if dev_remote:\n                excluded.add(\"dev-workspace\")\n\n            seen = set()\n            for root, dirnames, filenames in os.walk(workspace_path):\n                dirnames[:] = [d for d in dirnames if d not in excluded and not d.startswith('.')]\n\n                for filename in filenames:\n                    if filename.startswith('.'):\n                        continue\n                    candidate = Path(root) / filename\n                    suffix = candidate.suffix.lower()\n                    if filename in name_matches or suffix in ext_suffixes:\n                        resolved = candidate.resolve()\n                        if resolved not in seen:\n                            seen.add(resolved)\n                            files.append(candidate)\n        except Exception as e:\n            logger.error(f\"[watch] Error scanning files: {e}\")\n\n        return files",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_watch_loop_1320": {
      "name": "watch_loop",
      "type": "method",
      "start_line": 1320,
      "end_line": 1393,
      "content_hash": "f3e452aa5b32827af5bb30062e9d077eba96bb91",
      "content": "    def watch_loop(self, interval: int = 5):\n        \"\"\"Main file watching loop using existing detection and upload methods.\"\"\"\n        logger.info(f\"[watch] Starting file monitoring (interval: {interval}s)\")\n        logger.info(f\"[watch] Monitoring: {self.workspace_path}\")\n        logger.info(f\"[watch] Press Ctrl+C to stop\")\n\n        try:\n            while True:\n                try:\n                    # Use existing change detection over both filesystem and cached registry\n                    fs_files = self.get_all_code_files()\n                    path_map = {}\n                    for p in fs_files:\n                        try:\n                            resolved = p.resolve()\n                        except Exception:\n                            continue\n                        path_map[resolved] = p\n\n                    # Include any paths that are only present in the local cache (deleted files)\n                    cached_file_hashes = _load_local_cache_file_hashes(self.workspace_path, self.repo_name)\n                    for cached_abs in cached_file_hashes.keys():\n                        try:\n                            cached_path = Path(cached_abs)\n                            resolved = cached_path.resolve()\n                        except Exception:\n                            continue\n                        if resolved not in path_map:\n                            path_map[resolved] = cached_path\n\n                    all_paths = list(path_map.values())\n                    changes = self.detect_file_changes(all_paths)\n\n                    # Count only meaningful changes (exclude unchanged)\n                    meaningful_changes = len(changes.get(\"created\", [])) + len(changes.get(\"updated\", [])) + len(changes.get(\"deleted\", [])) + len(changes.get(\"moved\", []))\n\n                    if meaningful_changes > 0:\n                        logger.info(f\"[watch] Detected {meaningful_changes} changes: { {k: len(v) for k, v in changes.items() if k != 'unchanged'} }\")\n\n                        success = self.process_changes_and_upload(changes)\n\n                        if success:\n                            logger.info(f\"[watch] Successfully uploaded changes\")\n                        else:\n                            logger.error(f\"[watch] Failed to upload changes\")\n                    else:\n                        git_history = None\n                        try:\n                            git_history = _collect_git_history_for_workspace(self.workspace_path)\n                        except Exception:\n                            git_history = None\n\n                        if git_history:\n                            logger.info(\"[watch] Detected git history update; uploading git history metadata\")\n                            success = self.upload_git_history_only(git_history)\n                            if success:\n                                logger.info(\"[watch] Successfully uploaded git history metadata\")\n                            else:\n                                logger.error(\"[watch] Failed to upload git history metadata\")\n                        else:\n                            logger.debug(f\"[watch] No changes detected\")  # Debug level to avoid spam\n\n                    # Sleep until next check\n                    time.sleep(interval)\n\n                except KeyboardInterrupt:\n                    logger.info(f\"[watch] Received interrupt signal, stopping...\")\n                    break\n                except Exception as e:\n                    logger.error(f\"[watch] Error in watch loop: {e}\")\n                    time.sleep(interval)  # Continue even after errors\n\n        except KeyboardInterrupt:\n            logger.info(f\"[watch] File monitoring stopped by user\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_process_and_upload_changes_1395": {
      "name": "process_and_upload_changes",
      "type": "method",
      "start_line": 1395,
      "end_line": 1498,
      "content_hash": "0e855b4996c36e5d166566759099f896abbe2989",
      "content": "    def process_and_upload_changes(self, changed_paths: List[Path]) -> bool:\n        \"\"\"\n        Process changed paths and upload delta bundle if meaningful changes exist.\n        Includes comprehensive error handling and graceful fallback.\n\n        Args:\n            changed_paths: List of changed file paths\n\n        Returns:\n            True if upload was successful, False otherwise\n        \"\"\"\n        try:\n            logger.info(f\"[remote_upload] Processing {len(changed_paths)} changed paths\")\n\n            # Validate input\n            if not changed_paths:\n                logger.info(\"[remote_upload] No changed paths provided\")\n                return True\n\n            # Detect changes\n            try:\n                changes = self.detect_file_changes(changed_paths)\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error detecting file changes: {e}\")\n                return False\n\n            if not self.has_meaningful_changes(changes):\n                logger.info(\"[remote_upload] No meaningful changes detected, skipping upload\")\n                return True\n\n            # Log change summary\n            total_changes = sum(len(files) for op, files in changes.items() if op != \"unchanged\")\n            logger.info(f\"[remote_upload] Detected {total_changes} meaningful changes: \"\n                       f\"{len(changes['created'])} created, {len(changes['updated'])} updated, \"\n                       f\"{len(changes['deleted'])} deleted, {len(changes['moved'])} moved\")\n\n            # Create delta bundle\n            bundle_path = None\n            try:\n                bundle_path, manifest = self.create_delta_bundle(changes)\n                logger.info(f\"[remote_upload] Created delta bundle: {manifest['bundle_id']} \"\n                           f\"(size: {manifest['total_size_bytes']} bytes)\")\n\n                # Validate bundle was created successfully\n                if not bundle_path or not os.path.exists(bundle_path):\n                    raise RuntimeError(f\"Failed to create bundle at {bundle_path}\")\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error creating delta bundle: {e}\")\n                # Clean up any temporary files on failure\n                self.cleanup()\n                return False\n\n            # Upload bundle with retry logic\n            try:\n                response = self.upload_bundle(bundle_path, manifest)\n\n                if response.get(\"success\", False):\n                    processed_ops = response.get('processed_operations', {})\n                    logger.info(f\"[remote_upload] Successfully uploaded bundle {manifest['bundle_id']}\")\n                    logger.info(f\"[remote_upload] Processed operations: {processed_ops}\")\n\n                    # Clean up temporary bundle after successful upload\n                    try:\n                        if os.path.exists(bundle_path):\n                            os.remove(bundle_path)\n                            logger.debug(f\"[remote_upload] Cleaned up temporary bundle: {bundle_path}\")\n                        # Also clean up the entire temp directory if this is the last bundle\n                        self.cleanup()\n                    except Exception as cleanup_error:\n                        logger.warning(f\"[remote_upload] Failed to cleanup bundle {bundle_path}: {cleanup_error}\")\n\n                    return True\n                else:\n                    error = response.get(\"error\", {})\n                    error_code = error.get(\"code\", \"UNKNOWN\")\n                    error_msg = error.get(\"message\", \"Unknown error\")\n\n                    logger.error(f\"[remote_upload] Upload failed: {error_msg}\")\n\n                    # Handle specific error types\n                    # CLI is stateless - server handles sequence management\n                    if error_code in [\"BUNDLE_TOO_LARGE\", \"BUNDLE_NOT_FOUND\"]:\n                        # These are unrecoverable errors\n                        logger.error(f\"[remote_upload] Unrecoverable error ({error_code}): {error_msg}\")\n                        return False\n                    elif error_code in [\"TIMEOUT_ERROR\", \"CONNECTION_ERROR\", \"NETWORK_ERROR\"]:\n                        # These might be temporary, suggest fallback\n                        logger.warning(f\"[remote_upload] Network-related error ({error_code}): {error_msg}\")\n                        logger.warning(\"[remote_upload] Consider falling back to local mode if this persists\")\n                        return False\n                    else:\n                        # Other errors\n                        logger.error(f\"[remote_upload] Upload error ({error_code}): {error_msg}\")\n                        return False\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Unexpected error during upload: {e}\")\n                return False\n\n        except Exception as e:\n            logger.error(f\"[remote_upload] Critical error in process_and_upload_changes: {e}\")\n            logger.exception(\"[remote_upload] Full traceback:\")\n            return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_remote_config_1500": {
      "name": "get_remote_config",
      "type": "function",
      "start_line": 1500,
      "end_line": 1525,
      "content_hash": "5858a5d4a32b4004798da0dd5b39ec30adec2899",
      "content": "def get_remote_config(cli_path: Optional[str] = None) -> Dict[str, str]:\n    \"\"\"Get remote upload configuration from environment variables and command-line arguments.\"\"\"\n    # Use command-line path if provided, otherwise fall back to environment variables\n    if cli_path:\n        workspace_path = cli_path\n    else:\n        workspace_path = os.environ.get(\"WATCH_ROOT\", os.environ.get(\"WORKSPACE_PATH\", \"/work\"))\n\n    logical_repo_id = _compute_logical_repo_id(workspace_path)\n\n    # Use auto-generated collection name based on repo name\n    repo_name = _extract_repo_name_from_path(workspace_path)\n    # Fallback to directory name if repo detection fails\n    if not repo_name:\n        repo_name = Path(workspace_path).name\n    collection_name = get_collection_name(repo_name)\n\n    return {\n        \"upload_endpoint\": os.environ.get(\"REMOTE_UPLOAD_ENDPOINT\", \"http://localhost:8080\"),\n        \"workspace_path\": workspace_path,\n        \"collection_name\": collection_name,\n        \"logical_repo_id\": logical_repo_id,\n        # Use higher, more robust defaults but still allow env overrides\n        \"max_retries\": int(os.environ.get(\"REMOTE_UPLOAD_MAX_RETRIES\", \"5\")),\n        \"timeout\": int(os.environ.get(\"REMOTE_UPLOAD_TIMEOUT\", \"1800\")),\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_main_1528": {
      "name": "main",
      "type": "function",
      "start_line": 1528,
      "end_line": 1768,
      "content_hash": "3446ea3f87b5bfb2fed5774e479afb6504e9e9cb",
      "content": "def main():\n    \"\"\"Main entry point for the remote upload client.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Remote upload client for delta bundles in Context-Engine\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # Upload from current directory or environment variables\n  python remote_upload_client.py\n\n  # Upload from specific directory\n  python remote_upload_client.py --path /path/to/repo\n\n  # Upload from specific directory with custom endpoint\n  python remote_upload_client.py --path /path/to/repo --endpoint http://remote-server:8080\n\n  # Watch for file changes and upload automatically\n  python remote_upload_client.py --path /path/to/repo --watch\n\n  # Watch with custom interval (check every 3 seconds)\n  python remote_upload_client.py --path /path/to/repo --watch --interval 3\n        \"\"\"\n    )\n\n    parser.add_argument(\n        \"--path\",\n        type=str,\n        help=\"Path to the directory to upload (overrides WATCH_ROOT/WORKSPACE_PATH environment variables)\"\n    )\n\n    parser.add_argument(\n        \"--endpoint\",\n        type=str,\n        help=\"Remote upload endpoint (overrides REMOTE_UPLOAD_ENDPOINT environment variable)\"\n    )\n\n    parser.add_argument(\n        \"--max-retries\",\n        type=int,\n        help=\"Maximum number of upload retries (overrides REMOTE_UPLOAD_MAX_RETRIES environment variable)\"\n    )\n\n    parser.add_argument(\n        \"--timeout\",\n        type=int,\n        help=\"Request timeout in seconds (overrides REMOTE_UPLOAD_TIMEOUT environment variable)\"\n    )\n\n    parser.add_argument(\n        \"--force\",\n        action=\"store_true\",\n        help=\"Force upload of all files (ignore cached state and treat all files as new)\"\n    )\n\n    parser.add_argument(\n        \"--show-mapping\",\n        action=\"store_true\",\n        help=\"Print collection\u2194workspace mapping information and exit\"\n    )\n\n    parser.add_argument(\n        \"--watch\", \"-w\",\n        action=\"store_true\",\n        help=\"Watch for file changes and upload automatically (continuous mode)\"\n    )\n\n    parser.add_argument(\n        \"--interval\", \"-i\",\n        type=int,\n        default=5,\n        help=\"Watch interval in seconds (default: 5)\"\n    )\n\n    args = parser.parse_args()\n\n    # Validate path if provided\n    if args.path:\n        if not os.path.exists(args.path):\n            logger.error(f\"Path does not exist: {args.path}\")\n            return 1\n\n        if not os.path.isdir(args.path):\n            logger.error(f\"Path is not a directory: {args.path}\")\n            return 1\n\n        args.path = os.path.abspath(args.path)\n        logger.info(f\"Using specified path: {args.path}\")\n\n    # Get configuration\n    config = get_remote_config(args.path)\n\n    # Override with command-line arguments\n    if args.endpoint:\n        config[\"upload_endpoint\"] = args.endpoint\n    if args.max_retries is not None:\n        config[\"max_retries\"] = args.max_retries\n    if args.timeout is not None:\n        config[\"timeout\"] = args.timeout\n\n    logger.info(f\"Workspace path: {config['workspace_path']}\")\n    logger.info(f\"Collection name: {config['collection_name']}\")\n    logger.info(f\"Upload endpoint: {config['upload_endpoint']}\")\n\n    if args.show_mapping:\n        with RemoteUploadClient(\n            upload_endpoint=config[\"upload_endpoint\"],\n            workspace_path=config[\"workspace_path\"],\n            collection_name=config[\"collection_name\"],\n            max_retries=config[\"max_retries\"],\n            timeout=config[\"timeout\"],\n            logical_repo_id=config.get(\"logical_repo_id\"),\n        ) as client:\n            client.log_mapping_summary()\n        return 0\n\n    # Handle watch mode\n    if args.watch:\n        logger.info(\"Starting watch mode for continuous file monitoring\")\n        try:\n            with RemoteUploadClient(\n                upload_endpoint=config[\"upload_endpoint\"],\n                workspace_path=config[\"workspace_path\"],\n                collection_name=config[\"collection_name\"],\n                max_retries=config[\"max_retries\"],\n                timeout=config[\"timeout\"],\n                logical_repo_id=config.get(\"logical_repo_id\"),\n            ) as client:\n\n                logger.info(\"Remote upload client initialized successfully\")\n                client.log_mapping_summary()\n\n                # Test server connection first\n                logger.info(\"Checking server status...\")\n                status = client.get_server_status()\n                is_success = (\n                    isinstance(status, dict) and\n                    'workspace_path' in status and\n                    'collection_name' in status and\n                    status.get('status') == 'ready'\n                )\n                if not is_success:\n                    error = status.get(\"error\", {})\n                    logger.error(f\"Cannot connect to server: {error.get('message', 'Unknown error')}\")\n                    return 1\n\n                logger.info(\"Server connection successful\")\n                logger.info(f\"Starting file monitoring with {args.interval}s interval\")\n\n                # Start the watch loop\n                client.watch_loop(interval=args.interval)\n\n            return 0\n\n        except KeyboardInterrupt:\n            logger.info(\"Watch mode stopped by user\")\n            return 0\n        except Exception as e:\n            logger.error(f\"Watch mode failed: {e}\")\n            return 1\n\n    # Initialize client with context manager for cleanup\n    try:\n        with RemoteUploadClient(\n            upload_endpoint=config[\"upload_endpoint\"],\n            workspace_path=config[\"workspace_path\"],\n            collection_name=config[\"collection_name\"],\n            max_retries=config[\"max_retries\"],\n            timeout=config[\"timeout\"],\n            logical_repo_id=config.get(\"logical_repo_id\"),\n        ) as client:\n\n            logger.info(\"Remote upload client initialized successfully\")\n\n            client.log_mapping_summary()\n\n            # Test server connection\n            logger.info(\"Checking server status...\")\n            status = client.get_server_status()\n            # For delta endpoint, success is indicated by having expected fields (not a \"success\" boolean)\n            is_success = (\n                isinstance(status, dict) and\n                'workspace_path' in status and\n                'collection_name' in status and\n                status.get('status') == 'ready'\n            )\n            if not is_success:\n                error = status.get(\"error\", {})\n                logger.error(f\"Cannot connect to server: {error.get('message', 'Unknown error')}\")\n                return 1\n\n            logger.info(\"Server connection successful\")\n\n            # Scan repository and upload files\n            logger.info(\"Scanning repository for files...\")\n            workspace_path = Path(config['workspace_path'])\n\n            # Find all files in the repository\n            all_files = []\n            for file_path in workspace_path.rglob('*'):\n                if file_path.is_file() and not file_path.name.startswith('.'):\n                    rel_path = file_path.relative_to(workspace_path)\n                    # Skip .codebase directory and other metadata\n                    if not str(rel_path).startswith('.codebase'):\n                        all_files.append(file_path)\n\n            logger.info(f\"Found {len(all_files)} files to upload\")\n\n            if not all_files:\n                logger.warning(\"No files found to upload\")\n                return 0\n\n            # Detect changes (treat all files as changes for initial upload)\n            if args.force:\n                # Force mode: treat all files as created\n                changes = {\"created\": all_files, \"updated\": [], \"deleted\": [], \"moved\": [], \"unchanged\": []}\n            else:\n                changes = client.detect_file_changes(all_files)\n\n            if not client.has_meaningful_changes(changes):\n                logger.info(\"No meaningful changes to upload\")\n                return 0\n\n            logger.info(f\"Changes detected: {len(changes.get('created', []))} created, {len(changes.get('updated', []))} updated, {len(changes.get('deleted', []))} deleted\")\n\n            # Process and upload changes\n            logger.info(\"Uploading files to remote server...\")\n            success = client.process_changes_and_upload(changes)\n\n            if success:\n                logger.info(\"Repository upload completed successfully!\")\n                logger.info(f\"Collection name: {config['collection_name']}\")\n                logger.info(f\"Files uploaded: {len(all_files)}\")\n            else:\n                logger.error(\"Repository upload failed!\")\n                return 1\n\n            return 0\n\n    except Exception as e:\n        logger.error(f\"Failed to initialize remote upload client: {e}\")\n        return 1",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}