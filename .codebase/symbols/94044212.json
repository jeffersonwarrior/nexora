{
  "file_path": "/work/.local/tools/modelscan/sdk/stream/stream_test.go",
  "file_hash": "f07fa85c991f1982955558fa157e788fc576a0ce",
  "updated_at": "2025-12-26T17:34:23.453209",
  "symbols": {
    "function_TestNewStream_CreatesWithType_11": {
      "name": "TestNewStream_CreatesWithType",
      "type": "function",
      "start_line": 11,
      "end_line": 25,
      "content_hash": "66e39b79972d05b87002a6102f3a04d816f024d4",
      "content": "func TestNewStream_CreatesWithType(t *testing.T) {\n\treader := strings.NewReader(\"test data\")\n\tctx := context.Background()\n\n\tstream := NewStream(ctx, reader, StreamTypeSSE)\n\tdefer stream.Close()\n\n\tif stream.streamType != StreamTypeSSE {\n\t\tt.Errorf(\"Expected SSE stream type, got %s\", stream.streamType)\n\t}\n\tif stream.chunks == nil {\n\t\tt.Error(\"Chunks channel not initialized\")\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_SSE_ParsesDataLines_26": {
      "name": "TestStream_SSE_ParsesDataLines",
      "type": "function",
      "start_line": 26,
      "end_line": 61,
      "content_hash": "5c7c20cd91e373ce668c5c618273d4e714218223",
      "content": "func TestStream_SSE_ParsesDataLines(t *testing.T) {\n\tsseData := `data: {\"content\": \"Hello\"}\n\ndata: {\"content\": \" World\"}\n\ndata: [DONE]\n\n`\n\treader := strings.NewReader(sseData)\n\tctx := context.Background()\n\n\tstream := NewStream(ctx, reader, StreamTypeSSE)\n\tdefer stream.Close()\n\n\t// Collect chunks\n\tvar chunks []*Chunk\n\tfor chunk := range stream.Chunks() {\n\t\tchunks = append(chunks, chunk)\n\t}\n\n\tif len(chunks) < 2 {\n\t\tt.Fatalf(\"Expected at least 2 chunks, got %d\", len(chunks))\n\t}\n\n\t// First chunk\n\tif chunks[0].Type != ChunkTypeData {\n\t\tt.Errorf(\"Expected data chunk, got %s\", chunks[0].Type)\n\t}\n\n\t// Last chunk should be DONE\n\tlastChunk := chunks[len(chunks)-1]\n\tif lastChunk.Type != ChunkTypeDone {\n\t\tt.Errorf(\"Expected done chunk, got %s\", lastChunk.Type)\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_SSE_ExtractsOpenAIContent_62": {
      "name": "TestStream_SSE_ExtractsOpenAIContent",
      "type": "function",
      "start_line": 62,
      "end_line": 90,
      "content_hash": "b74f377753b8fffe5ed24ea895f74a7cb31dad76",
      "content": "func TestStream_SSE_ExtractsOpenAIContent(t *testing.T) {\n\tsseData := `data: {\"choices\":[{\"delta\":{\"content\":\"Hello\"}}]}\n\ndata: {\"choices\":[{\"delta\":{\"content\":\" World\"}}]}\n\ndata: [DONE]\n\n`\n\treader := strings.NewReader(sseData)\n\tctx := context.Background()\n\n\tstream := NewStream(ctx, reader, StreamTypeSSE)\n\tdefer stream.Close()\n\n\t// Collect content\n\tvar content strings.Builder\n\tfor chunk := range stream.Chunks() {\n\t\tif chunk.Type == ChunkTypeDone {\n\t\t\tbreak\n\t\t}\n\t\tcontent.WriteString(chunk.Data)\n\t}\n\n\tresult := content.String()\n\tif result != \"Hello World\" {\n\t\tt.Errorf(\"Expected 'Hello World', got '%s'\", result)\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_SSE_ExtractsAnthropicContent_91": {
      "name": "TestStream_SSE_ExtractsAnthropicContent",
      "type": "function",
      "start_line": 91,
      "end_line": 121,
      "content_hash": "c6ff5a96d455c9e40d3cb22247c4e908297d0131",
      "content": "func TestStream_SSE_ExtractsAnthropicContent(t *testing.T) {\n\tsseData := `data: {\"delta\":{\"text\":\"Hello\"}}\n\ndata: {\"delta\":{\"text\":\" from\"}}\n\ndata: {\"delta\":{\"text\":\" Claude\"}}\n\ndata: [DONE]\n\n`\n\treader := strings.NewReader(sseData)\n\tctx := context.Background()\n\n\tstream := NewStream(ctx, reader, StreamTypeSSE)\n\tdefer stream.Close()\n\n\t// Collect content\n\tvar content strings.Builder\n\tfor chunk := range stream.Chunks() {\n\t\tif chunk.Type == ChunkTypeDone {\n\t\t\tbreak\n\t\t}\n\t\tcontent.WriteString(chunk.Data)\n\t}\n\n\tresult := content.String()\n\tif result != \"Hello from Claude\" {\n\t\tt.Errorf(\"Expected 'Hello from Claude', got '%s'\", result)\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_SSE_HandlesMetadata_122": {
      "name": "TestStream_SSE_HandlesMetadata",
      "type": "function",
      "start_line": 122,
      "end_line": 144,
      "content_hash": "264891b92749fda5918b0fd6bb430964706a058b",
      "content": "func TestStream_SSE_HandlesMetadata(t *testing.T) {\n\tsseData := `event: message\nid: msg-123\ndata: {\"content\": \"Hello\"}\n\n`\n\treader := strings.NewReader(sseData)\n\tctx := context.Background()\n\n\tstream := NewStream(ctx, reader, StreamTypeSSE)\n\tdefer stream.Close()\n\n\t// Get first chunk\n\tchunk := <-stream.Chunks()\n\n\tif chunk.Metadata[\"event\"] != \"message\" {\n\t\tt.Errorf(\"Expected event=message, got %v\", chunk.Metadata[\"event\"])\n\t}\n\tif chunk.Metadata[\"id\"] != \"msg-123\" {\n\t\tt.Errorf(\"Expected id=msg-123, got %v\", chunk.Metadata[\"id\"])\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_HTTP_ReadsChunkedData_145": {
      "name": "TestStream_HTTP_ReadsChunkedData",
      "type": "function",
      "start_line": 145,
      "end_line": 164,
      "content_hash": "dae42178aa5854e8fe5e14ca16f4681021a674f7",
      "content": "func TestStream_HTTP_ReadsChunkedData(t *testing.T) {\n\thttpData := \"Hello World from HTTP chunked transfer\"\n\treader := strings.NewReader(httpData)\n\tctx := context.Background()\n\n\tstream := NewStream(ctx, reader, StreamTypeHTTP)\n\tdefer stream.Close()\n\n\t// Collect all data\n\tvar collected strings.Builder\n\tfor chunk := range stream.Chunks() {\n\t\tcollected.WriteString(chunk.Data)\n\t}\n\n\tresult := collected.String()\n\tif !strings.Contains(result, \"Hello World\") {\n\t\tt.Errorf(\"Expected 'Hello World' in result, got '%s'\", result)\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_Collect_AccumulatesAllChunks_165": {
      "name": "TestStream_Collect_AccumulatesAllChunks",
      "type": "function",
      "start_line": 165,
      "end_line": 187,
      "content_hash": "4d66e35f5bbbcc226c23db1cae811772c541b734",
      "content": "func TestStream_Collect_AccumulatesAllChunks(t *testing.T) {\n\tsseData := `data: {\"content\": \"Hello\"}\n\ndata: {\"content\": \" World\"}\n\ndata: [DONE]\n\n`\n\treader := strings.NewReader(sseData)\n\tctx := context.Background()\n\n\tstream := NewStream(ctx, reader, StreamTypeSSE)\n\n\tresult, err := stream.Collect()\n\tif err != nil {\n\t\tt.Fatalf(\"Collect failed: %v\", err)\n\t}\n\n\tif !strings.Contains(result, \"Hello\") || !strings.Contains(result, \"World\") {\n\t\tt.Errorf(\"Expected 'Hello World', got '%s'\", result)\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_Filter_OnlyMatchingChunks_188": {
      "name": "TestStream_Filter_OnlyMatchingChunks",
      "type": "function",
      "start_line": 188,
      "end_line": 219,
      "content_hash": "fed8309b76353f626561711855cbb70a5978e4d8",
      "content": "func TestStream_Filter_OnlyMatchingChunks(t *testing.T) {\n\tsseData := `data: {\"content\": \"Hello\"}\n\ndata: {\"content\": \" World\"}\n\ndata: {\"content\": \"!\"}\n\ndata: [DONE]\n\n`\n\treader := strings.NewReader(sseData)\n\tctx := context.Background()\n\n\tstream := NewStream(ctx, reader, StreamTypeSSE)\n\tdefer stream.Close()\n\n\t// Filter only chunks with more than 5 characters\n\tfiltered := stream.Filter(func(chunk *Chunk) bool {\n\t\treturn len(chunk.Data) > 2\n\t})\n\n\tvar count int\n\tfor range filtered.Chunks() {\n\t\tcount++\n\t}\n\n\t// Should filter out the \"!\" chunk and [DONE]\n\tif count != 2 {\n\t\tt.Errorf(\"Expected 2 filtered chunks, got %d\", count)\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_Map_TransformsChunks_220": {
      "name": "TestStream_Map_TransformsChunks",
      "type": "function",
      "start_line": 220,
      "end_line": 255,
      "content_hash": "56f5838d0fd5e6a84bad6183adb7f8bea1e86ebf",
      "content": "func TestStream_Map_TransformsChunks(t *testing.T) {\n\tsseData := `data: {\"content\": \"hello\"}\n\ndata: {\"content\": \" world\"}\n\ndata: [DONE]\n\n`\n\treader := strings.NewReader(sseData)\n\tctx := context.Background()\n\n\tstream := NewStream(ctx, reader, StreamTypeSSE)\n\tdefer stream.Close()\n\n\t// Map to uppercase\n\tmapped := stream.Map(func(chunk *Chunk) *Chunk {\n\t\tif chunk.Type == ChunkTypeData {\n\t\t\tchunk.Data = strings.ToUpper(chunk.Data)\n\t\t}\n\t\treturn chunk\n\t})\n\n\tvar collected strings.Builder\n\tfor chunk := range mapped.Chunks() {\n\t\tif chunk.Type == ChunkTypeDone {\n\t\t\tbreak\n\t\t}\n\t\tcollected.WriteString(chunk.Data)\n\t}\n\n\tresult := collected.String()\n\tif result != \"HELLO WORLD\" {\n\t\tt.Errorf(\"Expected 'HELLO WORLD', got '%s'\", result)\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_Tap_ObservesWithoutModifying_256": {
      "name": "TestStream_Tap_ObservesWithoutModifying",
      "type": "function",
      "start_line": 256,
      "end_line": 285,
      "content_hash": "49ead193106dfa0c7e71feb26576a76c8e5e356b",
      "content": "func TestStream_Tap_ObservesWithoutModifying(t *testing.T) {\n\tsseData := `data: {\"content\": \"Hello\"}\n\ndata: {\"content\": \" World\"}\n\ndata: [DONE]\n\n`\n\treader := strings.NewReader(sseData)\n\tctx := context.Background()\n\n\tstream := NewStream(ctx, reader, StreamTypeSSE)\n\tdefer stream.Close()\n\n\tvar observed []string\n\ttapped := stream.Tap(func(chunk *Chunk) {\n\t\tif chunk.Type == ChunkTypeData {\n\t\t\tobserved = append(observed, chunk.Data)\n\t\t}\n\t})\n\n\t// Consume the stream\n\tfor range tapped.Chunks() {\n\t}\n\n\tif len(observed) < 1 {\n\t\tt.Errorf(\"Expected to observe chunks, got %d\", len(observed))\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_Close_CancelsProcessing_286": {
      "name": "TestStream_Close_CancelsProcessing",
      "type": "function",
      "start_line": 286,
      "end_line": 311,
      "content_hash": "197a5f3ec004be2e6cb609bee28f28f6f5a2089a",
      "content": "func TestStream_Close_CancelsProcessing(t *testing.T) {\n\t// Slow reader that never completes\n\treader := &slowReader{delay: 100 * time.Millisecond}\n\tctx := context.Background()\n\n\tstream := NewStream(ctx, reader, StreamTypeHTTP)\n\n\t// Close immediately\n\tstream.Close()\n\n\t// Stream should be closed\n\tselect {\n\tcase _, ok := <-stream.Chunks():\n\t\tif ok {\n\t\t\tt.Error(\"Chunks channel still open after close\")\n\t\t}\n\tcase <-time.After(200 * time.Millisecond):\n\t\tt.Error(\"Stream did not close within timeout\")\n\t}\n\n\t// Error may be context.Canceled, which is expected\n\tif stream.Err() != nil && stream.Err() != context.Canceled {\n\t\tt.Errorf(\"Unexpected error: %v\", stream.Err())\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_ContextCancellation_StopsProcessing_312": {
      "name": "TestStream_ContextCancellation_StopsProcessing",
      "type": "function",
      "start_line": 312,
      "end_line": 336,
      "content_hash": "df6db257ceac181ff3f9dee353fa29f5281034d3",
      "content": "func TestStream_ContextCancellation_StopsProcessing(t *testing.T) {\n\treader := &slowReader{delay: 10 * time.Millisecond}\n\tctx, cancel := context.WithCancel(context.Background())\n\n\tstream := NewStream(ctx, reader, StreamTypeHTTP)\n\tdefer stream.Close()\n\n\t// Cancel context after a short delay\n\tgo func() {\n\t\ttime.Sleep(50 * time.Millisecond)\n\t\tcancel()\n\t}()\n\n\t// Try to read chunks\n\tchunkCount := 0\n\tfor range stream.Chunks() {\n\t\tchunkCount++\n\t}\n\n\t// Should stop due to cancellation\n\tif stream.Err() != context.Canceled {\n\t\tt.Errorf(\"Expected context canceled error, got %v\", stream.Err())\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_ExtractContent_HandlesMultipleFormats_337": {
      "name": "TestStream_ExtractContent_HandlesMultipleFormats",
      "type": "function",
      "start_line": 337,
      "end_line": 402,
      "content_hash": "916399554c21626f1c515e5e9de46c6db9c166e0",
      "content": "func TestStream_ExtractContent_HandlesMultipleFormats(t *testing.T) {\n\ttests := []struct {\n\t\tname     string\n\t\tdata     map[string]interface{}\n\t\texpected string\n\t}{\n\t\t{\n\t\t\tname: \"OpenAI format\",\n\t\t\tdata: map[string]interface{}{\n\t\t\t\t\"choices\": []interface{}{\n\t\t\t\t\tmap[string]interface{}{\n\t\t\t\t\t\t\"delta\": map[string]interface{}{\n\t\t\t\t\t\t\t\"content\": \"Hello OpenAI\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\texpected: \"Hello OpenAI\",\n\t\t},\n\t\t{\n\t\t\tname: \"Anthropic format\",\n\t\t\tdata: map[string]interface{}{\n\t\t\t\t\"delta\": map[string]interface{}{\n\t\t\t\t\t\"text\": \"Hello Anthropic\",\n\t\t\t\t},\n\t\t\t},\n\t\t\texpected: \"Hello Anthropic\",\n\t\t},\n\t\t{\n\t\t\tname: \"Google format\",\n\t\t\tdata: map[string]interface{}{\n\t\t\t\t\"candidates\": []interface{}{\n\t\t\t\t\tmap[string]interface{}{\n\t\t\t\t\t\t\"content\": map[string]interface{}{\n\t\t\t\t\t\t\t\"parts\": []interface{}{\n\t\t\t\t\t\t\t\tmap[string]interface{}{\n\t\t\t\t\t\t\t\t\t\"text\": \"Hello Google\",\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\texpected: \"Hello Google\",\n\t\t},\n\t\t{\n\t\t\tname: \"Generic text\",\n\t\t\tdata: map[string]interface{}{\n\t\t\t\t\"text\": \"Hello Generic\",\n\t\t\t},\n\t\t\texpected: \"Hello Generic\",\n\t\t},\n\t}\n\n\tstream := &Stream{}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tresult := stream.extractContent(tt.data)\n\t\t\tif result != tt.expected {\n\t\t\t\tt.Errorf(\"Expected '%s', got '%s'\", tt.expected, result)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// slowReader simulates a slow streaming response",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "struct_slowReader_403": {
      "name": "slowReader",
      "type": "struct",
      "start_line": 403,
      "end_line": 407,
      "content_hash": "bde041c9bd32a1de3a6c222bf4a12b4c0848b37d",
      "content": "type slowReader struct {\n\tdelay time.Duration\n\tcount int\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_Read_408": {
      "name": "Read",
      "type": "method",
      "start_line": 408,
      "end_line": 417,
      "content_hash": "29d752a4526ff752e137ab8ef27ce38240029057",
      "content": "func (sr *slowReader) Read(p []byte) (n int, err error) {\n\tif sr.count >= 10 {\n\t\treturn 0, io.EOF\n\t}\n\ttime.Sleep(sr.delay)\n\tcopy(p, []byte(\"chunk \"))\n\tsr.count++\n\treturn 6, nil\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_TestStream_ProcessWebSocket_418": {
      "name": "TestStream_ProcessWebSocket",
      "type": "function",
      "start_line": 418,
      "end_line": 437,
      "content_hash": "1306034679d313f7aee0f0eeebe3368888dc9796",
      "content": "func TestStream_ProcessWebSocket(t *testing.T) {\n\t// processWebSocket currently calls processHTTP internally\n\t// Test that it doesn't panic and works as expected\n\tresponseBody := `{\"choices\":[{\"delta\":{\"content\":\"Test\"}}]}`\n\tstream := NewStream(context.Background(), io.NopCloser(strings.NewReader(responseBody)), StreamTypeWebSocket)\n\tdefer stream.Close()\n\n\t// Collect chunks - just verify it works without panic\n\tvar chunkCount int\n\tfor chunk := range stream.Chunks() {\n\t\tif chunk.Type == ChunkTypeData {\n\t\t\tchunkCount++\n\t\t}\n\t}\n\n\t// WebSocket processing should work (at least 1 chunk)\n\tif chunkCount < 1 {\n\t\tt.Errorf(\"Expected at least 1 chunk from WebSocket processing, got %d\", chunkCount)\n\t}\n}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}