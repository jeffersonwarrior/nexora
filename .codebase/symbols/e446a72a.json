{
  "file_path": "/work/context-engine/scripts/rerank_tools/ab_test.py",
  "file_hash": "731da78d9b29dec274be8e6aa9474aca86d69bb2",
  "updated_at": "2025-12-26T17:34:23.661549",
  "symbols": {
    "class_VariantType_36": {
      "name": "VariantType",
      "type": "class",
      "start_line": 36,
      "end_line": 41,
      "content_hash": "6143ad4f2014ca978d9ffaed5f7999d7d3273a10",
      "content": "class VariantType(Enum):\n    \"\"\"Available reranker variants.\"\"\"\n    BASELINE = \"baseline\"  # No reranking, use initial scores\n    ONNX = \"onnx\"  # ONNX cross-encoder\n    RECURSIVE = \"recursive\"  # TRM-inspired recursive reranker\n    RECURSIVE_ONNX = \"recursive_onnx\"  # ONNX + recursive refinement",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_ABMetric_45": {
      "name": "ABMetric",
      "type": "class",
      "start_line": 45,
      "end_line": 54,
      "content_hash": "b33220f981110ce9660420891d2986d77a91067e",
      "content": "class ABMetric:\n    \"\"\"Single metric observation.\"\"\"\n    session_id: str\n    variant: str\n    timestamp: float\n    latency_ms: float = 0.0\n    clicked_rank: Optional[int] = None  # 1-indexed rank of clicked result\n    num_results: int = 0\n    query_length: int = 0\n    custom: Dict[str, Any] = field(default_factory=dict)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_VariantStats_58": {
      "name": "VariantStats",
      "type": "class",
      "start_line": 58,
      "end_line": 108,
      "content_hash": "a1a96538a08d2f9d5b8bffc78b9510fbf087de7b",
      "content": "class VariantStats:\n    \"\"\"Aggregated statistics for a variant.\"\"\"\n    variant: str\n    n_observations: int = 0\n    latency_sum: float = 0.0\n    latency_sq_sum: float = 0.0  # For std calculation\n    mrr_sum: float = 0.0  # Mean Reciprocal Rank\n    clicks_at_1: int = 0\n    clicks_at_3: int = 0\n    clicks_at_5: int = 0\n    total_clicks: int = 0\n\n    def add_observation(self, metric: ABMetric):\n        \"\"\"Add a single observation.\"\"\"\n        self.n_observations += 1\n        self.latency_sum += metric.latency_ms\n        self.latency_sq_sum += metric.latency_ms ** 2\n\n        if metric.clicked_rank is not None:\n            self.total_clicks += 1\n            self.mrr_sum += 1.0 / metric.clicked_rank\n            if metric.clicked_rank == 1:\n                self.clicks_at_1 += 1\n            if metric.clicked_rank <= 3:\n                self.clicks_at_3 += 1\n            if metric.clicked_rank <= 5:\n                self.clicks_at_5 += 1\n\n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary statistics.\"\"\"\n        if self.n_observations == 0:\n            return {\"variant\": self.variant, \"n_observations\": 0}\n\n        n = self.n_observations\n        mean_latency = self.latency_sum / n\n        var_latency = (self.latency_sq_sum / n) - (mean_latency ** 2)\n        std_latency = var_latency ** 0.5 if var_latency > 0 else 0\n\n        mrr = self.mrr_sum / self.total_clicks if self.total_clicks > 0 else 0\n\n        return {\n            \"variant\": self.variant,\n            \"n_observations\": n,\n            \"latency_ms_mean\": round(mean_latency, 2),\n            \"latency_ms_std\": round(std_latency, 2),\n            \"mrr\": round(mrr, 4),\n            \"click_rate_at_1\": round(self.clicks_at_1 / n, 4) if n > 0 else 0,\n            \"click_rate_at_3\": round(self.clicks_at_3 / n, 4) if n > 0 else 0,\n            \"click_rate_at_5\": round(self.clicks_at_5 / n, 4) if n > 0 else 0,\n            \"total_clicks\": self.total_clicks,\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_add_observation_70": {
      "name": "add_observation",
      "type": "method",
      "start_line": 70,
      "end_line": 84,
      "content_hash": "e54e0dd5e7372654ca37cad93144086b51951739",
      "content": "    def add_observation(self, metric: ABMetric):\n        \"\"\"Add a single observation.\"\"\"\n        self.n_observations += 1\n        self.latency_sum += metric.latency_ms\n        self.latency_sq_sum += metric.latency_ms ** 2\n\n        if metric.clicked_rank is not None:\n            self.total_clicks += 1\n            self.mrr_sum += 1.0 / metric.clicked_rank\n            if metric.clicked_rank == 1:\n                self.clicks_at_1 += 1\n            if metric.clicked_rank <= 3:\n                self.clicks_at_3 += 1\n            if metric.clicked_rank <= 5:\n                self.clicks_at_5 += 1",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_summary_86": {
      "name": "get_summary",
      "type": "method",
      "start_line": 86,
      "end_line": 108,
      "content_hash": "6545adad6a55d26b1aa650e15c60dd27bd201cb0",
      "content": "    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary statistics.\"\"\"\n        if self.n_observations == 0:\n            return {\"variant\": self.variant, \"n_observations\": 0}\n\n        n = self.n_observations\n        mean_latency = self.latency_sum / n\n        var_latency = (self.latency_sq_sum / n) - (mean_latency ** 2)\n        std_latency = var_latency ** 0.5 if var_latency > 0 else 0\n\n        mrr = self.mrr_sum / self.total_clicks if self.total_clicks > 0 else 0\n\n        return {\n            \"variant\": self.variant,\n            \"n_observations\": n,\n            \"latency_ms_mean\": round(mean_latency, 2),\n            \"latency_ms_std\": round(std_latency, 2),\n            \"mrr\": round(mrr, 4),\n            \"click_rate_at_1\": round(self.clicks_at_1 / n, 4) if n > 0 else 0,\n            \"click_rate_at_3\": round(self.clicks_at_3 / n, 4) if n > 0 else 0,\n            \"click_rate_at_5\": round(self.clicks_at_5 / n, 4) if n > 0 else 0,\n            \"total_clicks\": self.total_clicks,\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_RerankerVariant_111": {
      "name": "RerankerVariant",
      "type": "class",
      "start_line": 111,
      "end_line": 123,
      "content_hash": "19ff3740c86175a587f0f5a3b0ed6daaff4397cb",
      "content": "class RerankerVariant:\n    \"\"\"Wrapper for a reranker variant with timing.\"\"\"\n\n    def __init__(self, variant_type: VariantType, rerank_fn: Callable):\n        self.variant_type = variant_type\n        self.rerank_fn = rerank_fn\n\n    def rerank(self, query: str, candidates: List[Dict[str, Any]], **kwargs) -> tuple:\n        \"\"\"Rerank candidates and return (results, latency_ms).\"\"\"\n        start = time.perf_counter()\n        results = self.rerank_fn(query, candidates, **kwargs)\n        latency_ms = (time.perf_counter() - start) * 1000\n        return results, latency_ms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___114": {
      "name": "__init__",
      "type": "method",
      "start_line": 114,
      "end_line": 116,
      "content_hash": "512c064d65639e1bbdfcacc67cda313dc4c96ca8",
      "content": "    def __init__(self, variant_type: VariantType, rerank_fn: Callable):\n        self.variant_type = variant_type\n        self.rerank_fn = rerank_fn",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_rerank_118": {
      "name": "rerank",
      "type": "method",
      "start_line": 118,
      "end_line": 123,
      "content_hash": "e5dfdb8e2eeb4901a97b400963a5332a53b501b7",
      "content": "    def rerank(self, query: str, candidates: List[Dict[str, Any]], **kwargs) -> tuple:\n        \"\"\"Rerank candidates and return (results, latency_ms).\"\"\"\n        start = time.perf_counter()\n        results = self.rerank_fn(query, candidates, **kwargs)\n        latency_ms = (time.perf_counter() - start) * 1000\n        return results, latency_ms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_ABTestManager_126": {
      "name": "ABTestManager",
      "type": "class",
      "start_line": 126,
      "end_line": 311,
      "content_hash": "81baf0c390af9ffbf6c4cb6a5e6e9af75b54f76d",
      "content": "class ABTestManager:\n    \"\"\"\n    Manages A/B testing for reranker variants.\n\n    Features:\n    - Consistent hashing for session assignment (same session = same variant)\n    - Thread-safe metric logging\n    - Persistent storage of metrics\n    - Statistical comparison utilities\n    \"\"\"\n\n    def __init__(\n        self,\n        variants: Optional[List[VariantType]] = None,\n        weights: Optional[List[float]] = None,\n        log_path: str = \"data/ab_test_metrics.jsonl\",\n        experiment_id: Optional[str] = None,\n    ):\n        self.variants = variants or [VariantType.BASELINE, VariantType.RECURSIVE]\n        self.weights = weights or [1.0] * len(self.variants)\n        self.log_path = Path(log_path)\n        self.experiment_id = experiment_id or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n        # Normalize weights (guard against zero/negative total)\n        total_weight = sum(self.weights)\n        if total_weight <= 0:\n            total_weight = len(self.weights)  # Fallback to uniform\n            self.weights = [1.0] * len(self.variants)\n        self.cumulative_weights = []\n        cumsum = 0.0\n        for w in self.weights:\n            cumsum += w / total_weight\n            self.cumulative_weights.append(cumsum)\n\n        # In-memory stats\n        self.stats: Dict[str, VariantStats] = {\n            v.value: VariantStats(variant=v.value) for v in self.variants\n        }\n\n        # Thread safety\n        self._lock = threading.Lock()\n\n        # Variant implementations\n        self._variant_impls: Dict[VariantType, RerankerVariant] = {}\n        self._init_variants()\n\n    def _init_variants(self):\n        \"\"\"Initialize reranker implementations for each variant.\"\"\"\n        # Baseline: just sort by initial score\n        def baseline_rerank(query, candidates, **kwargs):\n            return sorted(candidates, key=lambda x: x.get(\"score\", 0), reverse=True)\n\n        self._variant_impls[VariantType.BASELINE] = RerankerVariant(\n            VariantType.BASELINE, baseline_rerank\n        )\n\n        # Recursive reranker\n        try:\n            try:\n                from scripts.rerank_recursive import rerank_recursive\n            except ImportError:\n                from rerank_recursive import rerank_recursive\n\n            self._variant_impls[VariantType.RECURSIVE] = RerankerVariant(\n                VariantType.RECURSIVE,\n                lambda q, c, **kw: rerank_recursive(q, c, n_iterations=3)\n            )\n        except ImportError:\n            pass\n\n        # ONNX reranker\n        try:\n            try:\n                from scripts.rerank_local import rerank_in_process\n            except ImportError:\n                from rerank_local import rerank_in_process\n\n            self._variant_impls[VariantType.ONNX] = RerankerVariant(\n                VariantType.ONNX,\n                lambda q, c, **kw: rerank_in_process(q, c, limit=len(c))\n            )\n        except ImportError:\n            pass\n\n    def _hash_to_bucket(self, session_id: str) -> float:\n        \"\"\"Hash session ID to a value in [0, 1) for consistent bucketing.\"\"\"\n        h = hashlib.md5(f\"{self.experiment_id}:{session_id}\".encode()).hexdigest()\n        return int(h[:8], 16) / (16 ** 8)\n\n    def get_variant_type(self, session_id: str) -> VariantType:\n        \"\"\"Get the variant type for a session (consistent assignment).\"\"\"\n        bucket = self._hash_to_bucket(session_id)\n        for i, threshold in enumerate(self.cumulative_weights):\n            if bucket < threshold:\n                return self.variants[i]\n        return self.variants[-1]\n\n    def get_variant(self, session_id: str) -> RerankerVariant:\n        \"\"\"Get the reranker variant for a session.\"\"\"\n        variant_type = self.get_variant_type(session_id)\n\n        if variant_type in self._variant_impls:\n            return self._variant_impls[variant_type]\n\n        # Fall back to baseline if variant not available\n        return self._variant_impls.get(VariantType.BASELINE,\n            RerankerVariant(VariantType.BASELINE, lambda q, c, **kw: c))\n\n    def log_metrics(\n        self,\n        session_id: str,\n        latency_ms: float = 0.0,\n        clicked_rank: Optional[int] = None,\n        num_results: int = 0,\n        query_length: int = 0,\n        **custom\n    ):\n        \"\"\"Log metrics for an observation.\"\"\"\n        variant_type = self.get_variant_type(session_id)\n\n        metric = ABMetric(\n            session_id=session_id,\n            variant=variant_type.value,\n            timestamp=time.time(),\n            latency_ms=latency_ms,\n            clicked_rank=clicked_rank,\n            num_results=num_results,\n            query_length=query_length,\n            custom=custom,\n        )\n\n        with self._lock:\n            # Update in-memory stats\n            if variant_type.value in self.stats:\n                self.stats[variant_type.value].add_observation(metric)\n\n            # Append to log file\n            self._append_metric(metric)\n\n    def _append_metric(self, metric: ABMetric):\n        \"\"\"Append metric to log file.\"\"\"\n        self.log_path.parent.mkdir(parents=True, exist_ok=True)\n\n        record = {\n            \"experiment_id\": self.experiment_id,\n            \"session_id\": metric.session_id,\n            \"variant\": metric.variant,\n            \"timestamp\": metric.timestamp,\n            \"latency_ms\": metric.latency_ms,\n            \"clicked_rank\": metric.clicked_rank,\n            \"num_results\": metric.num_results,\n            \"query_length\": metric.query_length,\n            **metric.custom,\n        }\n\n        with open(self.log_path, \"a\") as f:\n            f.write(json.dumps(record) + \"\\n\")\n\n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary statistics for all variants.\"\"\"\n        return {\n            \"experiment_id\": self.experiment_id,\n            \"variants\": {v: self.stats[v].get_summary() for v in self.stats},\n        }\n\n    def print_summary(self):\n        \"\"\"Print formatted summary.\"\"\"\n        summary = self.get_summary()\n\n        print(\"\\n\" + \"=\" * 70)\n        print(f\"A/B TEST RESULTS: {summary['experiment_id']}\")\n        print(\"=\" * 70)\n\n        print(f\"\\n{'Variant':<15} {'N':<8} {'Latency (ms)':<15} {'MRR':<10} {'CTR@1':<10}\")\n        print(\"-\" * 70)\n\n        for variant_name, stats in summary[\"variants\"].items():\n            n = stats.get(\"n_observations\", 0)\n            lat = f\"{stats.get('latency_ms_mean', 0):.1f} \u00b1 {stats.get('latency_ms_std', 0):.1f}\"\n            mrr = f\"{stats.get('mrr', 0):.4f}\"\n            ctr = f\"{stats.get('click_rate_at_1', 0):.2%}\"\n\n            print(f\"{variant_name:<15} {n:<8} {lat:<15} {mrr:<10} {ctr:<10}\")\n\n        print(\"-\" * 70)\n        print()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___137": {
      "name": "__init__",
      "type": "method",
      "start_line": 137,
      "end_line": 170,
      "content_hash": "f99feeb40632e020bbd8f8308e4f31ea3d75b87b",
      "content": "    def __init__(\n        self,\n        variants: Optional[List[VariantType]] = None,\n        weights: Optional[List[float]] = None,\n        log_path: str = \"data/ab_test_metrics.jsonl\",\n        experiment_id: Optional[str] = None,\n    ):\n        self.variants = variants or [VariantType.BASELINE, VariantType.RECURSIVE]\n        self.weights = weights or [1.0] * len(self.variants)\n        self.log_path = Path(log_path)\n        self.experiment_id = experiment_id or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n        # Normalize weights (guard against zero/negative total)\n        total_weight = sum(self.weights)\n        if total_weight <= 0:\n            total_weight = len(self.weights)  # Fallback to uniform\n            self.weights = [1.0] * len(self.variants)\n        self.cumulative_weights = []\n        cumsum = 0.0\n        for w in self.weights:\n            cumsum += w / total_weight\n            self.cumulative_weights.append(cumsum)\n\n        # In-memory stats\n        self.stats: Dict[str, VariantStats] = {\n            v.value: VariantStats(variant=v.value) for v in self.variants\n        }\n\n        # Thread safety\n        self._lock = threading.Lock()\n\n        # Variant implementations\n        self._variant_impls: Dict[VariantType, RerankerVariant] = {}\n        self._init_variants()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__init_variants_172": {
      "name": "_init_variants",
      "type": "method",
      "start_line": 172,
      "end_line": 208,
      "content_hash": "2f88e57aefdbc36f6322009a245623e25ee34008",
      "content": "    def _init_variants(self):\n        \"\"\"Initialize reranker implementations for each variant.\"\"\"\n        # Baseline: just sort by initial score\n        def baseline_rerank(query, candidates, **kwargs):\n            return sorted(candidates, key=lambda x: x.get(\"score\", 0), reverse=True)\n\n        self._variant_impls[VariantType.BASELINE] = RerankerVariant(\n            VariantType.BASELINE, baseline_rerank\n        )\n\n        # Recursive reranker\n        try:\n            try:\n                from scripts.rerank_recursive import rerank_recursive\n            except ImportError:\n                from rerank_recursive import rerank_recursive\n\n            self._variant_impls[VariantType.RECURSIVE] = RerankerVariant(\n                VariantType.RECURSIVE,\n                lambda q, c, **kw: rerank_recursive(q, c, n_iterations=3)\n            )\n        except ImportError:\n            pass\n\n        # ONNX reranker\n        try:\n            try:\n                from scripts.rerank_local import rerank_in_process\n            except ImportError:\n                from rerank_local import rerank_in_process\n\n            self._variant_impls[VariantType.ONNX] = RerankerVariant(\n                VariantType.ONNX,\n                lambda q, c, **kw: rerank_in_process(q, c, limit=len(c))\n            )\n        except ImportError:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_baseline_rerank_175": {
      "name": "baseline_rerank",
      "type": "method",
      "start_line": 175,
      "end_line": 176,
      "content_hash": "2e5faca854217b6352bc111495b7483630ccfba7",
      "content": "        def baseline_rerank(query, candidates, **kwargs):\n            return sorted(candidates, key=lambda x: x.get(\"score\", 0), reverse=True)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__hash_to_bucket_210": {
      "name": "_hash_to_bucket",
      "type": "method",
      "start_line": 210,
      "end_line": 213,
      "content_hash": "47c91c4608412e0d59a24d36fb33172a86facc3a",
      "content": "    def _hash_to_bucket(self, session_id: str) -> float:\n        \"\"\"Hash session ID to a value in [0, 1) for consistent bucketing.\"\"\"\n        h = hashlib.md5(f\"{self.experiment_id}:{session_id}\".encode()).hexdigest()\n        return int(h[:8], 16) / (16 ** 8)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_variant_type_215": {
      "name": "get_variant_type",
      "type": "method",
      "start_line": 215,
      "end_line": 221,
      "content_hash": "6067ab74c8763e6f290891991becc2fa9179326f",
      "content": "    def get_variant_type(self, session_id: str) -> VariantType:\n        \"\"\"Get the variant type for a session (consistent assignment).\"\"\"\n        bucket = self._hash_to_bucket(session_id)\n        for i, threshold in enumerate(self.cumulative_weights):\n            if bucket < threshold:\n                return self.variants[i]\n        return self.variants[-1]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_variant_223": {
      "name": "get_variant",
      "type": "method",
      "start_line": 223,
      "end_line": 232,
      "content_hash": "6ba01008f8c4291e1482021d4faba87f22ff53a1",
      "content": "    def get_variant(self, session_id: str) -> RerankerVariant:\n        \"\"\"Get the reranker variant for a session.\"\"\"\n        variant_type = self.get_variant_type(session_id)\n\n        if variant_type in self._variant_impls:\n            return self._variant_impls[variant_type]\n\n        # Fall back to baseline if variant not available\n        return self._variant_impls.get(VariantType.BASELINE,\n            RerankerVariant(VariantType.BASELINE, lambda q, c, **kw: c))",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_log_metrics_234": {
      "name": "log_metrics",
      "type": "method",
      "start_line": 234,
      "end_line": 263,
      "content_hash": "730cb0a6163039ad3a29fd305d70d6fb0ff059c9",
      "content": "    def log_metrics(\n        self,\n        session_id: str,\n        latency_ms: float = 0.0,\n        clicked_rank: Optional[int] = None,\n        num_results: int = 0,\n        query_length: int = 0,\n        **custom\n    ):\n        \"\"\"Log metrics for an observation.\"\"\"\n        variant_type = self.get_variant_type(session_id)\n\n        metric = ABMetric(\n            session_id=session_id,\n            variant=variant_type.value,\n            timestamp=time.time(),\n            latency_ms=latency_ms,\n            clicked_rank=clicked_rank,\n            num_results=num_results,\n            query_length=query_length,\n            custom=custom,\n        )\n\n        with self._lock:\n            # Update in-memory stats\n            if variant_type.value in self.stats:\n                self.stats[variant_type.value].add_observation(metric)\n\n            # Append to log file\n            self._append_metric(metric)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__append_metric_265": {
      "name": "_append_metric",
      "type": "method",
      "start_line": 265,
      "end_line": 282,
      "content_hash": "994434784ce641b33650f358baf1f4c304dacbb5",
      "content": "    def _append_metric(self, metric: ABMetric):\n        \"\"\"Append metric to log file.\"\"\"\n        self.log_path.parent.mkdir(parents=True, exist_ok=True)\n\n        record = {\n            \"experiment_id\": self.experiment_id,\n            \"session_id\": metric.session_id,\n            \"variant\": metric.variant,\n            \"timestamp\": metric.timestamp,\n            \"latency_ms\": metric.latency_ms,\n            \"clicked_rank\": metric.clicked_rank,\n            \"num_results\": metric.num_results,\n            \"query_length\": metric.query_length,\n            **metric.custom,\n        }\n\n        with open(self.log_path, \"a\") as f:\n            f.write(json.dumps(record) + \"\\n\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_summary_284": {
      "name": "get_summary",
      "type": "method",
      "start_line": 284,
      "end_line": 289,
      "content_hash": "0994c8f81fb2a1278e5358560228b1670abdedc9",
      "content": "    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary statistics for all variants.\"\"\"\n        return {\n            \"experiment_id\": self.experiment_id,\n            \"variants\": {v: self.stats[v].get_summary() for v in self.stats},\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_print_summary_291": {
      "name": "print_summary",
      "type": "method",
      "start_line": 291,
      "end_line": 311,
      "content_hash": "b21efb3766fee08e69ff5117eeb926daf78c98a5",
      "content": "    def print_summary(self):\n        \"\"\"Print formatted summary.\"\"\"\n        summary = self.get_summary()\n\n        print(\"\\n\" + \"=\" * 70)\n        print(f\"A/B TEST RESULTS: {summary['experiment_id']}\")\n        print(\"=\" * 70)\n\n        print(f\"\\n{'Variant':<15} {'N':<8} {'Latency (ms)':<15} {'MRR':<10} {'CTR@1':<10}\")\n        print(\"-\" * 70)\n\n        for variant_name, stats in summary[\"variants\"].items():\n            n = stats.get(\"n_observations\", 0)\n            lat = f\"{stats.get('latency_ms_mean', 0):.1f} \u00b1 {stats.get('latency_ms_std', 0):.1f}\"\n            mrr = f\"{stats.get('mrr', 0):.4f}\"\n            ctr = f\"{stats.get('click_rate_at_1', 0):.2%}\"\n\n            print(f\"{variant_name:<15} {n:<8} {lat:<15} {mrr:<10} {ctr:<10}\")\n\n        print(\"-\" * 70)\n        print()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_simulate_ab_test_314": {
      "name": "simulate_ab_test",
      "type": "function",
      "start_line": 314,
      "end_line": 356,
      "content_hash": "8c439d580927920a5382bd4944305190207ff342",
      "content": "def simulate_ab_test(n_sessions: int = 100, n_queries_per_session: int = 5):\n    \"\"\"Simulate an A/B test with synthetic data.\"\"\"\n    import random\n\n    ab = ABTestManager(\n        variants=[VariantType.BASELINE, VariantType.RECURSIVE],\n        weights=[0.5, 0.5],\n    )\n\n    print(f\"Simulating A/B test with {n_sessions} sessions...\")\n\n    for session_idx in range(n_sessions):\n        session_id = f\"session_{session_idx}\"\n        variant = ab.get_variant(session_id)\n\n        for query_idx in range(n_queries_per_session):\n            # Generate fake candidates\n            candidates = [\n                {\"path\": f\"file_{i}.py\", \"score\": random.random()}\n                for i in range(10)\n            ]\n\n            # Rerank\n            results, latency_ms = variant.rerank(\"test query\", candidates)\n\n            # Simulate click (higher probability for top results)\n            clicked_rank = None\n            for rank, result in enumerate(results[:5], 1):\n                if random.random() < 0.3 / rank:  # Decreasing probability\n                    clicked_rank = rank\n                    break\n\n            # Log metrics\n            ab.log_metrics(\n                session_id=session_id,\n                latency_ms=latency_ms,\n                clicked_rank=clicked_rank,\n                num_results=len(results),\n                query_length=len(\"test query\"),\n            )\n\n    ab.print_summary()\n    return ab",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}