{
  "file_path": "/work/external-deps/Context-Engine/scripts/ast_analyzer.py",
  "file_hash": "6e518f2c20c56148b8137b08742daa5069471069",
  "updated_at": "2025-12-26T17:34:20.687444",
  "symbols": {
    "function__load_ts_language_31": {
      "name": "_load_ts_language",
      "type": "function",
      "start_line": 31,
      "end_line": 59,
      "content_hash": "05e09322b907a239e1a325d52e62f64ce8e3518b",
      "content": "    def _load_ts_language(mod: Any, *, preferred: list[str] | None = None) -> Any | None:\n        \"\"\"Return a tree-sitter Language instance from a per-language package.\n\n        Different packages expose different entrypoints (e.g. language(),\n        language_typescript(), language_tsx()).\n        \"\"\"\n        preferred = preferred or []\n        candidates: list[Any] = []\n        if getattr(mod, \"language\", None) is not None and callable(getattr(mod, \"language\")):\n            candidates.append(getattr(mod, \"language\"))\n        for name in preferred:\n            fn = getattr(mod, name, None)\n            if fn is not None and callable(fn):\n                candidates.append(fn)\n        # Last resort: scan for any callable language* attribute\n        for name in dir(mod):\n            if not name.startswith(\"language\"):\n                continue\n            fn = getattr(mod, name, None)\n            if fn is not None and callable(fn):\n                candidates.append(fn)\n\n        for fn in candidates:\n            try:\n                raw_lang = fn()\n                return raw_lang if isinstance(raw_lang, Language) else Language(raw_lang)\n            except Exception:\n                continue\n        return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_CodeSymbol_116": {
      "name": "CodeSymbol",
      "type": "class",
      "start_line": 116,
      "end_line": 128,
      "content_hash": "48dc3a6bdc84b101a31747ce05e1b0356d36aa22",
      "content": "class CodeSymbol:\n    \"\"\"Represents a code symbol (function, class, method, etc).\"\"\"\n    name: str\n    kind: str  # function, class, method, interface, etc.\n    start_line: int\n    end_line: int\n    path: Optional[str] = None  # Fully qualified path (e.g., \"MyClass.method\")\n    docstring: Optional[str] = None\n    signature: Optional[str] = None\n    decorators: List[str] = field(default_factory=list)\n    parent: Optional[str] = None  # Parent class/module\n    complexity: int = 0  # Cyclomatic complexity estimate\n    content_hash: Optional[str] = None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_CallReference_132": {
      "name": "CallReference",
      "type": "class",
      "start_line": 132,
      "end_line": 137,
      "content_hash": "2c5f25326ca61a6499cda987767d4e5e787443d2",
      "content": "class CallReference:\n    \"\"\"Represents a function/method call.\"\"\"\n    caller: str  # Who is calling\n    callee: str  # What is being called\n    line: int\n    context: str  # e.g., \"function\", \"method\", \"module\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_ImportReference_141": {
      "name": "ImportReference",
      "type": "class",
      "start_line": 141,
      "end_line": 147,
      "content_hash": "3cd3cd6db108564875e47f4941eed35fccbd48be",
      "content": "class ImportReference:\n    \"\"\"Represents an import statement.\"\"\"\n    module: str\n    names: List[str]  # Specific imports (empty if import *)\n    line: int\n    alias: Optional[str] = None\n    is_from: bool = False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_CodeContext_151": {
      "name": "CodeContext",
      "type": "class",
      "start_line": 151,
      "end_line": 160,
      "content_hash": "8d0af218411fca1f6bc56a53bda02949cc69e570",
      "content": "class CodeContext:\n    \"\"\"Complete context for a code chunk.\"\"\"\n    chunk_text: str\n    start_line: int\n    end_line: int\n    symbols: List[CodeSymbol]\n    imports: List[ImportReference]\n    calls: List[CallReference]\n    dependencies: Set[str]  # Modules/files this depends on\n    is_semantic_unit: bool = True  # True if chunk respects boundaries",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_ASTAnalyzer_163": {
      "name": "ASTAnalyzer",
      "type": "class",
      "start_line": 163,
      "end_line": 794,
      "content_hash": "608d5c3a4742ef6d388ea59bf22adf893457aa9f",
      "content": "class ASTAnalyzer:\n    \"\"\"\n    Advanced AST-based code analyzer for semantic understanding.\n    \n    Features:\n    - Language-aware symbol extraction\n    - Call graph construction\n    - Dependency tracking\n    - Semantic chunking (preserve boundaries)\n    - Cross-reference analysis\n    \"\"\"\n    \n    def __init__(self, use_tree_sitter: bool = True):\n        \"\"\"\n        Initialize AST analyzer.\n        \n        Args:\n            use_tree_sitter: Use tree-sitter when available (fallback to ast module)\n        \"\"\"\n        self.use_tree_sitter = use_tree_sitter and _TS_AVAILABLE\n        self._parsers: Dict[str, Any] = {}\n        \n        # Language support matrix\n        self.supported_languages = {\n            \"python\": {\"ast\": True, \"tree_sitter\": True},\n            \"javascript\": {\"ast\": False, \"tree_sitter\": True},\n            \"typescript\": {\"ast\": False, \"tree_sitter\": True},\n            \"java\": {\"ast\": False, \"tree_sitter\": False},\n            \"go\": {\"ast\": False, \"tree_sitter\": False},\n            \"rust\": {\"ast\": False, \"tree_sitter\": False},\n            \"c\": {\"ast\": False, \"tree_sitter\": False},\n            \"cpp\": {\"ast\": False, \"tree_sitter\": False},\n        }\n        \n        logger.info(f\"ASTAnalyzer initialized: tree_sitter={self.use_tree_sitter}\")\n    \n    def analyze_file(\n        self, file_path: str, language: str, content: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Analyze a source file and extract semantic information.\n        \n        Args:\n            file_path: Path to the file\n            language: Programming language\n            content: Optional file content (if not provided, read from file)\n        \n        Returns:\n            Dict with symbols, imports, calls, and dependencies\n        \"\"\"\n        if content is None:\n            try:\n                content = Path(file_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n            except Exception as e:\n                logger.error(f\"Failed to read {file_path}: {e}\")\n                return self._empty_analysis()\n        \n        # Route to appropriate analyzer\n        if language == \"python\":\n            return self._analyze_python(content, file_path)\n        elif language in (\"javascript\", \"typescript\") and self.use_tree_sitter:\n            return self._analyze_js_ts(content, file_path, language)\n        else:\n            # Fallback to regex-based analysis\n            return self._analyze_generic(content, file_path, language)\n    \n    def extract_symbols_with_context(\n        self, file_path: str, language: str, content: Optional[str] = None\n    ) -> List[CodeSymbol]:\n        \"\"\"\n        Extract code symbols with full context (docstrings, signatures, etc).\n        \n        Returns:\n            List of CodeSymbol objects with rich metadata\n        \"\"\"\n        analysis = self.analyze_file(file_path, language, content)\n        return analysis.get(\"symbols\", [])\n    \n    def chunk_semantic(\n        self,\n        content: str,\n        language: str,\n        max_lines: int = 120,\n        overlap_lines: int = 20,\n        preserve_boundaries: bool = True\n    ) -> List[CodeContext]:\n        \"\"\"\n        Chunk code semantically, respecting function/class boundaries.\n        \n        Args:\n            content: Source code content\n            language: Programming language\n            max_lines: Maximum lines per chunk\n            overlap_lines: Overlap between chunks\n            preserve_boundaries: Try to keep complete functions/classes together\n        \n        Returns:\n            List of CodeContext objects with semantic chunks\n        \"\"\"\n        if not preserve_boundaries:\n            # Fall back to line-based chunking\n            return self._chunk_lines_simple(content, max_lines, overlap_lines)\n        \n        # Extract symbols\n        analysis = self.analyze_file(\"\", language, content)\n        symbols = analysis.get(\"symbols\", [])\n        \n        if not symbols:\n            # No symbols found, use line-based\n            return self._chunk_lines_simple(content, max_lines, overlap_lines)\n        \n        lines = content.splitlines()\n        chunks = []\n        \n        # Sort symbols by start line\n        symbols.sort(key=lambda s: s.start_line)\n        \n        i = 0\n        while i < len(symbols):\n            symbol = symbols[i]\n            \n            # Calculate chunk extent\n            chunk_start = symbol.start_line\n            chunk_end = symbol.end_line\n            symbols_in_chunk = [symbol]\n            \n            # Try to include adjacent small symbols\n            j = i + 1\n            while j < len(symbols):\n                next_symbol = symbols[j]\n                potential_end = next_symbol.end_line\n                \n                # Check if adding next symbol exceeds max_lines\n                if potential_end - chunk_start > max_lines:\n                    break\n                \n                # Check if next symbol is close enough (within overlap)\n                if next_symbol.start_line - chunk_end > overlap_lines:\n                    break\n                \n                # Include this symbol\n                chunk_end = potential_end\n                symbols_in_chunk.append(next_symbol)\n                j += 1\n            \n            # Create chunk\n            chunk_lines = lines[chunk_start - 1:chunk_end]\n            chunk_text = \"\\n\".join(chunk_lines)\n            \n            # Extract chunk-specific imports and calls\n            chunk_imports = [\n                imp for imp in analysis.get(\"imports\", [])\n                if chunk_start <= imp.line <= chunk_end\n            ]\n            chunk_calls = [\n                call for call in analysis.get(\"calls\", [])\n                if chunk_start <= call.line <= chunk_end\n            ]\n            \n            context = CodeContext(\n                chunk_text=chunk_text,\n                start_line=chunk_start,\n                end_line=chunk_end,\n                symbols=symbols_in_chunk,\n                imports=chunk_imports,\n                calls=chunk_calls,\n                dependencies=self._extract_dependencies(chunk_imports, chunk_calls),\n                is_semantic_unit=True\n            )\n            \n            chunks.append(context)\n            i = j if j > i else i + 1\n        \n        # Handle code not covered by symbols (module-level code, etc)\n        self._fill_gaps(chunks, lines, max_lines, overlap_lines, analysis)\n        \n        return chunks\n    \n    def build_call_graph(self, file_path: str, language: str) -> Dict[str, List[str]]:\n        \"\"\"\n        Build call graph: mapping of caller -> list of callees.\n        \n        Returns:\n            Dict mapping function names to list of functions they call\n        \"\"\"\n        analysis = self.analyze_file(file_path, language)\n        \n        call_graph = defaultdict(list)\n        for call in analysis.get(\"calls\", []):\n            call_graph[call.caller].append(call.callee)\n        \n        return dict(call_graph)\n    \n    def extract_dependencies(\n        self, file_path: str, language: str\n    ) -> Dict[str, List[str]]:\n        \"\"\"\n        Extract file dependencies (imports, includes).\n        \n        Returns:\n            Dict with 'modules' (external) and 'local' (same project) imports\n        \"\"\"\n        analysis = self.analyze_file(file_path, language)\n        imports = analysis.get(\"imports\", [])\n        \n        modules = []\n        local = []\n        \n        for imp in imports:\n            # Simple heuristic: relative imports or without dots are likely local\n            if imp.module.startswith(\".\") or \"/\" in imp.module:\n                local.append(imp.module)\n            else:\n                modules.append(imp.module)\n        \n        return {\n            \"modules\": list(set(modules)),\n            \"local\": list(set(local))\n        }\n    \n    # ---- Python-specific analysis (using ast module) ----\n    \n    def _analyze_python(self, content: str, file_path: str) -> Dict[str, Any]:\n        \"\"\"Analyze Python code using ast module.\"\"\"\n        try:\n            tree = ast.parse(content)\n        except SyntaxError as e:\n            logger.warning(f\"Python syntax error in {file_path}: {e}\")\n            return self._empty_analysis()\n        \n        symbols = []\n        imports = []\n        calls = []\n        \n        # Extract symbols\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                symbol = self._extract_python_function(node, content)\n                symbols.append(symbol)\n            elif isinstance(node, ast.ClassDef):\n                symbol = self._extract_python_class(node, content)\n                symbols.append(symbol)\n        \n        # Extract imports\n        for node in ast.walk(tree):\n            if isinstance(node, ast.Import):\n                for alias in node.names:\n                    imports.append(ImportReference(\n                        module=alias.name,\n                        names=[],\n                        alias=alias.asname,\n                        line=node.lineno,\n                        is_from=False\n                    ))\n            elif isinstance(node, ast.ImportFrom):\n                names = [alias.name for alias in node.names]\n                imports.append(ImportReference(\n                    module=node.module or \"\",\n                    names=names,\n                    alias=None,\n                    line=node.lineno,\n                    is_from=True\n                ))\n        \n        # Extract calls (simplified)\n        for node in ast.walk(tree):\n            if isinstance(node, ast.Call):\n                callee = self._get_call_name(node.func)\n                if callee:\n                    calls.append(CallReference(\n                        caller=\"\",  # Would need parent context\n                        callee=callee,\n                        line=node.lineno,\n                        context=\"call\"\n                    ))\n        \n        return {\n            \"symbols\": symbols,\n            \"imports\": imports,\n            \"calls\": calls,\n            \"language\": \"python\"\n        }\n    \n    def _extract_python_function(self, node: ast.FunctionDef, content: str) -> CodeSymbol:\n        \"\"\"Extract detailed function information from AST node.\"\"\"\n        # Get docstring\n        docstring = ast.get_docstring(node)\n        \n        # Get decorators\n        decorators = [self._get_decorator_name(d) for d in node.decorator_list]\n        \n        # Build signature\n        args = [arg.arg for arg in node.args.args]\n        signature = f\"def {node.name}({', '.join(args)})\"\n        \n        # Calculate complexity (simplified: count branches)\n        complexity = sum(\n            1 for n in ast.walk(node)\n            if isinstance(n, (ast.If, ast.For, ast.While, ast.Try, ast.With))\n        )\n        \n        # Content hash\n        lines = content.splitlines()\n        if node.lineno <= len(lines) and node.end_lineno <= len(lines):\n            func_content = \"\\n\".join(lines[node.lineno - 1:node.end_lineno])\n            content_hash = hashlib.md5(func_content.encode()).hexdigest()[:8]\n        else:\n            content_hash = None\n        \n        return CodeSymbol(\n            name=node.name,\n            kind=\"function\",\n            start_line=node.lineno,\n            end_line=node.end_lineno or node.lineno,\n            docstring=docstring,\n            signature=signature,\n            decorators=decorators,\n            complexity=complexity,\n            content_hash=content_hash\n        )\n    \n    def _extract_python_class(self, node: ast.ClassDef, content: str) -> CodeSymbol:\n        \"\"\"Extract detailed class information from AST node.\"\"\"\n        docstring = ast.get_docstring(node)\n        decorators = [self._get_decorator_name(d) for d in node.decorator_list]\n        \n        # Get base classes\n        bases = [self._get_name(base) for base in node.bases]\n        signature = f\"class {node.name}({', '.join(bases)})\" if bases else f\"class {node.name}\"\n        \n        # Count methods\n        methods = sum(\n            1 for n in node.body\n            if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))\n        )\n        \n        return CodeSymbol(\n            name=node.name,\n            kind=\"class\",\n            start_line=node.lineno,\n            end_line=node.end_lineno or node.lineno,\n            docstring=docstring,\n            signature=signature,\n            decorators=decorators,\n            complexity=methods\n        )\n    \n    def _get_decorator_name(self, node: ast.expr) -> str:\n        \"\"\"Extract decorator name from AST node.\"\"\"\n        if isinstance(node, ast.Name):\n            return node.id\n        elif isinstance(node, ast.Call):\n            return self._get_name(node.func)\n        return \"\"\n    \n    def _get_name(self, node: ast.expr) -> str:\n        \"\"\"Extract name from AST expression.\"\"\"\n        if isinstance(node, ast.Name):\n            return node.id\n        elif isinstance(node, ast.Attribute):\n            return f\"{self._get_name(node.value)}.{node.attr}\"\n        return \"\"\n    \n    def _get_call_name(self, node: ast.expr) -> str:\n        \"\"\"Extract function name from call node.\"\"\"\n        if isinstance(node, ast.Name):\n            return node.id\n        elif isinstance(node, ast.Attribute):\n            # Return just the method name for simplicity\n            return node.attr\n        return \"\"\n    \n    # ---- JavaScript/TypeScript analysis (using tree-sitter) ----\n    \n    def _analyze_js_ts(\n        self, content: str, file_path: str, language: str\n    ) -> Dict[str, Any]:\n        \"\"\"Analyze JavaScript/TypeScript using tree-sitter.\"\"\"\n        ts_lang_key = language if language in _TS_LANGUAGES else \"javascript\"\n        parser = self._get_ts_parser(ts_lang_key)\n        if not parser:\n            return self._empty_analysis()\n        \n        try:\n            tree = parser.parse(content.encode(\"utf-8\"))\n            root = tree.root_node\n        except Exception as e:\n            logger.warning(f\"Tree-sitter parse error in {file_path}: {e}\")\n            return self._empty_analysis()\n        \n        symbols = []\n        imports = []\n        calls = []\n        \n        def node_text(n):\n            return content.encode(\"utf-8\")[n.start_byte:n.end_byte].decode(\"utf-8\", errors=\"ignore\")\n        \n        def walk(node, parent_class=None):\n            node_type = node.type\n            \n            # Classes\n            if node_type == \"class_declaration\":\n                name_node = node.child_by_field_name(\"name\")\n                class_name = node_text(name_node) if name_node else \"\"\n                \n                symbols.append(CodeSymbol(\n                    name=class_name,\n                    kind=\"class\",\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1\n                ))\n                \n                # Walk class body\n                for child in node.children:\n                    walk(child, parent_class=class_name)\n                return\n            \n            # Functions\n            if node_type in (\"function_declaration\", \"arrow_function\", \"function_expression\"):\n                name_node = node.child_by_field_name(\"name\")\n                func_name = node_text(name_node) if name_node else \"<anonymous>\"\n                \n                symbols.append(CodeSymbol(\n                    name=func_name,\n                    kind=\"function\",\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1,\n                    parent=parent_class\n                ))\n            \n            # Methods\n            if node_type == \"method_definition\":\n                name_node = node.child_by_field_name(\"name\")\n                method_name = node_text(name_node) if name_node else \"\"\n                \n                symbols.append(CodeSymbol(\n                    name=method_name,\n                    kind=\"method\",\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1,\n                    parent=parent_class,\n                    path=f\"{parent_class}.{method_name}\" if parent_class else method_name\n                ))\n            \n            # Imports\n            if node_type == \"import_statement\":\n                source = node.child_by_field_name(\"source\")\n                if source:\n                    module = node_text(source).strip('\"\\'')\n                    imports.append(ImportReference(\n                        module=module,\n                        names=[],\n                        line=node.start_point[0] + 1,\n                        is_from=True\n                    ))\n            \n            # Recurse\n            for child in node.children:\n                walk(child, parent_class)\n        \n        walk(root)\n        \n        return {\n            \"symbols\": symbols,\n            \"imports\": imports,\n            \"calls\": calls,\n            \"language\": language\n        }\n    \n    def _get_ts_parser(self, language: str):\n        \"\"\"Get or create tree-sitter parser for language.\n\n        Uses tree-sitter 0.25+ API with pre-loaded Language objects.\n        \"\"\"\n        if language in self._parsers:\n            return self._parsers[language]\n\n        if not _TS_AVAILABLE or language not in _TS_LANGUAGES:\n            return None\n\n        try:\n            lang = _TS_LANGUAGES[language]\n            parser = Parser(lang)\n            self._parsers[language] = parser\n            return parser\n        except Exception as e:\n            logger.warning(f\"Failed to create tree-sitter parser for {language}: {e}\")\n            return None\n    \n    # ---- Generic/fallback analysis ----\n    \n    def _analyze_generic(\n        self, content: str, file_path: str, language: str\n    ) -> Dict[str, Any]:\n        \"\"\"Fallback regex-based analysis for unsupported languages.\"\"\"\n        symbols = []\n        lines = content.splitlines()\n        \n        # Very basic heuristics\n        for i, line in enumerate(lines, 1):\n            # Try to find function-like patterns\n            if re.match(r'^\\s*(def|function|func|fn)\\s+(\\w+)', line):\n                match = re.match(r'^\\s*(?:def|function|func|fn)\\s+(\\w+)', line)\n                if match:\n                    symbols.append(CodeSymbol(\n                        name=match.group(1),\n                        kind=\"function\",\n                        start_line=i,\n                        end_line=i  # Can't determine without parsing\n                    ))\n            \n            # Try to find class-like patterns\n            if re.match(r'^\\s*class\\s+(\\w+)', line):\n                match = re.match(r'^\\s*class\\s+(\\w+)', line)\n                if match:\n                    symbols.append(CodeSymbol(\n                        name=match.group(1),\n                        kind=\"class\",\n                        start_line=i,\n                        end_line=i\n                    ))\n        \n        return {\n            \"symbols\": symbols,\n            \"imports\": [],\n            \"calls\": [],\n            \"language\": language\n        }\n    \n    # ---- Helper methods ----\n    \n    def _empty_analysis(self) -> Dict[str, Any]:\n        \"\"\"Return empty analysis result.\"\"\"\n        return {\n            \"symbols\": [],\n            \"imports\": [],\n            \"calls\": [],\n            \"language\": \"unknown\"\n        }\n    \n    def _chunk_lines_simple(\n        self, content: str, max_lines: int, overlap: int\n    ) -> List[CodeContext]:\n        \"\"\"Simple line-based chunking fallback.\"\"\"\n        lines = content.splitlines()\n        chunks = []\n        \n        i = 0\n        while i < len(lines):\n            chunk_end = min(i + max_lines, len(lines))\n            chunk_lines = lines[i:chunk_end]\n            \n            chunks.append(CodeContext(\n                chunk_text=\"\\n\".join(chunk_lines),\n                start_line=i + 1,\n                end_line=chunk_end,\n                symbols=[],\n                imports=[],\n                calls=[],\n                dependencies=set(),\n                is_semantic_unit=False\n            ))\n            \n            i = chunk_end - overlap if chunk_end < len(lines) else chunk_end\n        \n        return chunks\n    \n    def _fill_gaps(\n        self,\n        chunks: List[CodeContext],\n        lines: List[str],\n        max_lines: int,\n        overlap: int,\n        analysis: Dict[str, Any]\n    ):\n        \"\"\"Fill gaps between symbol chunks with module-level code.\"\"\"\n        if not chunks:\n            return\n        \n        # Find uncovered regions\n        covered = set()\n        for chunk in chunks:\n            covered.update(range(chunk.start_line, chunk.end_line + 1))\n        \n        gaps = []\n        gap_start = None\n        for i in range(1, len(lines) + 1):\n            if i not in covered:\n                if gap_start is None:\n                    gap_start = i\n            else:\n                if gap_start is not None:\n                    gaps.append((gap_start, i - 1))\n                    gap_start = None\n        \n        if gap_start is not None:\n            gaps.append((gap_start, len(lines)))\n        \n        # Create chunks for gaps\n        for start, end in gaps:\n            if end - start + 1 < 3:  # Skip tiny gaps\n                continue\n            \n            gap_lines = lines[start - 1:end]\n            chunks.append(CodeContext(\n                chunk_text=\"\\n\".join(gap_lines),\n                start_line=start,\n                end_line=end,\n                symbols=[],\n                imports=[imp for imp in analysis.get(\"imports\", []) if start <= imp.line <= end],\n                calls=[],\n                dependencies=set(),\n                is_semantic_unit=False\n            ))\n        \n        # Re-sort chunks by start line\n        chunks.sort(key=lambda c: c.start_line)\n    \n    def _extract_dependencies(\n        self, imports: List[ImportReference], calls: List[CallReference]\n    ) -> Set[str]:\n        \"\"\"Extract unique dependencies from imports and calls.\"\"\"\n        deps = set()\n        \n        for imp in imports:\n            deps.add(imp.module)\n            deps.update(imp.names)\n        \n        for call in calls:\n            deps.add(call.callee)\n        \n        return deps",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___175": {
      "name": "__init__",
      "type": "method",
      "start_line": 175,
      "end_line": 197,
      "content_hash": "90c0ae8018a08d36230a3944bce7b9c0b203f707",
      "content": "    def __init__(self, use_tree_sitter: bool = True):\n        \"\"\"\n        Initialize AST analyzer.\n        \n        Args:\n            use_tree_sitter: Use tree-sitter when available (fallback to ast module)\n        \"\"\"\n        self.use_tree_sitter = use_tree_sitter and _TS_AVAILABLE\n        self._parsers: Dict[str, Any] = {}\n        \n        # Language support matrix\n        self.supported_languages = {\n            \"python\": {\"ast\": True, \"tree_sitter\": True},\n            \"javascript\": {\"ast\": False, \"tree_sitter\": True},\n            \"typescript\": {\"ast\": False, \"tree_sitter\": True},\n            \"java\": {\"ast\": False, \"tree_sitter\": False},\n            \"go\": {\"ast\": False, \"tree_sitter\": False},\n            \"rust\": {\"ast\": False, \"tree_sitter\": False},\n            \"c\": {\"ast\": False, \"tree_sitter\": False},\n            \"cpp\": {\"ast\": False, \"tree_sitter\": False},\n        }\n        \n        logger.info(f\"ASTAnalyzer initialized: tree_sitter={self.use_tree_sitter}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_analyze_file_199": {
      "name": "analyze_file",
      "type": "method",
      "start_line": 199,
      "end_line": 227,
      "content_hash": "fea2234f5d644b5afc0377f81183a7a3b0fa8edb",
      "content": "    def analyze_file(\n        self, file_path: str, language: str, content: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Analyze a source file and extract semantic information.\n        \n        Args:\n            file_path: Path to the file\n            language: Programming language\n            content: Optional file content (if not provided, read from file)\n        \n        Returns:\n            Dict with symbols, imports, calls, and dependencies\n        \"\"\"\n        if content is None:\n            try:\n                content = Path(file_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n            except Exception as e:\n                logger.error(f\"Failed to read {file_path}: {e}\")\n                return self._empty_analysis()\n        \n        # Route to appropriate analyzer\n        if language == \"python\":\n            return self._analyze_python(content, file_path)\n        elif language in (\"javascript\", \"typescript\") and self.use_tree_sitter:\n            return self._analyze_js_ts(content, file_path, language)\n        else:\n            # Fallback to regex-based analysis\n            return self._analyze_generic(content, file_path, language)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_extract_symbols_with_context_229": {
      "name": "extract_symbols_with_context",
      "type": "method",
      "start_line": 229,
      "end_line": 239,
      "content_hash": "1646e31eaa67963590f47be0e672668e0457fb78",
      "content": "    def extract_symbols_with_context(\n        self, file_path: str, language: str, content: Optional[str] = None\n    ) -> List[CodeSymbol]:\n        \"\"\"\n        Extract code symbols with full context (docstrings, signatures, etc).\n        \n        Returns:\n            List of CodeSymbol objects with rich metadata\n        \"\"\"\n        analysis = self.analyze_file(file_path, language, content)\n        return analysis.get(\"symbols\", [])",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_chunk_semantic_241": {
      "name": "chunk_semantic",
      "type": "method",
      "start_line": 241,
      "end_line": 339,
      "content_hash": "04569be6400d12a5d02db47cf49d1bfad575f1fe",
      "content": "    def chunk_semantic(\n        self,\n        content: str,\n        language: str,\n        max_lines: int = 120,\n        overlap_lines: int = 20,\n        preserve_boundaries: bool = True\n    ) -> List[CodeContext]:\n        \"\"\"\n        Chunk code semantically, respecting function/class boundaries.\n        \n        Args:\n            content: Source code content\n            language: Programming language\n            max_lines: Maximum lines per chunk\n            overlap_lines: Overlap between chunks\n            preserve_boundaries: Try to keep complete functions/classes together\n        \n        Returns:\n            List of CodeContext objects with semantic chunks\n        \"\"\"\n        if not preserve_boundaries:\n            # Fall back to line-based chunking\n            return self._chunk_lines_simple(content, max_lines, overlap_lines)\n        \n        # Extract symbols\n        analysis = self.analyze_file(\"\", language, content)\n        symbols = analysis.get(\"symbols\", [])\n        \n        if not symbols:\n            # No symbols found, use line-based\n            return self._chunk_lines_simple(content, max_lines, overlap_lines)\n        \n        lines = content.splitlines()\n        chunks = []\n        \n        # Sort symbols by start line\n        symbols.sort(key=lambda s: s.start_line)\n        \n        i = 0\n        while i < len(symbols):\n            symbol = symbols[i]\n            \n            # Calculate chunk extent\n            chunk_start = symbol.start_line\n            chunk_end = symbol.end_line\n            symbols_in_chunk = [symbol]\n            \n            # Try to include adjacent small symbols\n            j = i + 1\n            while j < len(symbols):\n                next_symbol = symbols[j]\n                potential_end = next_symbol.end_line\n                \n                # Check if adding next symbol exceeds max_lines\n                if potential_end - chunk_start > max_lines:\n                    break\n                \n                # Check if next symbol is close enough (within overlap)\n                if next_symbol.start_line - chunk_end > overlap_lines:\n                    break\n                \n                # Include this symbol\n                chunk_end = potential_end\n                symbols_in_chunk.append(next_symbol)\n                j += 1\n            \n            # Create chunk\n            chunk_lines = lines[chunk_start - 1:chunk_end]\n            chunk_text = \"\\n\".join(chunk_lines)\n            \n            # Extract chunk-specific imports and calls\n            chunk_imports = [\n                imp for imp in analysis.get(\"imports\", [])\n                if chunk_start <= imp.line <= chunk_end\n            ]\n            chunk_calls = [\n                call for call in analysis.get(\"calls\", [])\n                if chunk_start <= call.line <= chunk_end\n            ]\n            \n            context = CodeContext(\n                chunk_text=chunk_text,\n                start_line=chunk_start,\n                end_line=chunk_end,\n                symbols=symbols_in_chunk,\n                imports=chunk_imports,\n                calls=chunk_calls,\n                dependencies=self._extract_dependencies(chunk_imports, chunk_calls),\n                is_semantic_unit=True\n            )\n            \n            chunks.append(context)\n            i = j if j > i else i + 1\n        \n        # Handle code not covered by symbols (module-level code, etc)\n        self._fill_gaps(chunks, lines, max_lines, overlap_lines, analysis)\n        \n        return chunks",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_build_call_graph_341": {
      "name": "build_call_graph",
      "type": "method",
      "start_line": 341,
      "end_line": 354,
      "content_hash": "284b322d9789a5d9c6de4554a4143f064e203c51",
      "content": "    def build_call_graph(self, file_path: str, language: str) -> Dict[str, List[str]]:\n        \"\"\"\n        Build call graph: mapping of caller -> list of callees.\n        \n        Returns:\n            Dict mapping function names to list of functions they call\n        \"\"\"\n        analysis = self.analyze_file(file_path, language)\n        \n        call_graph = defaultdict(list)\n        for call in analysis.get(\"calls\", []):\n            call_graph[call.caller].append(call.callee)\n        \n        return dict(call_graph)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_extract_dependencies_356": {
      "name": "extract_dependencies",
      "type": "method",
      "start_line": 356,
      "end_line": 381,
      "content_hash": "8786d91516f9dd7736076d48e71fe24031153481",
      "content": "    def extract_dependencies(\n        self, file_path: str, language: str\n    ) -> Dict[str, List[str]]:\n        \"\"\"\n        Extract file dependencies (imports, includes).\n        \n        Returns:\n            Dict with 'modules' (external) and 'local' (same project) imports\n        \"\"\"\n        analysis = self.analyze_file(file_path, language)\n        imports = analysis.get(\"imports\", [])\n        \n        modules = []\n        local = []\n        \n        for imp in imports:\n            # Simple heuristic: relative imports or without dots are likely local\n            if imp.module.startswith(\".\") or \"/\" in imp.module:\n                local.append(imp.module)\n            else:\n                modules.append(imp.module)\n        \n        return {\n            \"modules\": list(set(modules)),\n            \"local\": list(set(local))\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__analyze_python_385": {
      "name": "_analyze_python",
      "type": "method",
      "start_line": 385,
      "end_line": 444,
      "content_hash": "6a29b0e4a6e5d545629b02bbeec9c32e7c674181",
      "content": "    def _analyze_python(self, content: str, file_path: str) -> Dict[str, Any]:\n        \"\"\"Analyze Python code using ast module.\"\"\"\n        try:\n            tree = ast.parse(content)\n        except SyntaxError as e:\n            logger.warning(f\"Python syntax error in {file_path}: {e}\")\n            return self._empty_analysis()\n        \n        symbols = []\n        imports = []\n        calls = []\n        \n        # Extract symbols\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                symbol = self._extract_python_function(node, content)\n                symbols.append(symbol)\n            elif isinstance(node, ast.ClassDef):\n                symbol = self._extract_python_class(node, content)\n                symbols.append(symbol)\n        \n        # Extract imports\n        for node in ast.walk(tree):\n            if isinstance(node, ast.Import):\n                for alias in node.names:\n                    imports.append(ImportReference(\n                        module=alias.name,\n                        names=[],\n                        alias=alias.asname,\n                        line=node.lineno,\n                        is_from=False\n                    ))\n            elif isinstance(node, ast.ImportFrom):\n                names = [alias.name for alias in node.names]\n                imports.append(ImportReference(\n                    module=node.module or \"\",\n                    names=names,\n                    alias=None,\n                    line=node.lineno,\n                    is_from=True\n                ))\n        \n        # Extract calls (simplified)\n        for node in ast.walk(tree):\n            if isinstance(node, ast.Call):\n                callee = self._get_call_name(node.func)\n                if callee:\n                    calls.append(CallReference(\n                        caller=\"\",  # Would need parent context\n                        callee=callee,\n                        line=node.lineno,\n                        context=\"call\"\n                    ))\n        \n        return {\n            \"symbols\": symbols,\n            \"imports\": imports,\n            \"calls\": calls,\n            \"language\": \"python\"\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__extract_python_function_446": {
      "name": "_extract_python_function",
      "type": "method",
      "start_line": 446,
      "end_line": 482,
      "content_hash": "06db1d3d55553542d0712d5c4bf6a2ba981af6d4",
      "content": "    def _extract_python_function(self, node: ast.FunctionDef, content: str) -> CodeSymbol:\n        \"\"\"Extract detailed function information from AST node.\"\"\"\n        # Get docstring\n        docstring = ast.get_docstring(node)\n        \n        # Get decorators\n        decorators = [self._get_decorator_name(d) for d in node.decorator_list]\n        \n        # Build signature\n        args = [arg.arg for arg in node.args.args]\n        signature = f\"def {node.name}({', '.join(args)})\"\n        \n        # Calculate complexity (simplified: count branches)\n        complexity = sum(\n            1 for n in ast.walk(node)\n            if isinstance(n, (ast.If, ast.For, ast.While, ast.Try, ast.With))\n        )\n        \n        # Content hash\n        lines = content.splitlines()\n        if node.lineno <= len(lines) and node.end_lineno <= len(lines):\n            func_content = \"\\n\".join(lines[node.lineno - 1:node.end_lineno])\n            content_hash = hashlib.md5(func_content.encode()).hexdigest()[:8]\n        else:\n            content_hash = None\n        \n        return CodeSymbol(\n            name=node.name,\n            kind=\"function\",\n            start_line=node.lineno,\n            end_line=node.end_lineno or node.lineno,\n            docstring=docstring,\n            signature=signature,\n            decorators=decorators,\n            complexity=complexity,\n            content_hash=content_hash\n        )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__extract_python_class_484": {
      "name": "_extract_python_class",
      "type": "method",
      "start_line": 484,
      "end_line": 508,
      "content_hash": "bbb5725a05f1440558669305ceec46ea47045b44",
      "content": "    def _extract_python_class(self, node: ast.ClassDef, content: str) -> CodeSymbol:\n        \"\"\"Extract detailed class information from AST node.\"\"\"\n        docstring = ast.get_docstring(node)\n        decorators = [self._get_decorator_name(d) for d in node.decorator_list]\n        \n        # Get base classes\n        bases = [self._get_name(base) for base in node.bases]\n        signature = f\"class {node.name}({', '.join(bases)})\" if bases else f\"class {node.name}\"\n        \n        # Count methods\n        methods = sum(\n            1 for n in node.body\n            if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))\n        )\n        \n        return CodeSymbol(\n            name=node.name,\n            kind=\"class\",\n            start_line=node.lineno,\n            end_line=node.end_lineno or node.lineno,\n            docstring=docstring,\n            signature=signature,\n            decorators=decorators,\n            complexity=methods\n        )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__get_decorator_name_510": {
      "name": "_get_decorator_name",
      "type": "method",
      "start_line": 510,
      "end_line": 516,
      "content_hash": "074d386951a077eb19dc13c44cdb14c864153fb4",
      "content": "    def _get_decorator_name(self, node: ast.expr) -> str:\n        \"\"\"Extract decorator name from AST node.\"\"\"\n        if isinstance(node, ast.Name):\n            return node.id\n        elif isinstance(node, ast.Call):\n            return self._get_name(node.func)\n        return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__get_name_518": {
      "name": "_get_name",
      "type": "method",
      "start_line": 518,
      "end_line": 524,
      "content_hash": "1fd5ede32a8a90f3e436418e6f478f7d366fa956",
      "content": "    def _get_name(self, node: ast.expr) -> str:\n        \"\"\"Extract name from AST expression.\"\"\"\n        if isinstance(node, ast.Name):\n            return node.id\n        elif isinstance(node, ast.Attribute):\n            return f\"{self._get_name(node.value)}.{node.attr}\"\n        return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__get_call_name_526": {
      "name": "_get_call_name",
      "type": "method",
      "start_line": 526,
      "end_line": 533,
      "content_hash": "04f148d7bda77c81e4edc25ea18af54fec879cd6",
      "content": "    def _get_call_name(self, node: ast.expr) -> str:\n        \"\"\"Extract function name from call node.\"\"\"\n        if isinstance(node, ast.Name):\n            return node.id\n        elif isinstance(node, ast.Attribute):\n            # Return just the method name for simplicity\n            return node.attr\n        return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__analyze_js_ts_537": {
      "name": "_analyze_js_ts",
      "type": "method",
      "start_line": 537,
      "end_line": 630,
      "content_hash": "b6be12f1bce8187485a51b5ba67d334036d12afa",
      "content": "    def _analyze_js_ts(\n        self, content: str, file_path: str, language: str\n    ) -> Dict[str, Any]:\n        \"\"\"Analyze JavaScript/TypeScript using tree-sitter.\"\"\"\n        ts_lang_key = language if language in _TS_LANGUAGES else \"javascript\"\n        parser = self._get_ts_parser(ts_lang_key)\n        if not parser:\n            return self._empty_analysis()\n        \n        try:\n            tree = parser.parse(content.encode(\"utf-8\"))\n            root = tree.root_node\n        except Exception as e:\n            logger.warning(f\"Tree-sitter parse error in {file_path}: {e}\")\n            return self._empty_analysis()\n        \n        symbols = []\n        imports = []\n        calls = []\n        \n        def node_text(n):\n            return content.encode(\"utf-8\")[n.start_byte:n.end_byte].decode(\"utf-8\", errors=\"ignore\")\n        \n        def walk(node, parent_class=None):\n            node_type = node.type\n            \n            # Classes\n            if node_type == \"class_declaration\":\n                name_node = node.child_by_field_name(\"name\")\n                class_name = node_text(name_node) if name_node else \"\"\n                \n                symbols.append(CodeSymbol(\n                    name=class_name,\n                    kind=\"class\",\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1\n                ))\n                \n                # Walk class body\n                for child in node.children:\n                    walk(child, parent_class=class_name)\n                return\n            \n            # Functions\n            if node_type in (\"function_declaration\", \"arrow_function\", \"function_expression\"):\n                name_node = node.child_by_field_name(\"name\")\n                func_name = node_text(name_node) if name_node else \"<anonymous>\"\n                \n                symbols.append(CodeSymbol(\n                    name=func_name,\n                    kind=\"function\",\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1,\n                    parent=parent_class\n                ))\n            \n            # Methods\n            if node_type == \"method_definition\":\n                name_node = node.child_by_field_name(\"name\")\n                method_name = node_text(name_node) if name_node else \"\"\n                \n                symbols.append(CodeSymbol(\n                    name=method_name,\n                    kind=\"method\",\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1,\n                    parent=parent_class,\n                    path=f\"{parent_class}.{method_name}\" if parent_class else method_name\n                ))\n            \n            # Imports\n            if node_type == \"import_statement\":\n                source = node.child_by_field_name(\"source\")\n                if source:\n                    module = node_text(source).strip('\"\\'')\n                    imports.append(ImportReference(\n                        module=module,\n                        names=[],\n                        line=node.start_point[0] + 1,\n                        is_from=True\n                    ))\n            \n            # Recurse\n            for child in node.children:\n                walk(child, parent_class)\n        \n        walk(root)\n        \n        return {\n            \"symbols\": symbols,\n            \"imports\": imports,\n            \"calls\": calls,\n            \"language\": language\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_node_text_557": {
      "name": "node_text",
      "type": "method",
      "start_line": 557,
      "end_line": 558,
      "content_hash": "acb9075a30b28b5198a859d39c2f641acfaed409",
      "content": "        def node_text(n):\n            return content.encode(\"utf-8\")[n.start_byte:n.end_byte].decode(\"utf-8\", errors=\"ignore\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_walk_560": {
      "name": "walk",
      "type": "method",
      "start_line": 560,
      "end_line": 621,
      "content_hash": "25b638eac1a361c132c36ae42fc1b92b03da17f4",
      "content": "        def walk(node, parent_class=None):\n            node_type = node.type\n            \n            # Classes\n            if node_type == \"class_declaration\":\n                name_node = node.child_by_field_name(\"name\")\n                class_name = node_text(name_node) if name_node else \"\"\n                \n                symbols.append(CodeSymbol(\n                    name=class_name,\n                    kind=\"class\",\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1\n                ))\n                \n                # Walk class body\n                for child in node.children:\n                    walk(child, parent_class=class_name)\n                return\n            \n            # Functions\n            if node_type in (\"function_declaration\", \"arrow_function\", \"function_expression\"):\n                name_node = node.child_by_field_name(\"name\")\n                func_name = node_text(name_node) if name_node else \"<anonymous>\"\n                \n                symbols.append(CodeSymbol(\n                    name=func_name,\n                    kind=\"function\",\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1,\n                    parent=parent_class\n                ))\n            \n            # Methods\n            if node_type == \"method_definition\":\n                name_node = node.child_by_field_name(\"name\")\n                method_name = node_text(name_node) if name_node else \"\"\n                \n                symbols.append(CodeSymbol(\n                    name=method_name,\n                    kind=\"method\",\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1,\n                    parent=parent_class,\n                    path=f\"{parent_class}.{method_name}\" if parent_class else method_name\n                ))\n            \n            # Imports\n            if node_type == \"import_statement\":\n                source = node.child_by_field_name(\"source\")\n                if source:\n                    module = node_text(source).strip('\"\\'')\n                    imports.append(ImportReference(\n                        module=module,\n                        names=[],\n                        line=node.start_point[0] + 1,\n                        is_from=True\n                    ))\n            \n            # Recurse\n            for child in node.children:\n                walk(child, parent_class)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__get_ts_parser_632": {
      "name": "_get_ts_parser",
      "type": "method",
      "start_line": 632,
      "end_line": 650,
      "content_hash": "f55c49ea06de440d620b337a9b8a7f60faec7844",
      "content": "    def _get_ts_parser(self, language: str):\n        \"\"\"Get or create tree-sitter parser for language.\n\n        Uses tree-sitter 0.25+ API with pre-loaded Language objects.\n        \"\"\"\n        if language in self._parsers:\n            return self._parsers[language]\n\n        if not _TS_AVAILABLE or language not in _TS_LANGUAGES:\n            return None\n\n        try:\n            lang = _TS_LANGUAGES[language]\n            parser = Parser(lang)\n            self._parsers[language] = parser\n            return parser\n        except Exception as e:\n            logger.warning(f\"Failed to create tree-sitter parser for {language}: {e}\")\n            return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__analyze_generic_654": {
      "name": "_analyze_generic",
      "type": "method",
      "start_line": 654,
      "end_line": 690,
      "content_hash": "2ad52dc202e672fb14b767389c032715e56a494a",
      "content": "    def _analyze_generic(\n        self, content: str, file_path: str, language: str\n    ) -> Dict[str, Any]:\n        \"\"\"Fallback regex-based analysis for unsupported languages.\"\"\"\n        symbols = []\n        lines = content.splitlines()\n        \n        # Very basic heuristics\n        for i, line in enumerate(lines, 1):\n            # Try to find function-like patterns\n            if re.match(r'^\\s*(def|function|func|fn)\\s+(\\w+)', line):\n                match = re.match(r'^\\s*(?:def|function|func|fn)\\s+(\\w+)', line)\n                if match:\n                    symbols.append(CodeSymbol(\n                        name=match.group(1),\n                        kind=\"function\",\n                        start_line=i,\n                        end_line=i  # Can't determine without parsing\n                    ))\n            \n            # Try to find class-like patterns\n            if re.match(r'^\\s*class\\s+(\\w+)', line):\n                match = re.match(r'^\\s*class\\s+(\\w+)', line)\n                if match:\n                    symbols.append(CodeSymbol(\n                        name=match.group(1),\n                        kind=\"class\",\n                        start_line=i,\n                        end_line=i\n                    ))\n        \n        return {\n            \"symbols\": symbols,\n            \"imports\": [],\n            \"calls\": [],\n            \"language\": language\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__empty_analysis_694": {
      "name": "_empty_analysis",
      "type": "method",
      "start_line": 694,
      "end_line": 701,
      "content_hash": "015f1540b0e9338571f079e87119faf0f07ee4ef",
      "content": "    def _empty_analysis(self) -> Dict[str, Any]:\n        \"\"\"Return empty analysis result.\"\"\"\n        return {\n            \"symbols\": [],\n            \"imports\": [],\n            \"calls\": [],\n            \"language\": \"unknown\"\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__chunk_lines_simple_703": {
      "name": "_chunk_lines_simple",
      "type": "method",
      "start_line": 703,
      "end_line": 728,
      "content_hash": "543917aa610d86b995a2203205a5b6cf495c870b",
      "content": "    def _chunk_lines_simple(\n        self, content: str, max_lines: int, overlap: int\n    ) -> List[CodeContext]:\n        \"\"\"Simple line-based chunking fallback.\"\"\"\n        lines = content.splitlines()\n        chunks = []\n        \n        i = 0\n        while i < len(lines):\n            chunk_end = min(i + max_lines, len(lines))\n            chunk_lines = lines[i:chunk_end]\n            \n            chunks.append(CodeContext(\n                chunk_text=\"\\n\".join(chunk_lines),\n                start_line=i + 1,\n                end_line=chunk_end,\n                symbols=[],\n                imports=[],\n                calls=[],\n                dependencies=set(),\n                is_semantic_unit=False\n            ))\n            \n            i = chunk_end - overlap if chunk_end < len(lines) else chunk_end\n        \n        return chunks",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__fill_gaps_730": {
      "name": "_fill_gaps",
      "type": "method",
      "start_line": 730,
      "end_line": 779,
      "content_hash": "192751044229ee827cc244786cfa60103758f095",
      "content": "    def _fill_gaps(\n        self,\n        chunks: List[CodeContext],\n        lines: List[str],\n        max_lines: int,\n        overlap: int,\n        analysis: Dict[str, Any]\n    ):\n        \"\"\"Fill gaps between symbol chunks with module-level code.\"\"\"\n        if not chunks:\n            return\n        \n        # Find uncovered regions\n        covered = set()\n        for chunk in chunks:\n            covered.update(range(chunk.start_line, chunk.end_line + 1))\n        \n        gaps = []\n        gap_start = None\n        for i in range(1, len(lines) + 1):\n            if i not in covered:\n                if gap_start is None:\n                    gap_start = i\n            else:\n                if gap_start is not None:\n                    gaps.append((gap_start, i - 1))\n                    gap_start = None\n        \n        if gap_start is not None:\n            gaps.append((gap_start, len(lines)))\n        \n        # Create chunks for gaps\n        for start, end in gaps:\n            if end - start + 1 < 3:  # Skip tiny gaps\n                continue\n            \n            gap_lines = lines[start - 1:end]\n            chunks.append(CodeContext(\n                chunk_text=\"\\n\".join(gap_lines),\n                start_line=start,\n                end_line=end,\n                symbols=[],\n                imports=[imp for imp in analysis.get(\"imports\", []) if start <= imp.line <= end],\n                calls=[],\n                dependencies=set(),\n                is_semantic_unit=False\n            ))\n        \n        # Re-sort chunks by start line\n        chunks.sort(key=lambda c: c.start_line)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__extract_dependencies_781": {
      "name": "_extract_dependencies",
      "type": "method",
      "start_line": 781,
      "end_line": 794,
      "content_hash": "42d47ecaa7c7f29329354451aeba0a2e2fe77301",
      "content": "    def _extract_dependencies(\n        self, imports: List[ImportReference], calls: List[CallReference]\n    ) -> Set[str]:\n        \"\"\"Extract unique dependencies from imports and calls.\"\"\"\n        deps = set()\n        \n        for imp in imports:\n            deps.add(imp.module)\n            deps.update(imp.names)\n        \n        for call in calls:\n            deps.add(call.callee)\n        \n        return deps",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_ast_analyzer_801": {
      "name": "get_ast_analyzer",
      "type": "function",
      "start_line": 801,
      "end_line": 809,
      "content_hash": "bf707a41df5b4972a3ed26b2e6792573c952bfb7",
      "content": "def get_ast_analyzer(reset: bool = False) -> ASTAnalyzer:\n    \"\"\"Get or create global AST analyzer instance.\"\"\"\n    global _analyzer\n    \n    if _analyzer is None or reset:\n        use_ts = os.environ.get(\"USE_TREE_SITTER\", \"1\").lower() in {\"1\", \"true\", \"yes\", \"on\"}\n        _analyzer = ASTAnalyzer(use_tree_sitter=use_ts)\n    \n    return _analyzer",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_extract_symbols_813": {
      "name": "extract_symbols",
      "type": "function",
      "start_line": 813,
      "end_line": 816,
      "content_hash": "67aea02c604e11c19536bab1fca15e254e2ed86e",
      "content": "def extract_symbols(file_path: str, language: str) -> List[CodeSymbol]:\n    \"\"\"Extract symbols from a file.\"\"\"\n    analyzer = get_ast_analyzer()\n    return analyzer.extract_symbols_with_context(file_path, language)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_chunk_code_semantically_819": {
      "name": "chunk_code_semantically",
      "type": "function",
      "start_line": 819,
      "end_line": 844,
      "content_hash": "8d5e0b53d7a11044d090c0fb56b51b08ced730c8",
      "content": "def chunk_code_semantically(\n    content: str,\n    language: str,\n    max_lines: int = 120,\n    overlap: int = 20\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Chunk code semantically, returning simplified dicts for indexing.\n    \n    Returns list of dicts compatible with existing chunking interface.\n    \"\"\"\n    analyzer = get_ast_analyzer()\n    contexts = analyzer.chunk_semantic(content, language, max_lines, overlap)\n    \n    # Convert to simple dict format\n    return [\n        {\n            \"text\": ctx.chunk_text,\n            \"start\": ctx.start_line,\n            \"end\": ctx.end_line,\n            \"is_semantic\": ctx.is_semantic_unit,\n            \"symbols\": [s.name for s in ctx.symbols],\n            \"symbol_types\": [s.kind for s in ctx.symbols]\n        }\n        for ctx in contexts\n    ]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}