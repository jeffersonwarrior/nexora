{
  "file_path": "/work/context-engine/tests/test_staging_lifecycle.py",
  "file_hash": "55786d134dd1418d5b88429114b55e7b152b7eae",
  "updated_at": "2025-12-26T17:34:24.090349",
  "symbols": {
    "function_staging_workspace_11": {
      "name": "staging_workspace",
      "type": "function",
      "start_line": 11,
      "end_line": 36,
      "content_hash": "c4563e12cc6e66e5febd3f0e1fadfd71e3a7d391",
      "content": "def staging_workspace(tmp_path: Path, monkeypatch: pytest.MonkeyPatch):\n    # Make workspace_state treat this temp dir as the workspace root.\n    monkeypatch.setenv(\"MULTI_REPO_MODE\", \"1\")\n    monkeypatch.setenv(\"WORKSPACE_PATH\", str(tmp_path))\n    monkeypatch.setenv(\"WATCH_ROOT\", str(tmp_path))\n    monkeypatch.setenv(\"WORK_DIR\", str(tmp_path))\n    monkeypatch.setenv(\"WORKDIR\", str(tmp_path))\n    monkeypatch.setenv(\"CTXCE_STAGING_ENABLED\", \"1\")\n\n    # Avoid any accidental external calls.\n    monkeypatch.setenv(\"HF_HUB_DISABLE_XET\", \"1\")\n    monkeypatch.setenv(\"TOKENIZERS_PARALLELISM\", \"false\")\n\n    repo_name = \"repo1\"\n    collection = \"coll1\"\n\n    repo_ws = tmp_path / repo_name\n    repo_ws.mkdir(parents=True)\n    (repo_ws / \"hello.txt\").write_text(\"hello\", encoding=\"utf-8\")\n\n    return {\n        \"root\": tmp_path,\n        \"repo_name\": repo_name,\n        \"collection\": collection,\n        \"repo_ws\": repo_ws,\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__read_repo_state_39": {
      "name": "_read_repo_state",
      "type": "function",
      "start_line": 39,
      "end_line": 41,
      "content_hash": "08465d04ab98341c288f0d42bdb32160c27438a5",
      "content": "def _read_repo_state(root: Path, repo_name: str) -> dict:\n    p = root / \".codebase\" / \"repos\" / repo_name / \"state.json\"\n    return json.loads(p.read_text(encoding=\"utf-8\"))",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__repo_state_path_44": {
      "name": "_repo_state_path",
      "type": "function",
      "start_line": 44,
      "end_line": 45,
      "content_hash": "c71d48f6df91473dd34b72d19541544c0e373498",
      "content": "def _repo_state_path(root: Path, repo_name: str) -> Path:\n    return root / \".codebase\" / \"repos\" / repo_name / \"state.json\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__seed_repo_state_48": {
      "name": "_seed_repo_state",
      "type": "function",
      "start_line": 48,
      "end_line": 71,
      "content_hash": "cb37f0a2948caf9e0af6997af6809a7eae9e9619",
      "content": "def _seed_repo_state(\n    workspace_state_module,\n    *,\n    repo_ws: Path,\n    repo_name: str,\n    collection: str,\n    active_env: Optional[Dict[str, str]] = None,\n    pending_env: Optional[Dict[str, str]] = None,\n) -> Dict:\n    active_env = active_env or {\"CTXCE_E2E_MARKER\": \"active\"}\n    pending_env = pending_env or {\"CTXCE_E2E_MARKER\": \"pending\"}\n    workspace_state_module.update_workspace_state(\n        workspace_path=str(repo_ws),\n        repo_name=repo_name,\n        updates={\n            \"qdrant_collection\": collection,\n            \"serving_collection\": collection,\n            \"serving_repo_slug\": repo_name,\n            \"active_repo_slug\": repo_name,\n            \"indexing_env\": active_env,\n            \"indexing_env_pending\": pending_env,\n        },\n    )\n    return {\"active\": active_env, \"pending\": pending_env}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_staging_start_promote_activate_and_abort_are_consistent_74": {
      "name": "test_staging_start_promote_activate_and_abort_are_consistent",
      "type": "function",
      "start_line": 74,
      "end_line": 253,
      "content_hash": "f8cf6ba4998eaa4b10436ef0767a6c4c588b8c3b",
      "content": "def test_staging_start_promote_activate_and_abort_are_consistent(\n    staging_workspace: dict, monkeypatch: pytest.MonkeyPatch\n):\n    from scripts import indexing_admin\n    from scripts import workspace_state\n\n    root: Path = staging_workspace[\"root\"]\n    repo_name: str = staging_workspace[\"repo_name\"]\n    collection: str = staging_workspace[\"collection\"]\n    repo_ws: Path = staging_workspace[\"repo_ws\"]\n\n    envs = _seed_repo_state(\n        workspace_state_module=workspace_state,\n        repo_ws=repo_ws,\n        repo_name=repo_name,\n        collection=collection,\n        active_env={\"CTXCE_ENV_ROLE\": \"active\", \"CTXCE_E2E_MARKER\": \"active\"},\n        pending_env={\"CTXCE_ENV_ROLE\": \"pending\", \"CTXCE_E2E_MARKER\": \"pending\"},\n    )\n    pending_env = envs[\"pending\"]\n    # Seed cache.json so staging clone should copy + retarget paths.\n    src_cache_dir = root / \".codebase\" / \"repos\" / repo_name\n    src_cache_dir.mkdir(parents=True, exist_ok=True)\n    original_path = (repo_ws / \"hello.txt\").resolve().as_posix()\n    (src_cache_dir / \"cache.json\").write_text(\n        json.dumps(\n            {\n                \"file_hashes\": {\n                    original_path: {\"hash\": \"abc123\", \"size\": 5, \"mtime\": 123456},\n                }\n            }\n        ),\n        encoding=\"utf-8\",\n    )\n\n    # Patch boundaries (no real Qdrant, no subprocess indexing).\n    calls = {\n        \"copy\": [],\n        \"recreate\": [],\n        \"spawn\": [],\n        \"delete\": [],\n        \"wait_clone\": 0,\n        \"normalize_schema\": 0,\n    }\n\n    def fake_mapping_index(*, work_dir: str):\n        return {\n            collection: [\n                {\n                    \"repo_name\": repo_name,\n                    \"container_path\": str(repo_ws),\n                }\n            ]\n        }\n\n    monkeypatch.setattr(indexing_admin, \"collection_mapping_index\", fake_mapping_index)\n    monkeypatch.setattr(indexing_admin, \"_get_collection_point_count\", lambda **_: 0)\n\n    def fake_copy_collection_qdrant(**kwargs):\n        calls[\"copy\"].append(kwargs)\n        return kwargs.get(\"target\")\n\n    def fake_recreate_collection_qdrant(**kwargs):\n        calls[\"recreate\"].append(kwargs)\n\n    def fake_spawn_ingest_code(**kwargs):\n        calls[\"spawn\"].append(kwargs)\n\n    def fake_delete_collection_qdrant(**kwargs):\n        calls[\"delete\"].append(kwargs)\n\n    def fake_wait_for_clone_points(**kwargs):\n        calls[\"wait_clone\"] += 1\n\n    def fake_normalize_cloned_collection_schema(**kwargs):\n        calls[\"normalize_schema\"] += 1\n\n    monkeypatch.setattr(indexing_admin, \"copy_collection_qdrant\", fake_copy_collection_qdrant)\n    monkeypatch.setattr(indexing_admin, \"recreate_collection_qdrant\", fake_recreate_collection_qdrant)\n    monkeypatch.setattr(indexing_admin, \"spawn_ingest_code\", fake_spawn_ingest_code)\n    monkeypatch.setattr(indexing_admin, \"delete_collection_qdrant\", fake_delete_collection_qdrant)\n    monkeypatch.setattr(indexing_admin, \"_wait_for_clone_points\", fake_wait_for_clone_points)\n    monkeypatch.setattr(\n        indexing_admin, \"_normalize_cloned_collection_schema\", fake_normalize_cloned_collection_schema\n    )\n\n    # === START staging ===\n    old_collection = f\"{collection}_old\"\n    indexing_admin.start_staging_rebuild(collection=collection, work_dir=str(root))\n\n    # Contract: copy collection called for primary -> _old.\n    assert calls[\"copy\"], \"Expected copy_collection_qdrant to be called\"\n    assert calls[\"copy\"][0][\"source\"] == collection\n    assert calls[\"copy\"][0][\"target\"] == old_collection\n\n    # Contract: workspace clone dir exists.\n    assert (root / f\"{repo_name}_old\").exists()\n\n    # Contract: staging metadata present and serving switched to _old.\n    st = _read_repo_state(root, repo_name)\n    assert st.get(\"serving_collection\") == old_collection\n    assert st.get(\"qdrant_collection\") == collection\n    assert isinstance(st.get(\"staging\"), dict)\n    assert st[\"staging\"].get(\"collection\") == old_collection\n    assert st[\"staging\"].get(\"environment\") == pending_env\n    assert (st.get(\"indexing_env\") or {}) == envs[\"active\"]\n    assert (st.get(\"indexing_env_pending\") or {}) == envs[\"pending\"]\n\n    # Contract: spawn_ingest_code called with env_overrides matching pending env and forced collection.\n    assert calls[\"spawn\"], \"Expected spawn_ingest_code to be called\"\n    spawn = calls[\"spawn\"][0]\n    assert spawn[\"collection\"] == collection\n    assert spawn[\"repo_name\"] == repo_name\n    assert spawn[\"env_overrides\"] == pending_env\n    assert calls[\"recreate\"], \"Expected recreate_collection_qdrant to be invoked\"\n    assert calls[\"recreate\"][0][\"collection\"] == collection\n\n    # Contract: _old repo state exists and preserves serving env (pending cleared).\n    old_state = _read_repo_state(root, f\"{repo_name}_old\")\n    assert old_state.get(\"qdrant_collection\") == old_collection\n    assert old_state.get(\"serving_collection\") == old_collection\n    assert old_state.get(\"indexing_env_pending\") in (None, {})\n    assert (old_state.get(\"indexing_env\") or {}) == envs[\"active\"]\n    # Cached file hashes should be copied and retargeted to the cloned workspace.\n    clone_cache_path = root / \".codebase\" / \"repos\" / f\"{repo_name}_old\" / \"cache.json\"\n    assert clone_cache_path.exists(), \"expected cloned cache.json\"\n    clone_cache = json.loads(clone_cache_path.read_text(encoding=\"utf-8\"))\n    clone_path = (root / f\"{repo_name}_old\" / \"hello.txt\").resolve().as_posix()\n    assert clone_path in clone_cache.get(\"file_hashes\", {}), \"clone cache missing retargeted path\"\n    assert original_path not in clone_cache.get(\"file_hashes\", {}), \"original path leaked into clone cache\"\n\n    # === PROMOTE pending env ===\n    workspace_state.promote_pending_indexing_config(workspace_path=str(repo_ws), repo_name=repo_name)\n    st = _read_repo_state(root, repo_name)\n    assert (st.get(\"indexing_env\") or {}).get(\"CTXCE_E2E_MARKER\") == \"pending\"\n    assert st.get(\"indexing_env_pending\") in (None, {})\n\n    # === ACTIVATE ===\n    indexing_admin.activate_staging_rebuild(collection=collection, work_dir=str(root))\n    st = _read_repo_state(root, repo_name)\n    assert st.get(\"staging\") is None\n    assert st.get(\"serving_collection\") == collection\n    assert st.get(\"serving_repo_slug\") == repo_name\n\n    # Cleanup: old workspace + old repo meta removed.\n    assert not (root / f\"{repo_name}_old\").exists()\n    assert not (root / \".codebase\" / \"repos\" / f\"{repo_name}_old\").exists()\n\n    # Contract: _old collection delete invoked.\n    assert any(d.get(\"collection\") == old_collection for d in calls[\"delete\"])\n\n    # Idempotent activate\n    indexing_admin.activate_staging_rebuild(collection=collection, work_dir=str(root))\n\n    # === ABORT scenario ===\n    # Recreate old artifacts to ensure abort cleans them.\n    workspace_state.update_workspace_state(\n        workspace_path=str(repo_ws),\n        repo_name=repo_name,\n        updates={\n            \"indexing_env_pending\": {\"CTXCE_E2E_MARKER\": \"pending2\"},\n        },\n    )\n\n    indexing_admin.start_staging_rebuild(collection=collection, work_dir=str(root))\n    assert (root / f\"{repo_name}_old\").exists()\n    assert (root / \".codebase\" / \"repos\" / f\"{repo_name}_old\").exists()\n\n    indexing_admin.abort_staging_rebuild(collection=collection, work_dir=str(root), delete_collection=True)\n\n    # Abort restores serving to primary, clears staging, and cleans up.\n    st = _read_repo_state(root, repo_name)\n    assert st.get(\"staging\") is None\n    assert st.get(\"serving_collection\") == collection\n    assert st.get(\"serving_repo_slug\") == repo_name\n    assert not (root / f\"{repo_name}_old\").exists()\n    assert not (root / \".codebase\" / \"repos\" / f\"{repo_name}_old\").exists()\n\n    # Idempotent abort\n    indexing_admin.abort_staging_rebuild(collection=collection, work_dir=str(root), delete_collection=True)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_mapping_index_119": {
      "name": "fake_mapping_index",
      "type": "function",
      "start_line": 119,
      "end_line": 127,
      "content_hash": "199b9cc6f15b3b5decb3441cf40a88e979d5c67f",
      "content": "    def fake_mapping_index(*, work_dir: str):\n        return {\n            collection: [\n                {\n                    \"repo_name\": repo_name,\n                    \"container_path\": str(repo_ws),\n                }\n            ]\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_copy_collection_qdrant_132": {
      "name": "fake_copy_collection_qdrant",
      "type": "function",
      "start_line": 132,
      "end_line": 134,
      "content_hash": "da30d22148a0cd29714a28624c124541a18e87e3",
      "content": "    def fake_copy_collection_qdrant(**kwargs):\n        calls[\"copy\"].append(kwargs)\n        return kwargs.get(\"target\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_recreate_collection_qdrant_136": {
      "name": "fake_recreate_collection_qdrant",
      "type": "function",
      "start_line": 136,
      "end_line": 137,
      "content_hash": "3624b22f42aed6610a1e4483780c4f31b9f67540",
      "content": "    def fake_recreate_collection_qdrant(**kwargs):\n        calls[\"recreate\"].append(kwargs)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_spawn_ingest_code_139": {
      "name": "fake_spawn_ingest_code",
      "type": "function",
      "start_line": 139,
      "end_line": 140,
      "content_hash": "c638dacdfbb277cdbad252d96e026b557e095cb6",
      "content": "    def fake_spawn_ingest_code(**kwargs):\n        calls[\"spawn\"].append(kwargs)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_delete_collection_qdrant_142": {
      "name": "fake_delete_collection_qdrant",
      "type": "function",
      "start_line": 142,
      "end_line": 143,
      "content_hash": "aca6c915dbb07257506f38c0b0a50e5ae8cda86e",
      "content": "    def fake_delete_collection_qdrant(**kwargs):\n        calls[\"delete\"].append(kwargs)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_wait_for_clone_points_145": {
      "name": "fake_wait_for_clone_points",
      "type": "function",
      "start_line": 145,
      "end_line": 146,
      "content_hash": "12aecc428d4f3b90091cbb705b52bd0e644387a8",
      "content": "    def fake_wait_for_clone_points(**kwargs):\n        calls[\"wait_clone\"] += 1",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_normalize_cloned_collection_schema_148": {
      "name": "fake_normalize_cloned_collection_schema",
      "type": "function",
      "start_line": 148,
      "end_line": 149,
      "content_hash": "a4efe66274f81cb034c48bd2e785c732d1db7bd5",
      "content": "    def fake_normalize_cloned_collection_schema(**kwargs):\n        calls[\"normalize_schema\"] += 1",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_update_workspace_state_allows_repo_state_updates_without_workspace_dir_256": {
      "name": "test_update_workspace_state_allows_repo_state_updates_without_workspace_dir",
      "type": "function",
      "start_line": 256,
      "end_line": 283,
      "content_hash": "262a16a6a465f3b4d079106fcdc3b93292988573",
      "content": "def test_update_workspace_state_allows_repo_state_updates_without_workspace_dir(tmp_path: Path, monkeypatch: pytest.MonkeyPatch):\n    from scripts import workspace_state\n\n    monkeypatch.delenv(\"WORKSPACE_PATH\", raising=False)\n    monkeypatch.delenv(\"WATCH_ROOT\", raising=False)\n    monkeypatch.delenv(\"WORK_DIR\", raising=False)\n    monkeypatch.delenv(\"WORKDIR\", raising=False)\n    monkeypatch.setenv(\"MULTI_REPO_MODE\", \"1\")\n    monkeypatch.setenv(\"WORKSPACE_PATH\", str(tmp_path))\n    monkeypatch.setenv(\"WATCH_ROOT\", str(tmp_path))\n\n    repo_name = \"missing_ws_repo\"\n    repo_state_dir = tmp_path / \".codebase\" / \"repos\" / repo_name\n    repo_state_dir.mkdir(parents=True)\n\n    # Ensure the repo workspace dir does NOT exist.\n    assert not (tmp_path / repo_name).exists()\n\n    # Should still write state because state dir exists.\n    workspace_state.update_workspace_state(\n        workspace_path=str(tmp_path),\n        repo_name=repo_name,\n        updates={\"indexing_env_pending\": {\"CTXCE_E2E_MARKER\": \"ok\"}},\n    )\n\n    assert _repo_state_path(tmp_path, repo_name).exists()\n    st = _read_repo_state(tmp_path, repo_name)\n    assert (st.get(\"indexing_env_pending\") or {}).get(\"CTXCE_E2E_MARKER\") == \"ok\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_cleanup_handles_read_only_dirs_286": {
      "name": "test_cleanup_handles_read_only_dirs",
      "type": "function",
      "start_line": 286,
      "end_line": 319,
      "content_hash": "ad1f0d5a95840d41a06886a13cf83223d4dfa3ae",
      "content": "def test_cleanup_handles_read_only_dirs(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):\n    from scripts import indexing_admin\n\n    deleted = []\n\n    def fake_chmod(path, mode):\n        deleted.append((\"chmod\", path, mode))\n        original_chmod(path, mode)\n\n    target_dir = tmp_path / \"repo_old\"\n    target_dir.mkdir()\n    (target_dir / \"file.txt\").write_text(\"hello\", encoding=\"utf-8\")\n\n    # Patch shutil.rmtree to fail once, then succeed after chmod.\n    call_count = {\"attempt\": 0}\n\n    def patched_rmtree(path):\n        if call_count[\"attempt\"] == 0:\n            call_count[\"attempt\"] += 1\n            deleted.append((\"rmtree_fail\", path))\n            raise PermissionError(\"nope\")\n        deleted.append((\"rmtree_success\", path))\n        return original_rmtree(path)\n\n    original_rmtree = indexing_admin.shutil.rmtree\n    original_chmod = indexing_admin.os.chmod\n    monkeypatch.setattr(indexing_admin.shutil, \"rmtree\", patched_rmtree)\n    monkeypatch.setattr(indexing_admin.os, \"chmod\", fake_chmod)\n\n    result = indexing_admin._delete_path_tree(target_dir)\n    assert result is True\n    assert not target_dir.exists()\n    # Ensure chmod was attempted before the second rmtree.\n    assert any(entry[0] == \"chmod\" for entry in deleted)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_chmod_291": {
      "name": "fake_chmod",
      "type": "function",
      "start_line": 291,
      "end_line": 293,
      "content_hash": "d1c4f2157f09573d0436dd489fc806f9ae596832",
      "content": "    def fake_chmod(path, mode):\n        deleted.append((\"chmod\", path, mode))\n        original_chmod(path, mode)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_patched_rmtree_302": {
      "name": "patched_rmtree",
      "type": "function",
      "start_line": 302,
      "end_line": 308,
      "content_hash": "8a20942b44f84e4857b7fbe2c00666d9f4749f07",
      "content": "    def patched_rmtree(path):\n        if call_count[\"attempt\"] == 0:\n            call_count[\"attempt\"] += 1\n            deleted.append((\"rmtree_fail\", path))\n            raise PermissionError(\"nope\")\n        deleted.append((\"rmtree_success\", path))\n        return original_rmtree(path)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_start_recreates_primary_before_spawn_322": {
      "name": "test_start_recreates_primary_before_spawn",
      "type": "function",
      "start_line": 322,
      "end_line": 369,
      "content_hash": "6d32af309125ce093e786d514ac06a5165ac3509",
      "content": "def test_start_recreates_primary_before_spawn(staging_workspace: dict, monkeypatch: pytest.MonkeyPatch):\n    from scripts import indexing_admin\n    from scripts import workspace_state\n\n    root: Path = staging_workspace[\"root\"]\n    repo_name: str = staging_workspace[\"repo_name\"]\n    collection: str = staging_workspace[\"collection\"]\n    repo_ws: Path = staging_workspace[\"repo_ws\"]\n\n    _seed_repo_state(\n        workspace_state_module=workspace_state,\n        repo_ws=repo_ws,\n        repo_name=repo_name,\n        collection=collection,\n        active_env={\"CTXCE_ENV_ROLE\": \"active\"},\n        pending_env={\"CTXCE_ENV_ROLE\": \"pending\"},\n    )\n\n    events: list[str] = []\n\n    monkeypatch.setattr(\n        indexing_admin,\n        \"collection_mapping_index\",\n        lambda *, work_dir: {\n            collection: [{\"repo_name\": repo_name, \"container_path\": str(repo_ws)}]\n        },\n    )\n    monkeypatch.setattr(indexing_admin, \"_get_collection_point_count\", lambda **_: 0)\n    monkeypatch.setattr(indexing_admin, \"_wait_for_clone_points\", lambda **_: events.append(\"wait\"))\n    monkeypatch.setattr(indexing_admin, \"_normalize_cloned_collection_schema\", lambda **_: events.append(\"normalize\"))\n    monkeypatch.setattr(\n        indexing_admin,\n        \"copy_collection_qdrant\",\n        lambda **kwargs: events.append(\"copy\"),\n    )\n\n    def fake_recreate(**kwargs):\n        events.append(\"recreate\")\n\n    def fake_spawn(**kwargs):\n        events.append(\"spawn\")\n\n    monkeypatch.setattr(indexing_admin, \"recreate_collection_qdrant\", fake_recreate)\n    monkeypatch.setattr(indexing_admin, \"spawn_ingest_code\", fake_spawn)\n\n    indexing_admin.start_staging_rebuild(collection=collection, work_dir=str(root))\n    assert \"recreate\" in events and \"spawn\" in events\n    assert events.index(\"recreate\") < events.index(\"spawn\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_recreate_358": {
      "name": "fake_recreate",
      "type": "function",
      "start_line": 358,
      "end_line": 359,
      "content_hash": "6b14afb318986cdd814d6c5a6d607511b9d799ae",
      "content": "    def fake_recreate(**kwargs):\n        events.append(\"recreate\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_spawn_361": {
      "name": "fake_spawn",
      "type": "function",
      "start_line": 361,
      "end_line": 362,
      "content_hash": "188b7e4dd4161f59b31ef145474b351f2008c3e5",
      "content": "    def fake_spawn(**kwargs):\n        events.append(\"spawn\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_start_handles_copy_failure_without_mutation_372": {
      "name": "test_start_handles_copy_failure_without_mutation",
      "type": "function",
      "start_line": 372,
      "end_line": 411,
      "content_hash": "43c80c4a1d85055d51a02a4ec8e54480dd54d0c8",
      "content": "def test_start_handles_copy_failure_without_mutation(staging_workspace: dict, monkeypatch: pytest.MonkeyPatch):\n    from scripts import indexing_admin\n    from scripts import workspace_state\n\n    root: Path = staging_workspace[\"root\"]\n    repo_name: str = staging_workspace[\"repo_name\"]\n    collection: str = staging_workspace[\"collection\"]\n    repo_ws: Path = staging_workspace[\"repo_ws\"]\n\n    _seed_repo_state(\n        workspace_state_module=workspace_state,\n        repo_ws=repo_ws,\n        repo_name=repo_name,\n        collection=collection,\n    )\n\n    initial_state = _read_repo_state(root, repo_name)\n\n    monkeypatch.setattr(\n        indexing_admin,\n        \"collection_mapping_index\",\n        lambda *, work_dir: {\n            collection: [{\"repo_name\": repo_name, \"container_path\": str(repo_ws)}]\n        },\n    )\n    monkeypatch.setattr(indexing_admin, \"_get_collection_point_count\", lambda **_: 0)\n\n    def fake_copy(**kwargs):\n        raise RuntimeError(\"copy failed\")\n\n    monkeypatch.setattr(indexing_admin, \"copy_collection_qdrant\", fake_copy)\n\n    with pytest.raises(RuntimeError, match=\"copy failed\"):\n        indexing_admin.start_staging_rebuild(collection=collection, work_dir=str(root))\n\n    # State should remain unchanged.\n    assert _read_repo_state(root, repo_name) == initial_state\n    # No *_old artifacts should exist.\n    assert not (root / f\"{repo_name}_old\").exists()\n    assert not (root / \".codebase\" / \"repos\" / f\"{repo_name}_old\").exists()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_copy_399": {
      "name": "fake_copy",
      "type": "function",
      "start_line": 399,
      "end_line": 400,
      "content_hash": "ded27ac67dc193942f31431309a3109528530cd6",
      "content": "    def fake_copy(**kwargs):\n        raise RuntimeError(\"copy failed\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_start_handles_clone_wait_failure_without_mutation_414": {
      "name": "test_start_handles_clone_wait_failure_without_mutation",
      "type": "function",
      "start_line": 414,
      "end_line": 452,
      "content_hash": "dcff53cd342028106acb8c47443fd8de76d7d32f",
      "content": "def test_start_handles_clone_wait_failure_without_mutation(staging_workspace: dict, monkeypatch: pytest.MonkeyPatch):\n    from scripts import indexing_admin\n    from scripts import workspace_state\n\n    root: Path = staging_workspace[\"root\"]\n    repo_name: str = staging_workspace[\"repo_name\"]\n    collection: str = staging_workspace[\"collection\"]\n    repo_ws: Path = staging_workspace[\"repo_ws\"]\n\n    _seed_repo_state(\n        workspace_state_module=workspace_state,\n        repo_ws=repo_ws,\n        repo_name=repo_name,\n        collection=collection,\n    )\n\n    initial_state = _read_repo_state(root, repo_name)\n\n    monkeypatch.setattr(\n        indexing_admin,\n        \"collection_mapping_index\",\n        lambda *, work_dir: {\n            collection: [{\"repo_name\": repo_name, \"container_path\": str(repo_ws)}]\n        },\n    )\n    monkeypatch.setattr(indexing_admin, \"_get_collection_point_count\", lambda **_: 123)\n    monkeypatch.setattr(indexing_admin, \"copy_collection_qdrant\", lambda **kwargs: None)\n\n    def fake_wait(**kwargs):\n        raise RuntimeError(\"wait failed\")\n\n    monkeypatch.setattr(indexing_admin, \"_wait_for_clone_points\", fake_wait)\n\n    with pytest.raises(RuntimeError, match=\"wait failed\"):\n        indexing_admin.start_staging_rebuild(collection=collection, work_dir=str(root))\n\n    assert _read_repo_state(root, repo_name) == initial_state\n    assert not (root / f\"{repo_name}_old\").exists()\n    assert not (root / \".codebase\" / \"repos\" / f\"{repo_name}_old\").exists()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_wait_442": {
      "name": "fake_wait",
      "type": "function",
      "start_line": 442,
      "end_line": 443,
      "content_hash": "6efbd7f5ab7d660d6a066b9f83cccde2cdc9c43b",
      "content": "    def fake_wait(**kwargs):\n        raise RuntimeError(\"wait failed\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_start_handles_spawn_failure_but_leaves_staging_state_455": {
      "name": "test_start_handles_spawn_failure_but_leaves_staging_state",
      "type": "function",
      "start_line": 455,
      "end_line": 496,
      "content_hash": "d17e4f23da81238aa2f59f4aab267f404ba93da2",
      "content": "def test_start_handles_spawn_failure_but_leaves_staging_state(staging_workspace: dict, monkeypatch: pytest.MonkeyPatch):\n    from scripts import indexing_admin\n    from scripts import workspace_state\n\n    root: Path = staging_workspace[\"root\"]\n    repo_name: str = staging_workspace[\"repo_name\"]\n    collection: str = staging_workspace[\"collection\"]\n    repo_ws: Path = staging_workspace[\"repo_ws\"]\n\n    _seed_repo_state(\n        workspace_state_module=workspace_state,\n        repo_ws=repo_ws,\n        repo_name=repo_name,\n        collection=collection,\n    )\n\n    monkeypatch.setattr(\n        indexing_admin,\n        \"collection_mapping_index\",\n        lambda *, work_dir: {\n            collection: [{\"repo_name\": repo_name, \"container_path\": str(repo_ws)}]\n        },\n    )\n    monkeypatch.setattr(indexing_admin, \"_get_collection_point_count\", lambda **_: 0)\n    monkeypatch.setattr(indexing_admin, \"copy_collection_qdrant\", lambda **kwargs: None)\n    monkeypatch.setattr(indexing_admin, \"_wait_for_clone_points\", lambda **kwargs: None)\n    monkeypatch.setattr(indexing_admin, \"_normalize_cloned_collection_schema\", lambda **kwargs: None)\n    monkeypatch.setattr(indexing_admin, \"recreate_collection_qdrant\", lambda **kwargs: None)\n\n    def failing_spawn(**kwargs):\n        raise RuntimeError(\"spawn failed\")\n\n    monkeypatch.setattr(indexing_admin, \"spawn_ingest_code\", failing_spawn)\n\n    with pytest.raises(RuntimeError, match=\"spawn failed\"):\n        indexing_admin.start_staging_rebuild(collection=collection, work_dir=str(root))\n\n    st = _read_repo_state(root, repo_name)\n    assert (root / f\"{repo_name}_old\").exists()\n    assert (root / \".codebase\" / \"repos\" / f\"{repo_name}_old\").exists()\n    assert st.get(\"serving_collection\") == f\"{collection}_old\"\n    assert isinstance(st.get(\"staging\"), dict)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_failing_spawn_484": {
      "name": "failing_spawn",
      "type": "function",
      "start_line": 484,
      "end_line": 485,
      "content_hash": "e59da9ba2d97226183674e3e4f44b1cea8244948",
      "content": "    def failing_spawn(**kwargs):\n        raise RuntimeError(\"spawn failed\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_admin_staging_endpoints_exercise_http_layer_499": {
      "name": "test_admin_staging_endpoints_exercise_http_layer",
      "type": "function",
      "start_line": 499,
      "end_line": 542,
      "content_hash": "f09afbc87c397d73d099f08453d0865844c1e899",
      "content": "def test_admin_staging_endpoints_exercise_http_layer(monkeypatch: pytest.MonkeyPatch):\n    from scripts import upload_service\n\n    calls = {\"start\": 0, \"activate\": 0, \"abort\": 0}\n\n    monkeypatch.setattr(upload_service, \"AUTH_ENABLED\", True)\n    monkeypatch.setattr(upload_service, \"_require_admin_session\", lambda request: {\"user_id\": \"admin\"})\n    monkeypatch.setattr(upload_service, \"is_staging_enabled\", lambda: True)\n    monkeypatch.setattr(upload_service, \"WORK_DIR\", \"/fake/work\")\n    monkeypatch.setenv(\"WORK_DIR\", \"/fake/work\")\n    monkeypatch.setenv(\"CTXCE_STAGING_ENABLED\", \"1\")\n\n    def fake_start(**kwargs):\n        calls[\"start\"] += 1\n        return f\"{kwargs['collection']}_old\"\n\n    def fake_activate(**kwargs):\n        calls[\"activate\"] += 1\n\n    def fake_abort(**kwargs):\n        calls[\"abort\"] += 1\n\n    monkeypatch.setattr(upload_service, \"start_staging_rebuild\", fake_start)\n    monkeypatch.setattr(upload_service, \"activate_staging_rebuild\", fake_activate)\n    monkeypatch.setattr(upload_service, \"abort_staging_rebuild\", fake_abort)\n    monkeypatch.setattr(\n        upload_service,\n        \"resolve_collection_root\",\n        lambda **kwargs: (\"/fake/root\", \"repo1\"),\n    )\n\n    client = TestClient(upload_service.app)\n\n    resp = client.post(\"/admin/staging/start\", data={\"collection\": \"coll1\"}, follow_redirects=False)\n    assert resp.status_code == 302\n    assert calls[\"start\"] == 1\n\n    resp = client.post(\"/admin/staging/activate\", data={\"collection\": \"coll1\"}, follow_redirects=False)\n    assert resp.status_code == 302\n    assert calls[\"activate\"] == 1\n\n    resp = client.post(\"/admin/staging/abort\", data={\"collection\": \"coll1\"}, follow_redirects=False)\n    assert resp.status_code == 302\n    assert calls[\"abort\"] == 1",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_start_511": {
      "name": "fake_start",
      "type": "function",
      "start_line": 511,
      "end_line": 513,
      "content_hash": "5792115cae5b47c626c06d8aa9becaaa509fba03",
      "content": "    def fake_start(**kwargs):\n        calls[\"start\"] += 1\n        return f\"{kwargs['collection']}_old\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_activate_515": {
      "name": "fake_activate",
      "type": "function",
      "start_line": 515,
      "end_line": 516,
      "content_hash": "d26700a88573a87d781b2538780a9612d0185971",
      "content": "    def fake_activate(**kwargs):\n        calls[\"activate\"] += 1",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_abort_518": {
      "name": "fake_abort",
      "type": "function",
      "start_line": 518,
      "end_line": 519,
      "content_hash": "0d8cfde7d1cf499adf57dc7491db4177c03f33d4",
      "content": "    def fake_abort(**kwargs):\n        calls[\"abort\"] += 1",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_watcher_collection_resolution_prefers_serving_state_when_staging_enabled_545": {
      "name": "test_watcher_collection_resolution_prefers_serving_state_when_staging_enabled",
      "type": "function",
      "start_line": 545,
      "end_line": 564,
      "content_hash": "a7be7ec09aacf3d968a8daecf0c15d08140d60db",
      "content": "def test_watcher_collection_resolution_prefers_serving_state_when_staging_enabled(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):\n    from scripts import watch_index\n\n    repo_path = tmp_path / \"repo1\"\n    repo_path.mkdir()\n\n    monkeypatch.setenv(\"MULTI_REPO_MODE\", \"1\")\n    monkeypatch.setenv(\"WORKSPACE_PATH\", str(tmp_path))\n    monkeypatch.setenv(\"WATCH_ROOT\", str(tmp_path))\n\n    monkeypatch.setattr(watch_index, \"_extract_repo_name_from_path\", lambda *_: \"repo1\")\n    monkeypatch.setattr(watch_index, \"is_multi_repo_mode\", lambda: True)\n    monkeypatch.setattr(\n        watch_index,\n        \"get_workspace_state\",\n        lambda workspace_path, repo_name: {\"serving_collection\": \"coll1_old\"},\n    )\n\n    coll = watch_index._get_collection_for_repo(repo_path)\n    assert coll == \"coll1_old\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_watcher_collection_resolution_falls_back_to_env_when_staging_disabled_567": {
      "name": "test_watcher_collection_resolution_falls_back_to_env_when_staging_disabled",
      "type": "function",
      "start_line": 567,
      "end_line": 585,
      "content_hash": "ac6b25357ddf4d8c1b4e674350443a8b854e9bb8",
      "content": "def test_watcher_collection_resolution_falls_back_to_env_when_staging_disabled(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):\n    from scripts import watch_index\n\n    repo_path = tmp_path / \"repo1\"\n    repo_path.mkdir()\n\n    monkeypatch.setenv(\"MULTI_REPO_MODE\", \"0\")\n    monkeypatch.setenv(\"COLLECTION_NAME\", \"env-coll\")\n\n    monkeypatch.setattr(watch_index, \"_extract_repo_name_from_path\", lambda *_: \"repo1\")\n    monkeypatch.setattr(watch_index, \"is_multi_repo_mode\", lambda: False)\n\n    def _fail_get_collection(repo_name: str) -> str:\n        raise RuntimeError(\"no mapping\")\n\n    monkeypatch.setattr(watch_index, \"get_collection_name\", _fail_get_collection)\n\n    coll = watch_index._get_collection_for_repo(repo_path)\n    assert coll == \"env-coll\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__fail_get_collection_579": {
      "name": "_fail_get_collection",
      "type": "function",
      "start_line": 579,
      "end_line": 580,
      "content_hash": "208ad0fbc5cc996441588f6da2e28daabc770c42",
      "content": "    def _fail_get_collection(repo_name: str) -> str:\n        raise RuntimeError(\"no mapping\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_spawn_ingest_code_applies_env_overrides_for_staging_588": {
      "name": "test_spawn_ingest_code_applies_env_overrides_for_staging",
      "type": "function",
      "start_line": 588,
      "end_line": 622,
      "content_hash": "3a17af3df1b4f9c20616e6b50ce337c76e5be0cd",
      "content": "def test_spawn_ingest_code_applies_env_overrides_for_staging(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):\n    from scripts import indexing_admin\n\n    monkeypatch.setenv(\"BASE_ONLY\", \"system\")\n\n    captured: Dict[str, Any] = {}\n\n    def fake_popen(cmd, env):\n        captured[\"cmd\"] = cmd\n        captured[\"env\"] = env\n        class _Proc:\n            pass\n        return _Proc()\n\n    monkeypatch.setattr(indexing_admin.subprocess, \"Popen\", fake_popen)\n    monkeypatch.setattr(indexing_admin, \"update_indexing_status\", lambda **_: None)\n\n    overrides = {\"PENDING_KEY\": \"pending\", \"BASE_ONLY\": \"override\"}\n\n    indexing_admin.spawn_ingest_code(\n        root=str(tmp_path / \"repo1\"),\n        work_dir=str(tmp_path),\n        collection=\"primary-coll\",\n        recreate=True,\n        repo_name=\"repo1\",\n        env_overrides=overrides,\n    )\n\n    env = captured[\"env\"]\n    assert env[\"PENDING_KEY\"] == \"pending\"\n    assert env[\"BASE_ONLY\"] == \"override\"\n    assert env[\"COLLECTION_NAME\"] == \"primary-coll\"\n    assert env[\"CTXCE_FORCE_COLLECTION_NAME\"] == \"1\"\n    assert env[\"WATCH_ROOT\"] == str(tmp_path)\n    assert env[\"WORKSPACE_PATH\"] == str(tmp_path)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_popen_595": {
      "name": "fake_popen",
      "type": "function",
      "start_line": 595,
      "end_line": 600,
      "content_hash": "9ca1e2e70f19bb6726819ef8ef40cb8e3c8eb710",
      "content": "    def fake_popen(cmd, env):\n        captured[\"cmd\"] = cmd\n        captured[\"env\"] = env\n        class _Proc:\n            pass\n        return _Proc()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class__Proc_598": {
      "name": "_Proc",
      "type": "class",
      "start_line": 598,
      "end_line": 599,
      "content_hash": "5eb4a4d12544c2c166858a0596f7cfbf25591ab4",
      "content": "        class _Proc:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_spawn_ingest_code_without_overrides_uses_system_env_625": {
      "name": "test_spawn_ingest_code_without_overrides_uses_system_env",
      "type": "function",
      "start_line": 625,
      "end_line": 653,
      "content_hash": "848c399fa8599ed759d6dc3c4908b3674282b4a4",
      "content": "def test_spawn_ingest_code_without_overrides_uses_system_env(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):\n    from scripts import indexing_admin\n\n    monkeypatch.setenv(\"BASE_ONLY\", \"system\")\n\n    captured: Dict[str, Any] = {}\n\n    def fake_popen(cmd, env):\n        captured[\"env\"] = env\n        class _Proc:\n            pass\n        return _Proc()\n\n    monkeypatch.setattr(indexing_admin.subprocess, \"Popen\", fake_popen)\n    monkeypatch.setattr(indexing_admin, \"update_indexing_status\", lambda **_: None)\n\n    indexing_admin.spawn_ingest_code(\n        root=str(tmp_path / \"repo1\"),\n        work_dir=str(tmp_path),\n        collection=\"primary-coll\",\n        recreate=False,\n        repo_name=\"repo1\",\n        env_overrides=None,\n    )\n\n    env = captured[\"env\"]\n    assert env[\"BASE_ONLY\"] == \"system\"\n    assert env[\"COLLECTION_NAME\"] == \"primary-coll\"\n    assert \"CTXCE_FORCE_COLLECTION_NAME\" not in env",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_popen_632": {
      "name": "fake_popen",
      "type": "function",
      "start_line": 632,
      "end_line": 636,
      "content_hash": "89f0e248294e2ea80559a4a48b1f4ecf4ab6b4ed",
      "content": "    def fake_popen(cmd, env):\n        captured[\"env\"] = env\n        class _Proc:\n            pass\n        return _Proc()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class__Proc_634": {
      "name": "_Proc",
      "type": "class",
      "start_line": 634,
      "end_line": 635,
      "content_hash": "5eb4a4d12544c2c166858a0596f7cfbf25591ab4",
      "content": "        class _Proc:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_promote_pending_env_without_pending_config_656": {
      "name": "test_promote_pending_env_without_pending_config",
      "type": "function",
      "start_line": 656,
      "end_line": 679,
      "content_hash": "914e39525f2c44ff57553a8755197f2b571ef564",
      "content": "def test_promote_pending_env_without_pending_config(staging_workspace: dict):\n    from scripts import workspace_state\n\n    root: Path = staging_workspace[\"root\"]\n    repo_name: str = staging_workspace[\"repo_name\"]\n    repo_ws: Path = staging_workspace[\"repo_ws\"]\n\n    workspace_state.update_workspace_state(\n        workspace_path=str(repo_ws),\n        repo_name=repo_name,\n        updates={\n            \"indexing_env\": {\"ROLE\": \"active\"},\n            \"indexing_env_pending\": {\"ROLE\": \"pending\"},\n            \"indexing_config\": {\"chunk\": \"current\"},\n            \"indexing_config_pending\": None,\n        },\n    )\n\n    workspace_state.promote_pending_indexing_config(workspace_path=str(repo_ws), repo_name=repo_name)\n\n    st = _read_repo_state(root, repo_name)\n    assert st.get(\"indexing_env\") == {\"ROLE\": \"pending\"}\n    assert st.get(\"indexing_config\") == {\"chunk\": \"current\"}\n    assert st.get(\"indexing_env_pending\") in (None, {})",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_promote_pending_config_without_pending_env_682": {
      "name": "test_promote_pending_config_without_pending_env",
      "type": "function",
      "start_line": 682,
      "end_line": 705,
      "content_hash": "be07f6a52061874b1b72c895e6330c399351915d",
      "content": "def test_promote_pending_config_without_pending_env(staging_workspace: dict):\n    from scripts import workspace_state\n\n    root: Path = staging_workspace[\"root\"]\n    repo_name: str = staging_workspace[\"repo_name\"]\n    repo_ws: Path = staging_workspace[\"repo_ws\"]\n\n    workspace_state.update_workspace_state(\n        workspace_path=str(repo_ws),\n        repo_name=repo_name,\n        updates={\n            \"indexing_env\": {\"ROLE\": \"active\"},\n            \"indexing_env_pending\": None,\n            \"indexing_config\": {\"chunk\": \"current\"},\n            \"indexing_config_pending\": {\"chunk\": \"pending\"},\n        },\n    )\n\n    workspace_state.promote_pending_indexing_config(workspace_path=str(repo_ws), repo_name=repo_name)\n\n    st = _read_repo_state(root, repo_name)\n    assert st.get(\"indexing_env\") == {\"ROLE\": \"active\"}\n    assert st.get(\"indexing_config\") == {\"chunk\": \"pending\"}\n    assert st.get(\"indexing_config_pending\") in (None, {})",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_resolve_codebase_root_prefers_env_path_708": {
      "name": "test_resolve_codebase_root_prefers_env_path",
      "type": "function",
      "start_line": 708,
      "end_line": 719,
      "content_hash": "1ee7d53c8131b60c64b8f8ee50071036e3c2aa45",
      "content": "def test_resolve_codebase_root_prefers_env_path(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):\n    from scripts import indexing_admin\n\n    work_root = tmp_path / \"work\" / \"repo\"\n    work_root.mkdir(parents=True)\n    env_root = tmp_path / \"codebase_home\"\n    (env_root / \".codebase\" / \"repos\").mkdir(parents=True)\n\n    monkeypatch.setenv(\"CTXCE_CODEBASE_ROOT\", str(env_root))\n\n    resolved = indexing_admin._resolve_codebase_root(work_root)\n    assert resolved == env_root",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_resolve_codebase_root_fallbacks_to_parent_722": {
      "name": "test_resolve_codebase_root_fallbacks_to_parent",
      "type": "function",
      "start_line": 722,
      "end_line": 738,
      "content_hash": "fffd76714ff1d6f10ca2555632eb6f2deede0173",
      "content": "def test_resolve_codebase_root_fallbacks_to_parent(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):\n    from scripts import indexing_admin\n\n    parent = tmp_path / \"workspace\"\n    repo = parent / \"repo\"\n    repo.mkdir(parents=True)\n    codebase_root = parent\n    codebase_dir = codebase_root / \".codebase\" / \"repos\"\n    codebase_dir.mkdir(parents=True)\n\n    monkeypatch.delenv(\"CTXCE_CODEBASE_ROOT\", raising=False)\n\n    resolved = indexing_admin._resolve_codebase_root(parent)\n    assert resolved == codebase_root\n\n    resolved_parent = indexing_admin._resolve_codebase_root(repo)\n    assert resolved_parent == codebase_root",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_admin_abort_endpoint_falls_back_to_clear_when_abort_helper_missing_741": {
      "name": "test_admin_abort_endpoint_falls_back_to_clear_when_abort_helper_missing",
      "type": "function",
      "start_line": 741,
      "end_line": 763,
      "content_hash": "190487115ab1a18d2045801e72fd97683c9ce636",
      "content": "def test_admin_abort_endpoint_falls_back_to_clear_when_abort_helper_missing(monkeypatch: pytest.MonkeyPatch):\n    from scripts import upload_service\n\n    calls = {\"clear\": []}\n\n    monkeypatch.setattr(upload_service, \"_require_admin_session\", lambda request: {\"user_id\": \"admin\"})\n    monkeypatch.setattr(upload_service, \"abort_staging_rebuild\", None)\n    monkeypatch.setattr(\n        upload_service,\n        \"clear_staging_collection\",\n        lambda workspace_path, repo_name: calls[\"clear\"].append((workspace_path, repo_name)),\n    )\n    monkeypatch.setattr(\n        upload_service,\n        \"resolve_collection_root\",\n        lambda **kwargs: (\"/fake/root\", \"repo1\"),\n    )\n\n    client = TestClient(upload_service.app)\n\n    resp = client.post(\"/admin/staging/abort\", data={\"collection\": \"coll1\"}, follow_redirects=False)\n    assert resp.status_code == 302\n    assert calls[\"clear\"] == [(\"/fake/root\", \"repo1\")]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_watcher_collection_reuse_logical_repo_766": {
      "name": "test_watcher_collection_reuse_logical_repo",
      "type": "function",
      "start_line": 766,
      "end_line": 800,
      "content_hash": "3ae2f8157f077401445e316d64ade5fe01163c23",
      "content": "def test_watcher_collection_reuse_logical_repo(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):\n    from scripts import watch_index\n\n    repo_path = tmp_path / \"repoA\"\n    repo_path.mkdir()\n\n    monkeypatch.setenv(\"MULTI_REPO_MODE\", \"1\")\n    monkeypatch.setenv(\"WORKSPACE_PATH\", str(tmp_path))\n    monkeypatch.setattr(watch_index, \"_extract_repo_name_from_path\", lambda *_: \"repoA\")\n    monkeypatch.setattr(watch_index, \"is_multi_repo_mode\", lambda: True)\n    monkeypatch.setattr(watch_index, \"logical_repo_reuse_enabled\", lambda: True)\n\n    def fake_get_state(ws_path, repo_name):\n        return {}\n\n    monkeypatch.setattr(watch_index, \"get_workspace_state\", fake_get_state)\n\n    def fake_ensure(state, ws_path):\n        new_state = dict(state or {})\n        new_state[\"logical_repo_id\"] = \"lrid\"\n        return new_state\n\n    monkeypatch.setattr(watch_index, \"ensure_logical_repo_id\", fake_ensure)\n    monkeypatch.setattr(watch_index, \"find_collection_for_logical_repo\", lambda lrid, search_root: \"reuse-coll\")\n\n    updates = []\n    monkeypatch.setattr(\n        watch_index,\n        \"update_workspace_state\",\n        lambda **kwargs: updates.append(kwargs),\n    )\n\n    coll = watch_index._get_collection_for_repo(repo_path)\n    assert coll == \"reuse-coll\"\n    assert updates and updates[0][\"updates\"][\"qdrant_collection\"] == \"reuse-coll\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_get_state_778": {
      "name": "fake_get_state",
      "type": "function",
      "start_line": 778,
      "end_line": 779,
      "content_hash": "d6e19ffdf98f237f7621992a97629c297645899f",
      "content": "    def fake_get_state(ws_path, repo_name):\n        return {}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_ensure_783": {
      "name": "fake_ensure",
      "type": "function",
      "start_line": 783,
      "end_line": 786,
      "content_hash": "93e7a8d8af4ae05021376672a4fe1b2edd367ecc",
      "content": "    def fake_ensure(state, ws_path):\n        new_state = dict(state or {})\n        new_state[\"logical_repo_id\"] = \"lrid\"\n        return new_state",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}