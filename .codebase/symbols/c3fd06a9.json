{
  "file_path": "/work/external-deps/Context-Engine/tests/test_smart_reindex_vectors.py",
  "file_hash": "ee723868113f67102d8bb4ba56bb7bb4b6163e44",
  "updated_at": "2025-12-26T17:34:23.914505",
  "symbols": {
    "function_test_smart_reindex_refreshes_lex_vector_for_reused_chunks_10": {
      "name": "test_smart_reindex_refreshes_lex_vector_for_reused_chunks",
      "type": "function",
      "start_line": 10,
      "end_line": 115,
      "content_hash": "33d7abcf0bdb712affd4d261da8ad9d915b7a0d7",
      "content": "def test_smart_reindex_refreshes_lex_vector_for_reused_chunks(tmp_path, monkeypatch):\n    \"\"\"When reusing an existing dense embedding, smart reindex must refresh LEX vector.\n\n    Otherwise pseudo/tags changes can drift from the stored lexical vector.\n    \"\"\"\n    # The smart reindex logic we test doesn't require the real library.\n    monkeypatch.setitem(sys.modules, \"fastembed\", SimpleNamespace(TextEmbedding=object))\n\n    from scripts import ingest_code\n\n    # Deterministic pseudo/tags so we can predict lexical vector.\n    monkeypatch.setattr(\n        ingest_code,\n        \"should_process_pseudo_for_chunk\",\n        lambda fp, ch, changed: (False, \"pseudo\", [\"tag\"]),\n    )\n\n    # Avoid touching any caches.\n    monkeypatch.setattr(ingest_code, \"get_cached_symbols\", lambda fp: {})\n    monkeypatch.setattr(ingest_code, \"compare_symbol_changes\", lambda a, b: ([], []))\n    monkeypatch.setattr(ingest_code, \"set_cached_pseudo\", None)\n    monkeypatch.setattr(ingest_code, \"set_cached_symbols\", None)\n    monkeypatch.setattr(ingest_code, \"set_cached_file_hash\", None)\n\n    # Force simple line chunking.\n    monkeypatch.setenv(\"INDEX_MICRO_CHUNKS\", \"0\")\n    monkeypatch.setenv(\"INDEX_SEMANTIC_CHUNKS\", \"0\")\n    monkeypatch.setenv(\"USE_TREE_SITTER\", \"0\")\n    monkeypatch.setenv(\"REFRAG_MODE\", \"0\")\n\n    code = \"def add(a, b):\\n    return a + b\\n\"\n    fp = tmp_path / \"x.py\"\n    fp.write_text(code, encoding=\"utf-8\")\n\n    # Compute the exact chunk text the indexer will use.\n    chunk = ingest_code.chunk_lines(code, max_lines=120, overlap=20)[0]\n    code_text = chunk[\"text\"]\n    info_text = ingest_code.build_information(\n        \"python\",\n        Path(fp),\n        chunk[\"start\"],\n        chunk[\"end\"],\n        code_text.splitlines()[0] if code_text else \"\",\n    )\n\n    dense_key = \"dense\"\n    old_lex = [0.0] * ingest_code.LEX_VECTOR_DIM\n    old_lex[0] = 1.0\n\n    existing_record = SimpleNamespace(\n        payload={\n            \"document\": info_text,\n            \"information\": info_text,\n            \"metadata\": {\n                \"path\": str(fp),\n                \"code\": code_text,\n                \"kind\": \"function\",\n                \"symbol\": \"add\",\n                \"start_line\": 1,\n            }\n        },\n        vector={dense_key: [0.1, 0.2, 0.3], ingest_code.LEX_VECTOR_NAME: old_lex},\n    )\n\n    class FakeClient:\n        def scroll(self, **kwargs):\n            # Return one existing point and then stop.\n            if getattr(self, \"_done\", False):\n                return ([], None)\n            self._done = True\n            return ([existing_record], None)\n\n    captured = {}\n\n    def fake_upsert_points(_client, _collection, points):\n        captured[\"points\"] = points\n\n    monkeypatch.setattr(ingest_code, \"upsert_points\", fake_upsert_points)\n    monkeypatch.setattr(ingest_code, \"delete_points_by_path\", lambda *a, **k: None)\n\n    # Model is unused in reuse-only path.\n    dummy_model = object()\n\n    status = ingest_code.process_file_with_smart_reindexing(\n        file_path=Path(fp),\n        text=code,\n        language=\"python\",\n        client=FakeClient(),\n        current_collection=\"c\",\n        per_file_repo=\"r\",\n        model=dummy_model,\n        vector_name=dense_key,\n    )\n\n    assert status == \"success\"\n    assert \"points\" in captured and len(captured[\"points\"]) == 1\n\n    out_vec = captured[\"points\"][0].vector\n    assert isinstance(out_vec, dict)\n    assert ingest_code.LEX_VECTOR_NAME in out_vec\n\n    expected_aug = (code_text or \"\") + \" pseudo\" + \" tag\"\n    expected_lex = ingest_code._lex_hash_vector_text(expected_aug)\n    assert out_vec[ingest_code.LEX_VECTOR_NAME] == expected_lex\n    # Make sure we didn't keep the old lex vector.\n    assert out_vec[ingest_code.LEX_VECTOR_NAME] != old_lex",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_FakeClient_74": {
      "name": "FakeClient",
      "type": "class",
      "start_line": 74,
      "end_line": 80,
      "content_hash": "6eecb78bfead1fc58a098287e082d50d17ee96e6",
      "content": "    class FakeClient:\n        def scroll(self, **kwargs):\n            # Return one existing point and then stop.\n            if getattr(self, \"_done\", False):\n                return ([], None)\n            self._done = True\n            return ([existing_record], None)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_scroll_75": {
      "name": "scroll",
      "type": "method",
      "start_line": 75,
      "end_line": 80,
      "content_hash": "a01511dc44ffdc4ff397364bf326c75671e5031b",
      "content": "        def scroll(self, **kwargs):\n            # Return one existing point and then stop.\n            if getattr(self, \"_done\", False):\n                return ([], None)\n            self._done = True\n            return ([existing_record], None)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_upsert_points_84": {
      "name": "fake_upsert_points",
      "type": "function",
      "start_line": 84,
      "end_line": 85,
      "content_hash": "aa2470b7d535c7d51febb99cd28f251ab8db7976",
      "content": "    def fake_upsert_points(_client, _collection, points):\n        captured[\"points\"] = points",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_smart_reindex_does_not_reuse_when_info_changes_118": {
      "name": "test_smart_reindex_does_not_reuse_when_info_changes",
      "type": "function",
      "start_line": 118,
      "end_line": 201,
      "content_hash": "d1e9df5f0730b4edb08c619e1564ea9e84ae297b",
      "content": "def test_smart_reindex_does_not_reuse_when_info_changes(tmp_path, monkeypatch):\n    \"\"\"Dense embeddings must not be reused if `information` differs.\"\"\"\n\n    monkeypatch.setitem(sys.modules, \"fastembed\", SimpleNamespace(TextEmbedding=object))\n\n    from scripts import ingest_code\n\n    # Avoid touching any caches.\n    monkeypatch.setattr(ingest_code, \"get_cached_symbols\", lambda fp: {})\n    monkeypatch.setattr(ingest_code, \"compare_symbol_changes\", lambda a, b: ([], []))\n    monkeypatch.setattr(ingest_code, \"set_cached_pseudo\", None)\n    monkeypatch.setattr(ingest_code, \"set_cached_symbols\", None)\n    monkeypatch.setattr(ingest_code, \"set_cached_file_hash\", None)\n\n    # Force simple line chunking.\n    monkeypatch.setenv(\"INDEX_MICRO_CHUNKS\", \"0\")\n    monkeypatch.setenv(\"INDEX_SEMANTIC_CHUNKS\", \"0\")\n    monkeypatch.setenv(\"USE_TREE_SITTER\", \"0\")\n    monkeypatch.setenv(\"REFRAG_MODE\", \"0\")\n\n    # Make build_information return a value that won't match the stored record.\n    old_info = \"old-info\"\n    new_info = \"new-info\"\n    monkeypatch.setattr(ingest_code, \"build_information\", lambda *a, **k: new_info)\n\n    code = \"def hi():\\n    return 1\\n\"\n    fp = tmp_path / \"x.py\"\n    fp.write_text(code, encoding=\"utf-8\")\n\n    chunk = ingest_code.chunk_lines(code, max_lines=120, overlap=20)[0]\n    code_text = chunk[\"text\"]\n\n    dense_key = \"dense\"\n    reused_dense = [0.123, 0.456]\n\n    existing_record = SimpleNamespace(\n        payload={\n            \"document\": old_info,\n            \"information\": old_info,\n            \"metadata\": {\n                \"path\": str(fp),\n                \"code\": code_text,\n                \"kind\": \"function\",\n                \"symbol\": \"hi\",\n                \"start_line\": 1,\n            },\n        },\n        vector={dense_key: reused_dense, ingest_code.LEX_VECTOR_NAME: [0.0] * ingest_code.LEX_VECTOR_DIM},\n    )\n\n    class FakeClient:\n        def scroll(self, **kwargs):\n            if getattr(self, \"_done\", False):\n                return ([], None)\n            self._done = True\n            return ([existing_record], None)\n\n    captured = {}\n\n    def fake_upsert_points(_client, _collection, points):\n        captured[\"points\"] = points\n\n    monkeypatch.setattr(ingest_code, \"upsert_points\", fake_upsert_points)\n    monkeypatch.setattr(ingest_code, \"delete_points_by_path\", lambda *a, **k: None)\n\n    embedded_vec = [9.9, 8.8]\n    monkeypatch.setattr(ingest_code, \"embed_batch\", lambda _model, texts: [embedded_vec for _ in texts])\n\n    status = ingest_code.process_file_with_smart_reindexing(\n        file_path=Path(fp),\n        text=code,\n        language=\"python\",\n        client=FakeClient(),\n        current_collection=\"c\",\n        per_file_repo=\"r\",\n        model=object(),\n        vector_name=dense_key,\n    )\n\n    assert status == \"success\"\n    assert len(captured[\"points\"]) == 1\n    out_vec = captured[\"points\"][0].vector\n    assert out_vec[dense_key] == embedded_vec\n    assert out_vec[dense_key] != reused_dense",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_FakeClient_168": {
      "name": "FakeClient",
      "type": "class",
      "start_line": 168,
      "end_line": 173,
      "content_hash": "a5b44eaf100294b1baad7087a0670d061cc53ec6",
      "content": "    class FakeClient:\n        def scroll(self, **kwargs):\n            if getattr(self, \"_done\", False):\n                return ([], None)\n            self._done = True\n            return ([existing_record], None)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_scroll_169": {
      "name": "scroll",
      "type": "method",
      "start_line": 169,
      "end_line": 173,
      "content_hash": "63eccbd2c7061b9c1ef88db2286c74294f507773",
      "content": "        def scroll(self, **kwargs):\n            if getattr(self, \"_done\", False):\n                return ([], None)\n            self._done = True\n            return ([existing_record], None)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_upsert_points_177": {
      "name": "fake_upsert_points",
      "type": "function",
      "start_line": 177,
      "end_line": 178,
      "content_hash": "aa2470b7d535c7d51febb99cd28f251ab8db7976",
      "content": "    def fake_upsert_points(_client, _collection, points):\n        captured[\"points\"] = points",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_smart_reindex_unnamed_reuse_requires_dense_vector_204": {
      "name": "test_smart_reindex_unnamed_reuse_requires_dense_vector",
      "type": "function",
      "start_line": 204,
      "end_line": 289,
      "content_hash": "c2dacb54ad67b75eb57d0456dd263052c7d40a68",
      "content": "def test_smart_reindex_unnamed_reuse_requires_dense_vector(tmp_path, monkeypatch):\n    \"\"\"If an existing unnamed-vector point has only lex/mini, re-embed instead of reusing [].\"\"\"\n\n    monkeypatch.setitem(sys.modules, \"fastembed\", SimpleNamespace(TextEmbedding=object))\n\n    from scripts import ingest_code\n\n    # Avoid touching any caches.\n    monkeypatch.setattr(ingest_code, \"get_cached_symbols\", lambda fp: {})\n    monkeypatch.setattr(ingest_code, \"compare_symbol_changes\", lambda a, b: ([], []))\n    monkeypatch.setattr(ingest_code, \"set_cached_pseudo\", None)\n    monkeypatch.setattr(ingest_code, \"set_cached_symbols\", None)\n    monkeypatch.setattr(ingest_code, \"set_cached_file_hash\", None)\n\n    # Force simple line chunking.\n    monkeypatch.setenv(\"INDEX_MICRO_CHUNKS\", \"0\")\n    monkeypatch.setenv(\"INDEX_SEMANTIC_CHUNKS\", \"0\")\n    monkeypatch.setenv(\"USE_TREE_SITTER\", \"0\")\n    monkeypatch.setenv(\"REFRAG_MODE\", \"0\")\n\n    code = \"def hi():\\n    return 1\\n\"\n    fp = tmp_path / \"x.py\"\n    fp.write_text(code, encoding=\"utf-8\")\n\n    chunk = ingest_code.chunk_lines(code, max_lines=120, overlap=20)[0]\n    code_text = chunk[\"text\"]\n    info_text = ingest_code.build_information(\n        \"python\",\n        Path(fp),\n        chunk[\"start\"],\n        chunk[\"end\"],\n        code_text.splitlines()[0] if code_text else \"\",\n    )\n\n    existing_record = SimpleNamespace(\n        payload={\n            \"document\": info_text,\n            \"information\": info_text,\n            \"metadata\": {\n                \"path\": str(fp),\n                \"code\": code_text,\n                \"kind\": \"function\",\n                \"symbol\": \"hi\",\n                \"start_line\": 1,\n            },\n        },\n        # Only lex/mini present: should not be reused as dense.\n        vector={\n            ingest_code.LEX_VECTOR_NAME: [0.0] * ingest_code.LEX_VECTOR_DIM,\n            ingest_code.MINI_VECTOR_NAME: [0.0] * ingest_code.MINI_VEC_DIM,\n        },\n    )\n\n    class FakeClient:\n        def scroll(self, **kwargs):\n            if getattr(self, \"_done\", False):\n                return ([], None)\n            self._done = True\n            return ([existing_record], None)\n\n    captured = {}\n\n    def fake_upsert_points(_client, _collection, points):\n        captured[\"points\"] = points\n\n    monkeypatch.setattr(ingest_code, \"upsert_points\", fake_upsert_points)\n    monkeypatch.setattr(ingest_code, \"delete_points_by_path\", lambda *a, **k: None)\n\n    embedded_vec = [7.7, 6.6]\n    monkeypatch.setattr(ingest_code, \"embed_batch\", lambda _model, texts: [embedded_vec for _ in texts])\n\n    status = ingest_code.process_file_with_smart_reindexing(\n        file_path=Path(fp),\n        text=code,\n        language=\"python\",\n        client=FakeClient(),\n        current_collection=\"c\",\n        per_file_repo=\"r\",\n        model=object(),\n        vector_name=None,\n    )\n\n    assert status == \"success\"\n    assert len(captured[\"points\"]) == 1\n    out_vec = captured[\"points\"][0].vector\n    assert out_vec == embedded_vec",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_FakeClient_257": {
      "name": "FakeClient",
      "type": "class",
      "start_line": 257,
      "end_line": 262,
      "content_hash": "a5b44eaf100294b1baad7087a0670d061cc53ec6",
      "content": "    class FakeClient:\n        def scroll(self, **kwargs):\n            if getattr(self, \"_done\", False):\n                return ([], None)\n            self._done = True\n            return ([existing_record], None)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_scroll_258": {
      "name": "scroll",
      "type": "method",
      "start_line": 258,
      "end_line": 262,
      "content_hash": "63eccbd2c7061b9c1ef88db2286c74294f507773",
      "content": "        def scroll(self, **kwargs):\n            if getattr(self, \"_done\", False):\n                return ([], None)\n            self._done = True\n            return ([existing_record], None)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fake_upsert_points_266": {
      "name": "fake_upsert_points",
      "type": "function",
      "start_line": 266,
      "end_line": 267,
      "content_hash": "aa2470b7d535c7d51febb99cd28f251ab8db7976",
      "content": "    def fake_upsert_points(_client, _collection, points):\n        captured[\"points\"] = points",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}