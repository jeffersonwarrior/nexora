{
  "file_path": "/work/external-deps/Context-Engine/tests/test_context_answer.py",
  "file_hash": "f00420c5df50748d2009eb8b2a816f1c1a0d10c0",
  "updated_at": "2025-12-26T17:34:20.237680",
  "symbols": {
    "function__fake_items_9": {
      "name": "_fake_items",
      "type": "function",
      "start_line": 9,
      "end_line": 27,
      "content_hash": "b1b888d24457d4f4997d4478a12ae50100d0c923",
      "content": "def _fake_items():\n    return [\n        {\n            \"score\": 1.0,\n            \"path\": \"/work/foo.py\",\n            \"symbol\": \"foo\",\n            \"start_line\": 10,\n            \"end_line\": 16,\n            \"span_budgeted\": True,\n        },\n        {\n            \"score\": 0.9,\n            \"path\": \"/work/bar.py\",\n            \"symbol\": \"bar\",\n            \"start_line\": 5,\n            \"end_line\": 8,\n            \"span_budgeted\": True,\n        },\n    ]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_context_answer_happy_path_31": {
      "name": "test_context_answer_happy_path",
      "type": "function",
      "start_line": 31,
      "end_line": 62,
      "content_hash": "aeb9d9e74afe52340ac5f3ff7b5078421b261910",
      "content": "def test_context_answer_happy_path(monkeypatch):\n    # Mock embedding model to avoid loading real model\n    monkeypatch.setattr(srv, \"_get_embedding_model\", lambda *a, **k: None)\n\n    # Fake retrieval output (already budgeted)\n    import scripts.hybrid_search as hs\n\n    monkeypatch.setattr(hs, \"run_hybrid_search\", lambda **k: _fake_items())\n\n    # Fake decoder\n    import scripts.refrag_llamacpp as ref\n\n    class FakeLlama:\n        def __init__(self, *a, **k):\n            pass\n\n        def generate_with_soft_embeddings(self, prompt: str, max_tokens: int = 256, **kw):\n            assert \"Sources:\" in prompt and \"[1]\" in prompt\n            return \"Answer using [1]\"\n\n    monkeypatch.setattr(ref, \"LlamaCppRefragClient\", FakeLlama)\n    monkeypatch.setattr(ref, \"is_decoder_enabled\", lambda: True)\n\n    out = srv.asyncio.get_event_loop().run_until_complete(\n        srv.context_answer(query=\"how to do x\", limit=2, per_path=1)\n    )\n\n    assert isinstance(out, dict)\n    assert out.get(\"answer\") and \"[1]\" in out[\"answer\"]\n    cits = out.get(\"citations\") or []\n    assert len(cits) >= 1\n    assert {\"path\", \"start_line\", \"end_line\"}.issubset(set(cits[0].keys()))",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_FakeLlama_43": {
      "name": "FakeLlama",
      "type": "class",
      "start_line": 43,
      "end_line": 49,
      "content_hash": "55e53a4b9274dcfbd2b96409685c00798721bc18",
      "content": "    class FakeLlama:\n        def __init__(self, *a, **k):\n            pass\n\n        def generate_with_soft_embeddings(self, prompt: str, max_tokens: int = 256, **kw):\n            assert \"Sources:\" in prompt and \"[1]\" in prompt\n            return \"Answer using [1]\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___44": {
      "name": "__init__",
      "type": "method",
      "start_line": 44,
      "end_line": 45,
      "content_hash": "2f01f3eeec882714bac9a504b0450cf9884e2078",
      "content": "        def __init__(self, *a, **k):\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_generate_with_soft_embeddings_47": {
      "name": "generate_with_soft_embeddings",
      "type": "method",
      "start_line": 47,
      "end_line": 49,
      "content_hash": "8401b52e22bb311b562ee601ca220b3117d1ce64",
      "content": "        def generate_with_soft_embeddings(self, prompt: str, max_tokens: int = 256, **kw):\n            assert \"Sources:\" in prompt and \"[1]\" in prompt\n            return \"Answer using [1]\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_context_answer_decoder_disabled_65": {
      "name": "test_context_answer_decoder_disabled",
      "type": "function",
      "start_line": 65,
      "end_line": 90,
      "content_hash": "f05d5e8a038d5894307958b4fb769489a7f1b453",
      "content": "def test_context_answer_decoder_disabled(monkeypatch):\n    # Mock embedding model to avoid loading real model\n    monkeypatch.setattr(srv, \"_get_embedding_model\", lambda *a, **k: None)\n\n    import scripts.hybrid_search as hs\n\n    monkeypatch.setattr(hs, \"run_hybrid_search\", lambda **k: _fake_items())\n\n    import scripts.refrag_llamacpp as ref\n\n    class FakeLlama:\n        def __init__(self, *a, **k):\n            pass\n\n        def generate_with_soft_embeddings(self, *a, **k):\n            return \"SHOULD_NOT_BE_CALLED\"\n\n    monkeypatch.setattr(ref, \"LlamaCppRefragClient\", FakeLlama)\n    monkeypatch.setattr(ref, \"is_decoder_enabled\", lambda: False)\n\n    out = srv.asyncio.get_event_loop().run_until_complete(\n        srv.context_answer(query=\"how to do y\", limit=1)\n    )\n\n    assert \"error\" in out\n    assert isinstance(out.get(\"citations\"), list)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_FakeLlama_75": {
      "name": "FakeLlama",
      "type": "class",
      "start_line": 75,
      "end_line": 80,
      "content_hash": "f76616d033f19eadcf6acadf8a937fdc00252104",
      "content": "    class FakeLlama:\n        def __init__(self, *a, **k):\n            pass\n\n        def generate_with_soft_embeddings(self, *a, **k):\n            return \"SHOULD_NOT_BE_CALLED\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___76": {
      "name": "__init__",
      "type": "method",
      "start_line": 76,
      "end_line": 77,
      "content_hash": "2f01f3eeec882714bac9a504b0450cf9884e2078",
      "content": "        def __init__(self, *a, **k):\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_generate_with_soft_embeddings_79": {
      "name": "generate_with_soft_embeddings",
      "type": "method",
      "start_line": 79,
      "end_line": 80,
      "content_hash": "b88972fe66b1d38a457faee90be93381fe3dcb65",
      "content": "        def generate_with_soft_embeddings(self, *a, **k):\n            return \"SHOULD_NOT_BE_CALLED\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_context_answer_prefers_identifier_spans_93": {
      "name": "test_context_answer_prefers_identifier_spans",
      "type": "function",
      "start_line": 93,
      "end_line": 139,
      "content_hash": "bbd8bc098fb52a4c3c6d1b5b4a7923b7c0dabe13",
      "content": "def test_context_answer_prefers_identifier_spans(monkeypatch):\n    # Mock embedding model to avoid loading real model\n    monkeypatch.setattr(srv, \"_get_embedding_model\", lambda *a, **k: None)\n\n    import scripts.hybrid_search as hs\n\n    def _items():\n        return [\n            {\n                \"score\": 1.0,\n                \"path\": \"/work/foo.py\",\n                \"symbol\": \"foo\",\n                \"start_line\": 10,\n                \"end_line\": 16,\n                \"text\": \"def helper():\\n    return 42\\n\",\n            },\n            {\n                \"score\": 0.8,\n                \"path\": \"/work/bar.py\",\n                \"symbol\": \"RRF_K\",\n                \"start_line\": 5,\n                \"end_line\": 9,\n                \"text\": \"RRF_K = 60\\n\",\n            },\n        ]\n\n    monkeypatch.setattr(hs, \"run_hybrid_search\", lambda **k: _items())\n\n    import scripts.refrag_llamacpp as ref\n\n    class FakeLlama:\n        def __init__(self, *a, **k):\n            pass\n\n        def generate_with_soft_embeddings(self, prompt: str, max_tokens: int = 256, **kw):\n            return \"Definition: \\\"RRF_K = 60\\\" [1]\\nUsage: Appears in code. [1]\"\n\n    monkeypatch.setattr(ref, \"LlamaCppRefragClient\", FakeLlama)\n    monkeypatch.setattr(ref, \"is_decoder_enabled\", lambda: True)\n\n    out = srv.asyncio.get_event_loop().run_until_complete(\n        srv.context_answer(query=\"what is RRF_K in hybrid_search.py?\", limit=1, per_path=1)\n    )\n\n    cits = out.get(\"citations\") or []\n    assert len(cits) == 1\n    assert cits[0][\"path\"] == \"/work/bar.py\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__items_99": {
      "name": "_items",
      "type": "function",
      "start_line": 99,
      "end_line": 117,
      "content_hash": "e4cda9bb137f79f1bd7793a140f8def732fba81a",
      "content": "    def _items():\n        return [\n            {\n                \"score\": 1.0,\n                \"path\": \"/work/foo.py\",\n                \"symbol\": \"foo\",\n                \"start_line\": 10,\n                \"end_line\": 16,\n                \"text\": \"def helper():\\n    return 42\\n\",\n            },\n            {\n                \"score\": 0.8,\n                \"path\": \"/work/bar.py\",\n                \"symbol\": \"RRF_K\",\n                \"start_line\": 5,\n                \"end_line\": 9,\n                \"text\": \"RRF_K = 60\\n\",\n            },\n        ]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_FakeLlama_123": {
      "name": "FakeLlama",
      "type": "class",
      "start_line": 123,
      "end_line": 128,
      "content_hash": "2a451ca096e0125f5b5b46cb880e8b59d4828373",
      "content": "    class FakeLlama:\n        def __init__(self, *a, **k):\n            pass\n\n        def generate_with_soft_embeddings(self, prompt: str, max_tokens: int = 256, **kw):\n            return \"Definition: \\\"RRF_K = 60\\\" [1]\\nUsage: Appears in code. [1]\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___124": {
      "name": "__init__",
      "type": "method",
      "start_line": 124,
      "end_line": 125,
      "content_hash": "2f01f3eeec882714bac9a504b0450cf9884e2078",
      "content": "        def __init__(self, *a, **k):\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_generate_with_soft_embeddings_127": {
      "name": "generate_with_soft_embeddings",
      "type": "method",
      "start_line": 127,
      "end_line": 128,
      "content_hash": "a6ea380d9ed6bb34ec6e22b49561c3f6e2aaf00f",
      "content": "        def generate_with_soft_embeddings(self, prompt: str, max_tokens: int = 256, **kw):\n            return \"Definition: \\\"RRF_K = 60\\\" [1]\\nUsage: Appears in code. [1]\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_context_answer_tier2_retry_without_gating_142": {
      "name": "test_context_answer_tier2_retry_without_gating",
      "type": "function",
      "start_line": 142,
      "end_line": 198,
      "content_hash": "e32d6fbf62fbd59f9dc895772ff8534338aec0c3",
      "content": "def test_context_answer_tier2_retry_without_gating(monkeypatch):\n    \"\"\"Tier 2 should retry run_hybrid_search with relaxed filters when Tier 1 yields zero.\"\"\"\n    # Mock embedding model to avoid loading real model\n    monkeypatch.setattr(srv, \"_get_embedding_model\", lambda *a, **k: None)\n\n    import scripts.hybrid_search as hs\n\n    calls = []\n\n    def _run_hybrid_search(**kwargs):\n        calls.append(kwargs)\n        # Only return results once Tier 2 relaxes the filters (path_glob None, symbol None)\n        if kwargs.get(\"path_glob\") is None and kwargs.get(\"symbol\") is None and len(calls) >= 1:\n            return [\n                {\n                    \"score\": 0.42,\n                    \"path\": \"/work/hybrid_search.py\",\n                    \"symbol\": \"RRF_K\",\n                    \"start_line\": 100,\n                    \"end_line\": 104,\n                    \"text\": \"RRF_K = 60\\n\",\n                }\n            ]\n        # All other calls (tier1/usage/targeted search) yield no hits\n        return []\n\n    monkeypatch.setattr(hs, \"run_hybrid_search\", _run_hybrid_search)\n\n    import scripts.refrag_llamacpp as ref\n\n    class FakeLlama:\n        def __init__(self, *a, **k):\n            pass\n\n        def generate_with_soft_embeddings(self, *a, **kw):\n            return \"Definition: \\\"RRF_K = 60\\\" [1]\\nUsage: Not found in provided snippets. [1]\"\n\n    monkeypatch.setattr(ref, \"LlamaCppRefragClient\", FakeLlama)\n    monkeypatch.setattr(ref, \"is_decoder_enabled\", lambda: True)\n\n    out = srv.asyncio.get_event_loop().run_until_complete(\n        srv.context_answer(query=\"RRF_K\", limit=1, per_path=1)\n    )\n\n    # Ensure Tier 2 was invoked (run_hybrid_search called twice)\n    assert len(calls) >= 3, \"Tier 2 fallback should re-run hybrid search\"\n\n    tier2_kwargs = calls[-1]\n    # Tier 2 should have relaxed filters\n    assert tier2_kwargs.get(\"path_glob\") is None\n    assert tier2_kwargs.get(\"symbol\") is None\n    assert tier2_kwargs.get(\"kind\") is None\n\n    # The final citations should come from the tier-2 hit\n    cits = out.get(\"citations\") or []\n    assert len(cits) == 1\n    assert cits[0][\"path\"].endswith(\"hybrid_search.py\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__run_hybrid_search_151": {
      "name": "_run_hybrid_search",
      "type": "function",
      "start_line": 151,
      "end_line": 166,
      "content_hash": "fc5ff5576369512ece43734f4123681693f4c5d7",
      "content": "    def _run_hybrid_search(**kwargs):\n        calls.append(kwargs)\n        # Only return results once Tier 2 relaxes the filters (path_glob None, symbol None)\n        if kwargs.get(\"path_glob\") is None and kwargs.get(\"symbol\") is None and len(calls) >= 1:\n            return [\n                {\n                    \"score\": 0.42,\n                    \"path\": \"/work/hybrid_search.py\",\n                    \"symbol\": \"RRF_K\",\n                    \"start_line\": 100,\n                    \"end_line\": 104,\n                    \"text\": \"RRF_K = 60\\n\",\n                }\n            ]\n        # All other calls (tier1/usage/targeted search) yield no hits\n        return []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_FakeLlama_172": {
      "name": "FakeLlama",
      "type": "class",
      "start_line": 172,
      "end_line": 177,
      "content_hash": "6b3f131d5a7e4490bb6832623330e7b72d627f6d",
      "content": "    class FakeLlama:\n        def __init__(self, *a, **k):\n            pass\n\n        def generate_with_soft_embeddings(self, *a, **kw):\n            return \"Definition: \\\"RRF_K = 60\\\" [1]\\nUsage: Not found in provided snippets. [1]\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___173": {
      "name": "__init__",
      "type": "method",
      "start_line": 173,
      "end_line": 174,
      "content_hash": "2f01f3eeec882714bac9a504b0450cf9884e2078",
      "content": "        def __init__(self, *a, **k):\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_generate_with_soft_embeddings_176": {
      "name": "generate_with_soft_embeddings",
      "type": "method",
      "start_line": 176,
      "end_line": 177,
      "content_hash": "bfb30a08d9c2b7bf64a2abec0fabe99eebd0929b",
      "content": "        def generate_with_soft_embeddings(self, *a, **kw):\n            return \"Definition: \\\"RRF_K = 60\\\" [1]\\nUsage: Not found in provided snippets. [1]\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_context_answer_env_lock_release_on_retrieval_exception_202": {
      "name": "test_context_answer_env_lock_release_on_retrieval_exception",
      "type": "function",
      "start_line": 202,
      "end_line": 254,
      "content_hash": "c06495fa7b360a9a4010cbc8b29ce0f89afd70eb",
      "content": "def test_context_answer_env_lock_release_on_retrieval_exception(monkeypatch):\n    # Mock embedding model to avoid loading real model\n    monkeypatch.setattr(srv, \"_get_embedding_model\", lambda *a, **k: None)\n\n    import os\n    # Force retrieval to raise and ensure env/lock are restored\n    prev = {k: os.environ.get(k) for k in (\n        \"REFRAG_MODE\", \"REFRAG_GATE_FIRST\", \"REFRAG_CANDIDATES\", \"COLLECTION_NAME\", \"MICRO_BUDGET_TOKENS\"\n    )}\n\n    def _raise_retrieval(*a, **k):\n        raise RuntimeError(\"boom\")\n\n    monkeypatch.setattr(srv, \"_ca_prepare_filters_and_retrieve\", _raise_retrieval)\n\n    out = srv.asyncio.get_event_loop().run_until_complete(\n        srv.context_answer(query=\"x\", limit=1, per_path=1)\n    )\n    assert \"error\" in out\n\n    # Lock should be free after failure\n    assert srv._ENV_LOCK.acquire(blocking=False), \"_ENV_LOCK should be released on exception\"\n    srv._ENV_LOCK.release()\n\n    # Env should be restored\n    for k, v in prev.items():\n        assert os.environ.get(k) == v\n\n    # Subsequent call should proceed (no deadlock)\n    def _fake_retrieval(*a, **k):\n        return {\n            \"items\": [],\n            \"eff_language\": None,\n            \"eff_path_glob\": None,\n            \"eff_not_glob\": None,\n            \"override_under\": False,\n            \"sym_arg\": None,\n            \"cwd_root\": \"/work\",\n            \"path_regex\": None,\n            \"ext\": None,\n            \"kind\": None,\n            \"case\": None,\n        }\n\n    monkeypatch.setattr(srv, \"_ca_prepare_filters_and_retrieve\", _fake_retrieval)\n\n    import scripts.refrag_llamacpp as ref\n    monkeypatch.setattr(ref, \"is_decoder_enabled\", lambda: False)\n\n    out2 = srv.asyncio.get_event_loop().run_until_complete(\n        srv.context_answer(query=\"x\", limit=1, per_path=1)\n    )\n    assert isinstance(out2, dict)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__raise_retrieval_212": {
      "name": "_raise_retrieval",
      "type": "function",
      "start_line": 212,
      "end_line": 213,
      "content_hash": "835d8baa782973a70b334392a94710cfe456d799",
      "content": "    def _raise_retrieval(*a, **k):\n        raise RuntimeError(\"boom\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__fake_retrieval_231": {
      "name": "_fake_retrieval",
      "type": "function",
      "start_line": 231,
      "end_line": 244,
      "content_hash": "26ced47f419a66513b4ada968274955efb3bc43a",
      "content": "    def _fake_retrieval(*a, **k):\n        return {\n            \"items\": [],\n            \"eff_language\": None,\n            \"eff_path_glob\": None,\n            \"eff_not_glob\": None,\n            \"override_under\": False,\n            \"sym_arg\": None,\n            \"cwd_root\": \"/work\",\n            \"path_regex\": None,\n            \"ext\": None,\n            \"kind\": None,\n            \"case\": None,\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}