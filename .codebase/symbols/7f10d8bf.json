{
  "file_path": "/work/context-engine/scripts/rerank_recursive/utils.py",
  "file_hash": "4f8629ab3d9de44f6b3cdadc6c72cb9cb711a8fb",
  "updated_at": "2025-12-26T17:34:22.781704",
  "symbols": {
    "function__split_identifier_27": {
      "name": "_split_identifier",
      "type": "function",
      "start_line": 27,
      "end_line": 75,
      "content_hash": "effd436c31201a42a6cc60f04e22511b6a447414",
      "content": "def _split_identifier(s: str) -> List[str]:\n    \"\"\"Split any identifier into tokens, handling all common conventions.\n\n    Handles: snake_case, kebab-case, camelCase, PascalCase, SCREAMING_CASE,\n    numbers, acronyms (XMLParser -> xml, parser), dot.notation, and mixed styles.\n\n    Special handling:\n    - Preserves meaningful acronyms (API, HTTP, JSON, XML, URL, etc.)\n    - Strips common prefixes (I for interface, _ for private)\n    - Handles version suffixes (v2, 2.0)\n    \"\"\"\n    if not s:\n        return []\n\n    # Strip common prefixes that don't add meaning\n    if len(s) > 1:\n        # Interface prefix (IUserService -> UserService)\n        if s[0] == 'I' and s[1].isupper():\n            s = s[1:]\n        # Private prefix (_private -> private)\n        elif s[0] == '_':\n            s = s.lstrip('_')\n        # Dollar prefix ($scope -> scope)\n        elif s[0] == '$':\n            s = s[1:]\n\n    # Insert space before uppercase letters that follow lowercase (camelCase)\n    s = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", s)\n    # Insert space before uppercase letters followed by lowercase (acronyms: XMLParser -> XML Parser)\n    s = re.sub(r\"([A-Z]+)([A-Z][a-z])\", r\"\\1 \\2\", s)\n    # Insert space around digit sequences (handler2 -> handler 2, v2 -> v 2)\n    s = re.sub(r\"([a-zA-Z])(\\d)\", r\"\\1 \\2\", s)\n    s = re.sub(r\"(\\d)([a-zA-Z])\", r\"\\1 \\2\", s)\n\n    # Split on separators: underscore, hyphen, dot, space\n    parts = re.split(r\"[_\\-.\\s]+\", s)\n    tokens = []\n    for part in parts:\n        part = part.strip().lower()\n        # Skip pure numbers and single chars (except meaningful ones)\n        if not part:\n            continue\n        if part.isdigit():\n            continue  # Skip version numbers like \"2\", \"18\"\n        if len(part) < 2:\n            continue\n        tokens.append(part)\n\n    return tokens",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__normalize_token_78": {
      "name": "_normalize_token",
      "type": "function",
      "start_line": 78,
      "end_line": 91,
      "content_hash": "2bc29ab38e35de653e9a76225ad792788ab63ee4",
      "content": "def _normalize_token(tok: str) -> Set[str]:\n    \"\"\"Return the token plus simple morphological variants.\"\"\"\n    forms = {tok}\n    # Simple plural/singular normalization\n    if tok.endswith('s') and len(tok) > 3:\n        forms.add(tok[:-1])  # services -> service\n    elif tok.endswith('es') and len(tok) > 4:\n        forms.add(tok[:-2])  # processes -> process\n    elif tok.endswith('ies') and len(tok) > 4:\n        forms.add(tok[:-3] + 'y')  # utilities -> utility\n    # Add singular -> plural\n    if not tok.endswith('s') and len(tok) > 2:\n        forms.add(tok + 's')\n    return forms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__tokenize_for_fname_boost_94": {
      "name": "_tokenize_for_fname_boost",
      "type": "function",
      "start_line": 94,
      "end_line": 114,
      "content_hash": "6c7e2d8ea6afb12917847b303171a92c39d04f34",
      "content": "def _tokenize_for_fname_boost(text: Any) -> Set[str]:\n    \"\"\"Robust tokenization for filename boosts.\n\n    Some MCP/IDE clients pass query strings that include quotes/brackets\n    or list-like wrappers. Regex tokenization is resilient to that.\n    \"\"\"\n    if not text:\n        return set()\n    try:\n        s = str(text)\n    except Exception:\n        return set()\n\n    # Split on any non-alphanumeric\n    raw_parts = re.split(r\"[^a-zA-Z0-9]+\", s)\n    tokens = set()\n    for part in raw_parts:\n        for tok in _split_identifier(part):\n            if len(tok) >= 3:  # Query tokens need 3+ chars\n                tokens.add(tok)\n    return tokens",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__candidate_path_for_fname_boost_117": {
      "name": "_candidate_path_for_fname_boost",
      "type": "function",
      "start_line": 117,
      "end_line": 137,
      "content_hash": "fada8795a42d9c813be9a529e239d9cbaadbfc22",
      "content": "def _candidate_path_for_fname_boost(candidate: Dict[str, Any]) -> str:\n    \"\"\"Best-effort extraction of a path/filename from candidate objects.\"\"\"\n    for key in (\"path\", \"rel_path\", \"host_path\", \"container_path\", \"client_path\"):\n        try:\n            val = candidate.get(key)\n        except Exception:\n            val = None\n        if isinstance(val, str) and val.strip():\n            return val\n\n    try:\n        md = candidate.get(\"metadata\") or {}\n        if isinstance(md, dict):\n            for key in (\"path\", \"rel_path\", \"host_path\", \"container_path\", \"client_path\"):\n                val = md.get(key)\n                if isinstance(val, str) and val.strip():\n                    return val\n    except Exception:\n        pass\n\n    return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__compute_fname_boost_140": {
      "name": "_compute_fname_boost",
      "type": "function",
      "start_line": 140,
      "end_line": 289,
      "content_hash": "7066e1cca1f3309fc4bef93b8b19e51c6d2157ab",
      "content": "def _compute_fname_boost(query: Any, candidate: Dict[str, Any], factor: float) -> float:\n    \"\"\"Compute filename/query correlation boost for a candidate.\n\n    Production-grade matching for real-world codebases at scale:\n\n    **Naming convention support:**\n    - snake_case, camelCase, PascalCase, kebab-case, SCREAMING_CASE\n    - Dot notation (com.company.auth.service)\n    - Mixed styles (legacy codebases)\n\n    **Smart tokenization:**\n    - Acronyms: XMLParser -> xml, parser; HTTPClient -> http, client\n    - Prefixes stripped: IService -> service, _private -> private\n    - Numbers separated: handler2 -> handler, React18 -> react\n\n    **Normalization:**\n    - Simple plural/singular normalization (services <-> service)\n\n    **Position-aware scoring:**\n    - Filename matches weighted higher than directory matches\n    - Deeper directories weighted less (noise reduction)\n\n    **Specificity weighting:**\n    - Common tokens (index, main, utils) weighted less\n    - Rare/specific tokens weighted more\n\n    **Scoring tiers:**\n    - Exact match: 1.0 \u00d7 factor\n    - Normalized match (morphology): 0.8 \u00d7 factor\n    - Substring containment: 0.4 \u00d7 factor\n    - Common token penalty: 0.5\u00d7 multiplier\n    - Filename bonus: 1.5\u00d7 multiplier for filename matches\n\n    Requires 2+ quality matches to trigger (prevents noise).\n    \"\"\"\n    if not factor or factor <= 0:\n        return 0.0\n\n    query_tokens = _tokenize_for_fname_boost(query)\n    if not query_tokens:\n        return 0.0\n\n    path = _candidate_path_for_fname_boost(candidate)\n    path = str(path or \"\")\n    if not path:\n        return 0.0\n\n    # Strip common prefixes that add noise (preserve case for splitting)\n    path_clean = path\n    path_lower = path.lower()\n    for prefix in (\"/work/\", \"/app/\", \"/src/\", \"/home/\", \"/var/\", \"/opt/\", \"/usr/\"):\n        if path_lower.startswith(prefix):\n            path_clean = path[len(prefix):]\n            break\n\n    # Split path into segments, track position for weighting\n    path_segments = re.split(r\"[/\\\\]\", path_clean)\n    path_segments = [s for s in path_segments if s]  # Remove empty\n\n    if not path_segments:\n        return 0.0\n\n    # Tokenize with position info: (token, is_filename, depth)\n    # Filename = last segment, depth = 0 for filename, 1 for parent, etc.\n    path_token_info: Dict[str, Dict[str, Any]] = {}  # token -> {is_filename, min_depth}\n\n    for i, segment in enumerate(reversed(path_segments)):\n        is_filename = (i == 0)\n        depth = i\n\n        # Strip extension from filename\n        if is_filename and \".\" in segment:\n            segment = segment.rsplit(\".\", 1)[0]\n\n        for tok in _split_identifier(segment):\n            if len(tok) >= 2:\n                if tok not in path_token_info:\n                    path_token_info[tok] = {\"is_filename\": is_filename, \"depth\": depth}\n                # Keep the most important occurrence (filename > dir, shallow > deep)\n                elif is_filename and not path_token_info[tok][\"is_filename\"]:\n                    path_token_info[tok] = {\"is_filename\": True, \"depth\": depth}\n\n    if not path_token_info:\n        return 0.0\n\n    path_tokens = set(path_token_info.keys())\n\n    # Build normalized lookup for path tokens\n    path_normalized: Dict[str, str] = {}  # normalized_form -> original_token\n    for ptok in path_tokens:\n        for form in _normalize_token(ptok):\n            if form not in path_normalized:\n                path_normalized[form] = ptok\n\n    # Score matches with quality tiers\n    score = 0.0\n    matched_query_tokens = set()\n\n    for qtok in query_tokens:\n        qtok_forms = _normalize_token(qtok)\n        match_score = 0.0\n        matched_ptok = None\n\n        # Tier 1: Exact match\n        if qtok in path_tokens:\n            match_score = 1.0\n            matched_ptok = qtok\n        else:\n            # Tier 2: Normalized match (plural/singular)\n            for qform in qtok_forms:\n                if qform in path_normalized:\n                    match_score = 0.8\n                    matched_ptok = path_normalized[qform]\n                    break\n\n            # Tier 3: Substring containment (if no normalized match)\n            if match_score == 0.0:\n                for ptok in path_tokens:\n                    if len(qtok) >= 4 and len(ptok) >= 4:\n                        if qtok in ptok or ptok in qtok:\n                            overlap = min(len(qtok), len(ptok))\n                            if overlap >= 4:\n                                match_score = 0.4\n                                matched_ptok = ptok\n                                break\n\n        if match_score > 0 and matched_ptok:\n            matched_query_tokens.add(qtok)\n\n            # Apply position bonus (filename matches worth more)\n            info = path_token_info.get(matched_ptok, {})\n            if info.get(\"is_filename\"):\n                match_score *= 1.5  # 50% bonus for filename match\n            else:\n                # Depth penalty for deep directories\n                depth = info.get(\"depth\", 0)\n                if depth > 2:\n                    match_score *= 0.8  # Slight penalty for deep paths\n\n            # Common token penalty\n            if qtok in _COMMON_TOKENS or matched_ptok in _COMMON_TOKENS:\n                match_score *= 0.5\n\n            score += match_score\n\n    # Require 2+ quality matches to trigger (prevents noise from single common word)\n    if len(matched_query_tokens) < 2:\n        return 0.0\n\n    return float(score * factor)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cache_key_303": {
      "name": "_cache_key",
      "type": "function",
      "start_line": 303,
      "end_line": 305,
      "content_hash": "3a56046a5077e2ffc3aa3568af9fc94f4bc54cc6",
      "content": "def _cache_key(text: str) -> str:\n    \"\"\"Generate deterministic cache key from text (process-stable, collision-resistant).\"\"\"\n    return hashlib.sha256(text.encode(\"utf-8\", errors=\"replace\")).hexdigest()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_cached_embedding_308": {
      "name": "_get_cached_embedding",
      "type": "function",
      "start_line": 308,
      "end_line": 312,
      "content_hash": "d691150c6222becf5e0bfa22c4e8d1fd650c8105",
      "content": "def _get_cached_embedding(text: str) -> Optional[np.ndarray]:\n    \"\"\"Get embedding from cache if exists.\"\"\"\n    key = _cache_key(text)\n    with _EMBEDDING_CACHE_LOCK:\n        return _EMBEDDING_CACHE.get(key)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cache_embedding_315": {
      "name": "_cache_embedding",
      "type": "function",
      "start_line": 315,
      "end_line": 324,
      "content_hash": "8f2a2316b7e786d9fb3888650e8d8975df521e5c",
      "content": "def _cache_embedding(text: str, embedding: np.ndarray):\n    \"\"\"Cache embedding for text.\"\"\"\n    key = _cache_key(text)\n    with _EMBEDDING_CACHE_LOCK:\n        if len(_EMBEDDING_CACHE) >= _EMBEDDING_CACHE_MAX_SIZE:\n            # Evict oldest 10%\n            keys_to_remove = list(_EMBEDDING_CACHE.keys())[:_EMBEDDING_CACHE_MAX_SIZE // 10]\n            for k in keys_to_remove:\n                del _EMBEDDING_CACHE[k]\n        _EMBEDDING_CACHE[key] = embedding",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}