{
  "file_path": "/work/context-engine/scripts/mcp_impl/search.py",
  "file_hash": "30d528f3621e29563372d956a80eec055a92b996",
  "updated_at": "2025-12-26T17:34:20.675272",
  "symbols": {
    "function__repo_search_impl_57": {
      "name": "_repo_search_impl",
      "type": "function",
      "start_line": 57,
      "end_line": 1353,
      "content_hash": "158603f547a638ed9e3430312d8be0d756c1ce6e",
      "content": "async def _repo_search_impl(\n    query: Any = None,\n    queries: Any = None,  # Alias for query (many clients use this)\n    limit: Any = None,\n    per_path: Any = None,\n    include_snippet: Any = None,\n    context_lines: Any = None,\n    rerank_enabled: Any = None,\n    rerank_top_n: Any = None,\n    rerank_return_m: Any = None,\n    rerank_timeout_ms: Any = None,\n    highlight_snippet: Any = None,\n    collection: Any = None,\n    workspace_path: Any = None,\n    mode: Any = None,\n    session: Any = None,\n    ctx: Any = None,  # MCP Context (passed from wrapper)\n    # Structured filters (optional; mirrors hybrid_search flags)\n    language: Any = None,\n    under: Any = None,\n    kind: Any = None,\n    symbol: Any = None,\n    # Additional structured parity\n    path_regex: Any = None,\n    path_glob: Any = None,\n    not_glob: Any = None,\n    ext: Any = None,\n    not_: Any = None,\n    case: Any = None,\n    # Repo scoping (cross-codebase isolation)\n    repo: Any = None,  # str, list[str], or \"*\" to search all repos\n    # Response shaping\n    compact: Any = None,\n    output_format: Any = None,  # \"json\" (default) or \"toon\" for token-efficient format\n    args: Any = None,  # Compatibility shim for mcp-remote/Claude wrappers that send args/kwargs\n    kwargs: Any = None,\n    # Injected dependencies from facade\n    *,\n    get_embedding_model_fn: Any = None,  # callable for _get_embedding_model\n    require_auth_session_fn: Any = None,  # callable for _require_auth_session\n    do_highlight_snippet_fn: Any = None,  # callable for _do_highlight_snippet\n    run_async_fn: Any = None,  # callable for _run_async (subprocess runner)\n) -> Dict[str, Any]:\n    \"\"\"Zero-config code search over repositories (hybrid: vector + lexical RRF, rerank ON by default).\n\n    When to use:\n    - Find relevant code spans quickly; prefer this over embedding-only search.\n    - Use context_answer when you need a synthesized explanation; use context_search to blend with memory notes.\n\n    Key parameters:\n    - query: str or list[str]. Multiple queries are fused; accepts \"queries\" alias.\n    - limit: int (default 10). Total results across files.\n    - per_path: int (default 2). Max results per file.\n    - include_snippet/context_lines: return inline snippets near hits when true.\n    - rerank_*: ONNX reranker is ON by default for best relevance; timeouts fall back to hybrid.\n    - output_format: \"json\" (default) or \"toon\" for token-efficient TOON format.\n      Set TOON_ENABLED=1 env var to enable TOON by default.\n    - collection: str. Target collection; defaults to workspace state or env COLLECTION_NAME.\n    - repo: str or list[str]. Filter by repo name(s). Use \"*\" to search all repos (disable auto-filter).\n      By default, auto-detects current repo from CURRENT_REPO env and filters to it.\n      Use repo=[\"frontend\",\"backend\"] to search related repos together.\n    - Filters (optional): language, under (path prefix), kind, symbol, ext, path_regex,\n      path_glob (str or list[str]), not_glob (str or list[str]), not_ (negative text), case.\n\n    Returns:\n    - Dict with keys:\n      - results: list of {score, path, symbol, start_line, end_line, why[, components][, relations][, related_paths][, snippet]}\n      - total: int; used_rerank: bool; rerank_counters: dict\n    - If compact=true (and snippets not requested), results contain only {path,start_line,end_line}.\n\n    Examples:\n    - path_glob=[\"scripts/**\",\"**/*.py\"], language=\"python\"\n    - symbol=\"context_answer\", under=\"scripts\"\n    \"\"\"\n    sess = require_auth_session_fn(session) if require_auth_session_fn else session\n\n    # Use injected run_async or fall back to module import\n    _run_async_fn = run_async_fn if run_async_fn is not None else _run_async\n\n    # Handle queries alias (explicit parameter)\n    if queries is not None and (query is None or (isinstance(query, str) and str(query).strip() == \"\")):\n        query = queries\n\n    # Accept common alias keys from clients (top-level)\n    try:\n        if kwargs and (\n            limit is None or (isinstance(limit, str) and str(limit).strip() == \"\")\n        ) and (\"top_k\" in kwargs):\n            limit = kwargs.get(\"top_k\")\n        if kwargs and (query is None or (isinstance(query, str) and str(query).strip() == \"\")):\n            q_alt = kwargs.get(\"q\") or kwargs.get(\"text\")\n            if q_alt is not None:\n                query = q_alt\n    except Exception:\n        pass\n\n    # Leniency: absorb nested 'kwargs' JSON payload some clients send\n    try:\n        _extra = _extract_kwargs_payload(kwargs)\n        if _extra:\n            if query is None or (isinstance(query, str) and query.strip() == \"\"):\n                query = _extra.get(\"query\") or _extra.get(\"queries\")\n            if limit in (None, \"\") and _extra.get(\"limit\") is not None:\n                limit = _extra.get(\"limit\")\n            if per_path in (None, \"\") and _extra.get(\"per_path\") is not None:\n                per_path = _extra.get(\"per_path\")\n            if (\n                include_snippet in (None, \"\")\n                and _extra.get(\"include_snippet\") is not None\n            ):\n                include_snippet = _extra.get(\"include_snippet\")\n            if context_lines in (None, \"\") and _extra.get(\"context_lines\") is not None:\n                context_lines = _extra.get(\"context_lines\")\n            if (\n                rerank_enabled in (None, \"\")\n                and _extra.get(\"rerank_enabled\") is not None\n            ):\n                rerank_enabled = _extra.get(\"rerank_enabled\")\n            if rerank_top_n in (None, \"\") and _extra.get(\"rerank_top_n\") is not None:\n                rerank_top_n = _extra.get(\"rerank_top_n\")\n            if (\n                rerank_return_m in (None, \"\")\n                and _extra.get(\"rerank_return_m\") is not None\n            ):\n                rerank_return_m = _extra.get(\"rerank_return_m\")\n            if (\n                rerank_timeout_ms in (None, \"\")\n                and _extra.get(\"rerank_timeout_ms\") is not None\n            ):\n                rerank_timeout_ms = _extra.get(\"rerank_timeout_ms\")\n            if (\n                highlight_snippet in (None, \"\")\n                and _extra.get(\"highlight_snippet\") is not None\n            ):\n                highlight_snippet = _extra.get(\"highlight_snippet\")\n            if (\n                collection is None\n                or (isinstance(collection, str) and collection.strip() == \"\")\n            ) and _extra.get(\"collection\"):\n                collection = _extra.get(\"collection\")\n            # Optional session token for session-scoped defaults\n            if (\n                (session is None) or (isinstance(session, str) and str(session).strip() == \"\")\n            ) and _extra.get(\"session\") is not None:\n                session = _extra.get(\"session\")\n\n            # Optional workspace_path routing\n            if (\n                (workspace_path is None)\n                or (\n                    isinstance(workspace_path, str)\n                    and str(workspace_path).strip() == \"\"\n                )\n            ) and _extra.get(\"workspace_path\") is not None:\n                workspace_path = _extra.get(\"workspace_path\")\n\n            if (\n                language is None\n                or (isinstance(language, str) and language.strip() == \"\")\n            ) and _extra.get(\"language\"):\n                language = _extra.get(\"language\")\n            if (\n                under is None or (isinstance(under, str) and under.strip() == \"\")\n            ) and _extra.get(\"under\"):\n                under = _extra.get(\"under\")\n            if (\n                kind is None or (isinstance(kind, str) and kind.strip() == \"\")\n            ) and _extra.get(\"kind\"):\n                kind = _extra.get(\"kind\")\n            if (\n                symbol is None or (isinstance(symbol, str) and symbol.strip() == \"\")\n            ) and _extra.get(\"symbol\"):\n                symbol = _extra.get(\"symbol\")\n            if (\n                path_regex is None\n                or (isinstance(path_regex, str) and path_regex.strip() == \"\")\n            ) and _extra.get(\"path_regex\"):\n                path_regex = _extra.get(\"path_regex\")\n            if path_glob in (None, \"\") and _extra.get(\"path_glob\") is not None:\n                path_glob = _extra.get(\"path_glob\")\n            if not_glob in (None, \"\") and _extra.get(\"not_glob\") is not None:\n                not_glob = _extra.get(\"not_glob\")\n            if (\n                ext is None or (isinstance(ext, str) and ext.strip() == \"\")\n            ) and _extra.get(\"ext\"):\n                ext = _extra.get(\"ext\")\n            if (not_ is None or (isinstance(not_, str) and not_.strip() == \"\")) and (\n                _extra.get(\"not\") or _extra.get(\"not_\")\n            ):\n                not_ = _extra.get(\"not\") or _extra.get(\"not_\")\n            if (\n                case is None or (isinstance(case, str) and case.strip() == \"\")\n            ) and _extra.get(\"case\"):\n                case = _extra.get(\"case\")\n            if compact in (None, \"\") and _extra.get(\"compact\") is not None:\n                compact = _extra.get(\"compact\")\n            # Optional mode hint: \"code_first\", \"docs_first\", \"balanced\"\n            if (\n                mode is None or (isinstance(mode, str) and str(mode).strip() == \"\")\n            ) and _extra.get(\"mode\") is not None:\n                mode = _extra.get(\"mode\")\n    except Exception:\n        pass\n\n    # Leniency shim: coerce null/invalid args to sane defaults so buggy clients don't fail schema\n    def _to_int(x, default):\n        try:\n            if x is None or (isinstance(x, str) and x.strip() == \"\"):\n                return default\n            return int(x)\n        except Exception:\n            return default\n\n    def _to_bool(x, default):\n        if x is None or (isinstance(x, str) and x.strip() == \"\"):\n            return default\n        if isinstance(x, bool):\n            return x\n        s = str(x).strip().lower()\n        if s in {\"1\", \"true\", \"yes\", \"on\"}:\n            return True\n        if s in {\"0\", \"false\", \"no\", \"off\"}:\n            return False\n        return default\n\n    # Session token (top-level or parsed from nested kwargs above)\n    sid = (str(session).strip() if session is not None else \"\")\n\n\n    def _to_str(x, default=\"\"):\n        if x is None:\n            return default\n        return str(x)\n\n    # Coerce incoming args (which may be null) to proper types\n    limit = _to_int(limit, 10)\n    per_path = _to_int(per_path, 2)\n    include_snippet = _to_bool(include_snippet, True)\n    context_lines = _to_int(context_lines, 2)\n    # Reranker: default ON; can be disabled via env or client args\n    rerank_env_default = str(\n        os.environ.get(\"RERANKER_ENABLED\", \"1\")\n    ).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n    rerank_enabled = _to_bool(rerank_enabled, rerank_env_default)\n    rerank_top_n = _to_int(\n        rerank_top_n, int(os.environ.get(\"RERANKER_TOPN\", \"50\") or 50)\n    )\n    rerank_return_m = _to_int(\n        rerank_return_m, int(os.environ.get(\"RERANKER_RETURN_M\", \"12\") or 12)\n    )\n    rerank_timeout_ms = _to_int(\n        rerank_timeout_ms, int(os.environ.get(\"RERANKER_TIMEOUT_MS\", \"120\") or 120)\n    )\n    highlight_snippet = _to_bool(highlight_snippet, True)\n\n    # Resolve collection and related hints: explicit > per-connection defaults > token defaults > env\n    coll_hint = _to_str(collection, \"\").strip()\n    mode_hint = _to_str(mode, \"\").strip()\n    under_hint = _to_str(under, \"\").strip()\n    lang_hint = _to_str(language, \"\").strip()\n\n    # 1) Per-connection defaults via ctx (no token required)\n    if ctx is not None and getattr(ctx, \"session\", None) is not None:\n        try:\n            with _SESSION_CTX_LOCK:\n                _d2 = SESSION_DEFAULTS_BY_SESSION.get(ctx.session) or {}\n                if not coll_hint:\n                    _sc2 = str((_d2.get(\"collection\") or \"\")).strip()\n                    if _sc2:\n                        coll_hint = _sc2\n                if not mode_hint:\n                    _sm2 = str((_d2.get(\"mode\") or \"\")).strip()\n                    if _sm2:\n                        mode_hint = _sm2\n                if not under_hint:\n                    _su2 = str((_d2.get(\"under\") or \"\")).strip()\n                    if _su2:\n                        under_hint = _su2\n                if not lang_hint:\n                    _sl2 = str((_d2.get(\"language\") or \"\")).strip()\n                    if _sl2:\n                        lang_hint = _sl2\n        except Exception:\n            pass\n\n    # 2) Legacy token-based defaults\n    if sid:\n        try:\n            with _SESSION_LOCK:\n                _d = SESSION_DEFAULTS.get(sid) or {}\n                if not coll_hint:\n                    _sc = str((_d.get(\"collection\") or \"\")).strip()\n                    if _sc:\n                        coll_hint = _sc\n                if not mode_hint:\n                    _sm = str((_d.get(\"mode\") or \"\")).strip()\n                    if _sm:\n                        mode_hint = _sm\n                if not under_hint:\n                    _su = str((_d.get(\"under\") or \"\")).strip()\n                    if _su:\n                        under_hint = _su\n                if not lang_hint:\n                    _sl = str((_d.get(\"language\") or \"\")).strip()\n                    if _sl:\n                        lang_hint = _sl\n        except Exception:\n            pass\n\n    # 3) Environment default (collection only for now)\n    env_coll = (os.environ.get(\"DEFAULT_COLLECTION\") or os.environ.get(\"COLLECTION_NAME\") or \"\").strip()\n    if (not coll_hint) and env_coll:\n        coll_hint = env_coll\n\n    # Final fallback\n    env_fallback = (os.environ.get(\"DEFAULT_COLLECTION\") or os.environ.get(\"COLLECTION_NAME\") or \"codebase\").strip()\n    collection = coll_hint or env_fallback\n\n    _require_collection_access((sess or {}).get(\"user_id\") if sess else None, collection, \"read\")\n\n    # Optional mode knob: \"code_first\" (default for IDE), \"docs_first\", \"balanced\"\n    if not mode:\n        mode = mode_hint\n    mode_str = _to_str(mode, \"\").strip().lower()\n\n    # Apply defaults for language / under when explicit args are empty\n    if not language:\n        language = lang_hint\n    if not under:\n        under = under_hint\n\n    language = _to_str(language, \"\").strip()\n    under = _to_str(under, \"\").strip()\n    kind = _to_str(kind, \"\").strip()\n    symbol = _to_str(symbol, \"\").strip()\n    path_regex = _to_str(path_regex, \"\").strip()\n\n    # Normalize globs to lists (accept string or list)\n    def _to_str_list(x):\n        if x is None:\n            return []\n        if isinstance(x, (list, tuple)):\n            out = []\n            for e in x:\n                s = str(e).strip()\n                if s:\n                    out.append(s)\n            return out\n        s = str(x).strip()\n        if not s:\n            return []\n        # support comma-separated shorthand\n        return [t.strip() for t in s.split(\",\") if t.strip()]\n\n    path_globs = _to_str_list(path_glob)\n    not_globs = _to_str_list(not_glob)\n    ext = _to_str(ext, \"\").strip()\n    not_ = _to_str(not_, \"\").strip()\n    case = _to_str(case, \"\").strip()\n\n    # Normalize repo filter: str, list[str], or \"*\" (search all)\n    # Default: auto-detect current repo unless REPO_AUTO_FILTER=0\n    repo_filter = None\n    if repo is not None:\n        if isinstance(repo, str):\n            r = repo.strip()\n            if r == \"*\":\n                repo_filter = \"*\"  # Explicit \"search all repos\"\n            elif r:\n                # Support comma-separated list\n                repo_filter = [x.strip() for x in r.split(\",\") if x.strip()]\n        elif isinstance(repo, (list, tuple)):\n            repo_filter = [str(x).strip() for x in repo if str(x).strip() and str(x).strip() != \"*\"]\n            if not repo_filter:\n                repo_filter = \"*\"  # Empty list after filtering means search all\n\n    # Auto-detect current repo if not explicitly specified and auto-filter is enabled\n    if repo_filter is None and str(os.environ.get(\"REPO_AUTO_FILTER\", \"1\")).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}:\n        detected_repo = _detect_current_repo()\n        if detected_repo:\n            repo_filter = [detected_repo]\n\n    compact_raw = compact\n    compact = _to_bool(compact, False)\n    # If snippets are requested, do not compact (we need snippet field in results)\n    if include_snippet:\n        compact = False\n\n    # Default behavior: exclude commit-history docs (which use path=\".git\") from\n    # generic repo_search calls, unless the caller explicitly asks for git\n    # content. This prevents normal code queries from surfacing commit-index\n    # points as if they were source files.\n    if (not language or language.lower() != \"git\") and (\n        not kind or kind.lower() != \"git_message\"\n    ):\n        if \".git\" not in not_globs:\n            not_globs.append(\".git\")\n\n    # Accept top-level alias `queries` as a drop-in for `query`\n    # Many clients send queries=[...] instead of query=[...]\n    if kwargs and \"queries\" in kwargs and kwargs.get(\"queries\") is not None:\n        query = kwargs.get(\"queries\")\n\n    # Normalize queries to a list[str] (robust for JSON strings and arrays)\n    queries: list[str] = []\n    if isinstance(query, (list, tuple)):\n        queries = [str(q).strip() for q in query if str(q).strip()]\n    elif isinstance(query, str):\n        queries = _to_str_list_relaxed(query)\n    elif query is not None:\n        s = str(query).strip()\n        if s:\n            queries = [s]\n\n    if not queries:\n        return {\"error\": \"query required\"}\n\n    # --- Code signal detection for intelligent targeting ---\n    # Analyze query for code-like patterns and extract potential symbols\n    code_signals = {\"has_code_signals\": False, \"signal_strength\": 0.0, \"extracted_symbols\": [], \"detected_patterns\": [], \"suggested_boosts\": {}}\n    try:\n        combined_query = \" \".join(queries)\n        code_signals = _detect_code_signals(combined_query)\n    except Exception:\n        pass\n\n    # If code signals detected and no explicit symbol filter, use extracted symbols for boosting\n    auto_symbol_hints: list[str] = []\n    if code_signals.get(\"has_code_signals\") and code_signals.get(\"extracted_symbols\"):\n        auto_symbol_hints = code_signals[\"extracted_symbols\"]\n\n    env = os.environ.copy()\n    env[\"QDRANT_URL\"] = QDRANT_URL\n    env[\"COLLECTION_NAME\"] = collection\n\n    # Apply dynamic boosts based on code signal strength\n    if code_signals.get(\"has_code_signals\"):\n        boosts = code_signals.get(\"suggested_boosts\", {})\n        # Boost symbol matching weight dynamically\n        if \"symbol_boost_multiplier\" in boosts:\n            base_sym_boost = float(os.environ.get(\"HYBRID_SYMBOL_BOOST\", \"0.15\"))\n            base_sym_eq_boost = float(os.environ.get(\"HYBRID_SYMBOL_EQUALITY_BOOST\", \"0.25\"))\n            mult = boosts[\"symbol_boost_multiplier\"]\n            env[\"HYBRID_SYMBOL_BOOST\"] = str(round(base_sym_boost * mult, 3))\n            env[\"HYBRID_SYMBOL_EQUALITY_BOOST\"] = str(round(base_sym_eq_boost * mult, 3))\n        # Boost implementation files over tests/docs when looking for code\n        if \"impl_boost_multiplier\" in boosts:\n            base_impl_boost = float(os.environ.get(\"HYBRID_IMPLEMENTATION_BOOST\", \"0.2\"))\n            mult = boosts[\"impl_boost_multiplier\"]\n            env[\"HYBRID_IMPLEMENTATION_BOOST\"] = str(round(base_impl_boost * mult, 3))\n\n    # Pass extracted symbols as additional search hints (augments existing queries)\n    if auto_symbol_hints:\n        env[\"CODE_SIGNAL_SYMBOLS\"] = \",\".join(auto_symbol_hints[:5])\n\n    results = []\n    json_lines = []\n\n    # In-process hybrid search (optional)\n\n    # Default subprocess result placeholder (for consistent response shape)\n    res = {\"ok\": True, \"code\": 0, \"stdout\": \"\", \"stderr\": \"\"}\n\n    use_hybrid_inproc = str(\n        os.environ.get(\"HYBRID_IN_PROCESS\", \"\")\n    ).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n    if use_hybrid_inproc:\n        try:\n            from scripts.hybrid_search import run_hybrid_search  # type: ignore\n\n            model_name = os.environ.get(\"EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\")\n            model = get_embedding_model_fn(model_name) if get_embedding_model_fn else None\n            # Ensure hybrid_search uses the intended collection when running in-process\n            prev_coll = os.environ.get(\"COLLECTION_NAME\")\n            # Determine effective hybrid candidate limit: if rerank is enabled, search up to rerank_top_n\n            try:\n                base_limit = int(limit)\n            except Exception:\n                base_limit = 10\n            eff_limit = base_limit\n            if rerank_enabled:\n                try:\n                    rt = int(rerank_top_n)\n                except Exception:\n                    rt = 0\n                if rt > eff_limit:\n                    eff_limit = rt\n            try:\n                os.environ[\"COLLECTION_NAME\"] = collection\n                # In-process path_glob/not_glob accept a single string; reduce list inputs safely\n                items = run_hybrid_search(\n                    queries=queries,\n                    limit=eff_limit,\n                    per_path=(\n                        int(per_path)\n                        if (per_path is not None and str(per_path).strip() != \"\")\n                        else 1\n                    ),\n                    language=language or None,\n                    under=under or None,\n                    kind=kind or None,\n                    symbol=symbol or None,\n                    ext=ext or None,\n                    not_filter=not_ or None,\n                    case=case or None,\n                    path_regex=path_regex or None,\n                    path_glob=(path_globs or None),\n                    not_glob=(not_globs or None),\n                    expand=str(os.environ.get(\"HYBRID_EXPAND\", \"1\")).strip().lower()\n                    in {\"1\", \"true\", \"yes\", \"on\"},\n                    model=model,\n                    mode=mode_str or None,\n                    repo=repo_filter,  # Cross-codebase isolation\n                )\n            finally:\n                if prev_coll is None:\n                    try:\n                        del os.environ[\"COLLECTION_NAME\"]\n                    except Exception:\n                        pass\n                else:\n                    os.environ[\"COLLECTION_NAME\"] = prev_coll\n            # items are already in structured dict form\n            json_lines = items  # reuse downstream shaping\n        except Exception as e:\n            # Fallback to subprocess path if in-process fails\n            logger.debug(f\"In-process hybrid search failed, falling back to subprocess: {type(e).__name__}: {e}\")\n            use_hybrid_inproc = False\n\n    if not use_hybrid_inproc:\n        # Try hybrid search via subprocess (JSONL output)\n        try:\n            base_limit = int(limit)\n        except Exception:\n            base_limit = 10\n        eff_limit = base_limit\n        if rerank_enabled:\n            try:\n                rt = int(rerank_top_n)\n            except Exception:\n                rt = 0\n            if rt > eff_limit:\n                eff_limit = rt\n        cmd = [\n            \"python\",\n            _work_script(\"hybrid_search.py\"),\n            \"--limit\",\n            str(eff_limit),\n            \"--json\",\n        ]\n        if per_path is not None and str(per_path).strip() != \"\":\n            cmd += [\"--per-path\", str(int(per_path))]\n        if language:\n            cmd += [\"--language\", language]\n        if under:\n            cmd += [\"--under\", under]\n        if kind:\n            cmd += [\"--kind\", kind]\n        if symbol:\n            cmd += [\"--symbol\", symbol]\n        if ext:\n            cmd += [\"--ext\", ext]\n        if not_:\n            cmd += [\"--not\", not_]\n        if case:\n            cmd += [\"--case\", case]\n        if path_regex:\n            cmd += [\"--path-regex\", path_regex]\n        for g in path_globs:\n            cmd += [\"--path-glob\", g]\n        for g in not_globs:\n            cmd += [\"--not-glob\", g]\n        for q in queries:\n            cmd += [\"--query\", q]\n        if collection:\n            cmd += [\"--collection\", str(collection)]\n\n        res = await _run_async_fn(cmd, env=env)\n        for line in (res.get(\"stdout\") or \"\").splitlines():\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                obj = json.loads(line)\n                json_lines.append(obj)\n            except json.JSONDecodeError:\n                continue\n        # Fallback: if subprocess yielded nothing (e.g., local dev without /work), try in-process once\n        if not json_lines:\n            try:\n                from scripts.hybrid_search import run_hybrid_search  # type: ignore\n\n                model_name = os.environ.get(\"EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\")\n                model = get_embedding_model_fn(model_name) if get_embedding_model_fn else None\n                items = run_hybrid_search(\n                    queries=queries,\n                    limit=int(limit),\n                    per_path=(\n                        int(per_path)\n                        if (per_path is not None and str(per_path).strip() != \"\")\n                        else 1\n                    ),\n                    language=language or None,\n                    under=under or None,\n                    kind=kind or None,\n                    symbol=symbol or None,\n                    ext=ext or None,\n                    not_filter=not_ or None,\n                    case=case or None,\n                    path_regex=path_regex or None,\n                    path_glob=(path_globs or None),\n                    not_glob=(not_globs or None),\n                    expand=str(os.environ.get(\"HYBRID_EXPAND\", \"0\")).strip().lower()\n                    in {\"1\", \"true\", \"yes\", \"on\"},\n                    model=model,\n                    mode=mode_str or None,\n                    repo=repo_filter,  # Cross-codebase isolation\n                )\n                json_lines = items\n            except Exception:\n                pass\n\n    # Optional rerank fallback path: if enabled, attempt; on timeout or error, keep hybrid\n    used_rerank = False\n    rerank_counters = {\n        \"inproc_hybrid\": 0,\n        \"inproc_dense\": 0,\n        \"subprocess\": 0,\n        \"timeout\": 0,\n        \"error\": 0,\n        \"learning\": 0,  # Learning-enabled recursive reranker\n    }\n    if rerank_enabled:\n        # Check for learning reranker mode (learns from ONNX teacher)\n        use_learning_rerank = str(\n            os.environ.get(\"RERANK_LEARNING\", \"\")\n        ).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n\n        if use_learning_rerank and json_lines:\n            try:\n                from scripts.rerank_recursive import rerank_with_learning\n\n                rq = queries[0] if queries else \"\"\n                cand_objs = list(json_lines[: int(rerank_top_n)])\n\n                # Run learning-enabled reranking (collection-aware for weight isolation)\n                reranked = rerank_with_learning(\n                    query=rq,\n                    candidates=cand_objs,\n                    limit=int(rerank_return_m),\n                    n_iterations=int(os.environ.get(\"RERANK_LEARNING_ITERS\", \"3\")),\n                    collection=collection or \"default\",\n                )\n\n                if reranked:\n                    # Format results for output\n                    tmp = []\n                    for obj in reranked:\n                        # Copy the list to avoid mutating the original object\n                        why_parts = list(obj.get(\"why\", []))\n                        why_parts.append(f\"learning:{obj.get('recursive_iterations', 0)}\")\n                        why_parts.append(f\"score:{float(obj.get('score', 0)):.3f}\")\n\n                        # Build components with optional fname_boost\n                        components = (obj.get(\"components\") or {}) | {\n                            \"learning_score\": float(obj.get(\"recursive_score\", 0)),\n                            \"refinement_iterations\": int(obj.get(\"recursive_iterations\", 0)),\n                        }\n                        if obj.get(\"fname_boost\"):\n                            components[\"fname_boost\"] = float(obj.get(\"fname_boost\", 0))\n                            why_parts.append(f\"fname:{float(obj.get('fname_boost', 0)):.2f}\")\n\n                        item = {\n                            \"score\": float(obj.get(\"score\", 0)),\n                            \"path\": obj.get(\"path\", \"\"),\n                            \"symbol\": obj.get(\"symbol\", \"\"),\n                            \"start_line\": int(obj.get(\"start_line\") or 0),\n                            \"end_line\": int(obj.get(\"end_line\") or 0),\n                            \"why\": why_parts,\n                            \"components\": components,\n                        }\n                        # Preserve dual-path metadata\n                        if obj.get(\"host_path\"):\n                            item[\"host_path\"] = obj[\"host_path\"]\n                        if obj.get(\"container_path\"):\n                            item[\"container_path\"] = obj[\"container_path\"]\n                        tmp.append(item)\n\n                    if tmp:\n                        results = tmp\n                        used_rerank = True\n                        rerank_counters[\"learning\"] += 1\n            except Exception:\n                pass  # Fall through to standard reranking\n\n        # Resolve in-process gating once and reuse\n        use_rerank_inproc = str(\n            os.environ.get(\"RERANK_IN_PROCESS\", \"\")\n        ).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n        # Prefer fusion-aware reranking over hybrid candidates when available, but only if in-process reranker is enabled\n        if use_rerank_inproc and not used_rerank:\n            try:\n                if json_lines:\n                    from scripts.rerank_local import rerank_local as _rr_local  # type: ignore\n                    import concurrent.futures as _fut\n\n                    rq = queries[0] if queries else \"\"\n                    # Prepare candidate docs from top-N hybrid hits (path+symbol + pseudo/tags + small snippet)\n                    cand_objs = list(json_lines[: int(rerank_top_n)])\n\n                    def _doc_for(obj: dict) -> str:\n                        path = str(obj.get(\"path\") or \"\")\n                        symbol = str(obj.get(\"symbol\") or \"\")\n                        header = f\"{symbol} \u2014 {path}\".strip()\n\n                        # Try to enrich with pseudo/tags from underlying payload when available.\n                        # We expect hybrid to have preserved metadata in obj[\"components\"] or\n                        # direct fields; if not, we fall back to header+code only.\n                        meta_lines: list[str] = [header] if header else []\n                        try:\n                            # Prefer explicit pseudo/tags fields on the top-level object when present\n                            pseudo_val = obj.get(\"pseudo\")\n                            tags_val = obj.get(\"tags\")\n                            if pseudo_val is None or tags_val is None:\n                                # Fallback: inspect a nested metadata view when present\n                                md = obj.get(\"metadata\") or {}\n                                if pseudo_val is None:\n                                    pseudo_val = md.get(\"pseudo\")\n                                if tags_val is None:\n                                    tags_val = md.get(\"tags\")\n                            pseudo_s = str(pseudo_val).strip() if pseudo_val is not None else \"\"\n                            if pseudo_s:\n                                # Keep pseudo short to avoid bloating rerank input\n                                meta_lines.append(f\"Summary: {pseudo_s[:256]}\")\n                            if tags_val:\n                                try:\n                                    if isinstance(tags_val, (list, tuple)):\n                                        tags_text = \", \".join(\n                                            str(x) for x in tags_val\n                                        )[:128]\n                                        if tags_text:\n                                            meta_lines.append(f\"Tags: {tags_text}\")\n                                    else:\n                                        tags_text = str(tags_val)[:128]\n                                        if tags_text:\n                                            meta_lines.append(f\"Tags: {tags_text}\")\n                                except Exception:\n                                    pass\n                        except Exception:\n                            # If any of the above fails, we just keep header-only\n                            pass\n\n                        sl = int(obj.get(\"start_line\") or 0)\n                        el = int(obj.get(\"end_line\") or 0)\n                        if not path or not sl:\n                            return \"\\n\".join(meta_lines) if meta_lines else header\n                        try:\n                            p = path\n                            if not os.path.isabs(p):\n                                p = os.path.join(\"/work\", p)\n                            realp = os.path.realpath(p)\n                            if not (realp == \"/work\" or realp.startswith(\"/work/\")):\n                                return \"\\n\".join(meta_lines) if meta_lines else header\n                            with open(\n                                realp, \"r\", encoding=\"utf-8\", errors=\"ignore\"\n                            ) as f:\n                                lines = f.readlines()\n                            ctx = (\n                                max(1, int(context_lines))\n                                if \"context_lines\" in locals()\n                                else 2\n                            )\n                            si = max(1, sl - ctx)\n                            ei = min(len(lines), max(sl, el) + ctx)\n                            snippet = \"\".join(lines[si - 1 : ei]).strip()\n                            if snippet:\n                                meta = \"\\n\".join(meta_lines) if meta_lines else header\n                                return (meta + \"\\n\\n\" + snippet).strip()\n                            return \"\\n\".join(meta_lines) if meta_lines else header\n                        except Exception:\n                            return \"\\n\".join(meta_lines) if meta_lines else header\n\n                    # Build docs concurrently\n                    max_workers = min(16, (os.cpu_count() or 4) * 4)\n                    with _fut.ThreadPoolExecutor(max_workers=max_workers) as ex:\n                        docs = list(ex.map(_doc_for, cand_objs))\n                    pairs = [(rq, d) for d in docs]\n                    scores = _rr_local(pairs)\n                    # Blend rerank with fusion score to preserve pre-rerank boosts\n                    # (symbol_exact, impl_boost, path boosts are otherwise lost)\n                    _rerank_blend = float(os.environ.get(\"RERANK_BLEND_WEIGHT\", \"0.6\") or 0.6)\n                    _rerank_blend = max(0.0, min(1.0, _rerank_blend))  # clamp [0,1]\n                    # Post-rerank symbol boost: apply symbol boosts directly to blended score\n                    # This ensures exact symbol matches rank higher even when reranker disagrees\n                    _post_symbol_boost = float(os.environ.get(\"POST_RERANK_SYMBOL_BOOST\", \"1.0\") or 1.0)\n                    blended = []\n                    for rr_score, obj in zip(scores, cand_objs):\n                        fusion_score = float(obj.get(\"score\", 0.0) or 0.0)\n                        # Normalize fusion_score to similar scale as rerank (rough heuristic)\n                        # Fusion scores are typically 0-3, rerank scores are -12 to 0\n                        # Shift fusion to negative range: fusion=2 -> -1, fusion=0 -> -3\n                        norm_fusion = fusion_score - 3.0\n                        blended_score = _rerank_blend * rr_score + (1.0 - _rerank_blend) * norm_fusion\n                        # Apply post-rerank symbol boost: extract symbol boosts from components\n                        # and add them directly to blended score (not diluted by blend weight)\n                        comps = obj.get(\"components\") or {}\n                        sym_sub = float(comps.get(\"symbol_substr\", 0.0) or 0.0)\n                        sym_eq = float(comps.get(\"symbol_exact\", 0.0) or 0.0)\n                        post_boost = (sym_sub + sym_eq) * _post_symbol_boost\n                        blended_score += post_boost\n                        blended.append((blended_score, rr_score, obj, post_boost))\n                    ranked = sorted(blended, key=lambda x: x[0], reverse=True)\n                    tmp = []\n                    for blended_s, rr_s, obj, post_b in ranked[: int(rerank_return_m)]:\n                        why_parts = obj.get(\"why\", []) + [f\"rerank_onnx:{float(rr_s):.3f}\", f\"blend:{float(blended_s):.3f}\"]\n                        if post_b > 0:\n                            why_parts.append(f\"post_sym:{float(post_b):.3f}\")\n                        item = {\n                            \"score\": float(blended_s),\n                            \"path\": obj.get(\"path\", \"\"),\n                            \"symbol\": obj.get(\"symbol\", \"\"),\n                            \"start_line\": int(obj.get(\"start_line\") or 0),\n                            \"end_line\": int(obj.get(\"end_line\") or 0),\n                            \"why\": why_parts,\n                            \"components\": (obj.get(\"components\") or {})\n                            | {\"rerank_onnx\": float(rr_s), \"blended\": float(blended_s), \"post_symbol_boost\": float(post_b)},\n                        }\n                        # Preserve dual-path metadata when available so clients can prefer host paths\n                        _hostp = obj.get(\"host_path\")\n                        _contp = obj.get(\"container_path\")\n                        if _hostp:\n                            item[\"host_path\"] = _hostp\n                        if _contp:\n                            item[\"container_path\"] = _contp\n                        tmp.append(item)\n                    if tmp:\n                        results = tmp\n                        used_rerank = True\n                        rerank_counters[\"inproc_hybrid\"] += 1\n            except Exception:\n                used_rerank = False\n        # Fallback paths (in-process reranker dense candidates, then subprocess)\n        if not used_rerank:\n            if use_rerank_inproc:\n                try:\n                    from scripts.rerank_local import rerank_in_process  # type: ignore\n\n                    model_name = os.environ.get(\n                        \"EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\"\n                    )\n                    model = get_embedding_model_fn(model_name) if get_embedding_model_fn else None\n                    rq = queries[0] if queries else \"\"\n                    items = rerank_in_process(\n                        query=rq,\n                        topk=int(rerank_top_n),\n                        limit=int(rerank_return_m),\n                        language=language or None,\n                        under=under or None,\n                        model=model,\n                        collection=collection,\n                    )\n                    if items:\n                        results = items\n                        used_rerank = True\n                        rerank_counters[\"inproc_dense\"] += 1\n                except Exception:\n                    use_rerank_inproc = False\n            if (not use_rerank_inproc) and (not used_rerank):\n                try:\n                    rq = queries[0] if queries else \"\"\n                    rcmd = [\n                        \"python\",\n                        _work_script(\"rerank_local.py\"),\n                        \"--query\",\n                        rq,\n                        \"--topk\",\n                        str(int(rerank_top_n)),\n                        \"--limit\",\n                        str(int(rerank_return_m)),\n                    ]\n                    if collection:\n                        rcmd += [\"--collection\", str(collection)]\n                    if language:\n                        rcmd += [\"--language\", language]\n                    if under:\n                        rcmd += [\"--under\", under]\n                    if os.environ.get(\"MCP_DEBUG_RERANK\", \"\").strip():\n                        try:\n                            logger.debug(\"RERANK_CMD\", extra={\"cmd\": \" \".join(rcmd)})\n                        except (ValueError, TypeError):\n                            pass\n                    _floor_ms = int(os.environ.get(\"RERANK_TIMEOUT_FLOOR_MS\", \"1000\"))\n                    try:\n                        _req_ms = int(rerank_timeout_ms)\n                    except Exception:\n                        _req_ms = _floor_ms\n                    _eff_ms = max(_floor_ms, _req_ms)\n                    _t_sec = max(0.1, _eff_ms / 1000.0)\n                    rres = await _run_async_fn(rcmd, env=env, timeout=_t_sec)\n                    if os.environ.get(\"MCP_DEBUG_RERANK\", \"\").strip():\n                        logger.debug(\n                            \"RERANK_RET\",\n                            extra={\n                                \"code\": rres.get(\"code\"),\n                                \"out_len\": len((rres.get(\"stdout\") or \"\").strip()),\n                                \"err_tail\": (rres.get(\"stderr\") or \"\")[-200:],\n                            },\n                        )\n                    if not rres.get(\"ok\"):\n                        _stderr = (rres.get(\"stderr\") or \"\").lower()\n                        if rres.get(\"code\") == -1 or \"timed out\" in _stderr:\n                            rerank_counters[\"timeout\"] += 1\n                    if rres.get(\"ok\") and (rres.get(\"stdout\") or \"\").strip():\n                        rerank_counters[\"subprocess\"] += 1\n                        tmp = []\n                        for ln in (rres.get(\"stdout\") or \"\").splitlines():\n                            parts = ln.strip().split(\"\\t\")\n                            if len(parts) != 4:\n                                continue\n                            score_s, path, symbol, range_s = parts\n                            try:\n                                start_s, end_s = range_s.split(\"-\", 1)\n                                start_line = int(start_s)\n                                end_line = int(end_s)\n                            except (ValueError, TypeError):\n                                start_line = 0\n                                end_line = 0\n                            try:\n                                score = float(score_s)\n                            except (ValueError, TypeError):\n                                score = 0.0\n                            item = {\n                                \"score\": score,\n                                \"path\": path,\n                                \"symbol\": symbol,\n                                \"start_line\": start_line,\n                                \"end_line\": end_line,\n                                \"why\": [f\"rerank_onnx:{score:.3f}\"],\n                            }\n                            tmp.append(item)\n                        if tmp:\n                            results = tmp\n                            used_rerank = True\n                            rerank_counters[\"subprocess\"] += 1\n                except Exception:\n                    rerank_counters[\"error\"] += 1\n                    used_rerank = False\n\n    if not used_rerank:\n        # Build results from hybrid JSON lines\n        for obj in json_lines:\n            item = {\n                \"score\": float(obj.get(\"score\", 0.0)),\n                \"path\": obj.get(\"path\", \"\"),\n                \"symbol\": obj.get(\"symbol\", \"\"),\n                \"start_line\": int(obj.get(\"start_line\") or 0),\n                \"end_line\": int(obj.get(\"end_line\") or 0),\n                \"why\": obj.get(\"why\", []),\n                \"components\": obj.get(\"components\", {}),\n            }\n            # Preserve dual-path metadata when available so clients can prefer host paths\n            _hostp = obj.get(\"host_path\")\n            _contp = obj.get(\"container_path\")\n            if _hostp:\n                item[\"host_path\"] = _hostp\n            if _contp:\n                item[\"container_path\"] = _contp\n            # Pass-through optional relation hints\n            if obj.get(\"relations\"):\n                item[\"relations\"] = obj.get(\"relations\")\n            if obj.get(\"related_paths\"):\n                item[\"related_paths\"] = obj.get(\"related_paths\")\n            if obj.get(\"span_budgeted\") is not None:\n                item[\"span_budgeted\"] = bool(obj.get(\"span_budgeted\"))\n            if obj.get(\"budget_tokens_used\") is not None:\n                item[\"budget_tokens_used\"] = int(obj.get(\"budget_tokens_used\"))\n            # Pass-through index-time pseudo/tags metadata so downstream consumers\n            # (e.g., MCP clients, rerankers, IDEs) can optionally incorporate\n            # GLM/LLM labels into their own scoring or display logic.\n            if obj.get(\"pseudo\") is not None:\n                item[\"pseudo\"] = obj.get(\"pseudo\")\n            if obj.get(\"tags\") is not None:\n                item[\"tags\"] = obj.get(\"tags\")\n            results.append(item)\n\n    # Mode-aware reordering: nudge core implementation code vs docs and non-core when requested\n    def _is_doc_path(p: str) -> bool:\n        pl = str(p or \"\").lower()\n        return (\n            \"readme\" in pl\n            or \"/docs/\" in pl\n            or \"/documentation/\" in pl\n            or pl.endswith(\".md\")\n            or pl.endswith(\".rst\")\n            or pl.endswith(\".txt\")\n        )\n\n    def _is_core_code_item(item: dict) -> bool:\n        \"\"\"Classify a result as core implementation code for mode-aware reordering.\n\n        This intentionally reuses hybrid_search's notion of core/test/vendor files\n        instead of duplicating extension and path heuristics here. We only apply\n        lightweight checks on top (docs/config/tests components) and delegate the\n        rest to helpers from hybrid_search when available.\n        \"\"\"\n        try:\n            raw_path = item.get(\"path\") or \"\"\n            p = str(raw_path)\n        except Exception:\n            return False\n        if not p:\n            return False\n        # Never treat docs as core code\n        if _is_doc_path(p):\n            return False\n\n        # Prefer items that were not explicitly tagged as docs/config/tests in hybrid components\n        comps = item.get(\"components\") or {}\n        try:\n            if comps:\n                if comps.get(\"config_penalty\") or comps.get(\"test_penalty\") or comps.get(\"doc_penalty\"):\n                    return False\n        except Exception:\n            pass\n\n        # Defer to hybrid_search helpers when available to avoid duplicating\n        # extension and path-based logic.\n        try:\n            from scripts.hybrid_search import (  # type: ignore\n                is_core_file as _hy_core_file,\n                is_test_file as _hy_is_test_file,\n                is_vendor_path as _hy_is_vendor_path,\n            )\n        except Exception:\n            _hy_core_file = None\n            _hy_is_test_file = None\n            _hy_is_vendor_path = None\n\n        if _hy_core_file:\n            try:\n                if not _hy_core_file(p):\n                    return False\n            except Exception:\n                return False\n        if _hy_is_test_file:\n            try:\n                if _hy_is_test_file(p):\n                    return False\n            except Exception:\n                pass\n        if _hy_is_vendor_path:\n            try:\n                if _hy_is_vendor_path(p):\n                    return False\n            except Exception:\n                pass\n\n        # If helper imports failed, fall back to a permissive classification:\n        # treat the item as core code (we already filtered obvious docs/config/tests).\n        return True\n\n    if mode_str in {\"code_first\", \"code-first\", \"code\"}:\n        core_items: list[dict] = []\n        other_code: list[dict] = []\n        doc_items: list[dict] = []\n        for it in results:\n            p = it.get(\"path\") or \"\"\n            if p and _is_doc_path(p):\n                doc_items.append(it)\n            elif _is_core_code_item(it):\n                core_items.append(it)\n            else:\n                other_code.append(it)\n        results = core_items + other_code + doc_items\n\n        try:\n            _min_core = int(os.environ.get(\"REPO_SEARCH_CODE_FIRST_MIN_CORE\", \"2\") or 0)\n        except Exception:\n            _min_core = 2\n        try:\n            _top_k = int(os.environ.get(\"REPO_SEARCH_CODE_FIRST_TOP_K\", \"8\") or 8)\n        except Exception:\n            _top_k = 8\n        if _min_core > 0 and results:\n            top_k = max(0, min(_top_k, len(results)))\n            if top_k > 0:\n                flags = [_is_core_code_item(it) for it in results]\n                cur_core = sum(1 for i in range(top_k) if flags[i])\n                if cur_core < _min_core:\n                    for src in range(top_k, len(results)):\n                        if not flags[src]:\n                            continue\n                        for dst in range(top_k - 1, -1, -1):\n                            if not flags[dst]:\n                                results[dst], results[src] = results[src], results[dst]\n                                flags[dst], flags[src] = flags[src], flags[dst]\n                                cur_core += 1\n                                break\n                        if cur_core >= _min_core:\n                            break\n    elif mode_str in {\"docs_first\", \"docs-first\", \"docs\"}:\n        core_items = []\n        other_code = []\n        doc_items = []\n        for it in results:\n            p = it.get(\"path\") or \"\"\n            if p and _is_doc_path(p):\n                doc_items.append(it)\n            elif _is_core_code_item(it):\n                core_items.append(it)\n            else:\n                other_code.append(it)\n        results = doc_items + core_items + other_code\n\n    # Enforce user-requested limit on final result count\n    try:\n        _limit_n = int(limit)\n    except Exception:\n        _limit_n = 0\n    if _limit_n > 0 and len(results) > _limit_n:\n        results = results[:_limit_n]\n\n    # Optionally add snippets (with highlighting)\n    toks = _tokens_from_queries(queries)\n    if include_snippet:\n        import concurrent.futures as _fut\n\n        def _read_snip(args):\n            i, item = args\n            try:\n                path = item.get(\"path\")\n                sl = int(item.get(\"start_line\") or 0)\n                el = int(item.get(\"end_line\") or 0)\n                if not path or not sl:\n                    return (i, \"\")\n                raw_path = (\n                    str(item.get(\"container_path\"))\n                    if item.get(\"container_path\")\n                    else str(path)\n                )\n                p = (\n                    raw_path\n                    if os.path.isabs(raw_path)\n                    else os.path.join(\"/work\", raw_path)\n                )\n                realp = os.path.realpath(p)\n                if not (realp == \"/work\" or realp.startswith(\"/work/\")):\n                    return (i, \"\")\n                with open(realp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                    lines = f.readlines()\n                ctx = max(1, int(context_lines))\n                si = max(1, sl - ctx)\n                ei = min(len(lines), max(sl, el) + ctx)\n                snippet = \"\".join(lines[si - 1 : ei])\n                if highlight_snippet:\n                    snippet = (\n                        do_highlight_snippet_fn(snippet, toks)\n                        if do_highlight_snippet_fn\n                        else snippet\n                    )\n                if len(snippet.encode(\"utf-8\", \"ignore\")) > SNIPPET_MAX_BYTES:\n                    _suffix = \"\\n...[snippet truncated]\"\n                    _sb = _suffix.encode(\"utf-8\")\n                    _bytes = snippet.encode(\"utf-8\", \"ignore\")\n                    _keep = max(0, SNIPPET_MAX_BYTES - len(_sb))\n                    _trimmed = _bytes[:_keep]\n                    snippet = _trimmed.decode(\"utf-8\", \"ignore\") + _suffix\n                return (i, snippet)\n            except Exception:\n                return (i, \"\")\n\n        max_workers = min(16, (os.cpu_count() or 4) * 4)\n        with _fut.ThreadPoolExecutor(max_workers=max_workers) as ex:\n            for i, snip in ex.map(_read_snip, list(enumerate(results))):\n                try:\n                    results[i][\"snippet\"] = snip\n                except Exception:\n                    pass\n\n    # Smart default: compact true for multi-query calls if compact not explicitly set\n    if (len(queries) > 1) and (\n        compact_raw is None\n        or (isinstance(compact_raw, str) and compact_raw.strip() == \"\")\n    ):\n        compact = True\n\n    # Compact mode: return only path and line range\n    if os.environ.get(\"DEBUG_REPO_SEARCH\"):\n        logger.debug(\n            \"DEBUG_REPO_SEARCH\",\n            extra={\n                \"count\": len(results),\n                \"sample\": [\n                    {\n                        \"path\": r.get(\"path\"),\n                        \"symbol\": r.get(\"symbol\"),\n                        \"range\": f\"{r.get('start_line')}-{r.get('end_line')}\",\n                    }\n                    for r in results[:5]\n                ],\n            },\n        )\n\n    # \u2500\u2500\u2500 Filename boost fallback \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # Apply filename-query correlation boost for results that don't have it yet.\n    # The learning reranker applies fname_boost when enabled; this catches:\n    #   - Reranking disabled\n    #   - Reranking timed out / failed\n    #   - Subprocess hybrid search without reranking\n    _fname_boost_factor = float(os.environ.get(\"FNAME_BOOST\", \"0.15\") or 0.15)\n    if _fname_boost_factor > 0 and results:\n        _q_str = \" \".join(queries).lower()\n        _q_toks = {t for t in re.findall(r\"[a-z0-9_]{3,}\", _q_str) if len(t) >= 3}\n        if _q_toks:\n            for r in results:\n                # Skip if fname_boost already applied by reranker\n                if r.get(\"fname_boost\") or (r.get(\"components\") or {}).get(\"fname_boost\"):\n                    continue\n\n                # Extract path from various possible keys\n                _path = \"\"\n                for _pk in (\"path\", \"rel_path\", \"host_path\", \"container_path\", \"client_path\"):\n                    _pv = r.get(_pk) or (r.get(\"metadata\") or {}).get(_pk)\n                    if isinstance(_pv, str) and _pv.strip():\n                        _path = _pv.lower()\n                        break\n                if not _path:\n                    continue\n\n                # Extract filename base (strip extension)\n                _fname = _path.rsplit(\"/\", 1)[-1] if \"/\" in _path else _path\n                _fname_base = re.sub(r\"\\.[^.]+$\", \"\", _fname)\n                _fname_toks = {t for t in re.split(r\"[_\\-.]\", _fname_base) if t and len(t) >= 3}\n\n                # Require 2+ matching tokens for boost\n                _match_count = len(_q_toks & _fname_toks)\n                if _match_count >= 2:\n                    _boost = float(_fname_boost_factor) * _match_count\n                    r[\"score\"] = float(r.get(\"score\", 0)) + _boost\n                    r[\"fname_boost\"] = _boost\n                    # Update components dict if present\n                    if \"components\" in r and isinstance(r[\"components\"], dict):\n                        r[\"components\"][\"fname_boost\"] = _boost\n                    # Update why array if present\n                    if \"why\" in r and isinstance(r[\"why\"], list):\n                        r[\"why\"].append(f\"fname:{_boost:.2f}\")\n        # Re-sort results by updated score so fname_boost affects ranking\n        results = sorted(results, key=lambda x: float(x.get(\"score\", 0)), reverse=True)\n\n    if compact:\n        results = [\n            {\n                \"path\": r.get(\"path\", \"\"),\n                \"start_line\": int(r.get(\"start_line\") or 0),\n                \"end_line\": int(r.get(\"end_line\") or 0),\n            }\n            for r in results\n        ]\n\n    response = {\n        \"args\": {\n            \"queries\": queries,\n            \"limit\": int(limit),\n            \"per_path\": int(per_path),\n            \"include_snippet\": bool(include_snippet),\n            \"context_lines\": int(context_lines),\n            \"rerank_enabled\": bool(rerank_enabled),\n            \"rerank_top_n\": int(rerank_top_n),\n            \"rerank_return_m\": int(rerank_return_m),\n            \"rerank_timeout_ms\": int(rerank_timeout_ms),\n            \"collection\": collection,\n            \"language\": language,\n            \"under\": under,\n            \"kind\": kind,\n            \"symbol\": symbol,\n            \"ext\": ext,\n            \"not\": not_,\n            \"case\": case,\n            \"path_regex\": path_regex,\n            \"path_glob\": path_globs,\n            \"not_glob\": not_globs,\n            # Echo the user-provided compact flag in args, normalized via _to_bool to respect strings like \"false\"/\"0\"\n            \"compact\": (_to_bool(compact_raw, compact)),\n        },\n        \"used_rerank\": bool(used_rerank),\n        \"rerank_counters\": rerank_counters,\n        \"code_signals\": code_signals if code_signals.get(\"has_code_signals\") else None,\n        \"total\": len(results),\n        \"results\": results,\n        **res,\n    }\n\n    # Apply TOON formatting if requested or enabled globally\n    # Full mode (compact=False) still saves tokens vs JSON while preserving all fields\n    if _should_use_toon(output_format):\n        return _format_results_as_toon(response, compact=bool(compact))\n    return response",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__to_int_262": {
      "name": "_to_int",
      "type": "function",
      "start_line": 262,
      "end_line": 268,
      "content_hash": "701ef16e1613d2eb06957edde5aff955573cadad",
      "content": "    def _to_int(x, default):\n        try:\n            if x is None or (isinstance(x, str) and x.strip() == \"\"):\n                return default\n            return int(x)\n        except Exception:\n            return default",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__to_bool_270": {
      "name": "_to_bool",
      "type": "function",
      "start_line": 270,
      "end_line": 280,
      "content_hash": "239cb886750e244d7b409643b0ce0568857e11bd",
      "content": "    def _to_bool(x, default):\n        if x is None or (isinstance(x, str) and x.strip() == \"\"):\n            return default\n        if isinstance(x, bool):\n            return x\n        s = str(x).strip().lower()\n        if s in {\"1\", \"true\", \"yes\", \"on\"}:\n            return True\n        if s in {\"0\", \"false\", \"no\", \"off\"}:\n            return False\n        return default",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__to_str_286": {
      "name": "_to_str",
      "type": "function",
      "start_line": 286,
      "end_line": 289,
      "content_hash": "02bdf44f5fdce271d2655dc9a2f76f20b3675e7a",
      "content": "    def _to_str(x, default=\"\"):\n        if x is None:\n            return default\n        return str(x)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__to_str_list_395": {
      "name": "_to_str_list",
      "type": "function",
      "start_line": 395,
      "end_line": 409,
      "content_hash": "4a5213ab3b7a5c81eef6506c790d3b1227501520",
      "content": "    def _to_str_list(x):\n        if x is None:\n            return []\n        if isinstance(x, (list, tuple)):\n            out = []\n            for e in x:\n                s = str(e).strip()\n                if s:\n                    out.append(s)\n            return out\n        s = str(x).strip()\n        if not s:\n            return []\n        # support comma-separated shorthand\n        return [t.strip() for t in s.split(\",\") if t.strip()]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__doc_for_767": {
      "name": "_doc_for",
      "type": "function",
      "start_line": 767,
      "end_line": 837,
      "content_hash": "ffbb6a5007465714545782d46dc37cdc90de645c",
      "content": "                    def _doc_for(obj: dict) -> str:\n                        path = str(obj.get(\"path\") or \"\")\n                        symbol = str(obj.get(\"symbol\") or \"\")\n                        header = f\"{symbol} \u2014 {path}\".strip()\n\n                        # Try to enrich with pseudo/tags from underlying payload when available.\n                        # We expect hybrid to have preserved metadata in obj[\"components\"] or\n                        # direct fields; if not, we fall back to header+code only.\n                        meta_lines: list[str] = [header] if header else []\n                        try:\n                            # Prefer explicit pseudo/tags fields on the top-level object when present\n                            pseudo_val = obj.get(\"pseudo\")\n                            tags_val = obj.get(\"tags\")\n                            if pseudo_val is None or tags_val is None:\n                                # Fallback: inspect a nested metadata view when present\n                                md = obj.get(\"metadata\") or {}\n                                if pseudo_val is None:\n                                    pseudo_val = md.get(\"pseudo\")\n                                if tags_val is None:\n                                    tags_val = md.get(\"tags\")\n                            pseudo_s = str(pseudo_val).strip() if pseudo_val is not None else \"\"\n                            if pseudo_s:\n                                # Keep pseudo short to avoid bloating rerank input\n                                meta_lines.append(f\"Summary: {pseudo_s[:256]}\")\n                            if tags_val:\n                                try:\n                                    if isinstance(tags_val, (list, tuple)):\n                                        tags_text = \", \".join(\n                                            str(x) for x in tags_val\n                                        )[:128]\n                                        if tags_text:\n                                            meta_lines.append(f\"Tags: {tags_text}\")\n                                    else:\n                                        tags_text = str(tags_val)[:128]\n                                        if tags_text:\n                                            meta_lines.append(f\"Tags: {tags_text}\")\n                                except Exception:\n                                    pass\n                        except Exception:\n                            # If any of the above fails, we just keep header-only\n                            pass\n\n                        sl = int(obj.get(\"start_line\") or 0)\n                        el = int(obj.get(\"end_line\") or 0)\n                        if not path or not sl:\n                            return \"\\n\".join(meta_lines) if meta_lines else header\n                        try:\n                            p = path\n                            if not os.path.isabs(p):\n                                p = os.path.join(\"/work\", p)\n                            realp = os.path.realpath(p)\n                            if not (realp == \"/work\" or realp.startswith(\"/work/\")):\n                                return \"\\n\".join(meta_lines) if meta_lines else header\n                            with open(\n                                realp, \"r\", encoding=\"utf-8\", errors=\"ignore\"\n                            ) as f:\n                                lines = f.readlines()\n                            ctx = (\n                                max(1, int(context_lines))\n                                if \"context_lines\" in locals()\n                                else 2\n                            )\n                            si = max(1, sl - ctx)\n                            ei = min(len(lines), max(sl, el) + ctx)\n                            snippet = \"\".join(lines[si - 1 : ei]).strip()\n                            if snippet:\n                                meta = \"\\n\".join(meta_lines) if meta_lines else header\n                                return (meta + \"\\n\\n\" + snippet).strip()\n                            return \"\\n\".join(meta_lines) if meta_lines else header\n                        except Exception:\n                            return \"\\n\".join(meta_lines) if meta_lines else header",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__is_doc_path_1043": {
      "name": "_is_doc_path",
      "type": "function",
      "start_line": 1043,
      "end_line": 1052,
      "content_hash": "d5dcb30433597cbb88ad9d63406c44bea4bf38e3",
      "content": "    def _is_doc_path(p: str) -> bool:\n        pl = str(p or \"\").lower()\n        return (\n            \"readme\" in pl\n            or \"/docs/\" in pl\n            or \"/documentation/\" in pl\n            or pl.endswith(\".md\")\n            or pl.endswith(\".rst\")\n            or pl.endswith(\".txt\")\n        )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__is_core_code_item_1054": {
      "name": "_is_core_code_item",
      "type": "function",
      "start_line": 1054,
      "end_line": 1116,
      "content_hash": "e487719fa86993914db8bdd3762517bd110df6ed",
      "content": "    def _is_core_code_item(item: dict) -> bool:\n        \"\"\"Classify a result as core implementation code for mode-aware reordering.\n\n        This intentionally reuses hybrid_search's notion of core/test/vendor files\n        instead of duplicating extension and path heuristics here. We only apply\n        lightweight checks on top (docs/config/tests components) and delegate the\n        rest to helpers from hybrid_search when available.\n        \"\"\"\n        try:\n            raw_path = item.get(\"path\") or \"\"\n            p = str(raw_path)\n        except Exception:\n            return False\n        if not p:\n            return False\n        # Never treat docs as core code\n        if _is_doc_path(p):\n            return False\n\n        # Prefer items that were not explicitly tagged as docs/config/tests in hybrid components\n        comps = item.get(\"components\") or {}\n        try:\n            if comps:\n                if comps.get(\"config_penalty\") or comps.get(\"test_penalty\") or comps.get(\"doc_penalty\"):\n                    return False\n        except Exception:\n            pass\n\n        # Defer to hybrid_search helpers when available to avoid duplicating\n        # extension and path-based logic.\n        try:\n            from scripts.hybrid_search import (  # type: ignore\n                is_core_file as _hy_core_file,\n                is_test_file as _hy_is_test_file,\n                is_vendor_path as _hy_is_vendor_path,\n            )\n        except Exception:\n            _hy_core_file = None\n            _hy_is_test_file = None\n            _hy_is_vendor_path = None\n\n        if _hy_core_file:\n            try:\n                if not _hy_core_file(p):\n                    return False\n            except Exception:\n                return False\n        if _hy_is_test_file:\n            try:\n                if _hy_is_test_file(p):\n                    return False\n            except Exception:\n                pass\n        if _hy_is_vendor_path:\n            try:\n                if _hy_is_vendor_path(p):\n                    return False\n            except Exception:\n                pass\n\n        # If helper imports failed, fall back to a permissive classification:\n        # treat the item as core code (we already filtered obvious docs/config/tests).\n        return True",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__read_snip_1184": {
      "name": "_read_snip",
      "type": "function",
      "start_line": 1184,
      "end_line": 1226,
      "content_hash": "bc7b6f1b07ca9c90308558d2f5c1593aa4d353e5",
      "content": "        def _read_snip(args):\n            i, item = args\n            try:\n                path = item.get(\"path\")\n                sl = int(item.get(\"start_line\") or 0)\n                el = int(item.get(\"end_line\") or 0)\n                if not path or not sl:\n                    return (i, \"\")\n                raw_path = (\n                    str(item.get(\"container_path\"))\n                    if item.get(\"container_path\")\n                    else str(path)\n                )\n                p = (\n                    raw_path\n                    if os.path.isabs(raw_path)\n                    else os.path.join(\"/work\", raw_path)\n                )\n                realp = os.path.realpath(p)\n                if not (realp == \"/work\" or realp.startswith(\"/work/\")):\n                    return (i, \"\")\n                with open(realp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                    lines = f.readlines()\n                ctx = max(1, int(context_lines))\n                si = max(1, sl - ctx)\n                ei = min(len(lines), max(sl, el) + ctx)\n                snippet = \"\".join(lines[si - 1 : ei])\n                if highlight_snippet:\n                    snippet = (\n                        do_highlight_snippet_fn(snippet, toks)\n                        if do_highlight_snippet_fn\n                        else snippet\n                    )\n                if len(snippet.encode(\"utf-8\", \"ignore\")) > SNIPPET_MAX_BYTES:\n                    _suffix = \"\\n...[snippet truncated]\"\n                    _sb = _suffix.encode(\"utf-8\")\n                    _bytes = snippet.encode(\"utf-8\", \"ignore\")\n                    _keep = max(0, SNIPPET_MAX_BYTES - len(_sb))\n                    _trimmed = _bytes[:_keep]\n                    snippet = _trimmed.decode(\"utf-8\", \"ignore\") + _suffix\n                return (i, snippet)\n            except Exception:\n                return (i, \"\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}