{
  "file_path": "/work/context-engine/scripts/ctx.py",
  "file_hash": "6f013d8eb6f6cf5970336f7d005f283e48867823",
  "updated_at": "2025-12-26T17:34:21.427965",
  "symbols": {
    "function__load_env_file_61": {
      "name": "_load_env_file",
      "type": "function",
      "start_line": 61,
      "end_line": 94,
      "content_hash": "d14aefa84e4740857dcd897da453f6ac5f0e7e59",
      "content": "def _load_env_file():\n\t\"\"\"Load .env file from workspace (if provided) or project root if it exists.\"\"\"\n\t# Prefer an explicit workspace root (set by the hook) when available,\n\t# otherwise fall back to the original project-root behavior based on this file.\n\tscript_dir = Path(__file__).resolve().parent\n\tcandidates = []\n\n\tworkspace_dir = os.environ.get(\"CTX_WORKSPACE_DIR\")\n\tif workspace_dir:\n\t\ttry:\n\t\t\tcandidates.append(Path(workspace_dir) / \".env\")\n\t\texcept Exception:\n\t\t\tpass\n\n\t# Original project-root-based .env (for CLI / repo-local usage)\n\tcandidates.append(script_dir.parent / \".env\")\n\n\tfor env_file in candidates:\n\t\tif not env_file.exists():\n\t\t\tcontinue\n\t\twith open(env_file) as f:\n\t\t\tfor line in f:\n\t\t\t\tline = line.strip()\n\t\t\t\tif not line or line.startswith(\"#\"):\n\t\t\t\t\tcontinue\n\t\t\t\tif \"=\" in line:\n\t\t\t\t\tkey, value = line.split(\"=\", 1)\n\t\t\t\t\tkey = key.strip()\n\t\t\t\t\tvalue = value.strip().strip('\"').strip(\"'\")\n\t\t\t\t\t# Only set if not already in environment\n\t\t\t\t\tif key and key not in os.environ:\n\t\t\t\t\t\tos.environ[key] = value\n\t\t# Only load the first existing .env\n\t\tbreak",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_resolve_decoder_url_114": {
      "name": "resolve_decoder_url",
      "type": "function",
      "start_line": 114,
      "end_line": 159,
      "content_hash": "c791a6addca275f202a0ca47861a363a17ad4fb7",
      "content": "def resolve_decoder_url() -> str:\n    \"\"\"Resolve decoder endpoint, honoring overrides and Ollama/GLM options.\n\n    Rules:\n    - DECODER_URL wins\n    - Otherwise, if OLLAMA_HOST is set, default to its /api/chat endpoint\n    - Otherwise, fall back to llama.cpp URL (GPU override if requested)\n    - Only append /completion for llama.cpp-style endpoints; leave Ollama/OpenAI paths untouched\n    \"\"\"\n    override = os.environ.get(\"DECODER_URL\", \"\").strip()\n    if override:\n        base = override\n    else:\n        ollama_host = os.environ.get(\"OLLAMA_HOST\", \"\").strip()\n        if ollama_host:\n            base = ollama_host.rstrip(\"/\")\n            if \"/api/\" not in base:\n                base = base + \"/api/chat\"\n        else:\n            use_gpu = str(os.environ.get(\"USE_GPU_DECODER\", \"0\")).strip().lower()\n            if use_gpu in {\"1\", \"true\", \"yes\", \"on\"}:\n                host = \"host.docker.internal\" if os.path.exists(\"/.dockerenv\") else \"localhost\"\n                base = f\"http://{host}:8081\"\n            else:\n                base = os.environ.get(\"LLAMACPP_URL\", \"http://localhost:8080\").strip()\n\n    base = base or \"http://localhost:11434/api/chat\"\n    parsed_base = urlparse(base)\n    if parsed_base.hostname == \"host.docker.internal\" and not os.path.exists(\"/.dockerenv\"):\n        try:\n            socket.gethostbyname(parsed_base.hostname)\n        except socket.gaierror:\n            base = base.replace(\"host.docker.internal\", \"localhost\")\n            sys.stderr.write(\"[DEBUG] decoder host.docker.internal not reachable; falling back to localhost\\n\")\n            sys.stderr.flush()\n    lowered = base.lower()\n    if (\n        \"ollama\" in lowered\n        or \"/api/chat\" in lowered\n        or \"/api/generate\" in lowered\n        or \"/v1/chat/completions\" in lowered\n    ):\n        return base\n    if base.endswith(\"/completion\"):\n        return base\n    return base.rstrip(\"/\") + \"/completion\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_parse_sse_response_170": {
      "name": "parse_sse_response",
      "type": "function",
      "start_line": 170,
      "end_line": 175,
      "content_hash": "16e94948e7b5610137c9085117b098d75dcb8afe",
      "content": "def parse_sse_response(text: str) -> Dict[str, Any]:\n    \"\"\"Parse SSE format response (event: message\\\\ndata: {...}).\"\"\"\n    for line in text.strip().split('\\n'):\n        if line.startswith('data: '):\n            return json.loads(line[6:])\n    raise ValueError(\"No data line found in SSE response\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_session_id_178": {
      "name": "get_session_id",
      "type": "function",
      "start_line": 178,
      "end_line": 216,
      "content_hash": "d44e3677a65ddc6dcc244307369057bf08ca1806",
      "content": "def get_session_id(timeout: int = 10) -> str:\n    \"\"\"Initialize MCP session and return session ID.\"\"\"\n    global _session_id\n    if _session_id:\n        return _session_id\n\n    payload = {\n        \"jsonrpc\": \"2.0\",\n        \"id\": 0,\n        \"method\": \"initialize\",\n        \"params\": {\n            \"protocolVersion\": \"2024-11-05\",\n            \"capabilities\": {},\n            \"clientInfo\": {\"name\": \"ctx-cli\", \"version\": \"1.0.0\"}\n        }\n    }\n\n    try:\n        req = request.Request(\n            MCP_URL,\n            data=json.dumps(payload).encode(),\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"Accept\": \"application/json, text/event-stream\"\n            }\n        )\n        with request.urlopen(req, timeout=timeout) as resp:\n            session_id = resp.headers.get(\"mcp-session-id\")\n            if not session_id:\n                raise RuntimeError(\"Server did not return session ID\")\n            # Read the initialization response to ensure session is fully established\n            init_response = resp.read().decode('utf-8')\n            # Wait a moment for session to be fully processed\n            import time\n            time.sleep(0.5)\n            _session_id = session_id\n            return session_id\n    except Exception as e:\n        raise RuntimeError(f\"Failed to initialize MCP session: {e}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_call_mcp_tool_219": {
      "name": "call_mcp_tool",
      "type": "function",
      "start_line": 219,
      "end_line": 240,
      "content_hash": "8bc4d69ae57b38626d3cc7eba49591573f6a6195",
      "content": "def call_mcp_tool(tool_name: str, params: Dict[str, Any], timeout: int = 30) -> Dict[str, Any]:\n    \"\"\"Call MCP tool via HTTP JSON-RPC with session management.\"\"\"\n    payload = {\n        \"jsonrpc\": \"2.0\",\n        \"id\": 1,\n        \"method\": \"tools/call\",\n        \"params\": {\"name\": tool_name, \"arguments\": params}\n    }\n\n    # Debug output (opt-in to avoid leaking queries in normal use)\n    debug_flag = os.environ.get(\"CTX_DEBUG\", \"\").strip().lower()\n    if debug_flag in {\"1\", \"true\", \"yes\", \"on\"}:\n        sys.stderr.write(f\"[DEBUG] Calling MCP tool '{tool_name}' at {MCP_URL}\\n\")\n        sys.stderr.write(f\"[DEBUG] Sending payload: {json.dumps(payload, indent=2)}\\n\")\n        sys.stderr.flush()\n\n    try:\n        return call_tool_http(MCP_URL, tool_name, params, timeout=float(timeout))\n    except Exception as e:\n        sys.stderr.write(f\"[ERROR] MCP call to '{tool_name}' at {MCP_URL} failed: {type(e).__name__}: {e}\\n\")\n        sys.stderr.flush()\n        return {\"error\": f\"Request failed: {str(e)}\"}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_parse_mcp_response_243": {
      "name": "parse_mcp_response",
      "type": "function",
      "start_line": 243,
      "end_line": 276,
      "content_hash": "8b74a74838245806931c2d7da20c6450899278b1",
      "content": "def parse_mcp_response(result: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n    \"\"\"Parse MCP response and extract the actual result.\n\n    Supports both text and json content items from FastMCP.\n    \"\"\"\n    if \"error\" in result:\n        return None\n\n    # FastMCP typically wraps results in a content array\n    res = result.get(\"result\", {})\n    content = res.get(\"content\", [])\n\n    # Some servers may return a dict directly (no content array)\n    if isinstance(res, dict) and content == [] and any(k in res for k in (\"results\", \"answer\", \"total\")):\n        return res\n\n    if not content:\n        return None\n\n    item = content[0] or {}\n\n    # Prefer typed JSON content\n    if isinstance(item, dict) and \"json\" in item:\n        return item.get(\"json\")\n\n    # Fallback: parse text as JSON or return raw text\n    text = item.get(\"text\", \"\") if isinstance(item, dict) else \"\"\n    if not text:\n        return None\n\n    try:\n        return json.loads(text)\n    except json.JSONDecodeError:\n        return {\"raw\": text}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_tool_error_279": {
      "name": "_extract_tool_error",
      "type": "function",
      "start_line": 279,
      "end_line": 300,
      "content_hash": "484d1710ce9f619ede6fe6fb4dc904ee844204b1",
      "content": "def _extract_tool_error(result: Dict[str, Any]) -> Optional[str]:\n    \"\"\"Best-effort extraction of a tool-level error message from MCP response.\n\n    Handles FastMCP-style isError + content.text as well as structuredContent.result.error.\n    Returns a human-readable error string when present, or None when no error detected.\n    \"\"\"\n    try:\n        res = result.get(\"result\") or {}\n        if isinstance(res, dict) and res.get(\"isError\") is True:\n            content = res.get(\"content\") or []\n            if isinstance(content, list) and content and isinstance(content[0], dict):\n                text = content[0].get(\"text\")\n                if isinstance(text, str) and text.strip():\n                    return text.strip()\n        sc = res.get(\"structuredContent\") or {}\n        rs = sc.get(\"result\") or {}\n        err = rs.get(\"error\")\n        if isinstance(err, str) and err.strip():\n            return err.strip()\n    except Exception:\n        return None\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__compress_snippet_303": {
      "name": "_compress_snippet",
      "type": "function",
      "start_line": 303,
      "end_line": 322,
      "content_hash": "d7cd8bbe5b11aaa6de93f295a25eaabcf9cb1467",
      "content": "def _compress_snippet(snippet: str, max_lines: int = 6) -> str:\n    \"\"\"Compact, high-signal subset of a code snippet.\n\n    Heuristics: prefer signatures, guards, returns/raises, asserts; fall back to head/tail.\n    \"\"\"\n    try:\n        raw_lines = [ln.rstrip() for ln in snippet.splitlines() if ln.strip()]\n        if not raw_lines:\n            return \"\"\n        keys = (\"def \", \"class \", \"return\", \"raise\", \"assert\", \"if \", \"except\", \"try:\")\n        scored = [(sum(k in ln for k in keys), idx, ln) for idx, ln in enumerate(raw_lines)]\n        keep_idx = sorted({idx for _, idx, _ in sorted(scored, key=lambda t: (-t[0], t[1]))[:max_lines]})\n        kept = [raw_lines[i] for i in keep_idx]\n        if not kept:\n            head = raw_lines[: max(1, max_lines // 2)]\n            tail = raw_lines[-(max_lines - len(head)) :]\n            kept = head + tail\n        return \"\\n\".join(kept[:max_lines])\n    except Exception:\n        return (snippet or \"\").splitlines()[0][:160]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_format_search_results_325": {
      "name": "format_search_results",
      "type": "function",
      "start_line": 325,
      "end_line": 368,
      "content_hash": "4bd0558c36394d78a17dbe41bce9d785a884ca72",
      "content": "def format_search_results(results: List[Dict[str, Any]], include_snippets: bool = False) -> str:\n    \"\"\"Format search results succinctly for LLM rewrite.\n\n    When include_snippets is False (default), only include headers with path and line ranges.\n    This keeps prompts small and fast for Granite via llama.cpp.\n    \"\"\"\n    lines: List[str] = []\n    for hit in results:\n        # Prefer the server-chosen display path; fall back to host/container paths\n        raw_path = (\n            hit.get(\"path\")\n            or hit.get(\"host_path\")\n            or hit.get(\"container_path\")\n            or \"unknown\"\n        )\n        path = raw_path\n        start = hit.get(\"start_line\", \"?\")\n        end = hit.get(\"end_line\", \"?\")\n        language = hit.get(\"language\") or \"\"\n        symbol = hit.get(\"symbol\") or \"\"\n        snippet = (hit.get(\"snippet\") or \"\").strip()\n\n        # Only include line ranges when both start and end are known\n        if start in (None, \"?\") or end in (None, \"?\"):\n            header = f\"- {path}\"\n        else:\n            header = f\"- {path}:{start}-{end}\"\n        meta: List[str] = []\n        if language:\n            meta.append(language)\n        if symbol:\n            meta.append(f\"{symbol}\")\n        if meta:\n            header += f\" ({', '.join(meta)})\"\n        lines.append(header)\n\n        if include_snippets and snippet:\n            compact = _compress_snippet(snippet, max_lines=6)\n            if compact:\n                for ln in compact.splitlines():\n                    # Inline compact snippet (no fences to keep token count small)\n                    lines.append(f\"    {ln}\")\n\n    return \"\\n\".join(lines)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ensure_two_paragraph_questions_372": {
      "name": "_ensure_two_paragraph_questions",
      "type": "function",
      "start_line": 372,
      "end_line": 438,
      "content_hash": "37d60983cc6ef145421d666002807c261344dbf1",
      "content": "def _ensure_two_paragraph_questions(text: str) -> str:\n    \"\"\"Normalize to at least two paragraphs.\n\n    - Collapse excessive whitespace\n    - For questions: ensure each paragraph ends with '?'\n    - For commands/instructions: ensure proper punctuation\n    - If only one paragraph, split heuristically or add a generic follow-up\n    \"\"\"\n    if not text:\n        return \"\"\n    # Normalize whitespace/newlines\n    t = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").strip()\n    # Collapse triple+ newlines to double\n    while \"\\n\\n\\n\" in t:\n        t = t.replace(\"\\n\\n\\n\", \"\\n\\n\")\n    raw_paras = [p.strip() for p in t.split(\"\\n\\n\") if p.strip()]\n\n    # Deduplicate paragraphs (case/whitespace insensitive, tolerance for near-duplicates)\n    paras: list[str] = []\n    dedup_keys: list[str] = []\n    for p in raw_paras:\n        key = re.sub(r\"\\s+\", \" \", p).strip().lower()\n        if any(difflib.SequenceMatcher(None, key, existing).ratio() >= 0.99 for existing in dedup_keys):\n            continue\n        dedup_keys.append(key)\n        paras.append(p)\n\n    def normalize_paragraph(s: str) -> str:\n        \"\"\"Ensure proper punctuation - keep questions as questions, commands as commands.\"\"\"\n        s = s.strip()\n        if not s:\n            return s\n        # If already ends with proper punctuation, keep as-is\n        if s[-1] in \"?!.\":\n            return s\n        # Check if it looks like a question (starts with question words or contains '?')\n        question_starters = (\"what\", \"how\", \"why\", \"when\", \"where\", \"who\", \"which\", \"can\", \"could\", \"would\", \"should\", \"is\", \"are\", \"does\", \"do\")\n        first_word = s.split()[0].lower() if s.split() else \"\"\n        if first_word in question_starters or \"?\" in s:\n            # It's a question - ensure it ends with '?'\n            if s[-1] in \".!:\":\n                return s[:-1].rstrip() + \"?\"\n            return s + \"?\"\n        # It's a command/statement - ensure it ends with '.'\n        if s[-1] in \":\":\n            return s[:-1].rstrip() + \".\"\n        return s + \".\"\n\n    max_paragraphs = 3\n    if len(paras) >= 2:\n        selected = [normalize_paragraph(p) for p in paras[:max_paragraphs]]\n        return \"\\n\\n\".join(selected)\n\n    # Single paragraph: try to split by sentence boundary\n    p = paras[0] if paras else t\n    # Naive sentence split\n    sentences = [s.strip() for s in p.replace(\"?\", \". \").replace(\"!\", \". \").split(\". \") if s.strip()]\n    if len(sentences) > 1:\n        half = max(1, len(sentences) // 2)\n        p1 = \". \".join(sentences[:half]).strip()\n        p2 = \". \".join(sentences[half:]).strip()\n    else:\n        p1 = p.strip()\n        p2 = (\n            \"Detail the exact systems involved (e.g., files, classes, state machines), how data flows between them, and any validation before emitting updates.\"\n        )\n    return normalize_paragraph(p1) + \"\\n\\n\" + normalize_paragraph(p2)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_normalize_paragraph_399": {
      "name": "normalize_paragraph",
      "type": "function",
      "start_line": 399,
      "end_line": 418,
      "content_hash": "efbe810a7cabefcb704f58669a210e7e4134d25f",
      "content": "    def normalize_paragraph(s: str) -> str:\n        \"\"\"Ensure proper punctuation - keep questions as questions, commands as commands.\"\"\"\n        s = s.strip()\n        if not s:\n            return s\n        # If already ends with proper punctuation, keep as-is\n        if s[-1] in \"?!.\":\n            return s\n        # Check if it looks like a question (starts with question words or contains '?')\n        question_starters = (\"what\", \"how\", \"why\", \"when\", \"where\", \"who\", \"which\", \"can\", \"could\", \"would\", \"should\", \"is\", \"are\", \"does\", \"do\")\n        first_word = s.split()[0].lower() if s.split() else \"\"\n        if first_word in question_starters or \"?\" in s:\n            # It's a question - ensure it ends with '?'\n            if s[-1] in \".!:\":\n                return s[:-1].rstrip() + \"?\"\n            return s + \"?\"\n        # It's a command/statement - ensure it ends with '.'\n        if s[-1] in \":\":\n            return s[:-1].rstrip() + \".\"\n        return s + \".\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_extract_allowed_citations_444": {
      "name": "extract_allowed_citations",
      "type": "function",
      "start_line": 444,
      "end_line": 470,
      "content_hash": "9172772d27f9038ee2898c4bd8156e2d4cd8125f",
      "content": "def extract_allowed_citations(context_text: str) -> tuple[Set[str], Set[str]]:\n    \"\"\"Extract allowed file paths and symbols from formatted context lines.\n\n    Parses lines produced by format_search_results. Returns (paths, symbols).\n    \"\"\"\n    allowed_paths: Set[str] = set()\n    allowed_symbols: Set[str] = set()\n    for raw in (context_text or \"\").splitlines():\n        line = (raw or \"\").strip()\n        if not line:\n            continue\n        if line.startswith(\"- \"):\n            header = line[2:].strip()\n            header_main = header.split(\" (\")[0]\n            path_part = header_main.split(\":\")[0]\n            if path_part:\n                allowed_paths.add(path_part)\n            # symbols are inside parens, after optional language\n            m = re.search(r\"\\(([^)]+)\\)\", header)\n            if m:\n                for part in m.group(1).split(\",\"):\n                    sym = part.strip()\n                    if sym and sym.lower() not in {\n                        \"python\", \"typescript\", \"javascript\", \"go\", \"rust\", \"java\", \"c\", \"c++\", \"c#\", \"shell\", \"bash\", \"markdown\", \"json\", \"yaml\", \"toml\"\n                    }:\n                        allowed_symbols.add(sym)\n    return allowed_paths, allowed_symbols",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_build_refined_query_473": {
      "name": "build_refined_query",
      "type": "function",
      "start_line": 473,
      "end_line": 484,
      "content_hash": "5600441790043bc7c73bf69e2057469ebaa59afc",
      "content": "def build_refined_query(original_query: str, allowed_paths: Set[str], allowed_symbols: Set[str], max_terms: int = 6) -> str:\n    \"\"\"Construct a grounded follow-up query using only known paths/symbols.\"\"\"\n    from os.path import basename\n    terms: list[str] = []\n    for p in list(allowed_paths)[: max_terms // 2]:\n        base = basename(p)\n        if base and base not in terms:\n            terms.append(base)\n    for s in list(allowed_symbols)[: max_terms - len(terms)]:\n        if s and s not in terms:\n            terms.append(s)\n    return (original_query or \"\").strip() + (\" \" + \" \".join(terms) if terms else \"\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__simple_tokenize_487": {
      "name": "_simple_tokenize",
      "type": "function",
      "start_line": 487,
      "end_line": 489,
      "content_hash": "b874e02e2aeff6b57ca63795c2da15f97da8f505",
      "content": "def _simple_tokenize(text: str) -> List[str]:\n    tokens = re.findall(r\"[A-Za-z0-9_]+\", text or \"\")\n    return [t.lower() for t in tokens if t]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__token_overlap_ratio_492": {
      "name": "_token_overlap_ratio",
      "type": "function",
      "start_line": 492,
      "end_line": 501,
      "content_hash": "a54bec699c84e49f0c2503a7cb39818b9aae0c97",
      "content": "def _token_overlap_ratio(a: str, b: str) -> float:\n    a_tokens = set(_simple_tokenize(a))\n    b_tokens = set(_simple_tokenize(b))\n    if not a_tokens or not b_tokens:\n        return 0.0\n    inter = len(a_tokens & b_tokens)\n    union = len(a_tokens | b_tokens)\n    if not union:\n        return 0.0\n    return inter / union",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__estimate_query_result_relevance_504": {
      "name": "_estimate_query_result_relevance",
      "type": "function",
      "start_line": 504,
      "end_line": 528,
      "content_hash": "5a7d198847de9f0c5eca863bb5d11eeb2903d3be",
      "content": "def _estimate_query_result_relevance(query: str, results: List[Dict[str, Any]]) -> float:\n    q_tokens = set(_simple_tokenize(query))\n    if not q_tokens or not results:\n        return 0.0\n    scores: List[float] = []\n    for hit in results[:5]:\n        parts: List[str] = []\n        for key in (\"path\", \"symbol\", \"snippet\"):\n            val = hit.get(key)\n            if isinstance(val, str):\n                parts.append(val)\n        if not parts:\n            continue\n        r_tokens = set()\n        for part in parts:\n            r_tokens.update(_simple_tokenize(part))\n        if not r_tokens:\n            continue\n        inter = len(q_tokens & r_tokens)\n        union = len(q_tokens | r_tokens)\n        if union:\n            scores.append(inter / union)\n    if not scores:\n        return 0.0\n    return sum(scores) / len(scores)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_sanitize_citations_531": {
      "name": "sanitize_citations",
      "type": "function",
      "start_line": 531,
      "end_line": 576,
      "content_hash": "2e3a52d8a07ad6a1838819e43ec6b4523cf53af8",
      "content": "def sanitize_citations(text: str, allowed_paths: Set[str]) -> str:\n    \"\"\"Replace path-like strings not present in allowed_paths with a neutral phrase.\n\n    Keeps exact paths and basenames that appear in allowed_paths; replaces others.\n    \"\"\"\n    if not text:\n        return text\n    from os.path import basename\n    allowed_set = set(allowed_paths or set())\n    basename_to_paths: Dict[str, Set[str]] = {}\n    for _p in allowed_set:\n        _b = basename(_p)\n        if _b:\n            basename_to_paths.setdefault(_b, set()).add(_p)\n\n    # For now, keep allowed paths exactly as they appear in the context refs.\n    # Earlier versions tried to be clever by rewriting absolute paths to\n    # workspace-relative forms (e.g., \"Context-Engine/scripts/ctx.py\"), which\n    # could produce confusing hybrids when multiple workspace roots or\n    # slugged/collection-hash directories were involved.  To simplify behavior\n    # and avoid mixing host/container/hash paths, we preserve the original\n    # full path strings for any citation that is known to come from the\n    # formatted context.\n    root = (os.environ.get(\"CTX_WORKSPACE_DIR\") or \"\").strip()\n\n    def _to_display_path(full_path: str) -> str:\n        # Identity mapping: leave allowed paths as-is so the LLM sees the same\n        # absolute/host paths that appeared in the Context refs.\n        return full_path\n\n    def _repl(m):\n        p = m.group(0)\n        if p in allowed_set:\n            return _to_display_path(p)\n        b = basename(p)\n        paths = basename_to_paths.get(b) if b else None\n        if paths:\n            if len(paths) == 1:\n                return _to_display_path(next(iter(paths)))\n            return p\n        return \"the referenced file\"\n\n    cleaned = re.sub(r\"/path/to/[^\\s]+\", \"the referenced file\", text)\n    # Simple path-like matcher: segments with a slash and a dot-ext\n    cleaned = re.sub(r\"(?<!\\w)([./\\w-]+/[./\\w-]+\\.[A-Za-z0-9_-]+|[A-Za-z0-9_.-]+\\.[A-Za-z0-9_-]+)\", _repl, cleaned)\n    return cleaned",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__to_display_path_556": {
      "name": "_to_display_path",
      "type": "function",
      "start_line": 556,
      "end_line": 559,
      "content_hash": "20a5fb6c866be6d947f2742d770c559e481d0030",
      "content": "    def _to_display_path(full_path: str) -> str:\n        # Identity mapping: leave allowed paths as-is so the LLM sees the same\n        # absolute/host paths that appeared in the Context refs.\n        return full_path",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__repl_561": {
      "name": "_repl",
      "type": "function",
      "start_line": 561,
      "end_line": 571,
      "content_hash": "b35f0aef8fa51c1ff2917e66ee982c71d51338a1",
      "content": "    def _repl(m):\n        p = m.group(0)\n        if p in allowed_set:\n            return _to_display_path(p)\n        b = basename(p)\n        paths = basename_to_paths.get(b) if b else None\n        if paths:\n            if len(paths) == 1:\n                return _to_display_path(next(iter(paths)))\n            return p\n        return \"the referenced file\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__load_user_preferences_580": {
      "name": "_load_user_preferences",
      "type": "function",
      "start_line": 580,
      "end_line": 598,
      "content_hash": "4a95b7f8023c7ababff54219c7600c3a7119a49c",
      "content": "def _load_user_preferences() -> dict:\n    \"\"\"Load user preferences from ~/.ctx_config.json if it exists.\n\n    Example config:\n    {\n        \"always_include_tests\": true,\n        \"prefer_bullet_commands\": true,\n        \"extra_instructions\": \"Always include error handling considerations\",\n        \"default_mode\": \"unicorn\",\n        \"streaming\": true\n    }\n    \"\"\"\n    if not os.path.exists(CTX_CONFIG_FILE):\n        return {}\n    try:\n        with open(CTX_CONFIG_FILE, 'r') as f:\n            return json.load(f)\n    except Exception:\n        return {}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__apply_user_preferences_601": {
      "name": "_apply_user_preferences",
      "type": "function",
      "start_line": 601,
      "end_line": 623,
      "content_hash": "a5c5ce43bfd1bc79981232e1315d23ea63007be3",
      "content": "def _apply_user_preferences(system_msg: str, user_msg: str, prefs: dict) -> tuple[str, str]:\n    \"\"\"Apply user preferences to system and user messages.\n\n    Allows personalization like:\n    - Always include test-plan paragraph\n    - Prefer bullet commands\n    - Custom instructions\n    \"\"\"\n    if not prefs:\n        return system_msg, user_msg\n\n    # Add extra instructions to system message\n    if prefs.get(\"extra_instructions\"):\n        system_msg += f\"\\n\\nUser preference: {prefs['extra_instructions']}\"\n\n    # Modify user message based on preferences\n    if prefs.get(\"always_include_tests\"):\n        user_msg += \"\\n\\nAlways include a paragraph about testing considerations and test cases.\"\n\n    if prefs.get(\"prefer_bullet_commands\"):\n        user_msg += \"\\n\\nFor commands/instructions, prefer bullet-point format for clarity.\"\n\n    return system_msg, user_msg",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__adaptive_context_sizing_626": {
      "name": "_adaptive_context_sizing",
      "type": "function",
      "start_line": 626,
      "end_line": 656,
      "content_hash": "4f50df46c79a2e0a0a26eb4a3223f775ee2ea08c",
      "content": "def _adaptive_context_sizing(query: str, filters: dict) -> dict:\n    \"\"\"Adaptively adjust limit and context_lines based on query characteristics.\n\n    - Short/vague queries \u2192 increase limit and context for richer grounding\n    - Queries with file/function names \u2192 lighter settings for speed\n    \"\"\"\n    import re\n    adjusted = dict(filters)\n\n    # Detect if query mentions specific files or functions\n    has_file_ref = bool(re.search(r'\\b\\w+\\.(py|js|ts|go|rs|java|cpp|c|h)\\b', query))\n    has_function_ref = bool(re.search(r'\\b(function|class|def|func|fn|method)\\s+\\w+', query))\n    is_specific = has_file_ref or has_function_ref\n\n    # Query length heuristic\n    word_count = len(query.split())\n    is_short = word_count < 5\n\n    # Adaptive sizing\n    if is_short and not is_specific:\n        # Short, vague query \u2192 need more context\n        adjusted[\"limit\"] = max(adjusted.get(\"limit\", DEFAULT_LIMIT), 6)\n        if adjusted.get(\"with_snippets\"):\n            adjusted[\"context_lines\"] = max(adjusted.get(\"context_lines\", DEFAULT_CONTEXT_LINES), 10)\n    elif is_specific:\n        # Specific query \u2192 can use lighter settings\n        adjusted[\"limit\"] = min(adjusted.get(\"limit\", DEFAULT_LIMIT), 4)\n        if adjusted.get(\"with_snippets\"):\n            adjusted[\"context_lines\"] = min(adjusted.get(\"context_lines\", DEFAULT_CONTEXT_LINES) or 8, 6)\n\n    return adjusted",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_enhance_prompt_659": {
      "name": "enhance_prompt",
      "type": "function",
      "start_line": 659,
      "end_line": 688,
      "content_hash": "fa7583b923e488e0378c36eea6cac8328e8ba73e",
      "content": "def enhance_prompt(query: str, **filters) -> str:\n    \"\"\"Retrieve context, invoke the LLM, and return a final enhanced prompt.\n\n    Uses adaptive context sizing to balance quality and speed.\n    \"\"\"\n    # Apply adaptive sizing\n    filters = _adaptive_context_sizing(query, filters)\n\n    context_text, context_note = fetch_context(query, **filters)\n\n    require_ctx_flag = os.environ.get(\"CTX_REQUIRE_CONTEXT\", \"\").strip().lower()\n    if require_ctx_flag in {\"1\", \"true\", \"yes\", \"on\"}:\n        has_real_context = bool((context_text or \"\").strip()) and not (\n            context_note and (\n                \"failed\" in context_note.lower()\n                or \"no relevant\" in context_note.lower()\n                or \"no data\" in context_note.lower()\n            )\n        )\n        if not has_real_context:\n            return (query or \"\").strip()\n\n    rewrite_opts = filters.get(\"rewrite_options\") or {}\n    rewritten = rewrite_prompt(\n        query,\n        context_text,\n        context_note,\n        max_tokens=rewrite_opts.get(\"max_tokens\"),\n    )\n    return rewritten.strip()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__generate_plan_691": {
      "name": "_generate_plan",
      "type": "function",
      "start_line": 691,
      "end_line": 813,
      "content_hash": "858f0f65f2d71ffdb2d59000e84c909e385fdf07",
      "content": "def _generate_plan(enhanced_prompt: str, context: str, note: str) -> str:\n    \"\"\"Generate a step-by-step execution plan for a command/instruction.\n\n    Uses the LLM to create a concrete action plan based on the enhanced prompt and code context.\n    Returns empty string if plan generation fails or is not applicable.\n    \"\"\"\n    import sys\n\n    # Detect if we have actual code context\n    has_code_context = bool((context or \"\").strip() and not (note and (\"failed\" in note.lower() or \"no relevant\" in note.lower() or \"no data\" in note.lower())))\n\n    if not has_code_context:\n        # No code context - skip plan generation\n        return \"\"\n\n    system_msg = (\n        \"You are a technical planning assistant. Your job is to create a step-by-step execution plan. \"\n        \"Given an enhanced prompt and code context, generate a numbered list of concrete steps to accomplish the task. \"\n        \"Each step should be specific and actionable. \"\n        \"Format: Start with 'EXECUTION PLAN:' followed by numbered steps (1., 2., 3., etc.). \"\n        \"Keep it concise - aim for 3-7 steps maximum. \"\n        \"Only reference files, functions, or code elements that appear in the provided context. \"\n        \"Do NOT invent file paths or function names. \"\n        \"Output format: plain text only, no markdown, no code fences.\"\n    )\n\n    user_msg = (\n        f\"Code context:\\n{context}\\n\\n\"\n        f\"Enhanced prompt:\\n{enhanced_prompt}\\n\\n\"\n        \"Generate a step-by-step execution plan to accomplish this task. \"\n        \"Use only the files and functions mentioned in the code context above. \"\n        \"Format as: EXECUTION PLAN: followed by numbered steps.\"\n    )\n\n    meta_prompt = (\n        \"<|start_of_role|>system<|end_of_role|>\" + system_msg + \"<|end_of_text|>\\n\"\n        \"<|start_of_role|>user<|end_of_role|>\" + user_msg + \"<|end_of_text|>\\n\"\n        \"<|start_of_role|>assistant<|end_of_role|>\"\n    )\n\n    runtime_kind = str(os.environ.get(\"REFRAG_RUNTIME\", \"llamacpp\")).strip().lower()\n\n    # GLM path mirrors rewrite_prompt behavior\n    if runtime_kind == \"glm\":\n        try:\n            from refrag_glm import GLMRefragClient  # type: ignore\n\n            client = GLMRefragClient()\n            response = client.client.chat.completions.create(\n                model=os.environ.get(\"GLM_MODEL\", \"glm-4.6\"),\n                messages=[\n                    {\"role\": \"system\", \"content\": system_msg},\n                    {\"role\": \"user\", \"content\": user_msg},\n                ],\n                max_tokens=200,\n                temperature=0.3,\n                stream=False,\n            )\n            plan = (\n                (response.choices[0].message.content if response and response.choices else \"\")\n                or \"\"\n            ).strip()\n            if not plan:\n                # Fall through to llama.cpp path\n                runtime_kind = \"llamacpp\"\n            else:\n                if \"EXECUTION PLAN\" not in plan.upper():\n                    plan = \"EXECUTION PLAN:\\n\" + plan\n                return plan\n        except Exception as e:\n            sys.stderr.write(f\"[DEBUG] Plan generation (GLM) failed, falling back to llama.cpp: {type(e).__name__}: {e}\\n\")\n            sys.stderr.flush()\n            runtime_kind = \"llamacpp\"\n\n    decoder_url = DECODER_URL\n    # Safety: restrict to local decoder hosts\n    parsed = urlparse(decoder_url)\n    if parsed.hostname not in {\"localhost\", \"127.0.0.1\", \"host.docker.internal\"}:\n        return \"\"\n\n    payload = {\n        \"prompt\": meta_prompt,\n        \"n_predict\": 200,  # Shorter for plan generation\n        \"temperature\": 0.3,  # Lower temperature for more focused plans\n        \"stream\": False,  # Silent plan generation\n    }\n\n    try:\n        req = request.Request(\n            decoder_url,\n            data=json.dumps(payload).encode(\"utf-8\"),\n            headers={\"Content-Type\": \"application/json\"},\n        )\n\n        # Use shorter timeout for plan generation (60 seconds instead of 300)\n        plan_timeout = min(60, DECODER_TIMEOUT)\n        with request.urlopen(req, timeout=plan_timeout) as resp:\n            raw = resp.read().decode(\"utf-8\", errors=\"ignore\")\n            data = json.loads(raw)\n\n            plan = (\n                (data.get(\"content\") if isinstance(data, dict) else None)\n                or ((data.get(\"choices\") or [{}])[0].get(\"content\") if isinstance(data, dict) else None)\n                or ((data.get(\"choices\") or [{}])[0].get(\"text\") if isinstance(data, dict) else None)\n                or (data.get(\"generated_text\") if isinstance(data, dict) else None)\n                or (data.get(\"text\") if isinstance(data, dict) else None)\n                or \"\"\n            )\n\n            plan = plan.strip()\n\n            # Relaxed validation: return any non-empty plan; add header if missing\n            if not plan:\n                return \"\"\n            if \"EXECUTION PLAN\" not in plan.upper():\n                plan = \"EXECUTION PLAN:\\n\" + plan\n            return plan\n\n    except Exception as e:\n        # Plan generation failed - not critical, just skip it\n        sys.stderr.write(f\"[DEBUG] Plan generation failed: {type(e).__name__}: {e}\\n\")\n        sys.stderr.flush()\n        return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__needs_polish_816": {
      "name": "_needs_polish",
      "type": "function",
      "start_line": 816,
      "end_line": 859,
      "content_hash": "fa4ca32238b6cb31dbba3af82002ceb7f3a8368e",
      "content": "def _needs_polish(text: str) -> bool:\n    \"\"\"Enhanced QA heuristic to decide if a third polishing pass is needed.\n\n    Checks for:\n    - Too short output\n    - Generic/vague language\n    - Missing concrete details\n    - Lack of code-specific references\n    \"\"\"\n    if not text:\n        return True\n    t = text.strip()\n\n    # Length check\n    if len(t) < 180:\n        return True\n\n    # Generic language cues (expanded list)\n    generic_cues = (\n        \"overall structure\", \"consider \", \"ensure \", \"improve its\",\n        \"you should\", \"it is important\", \"make sure\", \"be sure to\",\n        \"in general\", \"typically\", \"usually\", \"often\"\n    )\n    generic_count = sum(1 for cue in generic_cues if cue in t.lower())\n    if generic_count >= 3:\n        return True\n\n    # Check for concrete details (file paths, line numbers, function names, etc.)\n    import re\n    has_file_ref = bool(re.search(r'\\b\\w+\\.(py|js|ts|go|rs|java|cpp|c|h)\\b', t))\n    has_line_ref = bool(re.search(r'\\bline[s]?\\s+\\d+', t, re.IGNORECASE))\n    has_function_ref = bool(re.search(r'\\b(function|class|method|def|fn)\\s+\\w+', t))\n    has_concrete = has_file_ref or has_line_ref or has_function_ref\n\n    # If no concrete references and has generic language, needs polish\n    if not has_concrete and generic_count >= 2:\n        return True\n\n    # Check paragraph structure (should have at least 2 paragraphs)\n    paragraphs = [p.strip() for p in t.split('\\n\\n') if p.strip()]\n    if len(paragraphs) < 2:\n        return True\n\n    return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__dedup_paragraphs_862": {
      "name": "_dedup_paragraphs",
      "type": "function",
      "start_line": 862,
      "end_line": 891,
      "content_hash": "c0a3ad44f7dc66385d115d655f310347af6aefce",
      "content": "def _dedup_paragraphs(text: str, max_paragraphs: int = 3) -> str:\n    \"\"\"Deterministic paragraph-level deduplication and truncation.\n\n    - Split on double-newline boundaries\n    - Drop duplicate paragraphs beyond the first occurrence (case/whitespace insensitive)\n    - Cap total paragraphs to max_paragraphs\n    \"\"\"\n    if not text:\n        return \"\"\n\n    # Normalize newlines and split into paragraphs\n    t = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").strip()\n    raw_paras = [p.strip() for p in t.split(\"\\n\\n\") if p.strip()]\n    if not raw_paras:\n        return text.strip()\n\n    seen_keys: set[str] = set()\n    out: list[str] = []\n    for p in raw_paras:\n        key = re.sub(r\"\\s+\", \" \", p).strip().lower()\n        if key in seen_keys:\n            continue\n        seen_keys.add(key)\n        out.append(p)\n        if len(out) >= max_paragraphs:\n            break\n\n    if not out:\n        return text.strip()\n    return \"\\n\\n\".join(out)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_enhance_unicorn_894": {
      "name": "enhance_unicorn",
      "type": "function",
      "start_line": 894,
      "end_line": 1017,
      "content_hash": "1cc9fd3feb3a0b02e2c4ad9a3b04eaa52779854e",
      "content": "def enhance_unicorn(query: str, **filters) -> str:\n    \"\"\"Multi-pass staged enhancement for higher quality with optional plan generation.\n\n    Pass 1: rich snippets to draft sharper intent\n    Pass 2: refined retrieval using the draft, with even richer snippets to ground specifics\n    Pass 3: polish if output looks short/generic\n    Pass 4 (optional): generate execution plan if query is a command/instruction\n\n    Falls back to single-pass enhance_prompt if no context is available.\n    Stops immediately when repo search returns no hits to avoid hallucinated references.\n    \"\"\"\n    # ---- Pass 1: draft (rich snippets for grounding)\n    f1 = dict(filters)\n    rewrite_opts = f1.get(\"rewrite_options\") or {}\n    try:\n        max_budget = int(rewrite_opts.get(\"max_tokens\", DEFAULT_REWRITE_TOKENS))\n    except Exception:\n        max_budget = DEFAULT_REWRITE_TOKENS\n    f1.update({\n        \"with_snippets\": True,\n        \"limit\": max(1, min(int(f1.get(\"limit\", DEFAULT_LIMIT) or 3), 3)),\n        \"per_path\": 2,\n        \"context_lines\": 8,  # Rich context for understanding\n    })\n    ctx1, note1 = fetch_context(query, **f1)\n\n    # Early exit: if first pass has no context AND note indicates failure/no results, fall back immediately\n    has_context1 = bool((ctx1 or \"\").strip())\n    has_error1 = note1 and (\"failed\" in note1.lower() or \"no relevant\" in note1.lower() or \"no data\" in note1.lower())\n\n    if not has_context1:\n        # No context at all - fall back to single-pass with the diagnostic note\n        return enhance_prompt(query, **filters)\n\n    # Pass 1: silent (no streaming)\n    draft = rewrite_prompt(\n        query,\n        ctx1,\n        note1,\n        max_tokens=min(180, max_budget),\n        citation_policy=\"snippets\",\n        stream=False,\n    )\n\n    # Build a grounded follow-up query from original query + allowed paths/symbols\n    allowed_paths1, allowed_symbols1 = extract_allowed_citations(ctx1)\n    refined_query = build_refined_query(query, allowed_paths1, allowed_symbols1)\n\n    overlap = _token_overlap_ratio(query, draft)\n    sys.stderr.write(f\"[DEBUG] Unicorn draft similarity={overlap:.3f}\\n\")\n    sys.stderr.flush()\n    gate_flag = os.environ.get(\"CTX_DRAFT_SIM_GATE\", \"\").strip().lower()\n    if gate_flag in {\"1\", \"true\", \"yes\", \"on\"}:\n        try:\n            min_sim = float(os.environ.get(\"CTX_MIN_DRAFT_SIM\", \"0.4\"))\n        except Exception:\n            min_sim = 0.4\n        if overlap < min_sim:\n            sys.stderr.write(f\"[DEBUG] Draft similarity below threshold {min_sim:.3f}; reusing original query for pass2.\\n\")\n            sys.stderr.flush()\n            refined_query = query\n\n    # ---- Pass 2: refine (even richer snippets, focused results)\n    f2 = dict(filters)\n    f2.update({\n        \"with_snippets\": True,\n        \"limit\": 4,\n        \"per_path\": 1,\n        \"context_lines\": 12,  # Very rich context for detailed grounding\n    })\n    ctx2, note2 = fetch_context(refined_query, **f2)\n\n    # Check if second pass has context\n    has_context2 = bool((ctx2 or \"\").strip())\n\n    # If second-pass retrieval is empty, reuse first-pass context to avoid invented refs\n    if not has_context2:\n        ctx2 = ctx1\n        note2 = note1\n\n    # Pass 2: silent (no streaming). Use paths policy for clearer file/line anchoring.\n    final = rewrite_prompt(\n        draft,\n        ctx2,\n        note2,\n        max_tokens=min(300, max_budget),\n        citation_policy=\"paths\",\n        stream=False,\n    )\n\n    # ---- Pass 3: polish if clearly needed (optional via CTX_UNICORN_POLISH)\n    polish_flag = os.environ.get(\"CTX_UNICORN_POLISH\", \"1\").strip().lower()\n    if polish_flag in {\"1\", \"true\", \"yes\", \"on\"} and _needs_polish(final):\n        # Polish pass: silent (no streaming yet)\n        final = rewrite_prompt(final, ctx2, note2, max_tokens=140, citation_policy=\"snippets\", stream=False)\n\n    # ---- Pass 4: Generate execution plan if this is a command/instruction\n    plan = \"\"\n    is_command = not query.strip().endswith(\"?\")\n\n    # Only generate plan if we have actual code context (not just error notes)\n    has_real_context = has_context1 and bool((ctx2 or \"\").strip())\n\n    import sys as _sys\n    _sys.stderr.write(f\"[DEBUG] Plan generation: is_command={is_command}, has_real_context={has_real_context}\\n\")\n    _sys.stderr.flush()\n\n    if is_command and has_real_context:\n        # Generate a step-by-step execution plan based on code context\n        _sys.stderr.write(\"[DEBUG] Generating plan...\\n\")\n        _sys.stderr.flush()\n        plan = _generate_plan(final, ctx2, note2)\n        _sys.stderr.write(f\"[DEBUG] Plan length: {len(plan)} chars\\n\")\n        _sys.stderr.flush()\n\n    # Combine enhanced prompt with plan if available\n    if plan:\n        output = final + \"\\n\\n\" + plan\n    else:\n        output = final\n\n    # Sanitize citations on the final output and return\n    allowed_paths2, _ = extract_allowed_citations(ctx2)\n    return sanitize_citations(output.strip(), allowed_paths1.union(allowed_paths2))",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fetch_context_1020": {
      "name": "fetch_context",
      "type": "function",
      "start_line": 1020,
      "end_line": 1117,
      "content_hash": "26f49342fcbca538727457916be5ce0113555c9b",
      "content": "def fetch_context(query: str, **filters) -> Tuple[str, str]:\n    \"\"\"Fetch repository context text plus a note describing the status.\n\n    Defaults to header-only refs for speed unless with_snippets=True is provided.\n    Falls back to context_search (with memories) if repo_search returns no hits.\n    \"\"\"\n    with_snippets = bool(filters.get(\"with_snippets\", False))\n    # Resolve collection: explicit filter wins, then env COLLECTION_NAME, then default \"codebase\"\n    collection_name = filters.get(\"collection\") or os.environ.get(\"COLLECTION_NAME\", \"codebase\")\n\n    params = {\n        \"query\": query,\n        \"limit\": filters.get(\"limit\", DEFAULT_LIMIT),\n        \"include_snippet\": with_snippets,\n        \"context_lines\": filters.get(\"context_lines\", DEFAULT_CONTEXT_LINES),\n        \"collection\": collection_name,\n    }\n    for key in [\"language\", \"under\", \"path_glob\", \"not_glob\", \"kind\", \"symbol\", \"ext\"]:\n        if filters.get(key):\n            params[key] = filters[key]\n\n    result = call_mcp_tool(\"repo_search\", params)\n    if \"error\" in result:\n        error_msg = result.get('error', 'Unknown error')\n        sys.stderr.write(f\"[DEBUG] repo_search error: {error_msg}\\n\")\n        sys.stderr.flush()\n        return \"\", f\"Context retrieval failed: {error_msg}\"\n\n    # Surface tool-level errors (including auth failures) explicitly instead of\n    # silently treating them as \"no context\".\n    tool_err = _extract_tool_error(result)\n    if tool_err:\n        sys.stderr.write(f\"[DEBUG] repo_search tool error: {tool_err}\\n\")\n        sys.stderr.flush()\n        return \"\", f\"Context retrieval failed: {tool_err}\"\n\n    data = parse_mcp_response(result)\n    if not data:\n        sys.stderr.write(\"[DEBUG] repo_search returned no data\\n\")\n        sys.stderr.flush()\n        return \"\", \"Context retrieval returned no data.\"\n\n    hits = data.get(\"results\") or []\n    relevance = _estimate_query_result_relevance(query, hits)\n    sys.stderr.write(f\"[DEBUG] repo_search returned {len(hits)} hits (relevance={relevance:.3f})\\n\")\n    sys.stderr.flush()\n\n    # Optional path-level debug: sample raw paths coming back from MCP\n    debug_paths_flag = os.environ.get(\"CTX_DEBUG_PATHS\", \"\").strip().lower()\n    if debug_paths_flag in {\"1\", \"true\", \"yes\", \"on\"} and hits:\n        try:\n            sample = [\n                {\n                    \"path\": h.get(\"path\"),\n                    \"host_path\": h.get(\"host_path\"),\n                    \"container_path\": h.get(\"container_path\"),\n                    \"start_line\": h.get(\"start_line\"),\n                    \"end_line\": h.get(\"end_line\"),\n                    \"symbol\": h.get(\"symbol\"),\n                }\n                for h in hits[:5]\n            ]\n            sys.stderr.write(\"[DEBUG] repo_search sample paths:\\n\" + json.dumps(sample, indent=2) + \"\\n\")\n            sys.stderr.flush()\n        except Exception:\n            pass\n\n    gate_flag = os.environ.get(\"CTX_RELEVANCE_GATE\", \"\").strip().lower()\n    if hits and gate_flag in {\"1\", \"true\", \"yes\", \"on\"}:\n        try:\n            min_rel = float(os.environ.get(\"CTX_MIN_RELEVANCE\", \"0.15\"))\n        except Exception:\n            min_rel = 0.15\n        if relevance < min_rel:\n            sys.stderr.write(f\"[DEBUG] Relevance below threshold {min_rel:.3f}; treating as no relevant context.\\n\")\n            sys.stderr.flush()\n            return \"\", \"No relevant context found for the prompt (low retrieval relevance).\"\n\n    if not hits:\n        # Memory blending: try context_search with memories as fallback\n        memory_params = {\n            \"query\": query,\n            \"limit\": filters.get(\"limit\", DEFAULT_LIMIT),\n            \"include_memories\": True,\n            \"include_snippet\": with_snippets,\n            \"context_lines\": filters.get(\"context_lines\", DEFAULT_CONTEXT_LINES),\n            \"collection\": collection_name,\n        }\n        memory_result = call_mcp_tool(\"context_search\", memory_params)\n        if \"error\" not in memory_result:\n            memory_data = parse_mcp_response(memory_result)\n            if memory_data:\n                memory_hits = memory_data.get(\"results\") or []\n                if memory_hits:\n                    return format_search_results(memory_hits, include_snippets=with_snippets), \"Using memories and design docs\"\n        return \"\", \"No relevant context found for the prompt.\"\n\n    return format_search_results(hits, include_snippets=with_snippets), \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_rewrite_prompt_1120": {
      "name": "rewrite_prompt",
      "type": "function",
      "start_line": 1120,
      "end_line": 1443,
      "content_hash": "7843a5cbce2934f9941514a40ae7ae4b2240ede2",
      "content": "def rewrite_prompt(original_prompt: str, context: str, note: str, max_tokens: Optional[int], citation_policy: str = \"paths\", stream: bool = True) -> str:\n    \"\"\"Use the configured decoder (GLM or llama.cpp) to rewrite the prompt with repository context.\n\n    Returns ONLY the improved prompt text. Raises exception if decoder fails.\n    If stream=True (default), prints tokens as they arrive for instant feedback.\n    \"\"\"\n    import sys\n    ctx = (context or \"\").strip()\n    nt = (note or \"\").strip()\n    effective_context = ctx if ctx else (nt or \"No context available.\")\n\n    # Granite 4.0 chat template with explicit rewrite-only instruction\n    if (citation_policy or \"paths\") == \"snippets\":\n        policy_system = (\n            \"Use code snippets provided in Context refs to ground the rewrite. \"\n            \"Do NOT include file paths or line numbers. \"\n            \"You may quote very short code fragments directly from the snippets if essential, but never use markdown or code fences. \"\n            \"Never invent identifiers not present in the snippets. \"\n        )\n        policy_user = (\n            \"When relevant, reference concrete behaviors and small code fragments from the snippets above. \"\n            \"Do not mention file paths or line numbers. \"\n        )\n    else:\n        policy_system = (\n            \"If context is provided, use it to make the prompt more concrete by citing specific file paths, line ranges, and symbols that appear in the Context refs. \"\n            \"When you cite a file, use its full path exactly as it appears in the Context refs, including all directories and prefixes (for example, '/home/.../ctx.py'), rather than shortening it to just a filename. \"\n            \"Never invent references - only cite what appears verbatim in the Context refs. \"\n        )\n        policy_user = (\n            \"If the context above contains relevant references, cite concrete file paths, line ranges, and symbols in your rewrite. \"\n            \"When mentioning a file, use the full path exactly as shown in the Context refs (including directories), not a shortened form like 'ctx.py'. \"\n        )\n\n    # Detect if we have actual code context or just a diagnostic note\n    has_code_context = bool((ctx or \"\").strip() and not (nt and (\"failed\" in nt.lower() or \"no relevant\" in nt.lower() or \"no data\" in nt.lower())))\n\n    system_msg = (\n        \"You are a prompt rewriter. Your ONLY job is to rewrite prompts to be more specific and detailed. \"\n        \"CRITICAL: You must NEVER answer questions or execute commands. You must ONLY rewrite the prompt to be better and more specific. \"\n        \"ALWAYS enhance the prompt to be more detailed and actionable. \"\n        + policy_system\n    )\n\n    if has_code_context:\n        # We have real code context - encourage using it\n        system_msg += (\n            \"Use the provided context to make the prompt more concrete and specific. \"\n            \"Your rewrite must be at least two short paragraphs separated by a single blank line. \"\n            \"For questions: rewrite as more specific questions. For commands/instructions: rewrite as more detailed, specific instructions with concrete targets. \"\n            \"Each paragraph should explore different aspects of the topic. \"\n            \"Output format: plain text only, no markdown, no code fences, no answers, no explanations.\"\n        )\n    else:\n        # No code context - stay generic and don't invent details\n        system_msg += (\n            \"IMPORTANT: No code context is available for this query. \"\n            \"Do NOT invent file paths, line numbers, function names, or other specific code references. \"\n            \"Instead, rewrite the prompt to be more general and exploratory, asking about concepts, approaches, and best practices. \"\n            \"Your rewrite must be at least two short paragraphs separated by a single blank line. \"\n            \"For questions: expand into multiple related questions about the topic. For commands/instructions: expand into general guidance about the task. \"\n            \"Stay generic - do not hallucinate specific files, functions, or code locations. \"\n            \"Output format: plain text only, no markdown, no code fences, no answers, no explanations.\"\n        )\n\n    label = \"with snippets\" if \"\\n    \" in effective_context else \"headers only\"\n    user_msg = (\n        f\"Context refs ({label}):\\n{effective_context}\\n\\n\"\n        f\"Original prompt: {(original_prompt or '').strip()}\\n\\n\"\n        \"Rewrite this as a more specific, detailed prompt using at least two short paragraphs separated by a blank line. \"\n        + policy_user\n    )\n\n    if has_code_context:\n        user_msg += (\n            \"Use the context above to make the rewrite concrete and specific. \"\n            \"For questions: make them more specific and multi-faceted (each paragraph should be a question ending with '?'). \"\n            \"For commands/instructions: make them more detailed and concrete (specify exact functions, parameters, edge cases to handle). \"\n        )\n    else:\n        user_msg += (\n            \"Since no code context is available, keep the rewrite general and exploratory. \"\n            \"Do NOT invent specific file paths, line numbers, or function names. \"\n            \"For questions: expand into related conceptual questions. For commands/instructions: provide general guidance about the task. \"\n        )\n\n    user_msg += (\n        \"Remember: ONLY rewrite the prompt - do NOT answer questions or execute commands. \"\n        \"Avoid generic phrasing. No markdown or code fences.\"\n    )\n\n    # Apply user preferences if config exists\n    prefs = _load_user_preferences()\n    system_msg, user_msg = _apply_user_preferences(system_msg, user_msg, prefs)\n\n    # Override stream setting from preferences if specified\n    if prefs.get(\"streaming\") is not None:\n        stream = prefs.get(\"streaming\")\n\n    # Check which decoder runtime to use\n    runtime_kind = str(os.environ.get(\"REFRAG_RUNTIME\", \"llamacpp\")).strip().lower()\n\n    if runtime_kind == \"glm\":\n        from refrag_glm import GLMRefragClient  # type: ignore\n        client = GLMRefragClient()\n\n        # GLM uses OpenAI-style chat completions, convert context to user prompt format\n        # Note: For GLM, we need to convert the meta_prompt format to simple user message\n        user_msg = (\n            f\"Context refs:\\n{effective_context}\\n\\n\"\n            f\"Original prompt: {(original_prompt or '').strip()}\\n\\n\"\n            \"Rewrite this as a more specific, detailed prompt using at least two short paragraphs separated by a blank line. \"\n        )\n\n        if has_code_context:\n            user_msg += (\n                \"Use the context above to make the rewrite concrete and specific. \"\n                \"For questions: make them more specific and multi-faceted (each paragraph should be a question ending with '?'). \"\n                \"For commands/instructions: make them more detailed and concrete (specify exact functions, parameters, edge cases to handle). \"\n            )\n        else:\n            user_msg += (\n                \"Since no code context is available, keep the rewrite general and exploratory. \"\n                \"Do NOT invent specific file paths, line numbers, or function names. \"\n                \"For questions: expand into related conceptual questions. For commands/instructions: provide general guidance about the task. \"\n            )\n\n        # GLM API call\n        response = client.client.chat.completions.create(\n            model=os.environ.get(\"GLM_MODEL\", \"glm-4.6\"),\n            messages=[\n                {\"role\": \"system\", \"content\": system_msg},\n                {\"role\": \"user\", \"content\": user_msg}\n            ],\n            max_tokens=int(max_tokens or DEFAULT_REWRITE_TOKENS),\n            temperature=0.45,\n            stream=stream\n        )\n\n        enhanced = \"\"\n        if stream:\n            # Streaming mode for GLM\n            for chunk in response:\n                if chunk.choices[0].delta.content:\n                    token = chunk.choices[0].delta.content\n                    sys.stdout.write(token)\n                    sys.stdout.flush()\n                    enhanced += token\n            sys.stdout.write(\"\\n\")\n            sys.stdout.flush()\n        else:\n            # Non-streaming mode for GLM\n            enhanced = response.choices[0].message.content\n\n    else:\n        # Use local decoder (llama.cpp by default; Ollama supported when DECODER_URL points to /api/chat)\n        meta_prompt = (\n            \"<|start_of_role|>system<|end_of_role|>\" + system_msg + \"<|end_of_text|>\\n\"\n            \"<|start_of_role|>user<|end_of_role|>\" + user_msg + \"<|end_of_text|>\\n\"\n            \"<|start_of_role|>assistant<|end_of_role|>\"\n        )\n\n        decoder_url = DECODER_URL\n        # Safety: only allow local decoder hosts\n        parsed = urlparse(decoder_url)\n        if parsed.hostname not in {\"localhost\", \"127.0.0.1\", \"host.docker.internal\"}:\n            raise ValueError(f\"Unsafe decoder host: {parsed.hostname}\")\n\n        lowered_url = decoder_url.lower()\n        is_ollama = (\n            \"ollama\" in lowered_url\n            or \"/api/chat\" in lowered_url\n            or \"/api/generate\" in lowered_url\n            or \"/v1/chat/completions\" in lowered_url\n        )\n\n        enhanced = \"\"\n        try:\n            if is_ollama:\n                model = (\n                    os.environ.get(\"DECODER_MODEL\", \"\").strip()\n                    or os.environ.get(\"OLLAMA_MODEL\", \"\").strip()\n                    or \"llama3\"\n                )\n                payload = {\n                    \"model\": model,\n                    \"stream\": stream,\n                    \"options\": {\"temperature\": 0.45},\n                }\n                if max_tokens:\n                    payload[\"options\"][\"num_predict\"] = int(max_tokens)\n                if \"/api/chat\" in lowered_url or \"/v1/chat/completions\" in lowered_url:\n                    payload[\"messages\"] = [\n                        {\"role\": \"system\", \"content\": system_msg},\n                        {\"role\": \"user\", \"content\": user_msg},\n                    ]\n                else:\n                    payload[\"prompt\"] = f\"{system_msg}\\n\\n{user_msg}\"\n\n                req = request.Request(\n                    decoder_url,\n                    data=json.dumps(payload).encode(\"utf-8\"),\n                    headers={\"Content-Type\": \"application/json\"},\n                )\n\n                if stream:\n                    with request.urlopen(req, timeout=DECODER_TIMEOUT) as resp:\n                        for line in resp:\n                            line_str = line.decode(\"utf-8\", errors=\"ignore\").strip()\n                            if not line_str or line_str.startswith(\":\"):\n                                continue\n                            if line_str.startswith(\"data: \"):\n                                line_str = line_str[6:]\n                            try:\n                                chunk = json.loads(line_str)\n                            except json.JSONDecodeError:\n                                continue\n                            token = \"\"\n                            if isinstance(chunk, dict):\n                                token = (\n                                    (chunk.get(\"message\") or {}).get(\"content\", \"\")\n                                    or chunk.get(\"response\", \"\")\n                                )\n                            if token:\n                                sys.stdout.write(token)\n                                sys.stdout.flush()\n                                enhanced += token\n                            if chunk.get(\"done\") or chunk.get(\"stop\"):\n                                break\n                    sys.stdout.write(\"\\n\")\n                    sys.stdout.flush()\n                else:\n                    with request.urlopen(req, timeout=DECODER_TIMEOUT) as resp:\n                        raw = resp.read().decode(\"utf-8\", errors=\"ignore\")\n                        data = json.loads(raw or \"{}\")\n                        if isinstance(data, dict):\n                            enhanced = (\n                                (data.get(\"message\") or {}).get(\"content\")\n                                or data.get(\"response\")\n                                or ((data.get(\"choices\") or [{}])[0].get(\"message\") or {}).get(\"content\")\n                            )\n                        else:\n                            enhanced = None\n            else:\n                payload = {\n                    \"prompt\": meta_prompt,\n                    \"n_predict\": int(max_tokens or DEFAULT_REWRITE_TOKENS),\n                    \"temperature\": 0.45,\n                    \"stream\": stream,\n                }\n\n                req = request.Request(\n                    decoder_url,\n                    data=json.dumps(payload).encode(\"utf-8\"),\n                    headers={\"Content-Type\": \"application/json\"},\n                )\n\n                if stream:\n                    # Streaming mode: print tokens as they arrive for instant feedback\n                    with request.urlopen(req, timeout=DECODER_TIMEOUT) as resp:\n                        for line in resp:\n                            line_str = line.decode(\"utf-8\", errors=\"ignore\").strip()\n                            if not line_str or line_str.startswith(\":\"):\n                                continue\n                            if line_str.startswith(\"data: \"):\n                                line_str = line_str[6:]\n                            try:\n                                chunk = json.loads(line_str)\n                                token = chunk.get(\"content\", \"\")\n                                if token:\n                                    sys.stdout.write(token)\n                                    sys.stdout.flush()\n                                    enhanced += token\n                                if chunk.get(\"stop\", False):\n                                    break\n                            except json.JSONDecodeError as e:\n                                # Warn once per malformed line but keep streaming the final output only\n                                sys.stderr.write(f\"[WARN] decoder stream JSON decode failed: {str(e)}\\n\")\n                                sys.stderr.flush()\n                                continue\n                    sys.stdout.write(\"\\n\")\n                    sys.stdout.flush()\n                else:\n                    # Non-streaming mode: wait for full response\n                    with request.urlopen(req, timeout=DECODER_TIMEOUT) as resp:\n                        raw = resp.read().decode(\"utf-8\", errors=\"ignore\")\n                        data = json.loads(raw)\n\n                        # Extract content from llama.cpp response\n                        enhanced = (\n                            (data.get(\"content\") if isinstance(data, dict) else None)\n                            or ((data.get(\"choices\") or [{}])[0].get(\"content\") if isinstance(data, dict) else None)\n                            or ((data.get(\"choices\") or [{}])[0].get(\"text\") if isinstance(data, dict) else None)\n                            or (data.get(\"generated_text\") if isinstance(data, dict) else None)\n                            or (data.get(\"text\") if isinstance(data, dict) else None)\n                        )\n        except Exception as e:\n            body_detail = \"\"\n            if isinstance(e, HTTPError):\n                try:\n                    body_detail = e.read().decode(\"utf-8\", errors=\"ignore\").strip()\n                except Exception:\n                    body_detail = \"\"\n            msg = f\"[ERROR] Decoder call to {decoder_url} failed: {type(e).__name__}: {e}\"\n            if body_detail:\n                msg += f\" | body: {body_detail}\"\n            sys.stderr.write(msg + \"\\n\")\n            sys.stderr.flush()\n            raise\n\n    # Normalize and strip formatting / template artifacts from decoder output\n    enhanced = (enhanced or \"\")\n    enhanced = enhanced.replace(\"```\", \"\").replace(\"`\", \"\")\n    # Remove stray chat-template tags like <|user|>, <|assistant|>, etc.\n    enhanced = re.sub(r\"<\\|[^|>]+?\\|>\", \"\", enhanced)\n    enhanced = enhanced.strip()\n\n    if not enhanced:\n        raise ValueError(\"Decoder returned empty response\")\n\n    # Enforce at least two question paragraphs, then deduplicate and cap paragraphs\n    enhanced = _ensure_two_paragraph_questions(enhanced)\n    enhanced = _dedup_paragraphs(enhanced, max_paragraphs=3)\n    return enhanced",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_build_final_output_1449": {
      "name": "build_final_output",
      "type": "function",
      "start_line": 1449,
      "end_line": 1466,
      "content_hash": "dea8a36cb2be812cddb1becb9c1b298dab21aad6",
      "content": "def build_final_output(\n    rewritten_prompt: str, context: str, note: str, include_context: bool\n) -> str:\n    \"\"\"Combine LLM rewrite with optional supporting context for downstream tools.\"\"\"\n    improved = rewritten_prompt.strip() or \"No rewrite generated.\"\n    if not include_context:\n        return improved\n\n    context_block = context.strip() if context.strip() else (note or \"No supporting context.\")\n\n    return f\"\"\"# Improved Prompt\n{improved}\n\n---\n\n# Supporting Context\n{context_block}\n\"\"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_main_1469": {
      "name": "main",
      "type": "function",
      "start_line": 1469,
      "end_line": 1600,
      "content_hash": "ab05fb987ef7a3867fcd0dbdb9f500cd2e6f3cda",
      "content": "def main():\n    parser = argparse.ArgumentParser(\n        description=\"Context-aware prompt enhancer - rewrites questions and commands with codebase context\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # Questions: enhanced with specific details\n  ctx \"how does hybrid search work?\"\n\n  # Commands: enhanced with concrete implementation steps\n  ctx \"refactor ctx.py to improve modularity\"\n\n  # Unicorn mode: staged 2\u20133 pass enhancement for best results\n  ctx --unicorn \"refactor ctx.py\"\n\n  # Detail mode: include code snippets (slower but richer)\n  ctx --detail \"explain the caching logic\"\n\n  # Pipe to LLM or clipboard\n  ctx --cmd llm \"explain the caching logic\"\n  ctx --cmd pbcopy --language python \"fix bug in watcher\"\n        \"\"\"\n    )\n\n    parser.add_argument(\"query\", help=\"Your question or command to enhance\")\n\n    # Command execution\n    parser.add_argument(\"--cmd\", \"-c\", help=\"Command to pipe enhanced prompt to (e.g., llm, pbcopy)\")\n    parser.add_argument(\"--with-context\", action=\"store_true\",\n                        help=\"Append supporting context after the improved prompt\")\n    parser.add_argument(\"--unicorn\", action=\"store_true\",\n                        help=\"One-size 'amazing' mode: staged 2\u20133 calls for best prompts (keeps defaults unchanged)\")\n\n    # Search filters\n    parser.add_argument(\"--language\", \"-l\", help=\"Filter by language (e.g., python, typescript)\")\n    parser.add_argument(\"--under\", \"-u\", help=\"Filter by path prefix (e.g., scripts/)\")\n    parser.add_argument(\"--path-glob\", help=\"Filter by path glob pattern\")\n    parser.add_argument(\"--not-glob\", help=\"Exclude paths matching glob pattern\")\n    parser.add_argument(\"--kind\", help=\"Filter by symbol kind (e.g., function, class)\")\n    parser.add_argument(\"--symbol\", help=\"Filter by symbol name\")\n    parser.add_argument(\"--ext\", help=\"Filter by file extension\")\n    parser.add_argument(\"--collection\", help=\"Override collection name (default: env COLLECTION_NAME)\")\n\n    # Output control\n    parser.add_argument(\"--limit\", type=int, default=DEFAULT_LIMIT,\n                       help=f\"Max results (default: {DEFAULT_LIMIT})\")\n    parser.add_argument(\"--context-lines\", type=int, default=DEFAULT_CONTEXT_LINES,\n                       help=f\"Context lines for snippets (default: {DEFAULT_CONTEXT_LINES})\")\n    parser.add_argument(\"--per-path\", type=int,\n                       help=\"Limit results per file (default: server setting)\")\n    parser.add_argument(\"--rewrite-max-tokens\", type=int, default=DEFAULT_REWRITE_TOKENS,\n                       help=f\"Max tokens for LLM rewrite (default: {DEFAULT_REWRITE_TOKENS})\")\n\n    # Detail mode\n    parser.add_argument(\"--detail\", action=\"store_true\",\n                       help=\"Include short code snippets for richer rewrites (slower but more specific; auto-clamps to limit=4, per_path=1)\")\n\n    args = parser.parse_args()\n\n    # Build filter dict\n    filters = {\n        \"limit\": args.limit,\n        \"context_lines\": args.context_lines,\n        \"language\": args.language,\n        \"under\": args.under,\n        \"path_glob\": args.path_glob,\n        \"not_glob\": args.not_glob,\n        \"kind\": args.kind,\n        \"symbol\": args.symbol,\n        \"ext\": args.ext,\n        \"collection\": args.collection,\n        \"per_path\": args.per_path,\n        \"with_snippets\": args.detail,\n        \"rewrite_options\": {\n            \"max_tokens\": args.rewrite_max_tokens,\n        },\n    }\n\n    # If detail mode is on and context_lines equals the default (0), bump to 1 for a short snippet\n    if args.detail and args.context_lines == DEFAULT_CONTEXT_LINES:\n        filters[\"context_lines\"] = 1\n    # Clamp result counts in detail mode for latency\n    if args.detail:\n        try:\n            filters[\"limit\"] = max(1, min(int(filters.get(\"limit\", DEFAULT_LIMIT)), 4))\n        except Exception:\n            filters[\"limit\"] = 4\n        filters[\"per_path\"] = 1\n\n    # Remove None values\n    filters = {k: v for k, v in filters.items() if v is not None}\n\n    try:\n        # Enhance prompt\n        if args.unicorn:\n            output = enhance_unicorn(args.query, **filters)\n        else:\n            context_text, context_note = fetch_context(args.query, **filters)\n\n            # Derive allowed paths from the formatted context so we can validate/normalize\n            # any file-like mentions in the final rewrite.\n            allowed_paths, _ = extract_allowed_citations(context_text)\n\n            require_ctx_flag = os.environ.get(\"CTX_REQUIRE_CONTEXT\", \"\").strip().lower()\n            if require_ctx_flag in {\"1\", \"true\", \"yes\", \"on\"}:\n                has_real_context = bool((context_text or \"\").strip()) and not (\n                    context_note and (\n                        \"failed\" in context_note.lower()\n                        or \"no relevant\" in context_note.lower()\n                        or \"no data\" in context_note.lower()\n                    )\n                )\n                if not has_real_context:\n                    output = (args.query or \"\").strip()\n                else:\n                    rewritten = rewrite_prompt(args.query, context_text, context_note, max_tokens=args.rewrite_max_tokens)\n                    output = sanitize_citations(rewritten.strip(), allowed_paths)\n            else:\n                rewritten = rewrite_prompt(args.query, context_text, context_note, max_tokens=args.rewrite_max_tokens)\n                output = sanitize_citations(rewritten.strip(), allowed_paths)\n\n        if args.cmd:\n            subprocess.run(args.cmd, input=output.encode(\"utf-8\"), shell=True, check=False)\n        else:\n            print(output)\n\n    except KeyboardInterrupt:\n        print(\"\\nInterrupted.\", file=sys.stderr)\n        sys.exit(130)\n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        sys.exit(1)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}