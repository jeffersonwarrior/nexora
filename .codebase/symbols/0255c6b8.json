{
  "file_path": "/work/external-deps/helix-db/helix-db/src/helix_engine/tests/concurrency_tests/hnsw_concurrent_tests.rs",
  "file_hash": "21a58daef00295d30ac06dd299ccacd9bd1dc22d",
  "updated_at": "2025-12-26T17:34:20.875601",
  "symbols": {
    "function_setup_concurrent_storage_34": {
      "name": "setup_concurrent_storage",
      "type": "function",
      "start_line": 34,
      "end_line": 47,
      "content_hash": "435dda85577c3fcda6f25b50c2f96c89f1b8bdec",
      "content": "fn setup_concurrent_storage() -> (Arc<HelixGraphStorage>, TempDir) {\n    let temp_dir = tempfile::tempdir().unwrap();\n    let path = temp_dir.path().to_str().unwrap();\n\n    let mut config = Config::default();\n    config.db_max_size_gb = Some(1); // 1GB for concurrent testing\n\n    let version_info = VersionInfo::default();\n\n    let storage = HelixGraphStorage::new(path, config, version_info).unwrap();\n    (Arc::new(storage), temp_dir)\n}\n\n/// Generate a random vector of given dimensionality",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_random_vector_48": {
      "name": "random_vector",
      "type": "function",
      "start_line": 48,
      "end_line": 55,
      "content_hash": "c9e500d1d62c6487e5fa124277bddf3b9438b9bd",
      "content": "fn random_vector(dim: usize) -> Vec<f64> {\n    (0..dim)\n        .map(|_| rand::rng().random_range(0.0..1.0))\n        .collect()\n}\n\n#[test]\n#[serial(lmdb_stress)]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_concurrent_inserts_single_label_56": {
      "name": "test_concurrent_inserts_single_label",
      "type": "function",
      "start_line": 56,
      "end_line": 128,
      "content_hash": "6cdf8abd97ed7e6c08374ac9a4b4204e18c51331",
      "content": "fn test_concurrent_inserts_single_label() {\n    // Tests concurrent inserts from multiple threads to the same label\n    //\n    // RACE CONDITION: Entry point updates are not synchronized.\n    // Multiple threads could race to set the entry point.\n    //\n    // EXPECTED: All inserts should succeed, graph should remain consistent\n\n    let (storage, _temp_dir) = setup_concurrent_storage();\n\n    let num_threads = 4;\n    let vectors_per_thread = 25;\n    let barrier = Arc::new(Barrier::new(num_threads));\n\n    let handles: Vec<_> = (0..num_threads)\n        .map(|_thread_id| {\n            let storage = Arc::clone(&storage);\n            let barrier = Arc::clone(&barrier);\n            thread::spawn(move || {\n                // Wait for all threads to be ready\n                barrier.wait();\n\n                for _i in 0..vectors_per_thread {\n                    // Each insert needs its own write transaction (serialized by LMDB)\n                    let mut wtxn = storage.graph_env.write_txn().unwrap();\n                    let arena = Bump::new();\n                    let vector = random_vector(128);\n                    let data = arena.alloc_slice_copy(&vector);\n\n                    // Insert using G::new_mut\n                    G::new_mut(&storage, &arena, &mut wtxn)\n                        .insert_v::<Filter>(data, \"concurrent_test\", None)\n                        .collect::<Result<Vec<_>, _>>()\n                        .expect(\"Insert should succeed\");\n                    wtxn.commit().expect(\"Commit should succeed\");\n                }\n            })\n        })\n        .collect();\n\n    // Wait for all threads to complete\n    for handle in handles {\n        handle.join().unwrap();\n    }\n\n    // Verify: All vectors should be inserted and graph should be consistent\n    let rtxn = storage.graph_env.read_txn().unwrap();\n    let count = storage.vectors.num_inserted_vectors(&rtxn).unwrap();\n\n    // Note: count includes entry point (+1), so actual vectors inserted = count - 1\n    let expected_inserted = (num_threads * vectors_per_thread) as u64;\n    assert!(\n        count == expected_inserted || count == expected_inserted + 1,\n        \"Expected {} or {} vectors (with entry point), found {}\",\n        expected_inserted,\n        expected_inserted + 1,\n        count\n    );\n\n    // Additional consistency check: Verify we can perform searches (entry point exists implicitly)\n    let arena = Bump::new();\n    let query = arena.alloc_slice_copy(&[0.5; 128]);\n    let search_result = G::new(&storage, &rtxn, &arena)\n        .search_v::<Filter, _>(query, 10, \"concurrent_test\", None)\n        .collect::<Result<Vec<_>, _>>();\n    assert!(\n        search_result.is_ok(),\n        \"Should be able to search after concurrent inserts (entry point exists)\"\n    );\n}\n\n#[test]\n#[serial(lmdb_stress)]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_concurrent_searches_during_inserts_129": {
      "name": "test_concurrent_searches_during_inserts",
      "type": "function",
      "start_line": 129,
      "end_line": 270,
      "content_hash": "1e7e4f97efdcb0e8b93de13fcdc6a05a28634750",
      "content": "fn test_concurrent_searches_during_inserts() {\n    // Tests read-write conflicts: Concurrent searches while inserts happen\n    //\n    // EXPECTED BEHAVIOR:\n    // - Readers get snapshot isolation (MVCC)\n    // - Searches should return consistent results (no torn reads)\n    // - Number of results should increase over time as inserts complete\n\n    let (storage, _temp_dir) = setup_concurrent_storage();\n\n    // Initialize with some initial vectors\n    {\n        let mut txn = storage.graph_env.write_txn().unwrap();\n        let arena = Bump::new();\n        for _ in 0..50 {\n            let vector = random_vector(128);\n            let data = arena.alloc_slice_copy(&vector);\n            G::new_mut(&storage, &arena, &mut txn)\n                .insert_v::<Filter>(data, \"search_test\", None)\n                .collect::<Result<Vec<_>, _>>()\n                .unwrap();\n        }\n        txn.commit().unwrap();\n    }\n\n    let num_readers = 4;\n    let num_writers = 2;\n    let barrier = Arc::new(Barrier::new(num_readers + num_writers));\n    let query = Arc::new([0.5; 128]);\n\n    let mut handles = vec![];\n\n    // Spawn reader threads\n    for reader_id in 0..num_readers {\n        let storage = Arc::clone(&storage);\n        let barrier = Arc::clone(&barrier);\n        let query = Arc::clone(&query);\n\n        handles.push(thread::spawn(move || {\n            barrier.wait();\n\n            let mut total_searches = 0;\n            let mut total_results = 0;\n\n            for _ in 0..50 {\n                let rtxn = storage.graph_env.read_txn().unwrap();\n                let arena = Bump::new();\n                let query_data = arena.alloc_slice_copy(&query[..]);\n\n                match G::new(&storage, &rtxn, &arena)\n                    .search_v::<Filter, _>(query_data, 10, \"search_test\", None)\n                    .collect::<Result<Vec<_>, _>>()\n                {\n                    Ok(results) => {\n                        total_searches += 1;\n                        total_results += results.len();\n\n                        // Validate result consistency\n                        for (i, result) in results.iter().enumerate() {\n                            if let crate::helix_engine::traversal_core::traversal_value::TraversalValue::Vector(v) = result {\n                                assert!(\n                                    v.distance.is_some(),\n                                    \"Result {} should have distance\",\n                                    i\n                                );\n                            }\n                        }\n                    }\n                    Err(e) => {\n                        println!(\"Reader {} search failed: {:?}\", reader_id, e);\n                    }\n                }\n\n                // Small delay to allow writers to make progress\n                thread::sleep(std::time::Duration::from_millis(1));\n            }\n\n            println!(\n                \"Reader {} completed: {} searches, avg {} results\",\n                reader_id,\n                total_searches,\n                total_results / total_searches.max(1)\n            );\n        }));\n    }\n\n    // Spawn writer threads\n    for _writer_id in 0..num_writers {\n        let storage = Arc::clone(&storage);\n        let barrier = Arc::clone(&barrier);\n\n        handles.push(thread::spawn(move || {\n            barrier.wait();\n\n            for _i in 0..25 {\n                let mut wtxn = storage.graph_env.write_txn().unwrap();\n                let arena = Bump::new();\n\n                let vector = random_vector(128);\n                let data = arena.alloc_slice_copy(&vector);\n\n                G::new_mut(&storage, &arena, &mut wtxn)\n                    .insert_v::<Filter>(data, \"search_test\", None)\n                    .collect::<Result<Vec<_>, _>>()\n                    .expect(\"Insert should succeed\");\n                wtxn.commit().expect(\"Commit should succeed\");\n\n                thread::sleep(std::time::Duration::from_millis(2));\n            }\n        }));\n    }\n\n    // Wait for all threads\n    for handle in handles {\n        handle.join().unwrap();\n    }\n\n    // Final verification\n    let rtxn = storage.graph_env.read_txn().unwrap();\n    let final_count = storage.vectors.num_inserted_vectors(&rtxn).unwrap();\n\n    assert!(\n        final_count >= 50,\n        \"Should have at least initial 50 vectors, found {}\",\n        final_count\n    );\n\n    // Verify we can still search successfully\n    let arena = Bump::new();\n    let query_data = arena.alloc_slice_copy(&query[..]);\n    let results = G::new(&storage, &rtxn, &arena)\n        .search_v::<Filter, _>(query_data, 10, \"search_test\", None)\n        .collect::<Result<Vec<_>, _>>()\n        .unwrap();\n    assert!(\n        !results.is_empty(),\n        \"Should find results after concurrent operations\"\n    );\n}\n\n#[test]\n#[serial(lmdb_stress)]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_concurrent_inserts_multiple_labels_271": {
      "name": "test_concurrent_inserts_multiple_labels",
      "type": "function",
      "start_line": 271,
      "end_line": 350,
      "content_hash": "6fca000c566ed2e8820fa6e5c264e0dfd899e02c",
      "content": "fn test_concurrent_inserts_multiple_labels() {\n    // Tests concurrent inserts to different labels (should be independent)\n    //\n    // EXPECTED: No contention between different labels, all inserts succeed\n\n    let (storage, _temp_dir) = setup_concurrent_storage();\n\n    let num_labels = 4;\n    let vectors_per_label = 25;\n    let barrier = Arc::new(Barrier::new(num_labels));\n\n    let handles: Vec<_> = (0..num_labels)\n        .map(|label_id| {\n            let storage = Arc::clone(&storage);\n            let barrier = Arc::clone(&barrier);\n\n            thread::spawn(move || {\n                barrier.wait();\n\n                let label = format!(\"label_{}\", label_id);\n\n                for i in 0..vectors_per_label {\n                    let mut wtxn = storage.graph_env.write_txn().unwrap();\n                    let arena = Bump::new();\n\n                    let vector = random_vector(64);\n                    let data = arena.alloc_slice_copy(&vector);\n                    let label_ref = arena.alloc_str(&label);\n\n                    G::new_mut(&storage, &arena, &mut wtxn)\n                        .insert_v::<Filter>(data, label_ref, None)\n                        .collect::<Result<Vec<_>, _>>()\n                        .unwrap();\n                    wtxn.commit().unwrap();\n\n                    if i % 10 == 0 {\n                        println!(\"Label {} inserted {} vectors\", label, i);\n                    }\n                }\n            })\n        })\n        .collect();\n\n    for handle in handles {\n        handle.join().unwrap();\n    }\n\n    // Verify each label has correct count\n    let rtxn = storage.graph_env.read_txn().unwrap();\n\n    for label_id in 0..num_labels {\n        let label = format!(\"label_{}\", label_id);\n        let arena = Bump::new();\n\n        // Verify we can search for each label (entry point exists implicitly)\n        let query = arena.alloc_slice_copy(&[0.5; 64]);\n        let label_ref = arena.alloc_str(&label);\n        let search_result = G::new(&storage, &rtxn, &arena)\n            .search_v::<Filter, _>(query, 5, label_ref, None)\n            .collect::<Result<Vec<_>, _>>();\n        assert!(\n            search_result.is_ok(),\n            \"Should be able to search label {}\",\n            label\n        );\n    }\n\n    let total_count = storage.vectors.num_inserted_vectors(&rtxn).unwrap();\n    let expected_total = (num_labels * vectors_per_label) as u64;\n    assert!(\n        total_count == expected_total || total_count == expected_total + 1,\n        \"Expected {} or {} vectors (with entry point), found {}\",\n        expected_total,\n        expected_total + 1,\n        total_count\n    );\n}\n\n#[test]\n#[serial(lmdb_stress)]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_entry_point_consistency_351": {
      "name": "test_entry_point_consistency",
      "type": "function",
      "start_line": 351,
      "end_line": 426,
      "content_hash": "aa26d8271737f17a6a73e339843867dda0a575af",
      "content": "fn test_entry_point_consistency() {\n    // Tests entry point consistency under concurrent inserts\n    //\n    // CRITICAL: This tests the identified race condition where entry point\n    // updates have no synchronization. Multiple threads could race to set\n    // the entry point.\n    //\n    // EXPECTED: Entry point should always be a valid vector ID\n\n    let (storage, _temp_dir) = setup_concurrent_storage();\n\n    let num_threads = 8;\n    let vectors_per_thread = 10;\n    let barrier = Arc::new(Barrier::new(num_threads));\n\n    let handles: Vec<_> = (0..num_threads)\n        .map(|_| {\n            let storage = Arc::clone(&storage);\n            let barrier = Arc::clone(&barrier);\n\n            thread::spawn(move || {\n                barrier.wait();\n\n                for _ in 0..vectors_per_thread {\n                    let mut wtxn = storage.graph_env.write_txn().unwrap();\n                    let arena = Bump::new();\n\n                    let vector = random_vector(32);\n                    let data = arena.alloc_slice_copy(&vector);\n\n                    G::new_mut(&storage, &arena, &mut wtxn)\n                        .insert_v::<Filter>(data, \"entry_test\", None)\n                        .collect::<Result<Vec<_>, _>>()\n                        .unwrap();\n                    wtxn.commit().unwrap();\n                }\n            })\n        })\n        .collect();\n\n    for handle in handles {\n        handle.join().unwrap();\n    }\n\n    // Verify entry point is valid by performing a search\n    let rtxn = storage.graph_env.read_txn().unwrap();\n    let arena = Bump::new();\n\n    // If we can successfully search, entry point must be valid\n    let query = arena.alloc_slice_copy(&[0.5; 32]);\n    let search_result = G::new(&storage, &rtxn, &arena)\n        .search_v::<Filter, _>(query, 10, \"entry_test\", None)\n        .collect::<Result<Vec<_>, _>>();\n    assert!(\n        search_result.is_ok(),\n        \"Entry point should exist and be valid\"\n    );\n\n    let results = search_result.unwrap();\n    assert!(\n        !results.is_empty(),\n        \"Should return results if entry point is valid\"\n    );\n\n    // Verify results have valid properties\n    for result in results.iter() {\n        if let crate::helix_engine::traversal_core::traversal_value::TraversalValue::Vector(v) = result {\n            assert!(v.id > 0, \"Result ID should be valid\");\n            assert!(!v.deleted, \"Results should not be deleted\");\n            assert!(!v.data.is_empty(), \"Results should have data\");\n        }\n    }\n}\n\n#[test]\n#[serial(lmdb_stress)]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_graph_connectivity_after_concurrent_inserts_427": {
      "name": "test_graph_connectivity_after_concurrent_inserts",
      "type": "function",
      "start_line": 427,
      "end_line": 500,
      "content_hash": "9c5c275602a29dd212eae11822e9f07105c9ae10",
      "content": "fn test_graph_connectivity_after_concurrent_inserts() {\n    // Tests HNSW graph topology consistency after concurrent operations\n    //\n    // EXPECTED: Graph should remain connected (no orphaned nodes)\n    // All vectors should be reachable from entry point\n\n    let (storage, _temp_dir) = setup_concurrent_storage();\n\n    let num_threads = 4;\n    let vectors_per_thread = 20;\n    let barrier = Arc::new(Barrier::new(num_threads));\n\n    let handles: Vec<_> = (0..num_threads)\n        .map(|_| {\n            let storage = Arc::clone(&storage);\n            let barrier = Arc::clone(&barrier);\n\n            thread::spawn(move || {\n                barrier.wait();\n\n                for _ in 0..vectors_per_thread {\n                    let mut wtxn = storage.graph_env.write_txn().unwrap();\n                    let arena = Bump::new();\n\n                    let vector = random_vector(64);\n                    let data = arena.alloc_slice_copy(&vector);\n\n                    G::new_mut(&storage, &arena, &mut wtxn)\n                        .insert_v::<Filter>(data, \"connectivity_test\", None)\n                        .collect::<Result<Vec<_>, _>>()\n                        .unwrap();\n                    wtxn.commit().unwrap();\n                }\n            })\n        })\n        .collect();\n\n    for handle in handles {\n        handle.join().unwrap();\n    }\n\n    // Verify graph connectivity by performing searches from different query points\n    let rtxn = storage.graph_env.read_txn().unwrap();\n    let arena = Bump::new();\n\n    // Try multiple random queries - all should return results\n    for i in 0..10 {\n        let query = random_vector(64);\n        let query_data = arena.alloc_slice_copy(&query);\n        let results = G::new(&storage, &rtxn, &arena)\n            .search_v::<Filter, _>(query_data, 10, \"connectivity_test\", None)\n            .collect::<Result<Vec<_>, _>>()\n            .unwrap();\n\n        assert!(\n            !results.is_empty(),\n            \"Query {} should return results (graph should be connected)\",\n            i\n        );\n\n        // All results should have valid distances\n        for result in results {\n            if let crate::helix_engine::traversal_core::traversal_value::TraversalValue::Vector(v) = result {\n                assert!(\n                    v.distance.is_some() && v.distance.unwrap() >= 0.0,\n                    \"Result should have valid distance\"\n                );\n            }\n        }\n    }\n}\n\n#[test]\n#[serial(lmdb_stress)]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_transaction_isolation_501": {
      "name": "test_transaction_isolation",
      "type": "function",
      "start_line": 501,
      "end_line": 579,
      "content_hash": "dba5818367e906a446bec5d9c517b5f9eee229d0",
      "content": "fn test_transaction_isolation() {\n    // Tests MVCC snapshot isolation guarantees\n    //\n    // EXPECTED: Readers should see consistent snapshots even while writes occur\n\n    let (storage, _temp_dir) = setup_concurrent_storage();\n\n    // Initialize with known vectors\n    let initial_count = 10;\n    {\n        let mut txn = storage.graph_env.write_txn().unwrap();\n        let arena = Bump::new();\n        for _ in 0..initial_count {\n            let vector = random_vector(32);\n            let data = arena.alloc_slice_copy(&vector);\n            G::new_mut(&storage, &arena, &mut txn)\n                .insert_v::<Filter>(data, \"isolation_test\", None)\n                .collect::<Result<Vec<_>, _>>()\n                .unwrap();\n        }\n        txn.commit().unwrap();\n    }\n\n    // Start a long-lived read transaction\n    let rtxn = storage.graph_env.read_txn().unwrap();\n    let count_before = storage.vectors.num_inserted_vectors(&rtxn).unwrap();\n\n    // Entry point may be included in count (+1)\n    assert!(\n        count_before == initial_count || count_before == initial_count + 1,\n        \"Expected {} or {} (with entry point), got {}\",\n        initial_count,\n        initial_count + 1,\n        count_before\n    );\n\n    // In another thread, insert more vectors\n    let storage_clone = Arc::clone(&storage);\n    let handle = thread::spawn(move || {\n        for _ in 0..20 {\n            let mut wtxn = storage_clone.graph_env.write_txn().unwrap();\n            let arena = Bump::new();\n\n            let vector = random_vector(32);\n            let data = arena.alloc_slice_copy(&vector);\n            G::new_mut(&storage_clone, &arena, &mut wtxn)\n                .insert_v::<Filter>(data, \"isolation_test\", None)\n                .collect::<Result<Vec<_>, _>>()\n                .unwrap();\n            wtxn.commit().unwrap();\n        }\n    });\n\n    handle.join().unwrap();\n\n    // Original read transaction should still see the same count (snapshot isolation)\n    let count_after = storage.vectors.num_inserted_vectors(&rtxn).unwrap();\n    assert_eq!(\n        count_after, count_before,\n        \"Read transaction should see consistent snapshot\"\n    );\n\n    // New read transaction should see new vectors\n    drop(rtxn);\n\n    let rtxn_new = storage.graph_env.read_txn().unwrap();\n    let count_new = storage.vectors.num_inserted_vectors(&rtxn_new).unwrap();\n\n    // Entry point may be included in counts (+1)\n    let expected_new = initial_count + 20;\n    assert!(\n        count_new == expected_new\n            || count_new == expected_new + 1\n            || count_new == initial_count + 20 + 1,\n        \"Expected around {} vectors, got {}\",\n        expected_new,\n        count_new\n    );\n}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}