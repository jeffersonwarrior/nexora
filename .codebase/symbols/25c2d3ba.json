{
  "file_path": "/work/external-deps/Context-Engine/scripts/standalone_upload_client.py",
  "file_hash": "59d50cfd12d485d6ad694c10641ece4466eeea35",
  "updated_at": "2025-12-26T17:34:22.569505",
  "symbols": {
    "function_get_auth_session_34": {
      "name": "get_auth_session",
      "type": "function",
      "start_line": 34,
      "end_line": 35,
      "content_hash": "51be0198f6bb4895754d975d9c6cba5b52721835",
      "content": "    def get_auth_session(upload_endpoint: str) -> str:\n        return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_detect_language_141": {
      "name": "detect_language",
      "type": "function",
      "start_line": 141,
      "end_line": 155,
      "content_hash": "8819899d630d91b5a2d30e97c9c36c0d2aed9203",
      "content": "def detect_language(path: Path) -> str:\n    \"\"\"Detect language from file path, handling extensionless files like Dockerfile.*\"\"\"\n    # Check extension first\n    lang = CODE_EXTS.get(path.suffix.lower())\n    if lang:\n        return lang\n    # Check extensionless files by name (lowercase)\n    fname_lower = path.name.lower()\n    lang = EXTENSIONLESS_FILES.get(fname_lower)\n    if lang:\n        return lang\n    # Check Dockerfile.* pattern\n    if fname_lower.startswith(\"dockerfile\"):\n        return \"dockerfile\"\n    return \"unknown\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_hash_id_158": {
      "name": "hash_id",
      "type": "function",
      "start_line": 158,
      "end_line": 163,
      "content_hash": "c7d03fc422b65ec8a132af61357eda8d31fc3761",
      "content": "def hash_id(text: str, path: str, start: int, end: int) -> str:\n    \"\"\"Generate hash ID for content (from ingest_code.py).\"\"\"\n    h = hashlib.sha1(\n        f\"{path}:{start}-{end}\\n{text}\".encode(\"utf-8\", errors=\"ignore\")\n    ).hexdigest()\n    return h[:16]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_collection_name_165": {
      "name": "get_collection_name",
      "type": "function",
      "start_line": 165,
      "end_line": 174,
      "content_hash": "444768639b804f87264bfde992d9fd93fc281bb3",
      "content": "def get_collection_name(repo_name: Optional[str] = None) -> str:\n    \"\"\"Generate collection name with 8-char hash for local workspaces.\n\n    Simplified version from workspace_state.py.\n    \"\"\"\n    if not repo_name:\n        return \"default-collection\"\n    hash_obj = hashlib.sha256(repo_name.encode())\n    short_hash = hash_obj.hexdigest()[:8]\n    return f\"{repo_name}-{short_hash}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_repo_name_from_path_176": {
      "name": "_extract_repo_name_from_path",
      "type": "function",
      "start_line": 176,
      "end_line": 186,
      "content_hash": "8a5e22779cc3fb1ebc4006eb6950b34fdcc07585",
      "content": "def _extract_repo_name_from_path(workspace_path: str) -> str:\n    \"\"\"Extract repository name from workspace path.\n\n    Simplified version from workspace_state.py.\n    \"\"\"\n    try:\n        path = Path(workspace_path).resolve()\n        # Get the directory name as repo name\n        return path.name\n    except Exception:\n        return \"unknown-repo\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_SimpleHashCache_189": {
      "name": "SimpleHashCache",
      "type": "class",
      "start_line": 189,
      "end_line": 292,
      "content_hash": "20e27222c47c406ad7ee53b6c66948fbc93d79c7",
      "content": "class SimpleHashCache:\n    \"\"\"Simple file-based hash cache for tracking file changes.\"\"\"\n\n    def __init__(self, workspace_path: str, repo_name: str):\n        self.workspace_path = Path(workspace_path).resolve()\n        self.repo_name = repo_name\n        self.cache_dir = self.workspace_path / \".context-engine\"\n        self.cache_file = self.cache_dir / \"file_cache.json\"\n        self.cache_dir.mkdir(exist_ok=True)\n        # In-memory cache to avoid re-reading and re-validating on every access\n        self._cache_loaded = False\n        self._cache: Dict[str, str] = {}\n        self._stale_checked = False\n        self._load_cache()  # Load once on init\n\n    def _load_cache(self) -> Dict[str, str]:\n        \"\"\"Load cache from disk.\"\"\"\n        if self._cache_loaded:\n            return self._cache\n\n        if not self.cache_file.exists():\n            self._cache = {}\n            self._cache_loaded = True\n            return self._cache\n\n        try:\n            with open(self.cache_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                file_hashes = data.get(\"file_hashes\", {})\n                # Run stale check only once per process to avoid O(N^2) scans\n                if not self._stale_checked and self._cache_seems_stale(file_hashes):\n                    self._stale_checked = True\n                    logger.warning(\n                        \"[hash_cache] Detected stale cache with missing paths; resetting %s\",\n                        self.cache_file,\n                    )\n                    self._save_cache({})\n                    self._cache = {}\n                else:\n                    self._stale_checked = True\n                    self._cache = file_hashes if isinstance(file_hashes, dict) else {}\n        except Exception:\n            self._cache = {}\n\n        self._cache_loaded = True\n        return self._cache\n\n    def _save_cache(self, file_hashes: Dict[str, str]):\n        \"\"\"Save cache to disk.\"\"\"\n        # Keep in-memory view in sync\n        self._cache = file_hashes\n        self._cache_loaded = True\n        try:\n            data = {\n                \"file_hashes\": file_hashes,\n                \"updated_at\": datetime.now().isoformat()\n            }\n            with open(self.cache_file, 'w', encoding='utf-8') as f:\n                json.dump(data, f, indent=2)\n        except Exception:\n            pass\n\n    def get_hash(self, file_path: str) -> str:\n        \"\"\"Get cached file hash.\"\"\"\n        file_hashes = self._load_cache()\n        abs_path = str(Path(file_path).resolve())\n        return file_hashes.get(abs_path, \"\")\n\n    def set_hash(self, file_path: str, file_hash: str):\n        \"\"\"Set cached file hash.\"\"\"\n        file_hashes = self._load_cache()\n        abs_path = str(Path(file_path).resolve())\n        file_hashes[abs_path] = file_hash\n        self._cache = file_hashes\n        self._cache_loaded = True\n\n    def all_paths(self) -> List[str]:\n        \"\"\"Return all cached absolute file paths.\"\"\"\n        file_hashes = self._load_cache()\n        return list(file_hashes.keys())\n\n    def remove_hash(self, file_path: str) -> None:\n        \"\"\"Remove a cached file hash if present.\"\"\"\n        file_hashes = self._load_cache()\n        abs_path = str(Path(file_path).resolve())\n        if abs_path in file_hashes:\n            file_hashes.pop(abs_path, None)\n            self._cache = file_hashes\n            self._cache_loaded = True\n\n    def _cache_seems_stale(self, file_hashes: Dict[str, str]) -> bool:\n        \"\"\"Return True if a large portion of cached paths no longer exist on disk.\"\"\"\n        total = len(file_hashes)\n        if total == 0:\n            return False\n        missing = 0\n        for path_str in file_hashes.keys():\n            try:\n                if not Path(path_str).exists():\n                    missing += 1\n            except Exception:\n                missing += 1\n        missing_ratio = missing / total\n        return missing_ratio >= 0.25",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___192": {
      "name": "__init__",
      "type": "method",
      "start_line": 192,
      "end_line": 202,
      "content_hash": "33dfca57080c3c3e1a91c68fc02b6e9b8db11960",
      "content": "    def __init__(self, workspace_path: str, repo_name: str):\n        self.workspace_path = Path(workspace_path).resolve()\n        self.repo_name = repo_name\n        self.cache_dir = self.workspace_path / \".context-engine\"\n        self.cache_file = self.cache_dir / \"file_cache.json\"\n        self.cache_dir.mkdir(exist_ok=True)\n        # In-memory cache to avoid re-reading and re-validating on every access\n        self._cache_loaded = False\n        self._cache: Dict[str, str] = {}\n        self._stale_checked = False\n        self._load_cache()  # Load once on init",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__load_cache_204": {
      "name": "_load_cache",
      "type": "method",
      "start_line": 204,
      "end_line": 234,
      "content_hash": "3df44cfaa994601c227081541c13ad4198c7d12a",
      "content": "    def _load_cache(self) -> Dict[str, str]:\n        \"\"\"Load cache from disk.\"\"\"\n        if self._cache_loaded:\n            return self._cache\n\n        if not self.cache_file.exists():\n            self._cache = {}\n            self._cache_loaded = True\n            return self._cache\n\n        try:\n            with open(self.cache_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                file_hashes = data.get(\"file_hashes\", {})\n                # Run stale check only once per process to avoid O(N^2) scans\n                if not self._stale_checked and self._cache_seems_stale(file_hashes):\n                    self._stale_checked = True\n                    logger.warning(\n                        \"[hash_cache] Detected stale cache with missing paths; resetting %s\",\n                        self.cache_file,\n                    )\n                    self._save_cache({})\n                    self._cache = {}\n                else:\n                    self._stale_checked = True\n                    self._cache = file_hashes if isinstance(file_hashes, dict) else {}\n        except Exception:\n            self._cache = {}\n\n        self._cache_loaded = True\n        return self._cache",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__save_cache_236": {
      "name": "_save_cache",
      "type": "method",
      "start_line": 236,
      "end_line": 249,
      "content_hash": "0183bc3e699bf8f6d69763e19cec9bc097e5b03a",
      "content": "    def _save_cache(self, file_hashes: Dict[str, str]):\n        \"\"\"Save cache to disk.\"\"\"\n        # Keep in-memory view in sync\n        self._cache = file_hashes\n        self._cache_loaded = True\n        try:\n            data = {\n                \"file_hashes\": file_hashes,\n                \"updated_at\": datetime.now().isoformat()\n            }\n            with open(self.cache_file, 'w', encoding='utf-8') as f:\n                json.dump(data, f, indent=2)\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_hash_251": {
      "name": "get_hash",
      "type": "method",
      "start_line": 251,
      "end_line": 255,
      "content_hash": "202facf1768bb59eb841afd2433618d36dff9b82",
      "content": "    def get_hash(self, file_path: str) -> str:\n        \"\"\"Get cached file hash.\"\"\"\n        file_hashes = self._load_cache()\n        abs_path = str(Path(file_path).resolve())\n        return file_hashes.get(abs_path, \"\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_set_hash_257": {
      "name": "set_hash",
      "type": "method",
      "start_line": 257,
      "end_line": 263,
      "content_hash": "ef7d7adfee2b38ca0770944ca41d009877e9aa17",
      "content": "    def set_hash(self, file_path: str, file_hash: str):\n        \"\"\"Set cached file hash.\"\"\"\n        file_hashes = self._load_cache()\n        abs_path = str(Path(file_path).resolve())\n        file_hashes[abs_path] = file_hash\n        self._cache = file_hashes\n        self._cache_loaded = True",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_all_paths_265": {
      "name": "all_paths",
      "type": "method",
      "start_line": 265,
      "end_line": 268,
      "content_hash": "c7f80e86e6d655bd7db3c078c44d58d0a7167bb6",
      "content": "    def all_paths(self) -> List[str]:\n        \"\"\"Return all cached absolute file paths.\"\"\"\n        file_hashes = self._load_cache()\n        return list(file_hashes.keys())",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_remove_hash_270": {
      "name": "remove_hash",
      "type": "method",
      "start_line": 270,
      "end_line": 277,
      "content_hash": "645c01db8da485097a405ff054a95d6a55a7416c",
      "content": "    def remove_hash(self, file_path: str) -> None:\n        \"\"\"Remove a cached file hash if present.\"\"\"\n        file_hashes = self._load_cache()\n        abs_path = str(Path(file_path).resolve())\n        if abs_path in file_hashes:\n            file_hashes.pop(abs_path, None)\n            self._cache = file_hashes\n            self._cache_loaded = True",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__cache_seems_stale_279": {
      "name": "_cache_seems_stale",
      "type": "method",
      "start_line": 279,
      "end_line": 292,
      "content_hash": "89354ba4bcbbced01795f64bfe2090758385b977",
      "content": "    def _cache_seems_stale(self, file_hashes: Dict[str, str]) -> bool:\n        \"\"\"Return True if a large portion of cached paths no longer exist on disk.\"\"\"\n        total = len(file_hashes)\n        if total == 0:\n            return False\n        missing = 0\n        for path_str in file_hashes.keys():\n            try:\n                if not Path(path_str).exists():\n                    missing += 1\n            except Exception:\n                missing += 1\n        missing_ratio = missing / total\n        return missing_ratio >= 0.25",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_cached_file_hash_297": {
      "name": "get_cached_file_hash",
      "type": "function",
      "start_line": 297,
      "end_line": 302,
      "content_hash": "54d22f66e2b876c412147ffd3d0eed1aa4d43f40",
      "content": "def get_cached_file_hash(file_path: str, repo_name: Optional[str] = None) -> str:\n    \"\"\"Get cached file hash for tracking changes.\"\"\"\n    global _hash_cache\n    if _hash_cache:\n        return _hash_cache.get_hash(file_path)\n    return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_set_cached_file_hash_304": {
      "name": "set_cached_file_hash",
      "type": "function",
      "start_line": 304,
      "end_line": 308,
      "content_hash": "bb939a07ed1445c2722e145368fca83ad625af16",
      "content": "def set_cached_file_hash(file_path: str, file_hash: str, repo_name: Optional[str] = None):\n    \"\"\"Set cached file hash for tracking changes.\"\"\"\n    global _hash_cache\n    if _hash_cache:\n        _hash_cache.set_hash(file_path, file_hash)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_all_cached_paths_311": {
      "name": "get_all_cached_paths",
      "type": "function",
      "start_line": 311,
      "end_line": 320,
      "content_hash": "7502e0beb90543e882c9c12722534de8620ccbd6",
      "content": "def get_all_cached_paths(repo_name: Optional[str] = None) -> List[str]:\n    \"\"\"Return all tracked file paths from the local cache.\n\n    The repo_name parameter is accepted for API symmetry with the non-standalone\n    client but is not used here, since this cache is always per-workspace.\n    \"\"\"\n    global _hash_cache\n    if _hash_cache:\n        return _hash_cache.all_paths()\n    return []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_remove_cached_file_323": {
      "name": "remove_cached_file",
      "type": "function",
      "start_line": 323,
      "end_line": 327,
      "content_hash": "003608f5813ab3a6e05ae6686289c99059edb012",
      "content": "def remove_cached_file(file_path: str, repo_name: Optional[str] = None) -> None:\n    \"\"\"Remove a file entry from the local cache if present.\"\"\"\n    global _hash_cache\n    if _hash_cache:\n        _hash_cache.remove_hash(file_path)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__find_git_root_330": {
      "name": "_find_git_root",
      "type": "function",
      "start_line": 330,
      "end_line": 349,
      "content_hash": "66cfd8f7748f85044bc03a22a7b4b1270b47eb0e",
      "content": "def _find_git_root(start: Path) -> Optional[Path]:\n    \"\"\"Best-effort detection of the git repository root for a workspace.\n\n    Walks up from the given path looking for a .git directory. Returns None if\n    no repo is found or git metadata is unavailable.\n    \"\"\"\n    try:\n        cur = start.resolve()\n    except Exception:\n        cur = start\n    try:\n        for p in [cur] + list(cur.parents):\n            try:\n                if (p / \".git\").exists():\n                    return p\n            except Exception:\n                continue\n    except Exception:\n        return None\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__compute_logical_repo_id_352": {
      "name": "_compute_logical_repo_id",
      "type": "function",
      "start_line": 352,
      "end_line": 379,
      "content_hash": "f2d72f56edc79aebaf24b62298dc6c75ca63b0f1",
      "content": "def _compute_logical_repo_id(workspace_path: str) -> str:\n    try:\n        p = Path(workspace_path).resolve()\n    except Exception:\n        p = Path(workspace_path)\n\n    try:\n        r = subprocess.run(\n            [\"git\", \"-C\", str(p), \"rev-parse\", \"--git-common-dir\"],\n            capture_output=True,\n            text=True,\n        )\n        raw = (r.stdout or \"\").strip()\n        if r.returncode == 0 and raw:\n            common = Path(raw)\n            if not common.is_absolute():\n                base = p if p.is_dir() else p.parent\n                common = base / common\n            key = str(common.resolve())\n            prefix = \"git:\"\n        else:\n            raise RuntimeError\n    except Exception:\n        key = str(p)\n        prefix = \"fs:\"\n\n    h = hashlib.sha1(key.encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:16]\n    return f\"{prefix}{h}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__redact_emails_382": {
      "name": "_redact_emails",
      "type": "function",
      "start_line": 382,
      "end_line": 389,
      "content_hash": "1f113388bb1ba7d500a082b24e2b2ec271b8bc3c",
      "content": "def _redact_emails(text: str) -> str:\n    \"\"\"Redact email addresses from commit messages for privacy.\"\"\"\n    try:\n        return re.sub(\n            r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", \"<redacted>\", text or \"\",\n        )\n    except Exception:\n        return text",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__collect_git_history_for_workspace_392": {
      "name": "_collect_git_history_for_workspace",
      "type": "function",
      "start_line": 392,
      "end_line": 615,
      "content_hash": "e4ad3c2943e14cd11bbc1383379db5296536e629",
      "content": "def _collect_git_history_for_workspace(workspace_path: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Best-effort collection of recent git history for a workspace.\n\n    Uses REMOTE_UPLOAD_GIT_MAX_COMMITS (0/empty disables) and\n    REMOTE_UPLOAD_GIT_SINCE (optional) to bound history. Returns a\n    serializable dict suitable for writing as metadata/git_history.json, or\n    None when git metadata is unavailable.\n    \"\"\"\n    # Read configuration from environment\n    try:\n        raw_max = (os.environ.get(\"REMOTE_UPLOAD_GIT_MAX_COMMITS\", \"\") or \"\").strip()\n        max_commits = int(raw_max) if raw_max else 0\n    except Exception:\n        max_commits = 0\n    since = (os.environ.get(\"REMOTE_UPLOAD_GIT_SINCE\", \"\") or \"\").strip()\n    force_full = str(os.environ.get(\"REMOTE_UPLOAD_GIT_FORCE\", \"\") or \"\").strip().lower() in {\n        \"1\",\n        \"true\",\n        \"yes\",\n        \"on\",\n    }\n\n    if max_commits <= 0:\n        return None\n\n    root = _find_git_root(Path(workspace_path))\n    if not root:\n        return None\n\n    # Git history cache: avoid emitting identical manifests when HEAD/settings are unchanged\n    base = Path(os.environ.get(\"WORKSPACE_PATH\") or workspace_path).resolve()\n    git_cache_path = base / \".context-engine\" / \"git_history_cache.json\"\n    current_head = \"\"\n    try:\n        head_proc = subprocess.run(\n            [\"git\", \"rev-parse\", \"HEAD\"],\n            cwd=str(root),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            encoding=\"utf-8\",\n            errors=\"replace\",\n        )\n        if head_proc.returncode == 0 and head_proc.stdout.strip():\n            current_head = head_proc.stdout.strip()\n    except Exception:\n        current_head = \"\"\n\n    cache: Dict[str, Any] = {}\n    if not force_full:\n        try:\n            if git_cache_path.exists():\n                with git_cache_path.open(\"r\", encoding=\"utf-8\") as f:\n                    obj = json.load(f)\n                    if isinstance(obj, dict):\n                        cache = obj\n        except Exception:\n            cache = {}\n\n        if current_head and cache.get(\"last_head\") == current_head and cache.get(\"max_commits\") == max_commits and str(cache.get(\"since\") or \"\") == since:\n            return None\n\n    base_head = \"\"\n    prev_head = \"\"\n    if not force_full:\n        try:\n            prev_head = str(cache.get(\"last_head\") or \"\").strip()\n            if current_head and prev_head and prev_head != current_head:\n                base_head = prev_head\n        except Exception:\n            base_head = \"\"\n\n    snapshot_mode = bool(force_full)\n    if not snapshot_mode and current_head and prev_head and prev_head != current_head:\n        try:\n            anc = subprocess.run(\n                [\"git\", \"merge-base\", \"--is-ancestor\", prev_head, current_head],\n                cwd=str(root),\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                encoding=\"utf-8\",\n                errors=\"replace\",\n            )\n            if anc.returncode != 0:\n                snapshot_mode = True\n                base_head = \"\"\n        except Exception:\n            pass\n\n    # Build git rev-list command (simple HEAD-based history)\n    cmd: List[str] = [\"git\", \"rev-list\", \"--no-merges\"]\n    if since:\n        cmd.append(f\"--since={since}\")\n    if base_head and current_head:\n        cmd.append(f\"{base_head}..{current_head}\")\n    else:\n        cmd.append(\"HEAD\")\n\n    try:\n        proc = subprocess.run(\n            cmd,\n            cwd=str(root),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            encoding=\"utf-8\",\n            errors=\"replace\",\n        )\n        if proc.returncode != 0 or not proc.stdout.strip():\n            return None\n        commits = [l.strip() for l in proc.stdout.splitlines() if l.strip()]\n    except Exception:\n        return None\n\n    if not commits:\n        return None\n    if len(commits) > max_commits:\n        commits = commits[:max_commits]\n\n    records: List[Dict[str, Any]] = []\n    for sha in commits:\n        try:\n            fmt = \"%H%x1f%an%x1f%ae%x1f%ad%x1f%s%x1f%b\"\n            show_proc = subprocess.run(\n                [\"git\", \"show\", \"-s\", f\"--format={fmt}\", sha],\n                cwd=str(root),\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                encoding=\"utf-8\",\n                errors=\"replace\",\n            )\n            if show_proc.returncode != 0 or not show_proc.stdout.strip():\n                continue\n            parts = show_proc.stdout.strip().split(\"\\x1f\")\n            c_sha, an, _ae, ad, subj, body = (parts + [\"\"] * 6)[:6]\n\n            files_proc = subprocess.run(\n                [\"git\", \"diff-tree\", \"--no-commit-id\", \"--name-only\", \"-r\", sha],\n                cwd=str(root),\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                encoding=\"utf-8\",\n                errors=\"replace\",\n            )\n            files: List[str] = []\n            if files_proc.returncode == 0 and files_proc.stdout:\n                files = [f for f in files_proc.stdout.splitlines() if f]\n\n            diff_text = \"\"\n            try:\n                diff_proc = subprocess.run(\n                    [\"git\", \"show\", \"--stat\", \"--patch\", \"--unified=3\", sha],\n                    cwd=str(root),\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                    text=True,\n                    encoding=\"utf-8\",\n                    errors=\"replace\",\n                )\n                if diff_proc.returncode == 0 and diff_proc.stdout:\n                    try:\n                        max_chars = int(os.environ.get(\"COMMIT_SUMMARY_DIFF_CHARS\", \"6000\") or 6000)\n                    except Exception:\n                        max_chars = 6000\n                    diff_text = diff_proc.stdout[:max_chars]\n            except Exception:\n                diff_text = \"\"\n\n            msg = _redact_emails((subj + (\"\\n\" + body if body else \"\")).strip())\n            if len(msg) > 2000:\n                msg = msg[:2000] + \"\\u2026\"\n\n            records.append(\n                {\n                    \"commit_id\": c_sha or sha,\n                    \"author_name\": an,\n                    \"authored_date\": ad,\n                    \"message\": msg,\n                    \"files\": files,\n                    \"diff\": diff_text,\n                }\n            )\n        except Exception:\n            continue\n\n    if not records:\n        return None\n\n    try:\n        repo_name = root.name\n    except Exception:\n        repo_name = \"workspace\"\n\n    manifest = {\n        \"version\": 1,\n        \"repo_name\": repo_name,\n        \"generated_at\": datetime.now().isoformat(),\n        \"head\": current_head,\n        \"prev_head\": prev_head,\n        \"base_head\": base_head,\n        \"mode\": \"snapshot\" if snapshot_mode else \"delta\",\n        \"max_commits\": max_commits,\n        \"since\": since,\n        \"commits\": records,\n    }\n\n    # Update git history cache with the HEAD and settings used for this manifest\n    try:\n        git_cache_path.parent.mkdir(parents=True, exist_ok=True)\n        cache_out = {\n            \"last_head\": current_head or (commits[0] if commits else \"\"),\n            \"max_commits\": max_commits,\n            \"since\": since,\n            \"updated_at\": datetime.now().isoformat(),\n        }\n        with git_cache_path.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(cache_out, f, indent=2)\n    except Exception:\n        pass\n\n    return manifest",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_RemoteUploadClient_618": {
      "name": "RemoteUploadClient",
      "type": "class",
      "start_line": 618,
      "end_line": 1709,
      "content_hash": "ef8dfcd55281bc81f4d193a5d71ef1981c64a855",
      "content": "class RemoteUploadClient:\n    \"\"\"Client for uploading delta bundles to remote server.\"\"\"\n\n    def _translate_to_container_path(self, host_path: str) -> str:\n        \"\"\"Translate host path to container path for API communication.\"\"\"\n        host_root = (os.environ.get(\"HOST_ROOT\", \"\") or \"/home/coder/project/Context-Engine/dev-workspace\").strip()\n        container_root = (os.environ.get(\"CONTAINER_ROOT\", \"/work\") or \"/work\").strip()\n\n        host_path_obj = Path(host_path)\n        if host_root:\n            try:\n                host_root_obj = Path(host_root)\n                relative = host_path_obj.relative_to(host_root_obj)\n                container = PurePosixPath(container_root)\n                if relative.parts:\n                    container = container.joinpath(*relative.parts)\n                return str(container)\n            except ValueError:\n                pass\n            except Exception:\n                pass\n\n        # Fallback: strip drive/anchor and map to /work/<repo-name>\n        try:\n            container = PurePosixPath(container_root)\n            usable_parts = [part for part in host_path_obj.parts if part not in (host_path_obj.anchor, host_path_obj.drive)]\n            if usable_parts:\n                repo_name = usable_parts[-1]\n                return str(container.joinpath(repo_name))\n        except Exception:\n            pass\n\n        return host_path.replace('\\\\', '/').replace(':', '')\n\n    def __init__(self, upload_endpoint: str, workspace_path: str, collection_name: str,\n                 max_retries: int = 3, timeout: int = 30, metadata_path: Optional[str] = None,\n                 logical_repo_id: Optional[str] = None):\n        \"\"\"Initialize remote upload client.\"\"\"\n        self.upload_endpoint = upload_endpoint.rstrip('/')\n        self.workspace_path = workspace_path\n        self.collection_name = collection_name\n        self.max_retries = max_retries\n        self.timeout = timeout\n        self.temp_dir = None\n        self.logical_repo_id = logical_repo_id\n\n        # Set environment variables for cache functions\n        os.environ[\"WORKSPACE_PATH\"] = workspace_path\n\n        # Store repo name and initialize hash cache\n        self.repo_name = _extract_repo_name_from_path(workspace_path)\n        # Fallback to directory name if repo detection fails (for non-git repos)\n        if not self.repo_name:\n            self.repo_name = Path(workspace_path).name\n        global _hash_cache\n        _hash_cache = SimpleHashCache(workspace_path, self.repo_name)\n\n        # In-memory stat cache to avoid rehashing unchanged files on every watch iteration\n        self._stat_cache: Dict[str, Tuple[int, int]] = {}\n\n        # Setup HTTP session with simple retry\n        self.session = requests.Session()\n        retry_strategy = Retry(total=max_retries, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        self.session.mount(\"http://\", adapter)\n        self.session.mount(\"https://\", adapter)\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit with cleanup.\"\"\"\n        self.cleanup()\n\n    def cleanup(self):\n        \"\"\"Clean up temporary directories.\"\"\"\n        if self.temp_dir and os.path.exists(self.temp_dir):\n            try:\n                import shutil\n                shutil.rmtree(self.temp_dir)\n                logger.debug(f\"[remote_upload] Cleaned up temporary directory: {self.temp_dir}\")\n            except Exception as e:\n                logger.warning(f\"[remote_upload] Failed to cleanup temp directory {self.temp_dir}: {e}\")\n            finally:\n                self.temp_dir = None\n\n    def get_mapping_summary(self) -> Dict[str, Any]:\n        \"\"\"Return derived collection mapping details.\"\"\"\n        container_path = self._translate_to_container_path(self.workspace_path)\n        return {\n            \"repo_name\": self.repo_name,\n            \"collection_name\": self.collection_name,\n            \"source_path\": self.workspace_path,\n            \"container_path\": container_path,\n            \"upload_endpoint\": self.upload_endpoint,\n        }\n\n    def log_mapping_summary(self) -> None:\n        \"\"\"Log mapping summary for user visibility.\"\"\"\n        info = self.get_mapping_summary()\n        logger.info(\"[remote_upload] Collection mapping:\")\n        logger.info(f\"  repo_name: {info['repo_name']}\")\n        logger.info(f\"  collection_name: {info['collection_name']}\")\n        logger.info(f\"  source_path: {info['source_path']}\")\n        logger.info(f\"  container_path: {info['container_path']}\")\n\n    def _get_temp_bundle_dir(self) -> Path:\n        \"\"\"Get or create temporary directory for bundle creation.\"\"\"\n        if not self.temp_dir:\n            self.temp_dir = tempfile.mkdtemp(prefix=\"delta_bundle_\")\n        return Path(self.temp_dir)\n\n    # CLI is stateless - sequence tracking is handled by server\n\n    def detect_file_changes(self, changed_paths: List[Path]) -> Dict[str, List]:\n        \"\"\"\n        Detect what type of changes occurred for each file path.\n\n        Args:\n            changed_paths: List of changed file paths\n\n        Returns:\n            Dictionary with change types: created, updated, deleted, moved, unchanged\n        \"\"\"\n        changes = {\n            \"created\": [],\n            \"updated\": [],\n            \"deleted\": [],\n            \"moved\": [],\n            \"unchanged\": []\n        }\n\n        for path in changed_paths:\n            try:\n                abs_path = str(path.resolve())\n            except Exception:\n                # Skip paths that cannot be resolved\n                continue\n\n            cached_hash = get_cached_file_hash(abs_path, self.repo_name)\n\n            if not path.exists():\n                # File was deleted\n                if cached_hash:\n                    changes[\"deleted\"].append(path)\n                # Remove from in-memory stat cache if present\n                try:\n                    if abs_path in self._stat_cache:\n                        self._stat_cache.pop(abs_path, None)\n                except Exception:\n                    pass\n                continue\n\n            # File exists - use stat to avoid unnecessary re-hashing when possible\n            try:\n                stat = path.stat()\n            except Exception:\n                # Skip files we can't stat\n                continue\n\n            prev_mtime_ns = prev_size = None\n            try:\n                prev_mtime_ns, prev_size = self._stat_cache.get(abs_path, (None, None))\n            except Exception:\n                prev_mtime_ns, prev_size = None, None\n\n            # If mtime and size are unchanged and we have a cached hash, treat as unchanged\n            if prev_mtime_ns == getattr(stat, \"st_mtime_ns\", None) and prev_size == stat.st_size and cached_hash:\n                changes[\"unchanged\"].append(path)\n                continue\n\n            # Stat changed or no prior entry \u2013 hash content to classify change\n            try:\n                with open(path, 'rb') as f:\n                    content = f.read()\n                current_hash = hashlib.sha1(content).hexdigest()\n            except Exception:\n                # Skip files that can't be read\n                continue\n\n            if not cached_hash:\n                # New file\n                changes[\"created\"].append(path)\n            elif cached_hash != current_hash:\n                # Modified file\n                changes[\"updated\"].append(path)\n            else:\n                # Unchanged (content same despite stat change)\n                changes[\"unchanged\"].append(path)\n\n            # Update caches\n            try:\n                self._stat_cache[abs_path] = (getattr(stat, \"st_mtime_ns\", int(stat.st_mtime * 1e9)), stat.st_size)\n            except Exception:\n                pass\n            set_cached_file_hash(abs_path, current_hash, self.repo_name)\n\n        # Detect moves by looking for files with same content hash\n        # but different paths (requires additional tracking)\n        changes[\"moved\"] = self._detect_moves(changes[\"created\"], changes[\"deleted\"])\n\n        return changes\n\n    def _detect_moves(self, created_files: List[Path], deleted_files: List[Path]) -> List[Tuple[Path, Path]]:\n        \"\"\"\n        Detect file moves by matching content hashes between created and deleted files.\n\n        Args:\n            created_files: List of newly created files\n            deleted_files: List of deleted files\n\n        Returns:\n            List of (source, destination) path tuples for detected moves\n        \"\"\"\n        moves = []\n        deleted_hashes = {}\n\n        # Build hash map for deleted files\n        for deleted_path in deleted_files:\n            try:\n                # Try to get cached hash first, fallback to file content\n                cached_hash = get_cached_file_hash(str(deleted_path), self.repo_name)\n                if cached_hash:\n                    deleted_hashes[cached_hash] = deleted_path\n                    continue\n\n                # If no cached hash, try to read from file if it still exists\n                if deleted_path.exists():\n                    with open(deleted_path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    deleted_hashes[file_hash] = deleted_path\n            except Exception:\n                continue\n\n        # Match created files with deleted files by hash\n        for created_path in created_files:\n            try:\n                with open(created_path, 'rb') as f:\n                    content = f.read()\n                file_hash = hashlib.sha1(content).hexdigest()\n\n                if file_hash in deleted_hashes:\n                    source_path = deleted_hashes[file_hash]\n                    moves.append((source_path, created_path))\n                    # Remove from consideration\n                    del deleted_hashes[file_hash]\n            except Exception:\n                continue\n\n        return moves\n\n    def create_delta_bundle(\n        self,\n        changes: Dict[str, List],\n        git_history: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"\n        Create a delta bundle from detected changes.\n\n        Args:\n            changes: Dictionary of file changes by type\n\n        Returns:\n            Tuple of (bundle_path, manifest_metadata)\n        \"\"\"\n        bundle_id = str(uuid.uuid4())\n        # CLI is stateless - server handles sequence numbers\n        created_at = datetime.now().isoformat()\n\n        # Create temporary directory for bundle\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Create directory structure\n            files_dir = temp_path / \"files\"\n            metadata_dir = temp_path / \"metadata\"\n            files_dir.mkdir()\n            metadata_dir.mkdir()\n\n            # Create subdirectories\n            (files_dir / \"created\").mkdir()\n            (files_dir / \"updated\").mkdir()\n            (files_dir / \"moved\").mkdir()\n\n            operations = []\n            total_size = 0\n            file_hashes = {}\n\n            # Process created files\n            for path in changes[\"created\"]:\n                rel_path = path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    with open(path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    content_hash = f\"sha1:{file_hash}\"\n\n                    # Write file to bundle\n                    bundle_file_path = files_dir / \"created\" / rel_path\n                    bundle_file_path.parent.mkdir(parents=True, exist_ok=True)\n                    bundle_file_path.write_bytes(content)\n\n                    # Get file info\n                    stat = path.stat()\n                    language = detect_language(path)\n\n                    operation = {\n                        \"operation\": \"created\",\n                        \"path\": rel_path,\n                        \"relative_path\": rel_path,\n                        \"absolute_path\": str(path.resolve()),\n                        \"size_bytes\": stat.st_size,\n                        \"content_hash\": content_hash,\n                        \"file_hash\": f\"sha1:{hash_id(content.decode('utf-8', errors='ignore'), rel_path, 1, len(content.splitlines()))}\",\n                        \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                        \"language\": language\n                    }\n                    operations.append(operation)\n                    file_hashes[rel_path] = f\"sha1:{file_hash}\"\n                    total_size += stat.st_size\n                    set_cached_file_hash(str(path.resolve()), file_hash, self.repo_name)\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing created file {path}: {e}\")\n                    continue\n\n            # Process updated files\n            for path in changes[\"updated\"]:\n                rel_path = path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    with open(path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    content_hash = f\"sha1:{file_hash}\"\n                    previous_hash = get_cached_file_hash(str(path.resolve()), self.repo_name)\n\n                    # Write file to bundle\n                    bundle_file_path = files_dir / \"updated\" / rel_path\n                    bundle_file_path.parent.mkdir(parents=True, exist_ok=True)\n                    bundle_file_path.write_bytes(content)\n\n                    # Get file info\n                    stat = path.stat()\n                    language = detect_language(path)\n\n                    operation = {\n                        \"operation\": \"updated\",\n                        \"path\": rel_path,\n                        \"relative_path\": rel_path,\n                        \"absolute_path\": str(path.resolve()),\n                        \"size_bytes\": stat.st_size,\n                        \"content_hash\": content_hash,\n                        \"previous_hash\": f\"sha1:{previous_hash}\" if previous_hash else None,\n                        \"file_hash\": f\"sha1:{hash_id(content.decode('utf-8', errors='ignore'), rel_path, 1, len(content.splitlines()))}\",\n                        \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                        \"language\": language\n                    }\n                    operations.append(operation)\n                    file_hashes[rel_path] = f\"sha1:{file_hash}\"\n                    total_size += stat.st_size\n                    set_cached_file_hash(str(path.resolve()), file_hash, self.repo_name)\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing updated file {path}: {e}\")\n                    continue\n\n            # Process moved files\n            for source_path, dest_path in changes[\"moved\"]:\n                dest_rel_path = dest_path.relative_to(Path(self.workspace_path)).as_posix()\n                source_rel_path = source_path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    with open(dest_path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    content_hash = f\"sha1:{file_hash}\"\n\n                    # Write file to bundle\n                    bundle_file_path = files_dir / \"moved\" / dest_rel_path\n                    bundle_file_path.parent.mkdir(parents=True, exist_ok=True)\n                    bundle_file_path.write_bytes(content)\n\n                    # Get file info\n                    stat = dest_path.stat()\n                    language = detect_language(dest_path)\n\n                    operation = {\n                        \"operation\": \"moved\",\n                        \"path\": dest_rel_path,\n                        \"relative_path\": dest_rel_path,\n                        \"absolute_path\": str(dest_path.resolve()),\n                        \"source_path\": source_rel_path,\n                        \"source_relative_path\": source_rel_path,\n                        \"source_absolute_path\": str(source_path.resolve()),\n                        \"size_bytes\": stat.st_size,\n                        \"content_hash\": content_hash,\n                        \"file_hash\": f\"sha1:{hash_id(content.decode('utf-8', errors='ignore'), dest_rel_path, 1, len(content.splitlines()))}\",\n                        \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                        \"language\": language\n                    }\n                    operations.append(operation)\n                    file_hashes[dest_rel_path] = f\"sha1:{file_hash}\"\n                    total_size += stat.st_size\n                    set_cached_file_hash(str(dest_path.resolve()), file_hash, self.repo_name)\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing moved file {source_path} -> {dest_path}: {e}\")\n                    continue\n\n            # Process deleted files\n            for path in changes[\"deleted\"]:\n                rel_path = path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    previous_hash = get_cached_file_hash(str(path.resolve()), self.repo_name)\n\n                    operation = {\n                        \"operation\": \"deleted\",\n                        \"path\": rel_path,\n                        \"relative_path\": rel_path,\n                        \"absolute_path\": str(path.resolve()),\n                        \"previous_hash\": f\"sha1:{previous_hash}\" if previous_hash else None,\n                        \"file_hash\": None,\n                        \"modified_time\": datetime.now().isoformat(),\n                        \"language\": detect_language(path)\n                    }\n                    operations.append(operation)\n                    # Once a delete operation has been recorded, drop the cache entry\n                    # so subsequent scans do not keep re-reporting the same deletion.\n                    remove_cached_file(str(path.resolve()), self.repo_name)\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing deleted file {path}: {e}\")\n                    continue\n\n            # Create manifest\n            manifest = {\n                \"version\": \"1.0\",\n                \"bundle_id\": bundle_id,\n                \"workspace_path\": self.workspace_path,\n                \"collection_name\": self.collection_name,\n                \"created_at\": created_at,\n                # CLI is stateless - server handles sequence numbers\n                \"sequence_number\": None,  # Server will assign\n                \"parent_sequence\": None,   # Server will determine\n                \"operations\": {\n                    \"created\": len(changes[\"created\"]),\n                    \"updated\": len(changes[\"updated\"]),\n                    \"deleted\": len(changes[\"deleted\"]),\n                    \"moved\": len(changes[\"moved\"])\n                },\n                \"total_files\": len(operations),\n                \"total_size_bytes\": total_size,\n                \"compression\": \"gzip\",\n                \"encoding\": \"utf-8\"\n            }\n\n            # Write manifest\n            (temp_path / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n\n            # Write operations metadata\n            operations_metadata = {\n                \"operations\": operations\n            }\n            (metadata_dir / \"operations.json\").write_text(json.dumps(operations_metadata, indent=2))\n\n            # Write hashes\n            hashes_metadata = {\n                \"workspace_path\": self.workspace_path,\n                \"updated_at\": created_at,\n                \"file_hashes\": file_hashes\n            }\n            (metadata_dir / \"hashes.json\").write_text(json.dumps(hashes_metadata, indent=2))\n\n            try:\n                if git_history is None:\n                    git_history = _collect_git_history_for_workspace(self.workspace_path)\n                if git_history:\n                    (metadata_dir / \"git_history.json\").write_text(\n                        json.dumps(git_history, indent=2)\n                    )\n            except Exception:\n                pass\n\n            # Create tarball in temporary directory\n            temp_bundle_dir = self._get_temp_bundle_dir()\n            bundle_path = temp_bundle_dir / f\"{bundle_id}.tar.gz\"\n            with tarfile.open(bundle_path, \"w:gz\") as tar:\n                tar.add(temp_path, arcname=f\"{bundle_id}\")\n\n            return str(bundle_path), manifest\n\n    def upload_bundle(self, bundle_path: str, manifest: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Upload delta bundle to remote server with exponential backoff retry.\n\n        Args:\n            bundle_path: Path to the bundle tarball\n            manifest: Bundle manifest metadata\n\n        Returns:\n            Server response dictionary\n        \"\"\"\n        last_error = None\n\n        for attempt in range(self.max_retries + 1):\n            try:\n                # Simple exponential backoff\n                if attempt > 0:\n                    delay = min(2 ** (attempt - 1), 30)  # 1, 2, 4, 8... capped at 30s\n                    logger.info(f\"[remote_upload] Retry attempt {attempt + 1}/{self.max_retries + 1} after {delay}s delay\")\n                    time.sleep(delay)\n\n                # Verify bundle exists\n                if not os.path.exists(bundle_path):\n                    return {\"success\": False, \"error\": {\"code\": \"BUNDLE_NOT_FOUND\", \"message\": f\"Bundle not found: {bundle_path}\"}}\n\n                # Check bundle size (server-side enforcement)\n                bundle_size = os.path.getsize(bundle_path)\n\n                files = {\n                    \"bundle\": open(bundle_path, \"rb\"),\n                }\n                data = {\n                    \"workspace_path\": self._translate_to_container_path(self.workspace_path),\n                    \"collection_name\": self.collection_name,\n                    \"sequence_number\": manifest.get(\"sequence_number\"),\n                    \"force\": False,\n                    \"source_path\": self.workspace_path,\n                    \"logical_repo_id\": _compute_logical_repo_id(self.workspace_path),\n                }\n\n                sess = get_auth_session(self.upload_endpoint)\n                if sess:\n                    data[\"session\"] = sess\n\n                if getattr(self, \"logical_repo_id\", None):\n                    data['logical_repo_id'] = self.logical_repo_id\n\n                logger.info(f\"[remote_upload] Uploading bundle {manifest['bundle_id']} (size: {bundle_size} bytes)\")\n\n                response = self.session.post(\n                    f\"{self.upload_endpoint}/api/v1/delta/upload\",\n                    files=files,\n                    data=data,\n                    timeout=(10, self.timeout)\n                )\n\n                result = None\n                try:\n                    result = response.json()\n                except Exception:\n                    result = None\n\n                if response.status_code == 200 and isinstance(result, dict) and result.get(\"success\", False):\n                    logger.info(f\"[remote_upload] Successfully uploaded bundle {manifest['bundle_id']}\")\n                    seq = result.get(\"sequence_number\")\n                    if seq is not None:\n                        try:\n                            manifest[\"sequence\"] = seq\n                        except Exception:\n                            pass\n                    return result\n\n                # Handle error\n                error_msg = f\"Upload failed with status {response.status_code}\"\n                try:\n                    error_detail = result if isinstance(result, dict) else response.json()\n                    error_detail_msg = error_detail.get('error', {}).get('message', 'Unknown error')\n                    error_msg += f\": {error_detail_msg}\"\n                    error_code = error_detail.get('error', {}).get('code', 'HTTP_ERROR')\n                except Exception:\n                    error_msg += f\": {response.text[:200]}\"\n                    error_code = \"HTTP_ERROR\"\n\n                # Special-case 401 to make auth issues obvious to users\n                if response.status_code == 401:\n                    if error_code in {None, \"HTTP_ERROR\"}:\n                        error_code = \"UNAUTHORIZED\"\n                    # Always append a clear hint for auth failures\n                    error_msg += \" (unauthorized; please log in with `ctxce auth login` and retry)\"\n\n                last_error = {\"success\": False, \"error\": {\"code\": error_code, \"message\": error_msg, \"status_code\": response.status_code}}\n\n                # Don't retry on client errors (except 429)\n                if 400 <= response.status_code < 500 and response.status_code != 429:\n                    return last_error\n\n                logger.warning(f\"[remote_upload] Upload attempt {attempt + 1} failed: {error_msg}\")\n\n            except requests.exceptions.ConnectTimeout as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"TIMEOUT_ERROR\", \"message\": f\"Upload timeout: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Upload timeout on attempt {attempt + 1}: {e}\")\n\n            except requests.exceptions.ReadTimeout as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"TIMEOUT_ERROR\", \"message\": f\"Upload timeout: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Upload read timeout on attempt {attempt + 1}: {e}\")\n                \n                # After read timeout, poll to check if server processed the bundle\n                logger.info(f\"[remote_upload] Read timeout occurred, polling server to check if bundle was processed...\")\n                poll_result = self._poll_after_timeout(manifest)\n                if poll_result.get(\"success\"):\n                    logger.info(f\"[remote_upload] Server confirmed processing of bundle {manifest['bundle_id']} after timeout\")\n                    return poll_result\n                \n                logger.warning(f\"[remote_upload] Server did not process bundle after timeout, proceeding with failure\")\n                break\n\n            except requests.exceptions.Timeout as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"TIMEOUT_ERROR\", \"message\": f\"Upload timeout: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Upload timeout on attempt {attempt + 1}: {e}\")\n                \n                # For generic timeout, also try polling\n                logger.info(f\"[remote_upload] Timeout occurred, polling server to check if bundle was processed...\")\n                poll_result = self._poll_after_timeout(manifest)\n                if poll_result.get(\"success\"):\n                    logger.info(f\"[remote_upload] Server confirmed processing of bundle {manifest['bundle_id']} after timeout\")\n                    return poll_result\n                \n                logger.warning(f\"[remote_upload] Server did not process bundle after timeout, proceeding with failure\")\n                break\n\n            except requests.exceptions.ConnectionError as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"CONNECTION_ERROR\", \"message\": f\"Connection error: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Connection error on attempt {attempt + 1}: {e}\")\n\n            except requests.exceptions.RequestException as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"NETWORK_ERROR\", \"message\": f\"Network error: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Network error on attempt {attempt + 1}: {e}\")\n\n            except Exception as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"UPLOAD_ERROR\", \"message\": f\"Upload error: {str(e)}\"}}\n                logger.error(f\"[remote_upload] Unexpected error on attempt {attempt + 1}: {e}\")\n\n        # All retries exhausted\n        logger.error(f\"[remote_upload] All {self.max_retries + 1} upload attempts failed for bundle {manifest.get('bundle_id', 'unknown')}\")\n        return last_error or {\n            \"success\": False,\n            \"error\": {\n                \"code\": \"MAX_RETRIES_EXCEEDED\",\n                \"message\": f\"Upload failed after {self.max_retries + 1} attempts\"\n            }\n        }\n\n    def _poll_after_timeout(self, manifest: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Poll server status after a timeout to check if bundle was processed.\n        \n        Args:\n            manifest: Bundle manifest containing sequence information\n            \n        Returns:\n            Dictionary indicating success if bundle was processed\n        \"\"\"\n        try:\n            # Get current server status to know the expected sequence\n            status = self.get_server_status()\n            if not status.get(\"success\"):\n                return {\"success\": False, \"error\": status.get(\"error\", {\"code\": \"UNKNOWN\", \"message\": \"Failed to get status\"})}\n\n            current_sequence = status.get(\"last_sequence\", 0)\n            expected_sequence = manifest.get(\"sequence\", current_sequence + 1)\n\n            logger.info(f\"[remote_upload] Current server sequence: {current_sequence}, expected: {expected_sequence}\")\n\n            # If server is already at expected sequence, bundle was processed\n            if current_sequence >= expected_sequence:\n                return {\n                    \"success\": True,\n                    \"message\": f\"Bundle processed (server at sequence {current_sequence})\",\n                    \"sequence\": current_sequence,\n                }\n\n            # Poll window is configurable via REMOTE_UPLOAD_POLL_MAX_SECS (seconds).\n            # Values <= 0 mean \"no timeout\" (poll until success or process exit).\n            try:\n                max_poll_time = int(os.environ.get(\"REMOTE_UPLOAD_POLL_MAX_SECS\", \"300\"))\n            except Exception:\n                max_poll_time = 300\n            poll_interval = 5\n            start_time = time.time()\n\n            while True:\n                elapsed = time.time() - start_time\n                if max_poll_time > 0 and elapsed >= max_poll_time:\n                    logger.warning(\n                        f\"[remote_upload] Polling timed out after {int(elapsed)}s (limit={max_poll_time}s), bundle was not confirmed as processed\"\n                    )\n                    return {\n                        \"success\": False,\n                        \"error\": {\n                            \"code\": \"POLL_TIMEOUT\",\n                            \"message\": f\"Bundle not confirmed processed after polling for {int(elapsed)}s (limit={max_poll_time}s)\",\n                        },\n                    }\n\n                logger.info(\n                    f\"[remote_upload] Polling server status... (elapsed: {int(elapsed)}s, limit={'no-limit' if max_poll_time <= 0 else max_poll_time}s)\"\n                )\n                time.sleep(poll_interval)\n\n                status = self.get_server_status()\n                if status.get(\"success\"):\n                    new_sequence = status.get(\"last_sequence\", 0)\n                    if new_sequence >= expected_sequence:\n                        logger.info(\n                            f\"[remote_upload] Server sequence advanced to {new_sequence}, bundle was processed!\"\n                        )\n                        return {\n                            \"success\": True,\n                            \"message\": f\"Bundle processed after timeout (server at sequence {new_sequence})\",\n                            \"sequence\": new_sequence,\n                        }\n                    logger.debug(\n                        f\"[remote_upload] Server sequence still at {new_sequence}, continuing to poll...\"\n                    )\n                else:\n                    logger.warning(\n                        f\"[remote_upload] Failed to get server status during poll: {status.get('error', {}).get('message', 'Unknown')}\"\n                    )\n\n        except Exception as e:\n            logger.error(f\"[remote_upload] Error during post-timeout polling: {e}\")\n            return {\"success\": False, \"error\": {\"code\": \"POLL_ERROR\", \"message\": f\"Polling error: {str(e)}\"}}\n\n    def get_server_status(self) -> Dict[str, Any]:\n        \"\"\"Get server status with simplified error handling.\"\"\"\n        try:\n            container_workspace_path = self._translate_to_container_path(self.workspace_path)\n            connect_timeout = min(self.timeout, 10)\n            # Allow slower responses (e.g., cold starts/large collections) before bailing\n            read_timeout = max(self.timeout, 30)\n            response = self.session.get(\n                f\"{self.upload_endpoint}/api/v1/delta/status\",\n                params={'workspace_path': container_workspace_path},\n                timeout=(connect_timeout, read_timeout)\n            )\n\n            if response.status_code == 200:\n                return response.json()\n\n            # Handle error response\n            error_msg = f\"Status check failed with HTTP {response.status_code}\"\n            try:\n                error_detail = response.json()\n                error_msg += f\": {error_detail.get('error', {}).get('message', 'Unknown error')}\"\n            except:\n                error_msg += f\": {response.text[:100]}\"\n\n            return {\"success\": False, \"error\": {\"code\": \"STATUS_ERROR\", \"message\": error_msg}}\n\n        except requests.exceptions.Timeout:\n            return {\"success\": False, \"error\": {\"code\": \"STATUS_TIMEOUT\", \"message\": \"Status check timeout\"}}\n        except requests.exceptions.ConnectionError:\n            return {\"success\": False, \"error\": {\"code\": \"CONNECTION_ERROR\", \"message\": f\"Cannot connect to server\"}}\n        except Exception as e:\n            return {\"success\": False, \"error\": {\"code\": \"STATUS_CHECK_ERROR\", \"message\": f\"Status check error: {str(e)}\"}}\n\n    def has_meaningful_changes(self, changes: Dict[str, List]) -> bool:\n        \"\"\"Check if changes warrant a delta upload.\"\"\"\n        total_changes = sum(len(files) for op, files in changes.items() if op != \"unchanged\")\n        return total_changes > 0\n\n    def upload_git_history_only(self, git_history: Dict[str, Any]) -> bool:\n        try:\n            empty_changes = {\n                \"created\": [],\n                \"updated\": [],\n                \"deleted\": [],\n                \"moved\": [],\n                \"unchanged\": [],\n            }\n            bundle_path, manifest = self.create_delta_bundle(\n                empty_changes,\n                git_history=git_history,\n            )\n            response = self.upload_bundle(bundle_path, manifest)\n            if response.get(\"success\", False):\n                try:\n                    if os.path.exists(bundle_path):\n                        os.remove(bundle_path)\n                    self.cleanup()\n                except Exception:\n                    pass\n                return True\n            return False\n        except Exception as e:\n            logger.error(f\"[remote_upload] Error uploading git history metadata: {e}\")\n            return False\n\n    def process_changes_and_upload(self, changes: Dict[str, List]) -> bool:\n        \"\"\"\n        Process pre-computed changes and upload delta bundle.\n        Includes comprehensive error handling and graceful fallback.\n\n        Args:\n            changes: Dictionary of file changes by type\n\n        Returns:\n            True if upload was successful, False otherwise\n        \"\"\"\n        try:\n            logger.info(f\"[remote_upload] Processing pre-computed changes\")\n\n            # Validate input\n            if not changes:\n                logger.info(\"[remote_upload] No changes provided\")\n                return True\n\n            if not self.has_meaningful_changes(changes):\n                logger.info(\"[remote_upload] No meaningful changes detected, skipping upload\")\n                return True\n\n            # Log change summary\n            total_changes = sum(len(files) for op, files in changes.items() if op != \"unchanged\")\n            logger.info(f\"[remote_upload] Detected {total_changes} meaningful changes: \"\n                       f\"{len(changes['created'])} created, {len(changes['updated'])} updated, \"\n                       f\"{len(changes['deleted'])} deleted, {len(changes['moved'])} moved\")\n\n            # Create delta bundle\n            bundle_path = None\n            try:\n                bundle_path, manifest = self.create_delta_bundle(changes)\n                logger.info(f\"[remote_upload] Created delta bundle: {manifest['bundle_id']} \"\n                           f\"(size: {manifest['total_size_bytes']} bytes)\")\n\n                # Validate bundle was created successfully\n                if not bundle_path or not os.path.exists(bundle_path):\n                    raise RuntimeError(f\"Failed to create bundle at {bundle_path}\")\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error creating delta bundle: {e}\")\n                # Clean up any temporary files on failure\n                self.cleanup()\n                return False\n\n            # Upload bundle with retry logic\n            try:\n                response = self.upload_bundle(bundle_path, manifest)\n\n                if response.get(\"success\", False):\n                    processed_ops = response.get('processed_operations', {})\n                    logger.info(f\"[remote_upload] Successfully uploaded bundle {manifest['bundle_id']}\")\n                    logger.info(f\"[remote_upload] Processed operations: {processed_ops}\")\n\n                    # Clean up temporary bundle after successful upload\n                    try:\n                        if os.path.exists(bundle_path):\n                            os.remove(bundle_path)\n                            logger.debug(f\"[remote_upload] Cleaned up temporary bundle: {bundle_path}\")\n                        # Also clean up the entire temp directory if this is the last bundle\n                        self.cleanup()\n                    except Exception as cleanup_error:\n                        logger.warning(f\"[remote_upload] Failed to cleanup bundle {bundle_path}: {cleanup_error}\")\n\n                    return True\n                else:\n                    error_msg = response.get('error', {}).get('message', 'Unknown upload error')\n                    logger.error(f\"[remote_upload] Upload failed: {error_msg}\")\n                    return False\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error uploading bundle: {e}\")\n                return False\n\n        except Exception as e:\n            logger.error(f\"[remote_upload] Unexpected error in process_changes_and_upload: {e}\")\n            return False\n\n    def watch_loop(self, interval: int = 5):\n        \"\"\"Main file watching loop using existing detection and upload methods.\"\"\"\n        logger.info(f\"[watch] Starting file monitoring (interval: {interval}s)\")\n        logger.info(f\"[watch] Monitoring: {self.workspace_path}\")\n        logger.info(f\"[watch] Press Ctrl+C to stop\")\n\n        try:\n            while True:\n                try:\n                    # Use existing change detection over both filesystem and cached registry\n                    fs_files = self.get_all_code_files()\n                    path_map = {}\n                    for p in fs_files:\n                        try:\n                            resolved = p.resolve()\n                        except Exception:\n                            continue\n                        path_map[resolved] = p\n\n                    # Include any paths that are only present in the local cache (deleted files)\n                    for cached_abs in get_all_cached_paths(self.repo_name):\n                        try:\n                            cached_path = Path(cached_abs)\n                            resolved = cached_path.resolve()\n                        except Exception:\n                            continue\n                        if resolved not in path_map:\n                            path_map[resolved] = cached_path\n\n                    all_paths = list(path_map.values())\n                    changes = self.detect_file_changes(all_paths)\n\n                    # Count only meaningful changes (exclude unchanged)\n                    meaningful_changes = len(changes.get(\"created\", [])) + len(changes.get(\"updated\", [])) + len(changes.get(\"deleted\", [])) + len(changes.get(\"moved\", []))\n\n                    if meaningful_changes > 0:\n                        logger.info(f\"[watch] Detected {meaningful_changes} changes: { {k: len(v) for k, v in changes.items() if k != 'unchanged'} }\")\n\n                        success = self.process_changes_and_upload(changes)\n\n                        if success:\n                            logger.info(f\"[watch] Successfully uploaded changes\")\n                        else:\n                            logger.error(f\"[watch] Failed to upload changes\")\n                    else:\n                        git_history = None\n                        try:\n                            git_history = _collect_git_history_for_workspace(self.workspace_path)\n                        except Exception:\n                            git_history = None\n\n                        if git_history:\n                            logger.info(\"[watch] Detected git history update; uploading git history metadata\")\n                            success = self.upload_git_history_only(git_history)\n                            if success:\n                                logger.info(\"[watch] Successfully uploaded git history metadata\")\n                            else:\n                                logger.error(\"[watch] Failed to upload git history metadata\")\n                        else:\n                            logger.debug(f\"[watch] No changes detected\")  # Debug level to avoid spam\n\n                    # Sleep until next check\n                    time.sleep(interval)\n\n                except KeyboardInterrupt:\n                    logger.info(f\"[watch] Received interrupt signal, stopping...\")\n                    break\n                except Exception as e:\n                    logger.error(f\"[watch] Error in watch loop: {e}\")\n                    time.sleep(interval)  # Continue even after errors\n\n        except KeyboardInterrupt:\n            logger.info(f\"[watch] File monitoring stopped by user\")\n\n    def get_all_code_files(self) -> List[Path]:\n        \"\"\"Get all code files in the workspace, excluding heavy/third-party dirs.\"\"\"\n        files: List[Path] = []\n        try:\n            workspace_path = Path(self.workspace_path)\n            if not workspace_path.exists():\n                return files\n\n            # Single walk with early pruning and set-based matching to reduce IO\n            ext_suffixes = {str(ext).lower() for ext in CODE_EXTS if str(ext).startswith('.')}\n            extensionless_names = set(EXTENSIONLESS_FILES.keys())\n            # Always exclude dev-workspace to prevent recursive upload loops\n            # (upload service creates dev-workspace/<collection>/ which would otherwise get re-uploaded)\n            excluded = {\n                \"node_modules\", \"vendor\", \"dist\", \"build\", \"target\", \"out\",\n                \".git\", \".hg\", \".svn\", \".vscode\", \".idea\", \".venv\", \"venv\",\n                \"__pycache__\", \".pytest_cache\", \".mypy_cache\", \".cache\",\n                \".context-engine\", \".context-engine-uploader\", \".codebase\",\n                \"dev-workspace\"\n            }\n\n            seen = set()\n            for root, dirnames, filenames in os.walk(workspace_path):\n                # Prune heavy/hidden directories before descending\n                dirnames[:] = [d for d in dirnames if d not in excluded and not d.startswith('.')]\n\n                for filename in filenames:\n                    # Allow dotfiles that are in EXTENSIONLESS_FILES (e.g., .gitignore)\n                    fname_lower = filename.lower()\n                    if filename.startswith('.') and fname_lower not in extensionless_names:\n                        continue\n                    candidate = Path(root) / filename\n                    suffix = candidate.suffix.lower()\n                    # Match by extension, extensionless name, or Dockerfile.* prefix\n                    if (suffix in ext_suffixes or\n                        fname_lower in extensionless_names or\n                        fname_lower.startswith(\"dockerfile\")):\n                        resolved = candidate.resolve()\n                        if resolved not in seen:\n                            seen.add(resolved)\n                            files.append(candidate)\n        except Exception as e:\n            logger.error(f\"[watch] Error scanning files: {e}\")\n\n        return files\n\n    def process_and_upload_changes(self, changed_paths: List[Path]) -> bool:\n        \"\"\"\n        Process changed paths and upload delta bundle if meaningful changes exist.\n        Includes comprehensive error handling and graceful fallback.\n\n        Args:\n            changed_paths: List of changed file paths\n\n        Returns:\n            True if upload was successful, False otherwise\n        \"\"\"\n        try:\n            logger.info(f\"[remote_upload] Processing {len(changed_paths)} changed paths\")\n\n            # Validate input\n            if not changed_paths:\n                logger.info(\"[remote_upload] No changed paths provided\")\n                return True\n\n            # Detect changes\n            try:\n                changes = self.detect_file_changes(changed_paths)\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error detecting file changes: {e}\")\n                return False\n\n            if not self.has_meaningful_changes(changes):\n                logger.info(\"[remote_upload] No meaningful changes detected, skipping upload\")\n                return True\n\n            # Log change summary\n            total_changes = sum(len(files) for op, files in changes.items() if op != \"unchanged\")\n            logger.info(f\"[remote_upload] Detected {total_changes} meaningful changes: \"\n                       f\"{len(changes['created'])} created, {len(changes['updated'])} updated, \"\n                       f\"{len(changes['deleted'])} deleted, {len(changes['moved'])} moved\")\n\n            # Create delta bundle\n            bundle_path = None\n            try:\n                bundle_path, manifest = self.create_delta_bundle(changes)\n                logger.info(f\"[remote_upload] Created delta bundle: {manifest['bundle_id']} \"\n                           f\"(size: {manifest['total_size_bytes']} bytes)\")\n\n                # Validate bundle was created successfully\n                if not bundle_path or not os.path.exists(bundle_path):\n                    raise RuntimeError(f\"Failed to create bundle at {bundle_path}\")\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error creating delta bundle: {e}\")\n                # Clean up any temporary files on failure\n                self.cleanup()\n                return False\n\n            # Upload bundle with retry logic\n            try:\n                response = self.upload_bundle(bundle_path, manifest)\n\n                if response.get(\"success\", False):\n                    processed_ops = response.get('processed_operations', {})\n                    logger.info(f\"[remote_upload] Successfully uploaded bundle {manifest['bundle_id']}\")\n                    logger.info(f\"[remote_upload] Processed operations: {processed_ops}\")\n\n                    # Clean up temporary bundle after successful upload\n                    try:\n                        if os.path.exists(bundle_path):\n                            os.remove(bundle_path)\n                            logger.debug(f\"[remote_upload] Cleaned up temporary bundle: {bundle_path}\")\n                        # Also clean up the entire temp directory if this is the last bundle\n                        self.cleanup()\n                    except Exception as cleanup_error:\n                        logger.warning(f\"[remote_upload] Failed to cleanup bundle {bundle_path}: {cleanup_error}\")\n\n                    return True\n                else:\n                    error = response.get(\"error\", {})\n                    error_code = error.get(\"code\", \"UNKNOWN\")\n                    error_msg = error.get(\"message\", \"Unknown error\")\n\n                    logger.error(f\"[remote_upload] Upload failed: {error_msg}\")\n\n                    # Handle specific error types\n                    # CLI is stateless - server handles sequence management\n                    if error_code in [\"BUNDLE_TOO_LARGE\", \"BUNDLE_NOT_FOUND\"]:\n                        # These are unrecoverable errors\n                        logger.error(f\"[remote_upload] Unrecoverable error ({error_code}): {error_msg}\")\n                        return False\n                    elif error_code in [\"TIMEOUT_ERROR\", \"CONNECTION_ERROR\", \"NETWORK_ERROR\"]:\n                        # These might be temporary, suggest fallback\n                        logger.warning(f\"[remote_upload] Network-related error ({error_code}): {error_msg}\")\n                        logger.warning(\"[remote_upload] Consider falling back to local mode if this persists\")\n                        return False\n                    else:\n                        # Other errors\n                        logger.error(f\"[remote_upload] Upload error ({error_code}): {error_msg}\")\n                        return False\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Unexpected error during upload: {e}\")\n                return False\n\n        except Exception as e:\n            logger.error(f\"[remote_upload] Critical error in process_and_upload_changes: {e}\")\n            logger.exception(\"[remote_upload] Full traceback:\")\n            return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__translate_to_container_path_621": {
      "name": "_translate_to_container_path",
      "type": "method",
      "start_line": 621,
      "end_line": 650,
      "content_hash": "36bb45d2c746bca9b7e4e08f3a7b43dfa0bcf902",
      "content": "    def _translate_to_container_path(self, host_path: str) -> str:\n        \"\"\"Translate host path to container path for API communication.\"\"\"\n        host_root = (os.environ.get(\"HOST_ROOT\", \"\") or \"/home/coder/project/Context-Engine/dev-workspace\").strip()\n        container_root = (os.environ.get(\"CONTAINER_ROOT\", \"/work\") or \"/work\").strip()\n\n        host_path_obj = Path(host_path)\n        if host_root:\n            try:\n                host_root_obj = Path(host_root)\n                relative = host_path_obj.relative_to(host_root_obj)\n                container = PurePosixPath(container_root)\n                if relative.parts:\n                    container = container.joinpath(*relative.parts)\n                return str(container)\n            except ValueError:\n                pass\n            except Exception:\n                pass\n\n        # Fallback: strip drive/anchor and map to /work/<repo-name>\n        try:\n            container = PurePosixPath(container_root)\n            usable_parts = [part for part in host_path_obj.parts if part not in (host_path_obj.anchor, host_path_obj.drive)]\n            if usable_parts:\n                repo_name = usable_parts[-1]\n                return str(container.joinpath(repo_name))\n        except Exception:\n            pass\n\n        return host_path.replace('\\\\', '/').replace(':', '')",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___652": {
      "name": "__init__",
      "type": "method",
      "start_line": 652,
      "end_line": 683,
      "content_hash": "4e1c3637dab4dc0751ff4e8b210094b64fc30ece",
      "content": "    def __init__(self, upload_endpoint: str, workspace_path: str, collection_name: str,\n                 max_retries: int = 3, timeout: int = 30, metadata_path: Optional[str] = None,\n                 logical_repo_id: Optional[str] = None):\n        \"\"\"Initialize remote upload client.\"\"\"\n        self.upload_endpoint = upload_endpoint.rstrip('/')\n        self.workspace_path = workspace_path\n        self.collection_name = collection_name\n        self.max_retries = max_retries\n        self.timeout = timeout\n        self.temp_dir = None\n        self.logical_repo_id = logical_repo_id\n\n        # Set environment variables for cache functions\n        os.environ[\"WORKSPACE_PATH\"] = workspace_path\n\n        # Store repo name and initialize hash cache\n        self.repo_name = _extract_repo_name_from_path(workspace_path)\n        # Fallback to directory name if repo detection fails (for non-git repos)\n        if not self.repo_name:\n            self.repo_name = Path(workspace_path).name\n        global _hash_cache\n        _hash_cache = SimpleHashCache(workspace_path, self.repo_name)\n\n        # In-memory stat cache to avoid rehashing unchanged files on every watch iteration\n        self._stat_cache: Dict[str, Tuple[int, int]] = {}\n\n        # Setup HTTP session with simple retry\n        self.session = requests.Session()\n        retry_strategy = Retry(total=max_retries, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        self.session.mount(\"http://\", adapter)\n        self.session.mount(\"https://\", adapter)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___enter___685": {
      "name": "__enter__",
      "type": "method",
      "start_line": 685,
      "end_line": 687,
      "content_hash": "96b998e00a1972043cbb74bb1fc33857195294eb",
      "content": "    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___exit___689": {
      "name": "__exit__",
      "type": "method",
      "start_line": 689,
      "end_line": 691,
      "content_hash": "639ce50895d8ab935a06836f17785c850314ed6f",
      "content": "    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit with cleanup.\"\"\"\n        self.cleanup()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_cleanup_693": {
      "name": "cleanup",
      "type": "method",
      "start_line": 693,
      "end_line": 703,
      "content_hash": "b1ed05095aafada5d596ff64fcd44c2985d51af4",
      "content": "    def cleanup(self):\n        \"\"\"Clean up temporary directories.\"\"\"\n        if self.temp_dir and os.path.exists(self.temp_dir):\n            try:\n                import shutil\n                shutil.rmtree(self.temp_dir)\n                logger.debug(f\"[remote_upload] Cleaned up temporary directory: {self.temp_dir}\")\n            except Exception as e:\n                logger.warning(f\"[remote_upload] Failed to cleanup temp directory {self.temp_dir}: {e}\")\n            finally:\n                self.temp_dir = None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_mapping_summary_705": {
      "name": "get_mapping_summary",
      "type": "method",
      "start_line": 705,
      "end_line": 714,
      "content_hash": "f92e3ff31dda930efc3d4740721f28af619bec84",
      "content": "    def get_mapping_summary(self) -> Dict[str, Any]:\n        \"\"\"Return derived collection mapping details.\"\"\"\n        container_path = self._translate_to_container_path(self.workspace_path)\n        return {\n            \"repo_name\": self.repo_name,\n            \"collection_name\": self.collection_name,\n            \"source_path\": self.workspace_path,\n            \"container_path\": container_path,\n            \"upload_endpoint\": self.upload_endpoint,\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_log_mapping_summary_716": {
      "name": "log_mapping_summary",
      "type": "method",
      "start_line": 716,
      "end_line": 723,
      "content_hash": "b9d32cf6a2fdfe8b722ea13116ab633047b95689",
      "content": "    def log_mapping_summary(self) -> None:\n        \"\"\"Log mapping summary for user visibility.\"\"\"\n        info = self.get_mapping_summary()\n        logger.info(\"[remote_upload] Collection mapping:\")\n        logger.info(f\"  repo_name: {info['repo_name']}\")\n        logger.info(f\"  collection_name: {info['collection_name']}\")\n        logger.info(f\"  source_path: {info['source_path']}\")\n        logger.info(f\"  container_path: {info['container_path']}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__get_temp_bundle_dir_725": {
      "name": "_get_temp_bundle_dir",
      "type": "method",
      "start_line": 725,
      "end_line": 729,
      "content_hash": "a68050d291a9f459916f07bd90da8d7d834ca82a",
      "content": "    def _get_temp_bundle_dir(self) -> Path:\n        \"\"\"Get or create temporary directory for bundle creation.\"\"\"\n        if not self.temp_dir:\n            self.temp_dir = tempfile.mkdtemp(prefix=\"delta_bundle_\")\n        return Path(self.temp_dir)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_detect_file_changes_733": {
      "name": "detect_file_changes",
      "type": "method",
      "start_line": 733,
      "end_line": 820,
      "content_hash": "a84125d7acad35d3b5e7aae263248eaa198bcb4e",
      "content": "    def detect_file_changes(self, changed_paths: List[Path]) -> Dict[str, List]:\n        \"\"\"\n        Detect what type of changes occurred for each file path.\n\n        Args:\n            changed_paths: List of changed file paths\n\n        Returns:\n            Dictionary with change types: created, updated, deleted, moved, unchanged\n        \"\"\"\n        changes = {\n            \"created\": [],\n            \"updated\": [],\n            \"deleted\": [],\n            \"moved\": [],\n            \"unchanged\": []\n        }\n\n        for path in changed_paths:\n            try:\n                abs_path = str(path.resolve())\n            except Exception:\n                # Skip paths that cannot be resolved\n                continue\n\n            cached_hash = get_cached_file_hash(abs_path, self.repo_name)\n\n            if not path.exists():\n                # File was deleted\n                if cached_hash:\n                    changes[\"deleted\"].append(path)\n                # Remove from in-memory stat cache if present\n                try:\n                    if abs_path in self._stat_cache:\n                        self._stat_cache.pop(abs_path, None)\n                except Exception:\n                    pass\n                continue\n\n            # File exists - use stat to avoid unnecessary re-hashing when possible\n            try:\n                stat = path.stat()\n            except Exception:\n                # Skip files we can't stat\n                continue\n\n            prev_mtime_ns = prev_size = None\n            try:\n                prev_mtime_ns, prev_size = self._stat_cache.get(abs_path, (None, None))\n            except Exception:\n                prev_mtime_ns, prev_size = None, None\n\n            # If mtime and size are unchanged and we have a cached hash, treat as unchanged\n            if prev_mtime_ns == getattr(stat, \"st_mtime_ns\", None) and prev_size == stat.st_size and cached_hash:\n                changes[\"unchanged\"].append(path)\n                continue\n\n            # Stat changed or no prior entry \u2013 hash content to classify change\n            try:\n                with open(path, 'rb') as f:\n                    content = f.read()\n                current_hash = hashlib.sha1(content).hexdigest()\n            except Exception:\n                # Skip files that can't be read\n                continue\n\n            if not cached_hash:\n                # New file\n                changes[\"created\"].append(path)\n            elif cached_hash != current_hash:\n                # Modified file\n                changes[\"updated\"].append(path)\n            else:\n                # Unchanged (content same despite stat change)\n                changes[\"unchanged\"].append(path)\n\n            # Update caches\n            try:\n                self._stat_cache[abs_path] = (getattr(stat, \"st_mtime_ns\", int(stat.st_mtime * 1e9)), stat.st_size)\n            except Exception:\n                pass\n            set_cached_file_hash(abs_path, current_hash, self.repo_name)\n\n        # Detect moves by looking for files with same content hash\n        # but different paths (requires additional tracking)\n        changes[\"moved\"] = self._detect_moves(changes[\"created\"], changes[\"deleted\"])\n\n        return changes",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__detect_moves_822": {
      "name": "_detect_moves",
      "type": "method",
      "start_line": 822,
      "end_line": 869,
      "content_hash": "8d8677570a24d3960edf8cfac0062bc9d9794be0",
      "content": "    def _detect_moves(self, created_files: List[Path], deleted_files: List[Path]) -> List[Tuple[Path, Path]]:\n        \"\"\"\n        Detect file moves by matching content hashes between created and deleted files.\n\n        Args:\n            created_files: List of newly created files\n            deleted_files: List of deleted files\n\n        Returns:\n            List of (source, destination) path tuples for detected moves\n        \"\"\"\n        moves = []\n        deleted_hashes = {}\n\n        # Build hash map for deleted files\n        for deleted_path in deleted_files:\n            try:\n                # Try to get cached hash first, fallback to file content\n                cached_hash = get_cached_file_hash(str(deleted_path), self.repo_name)\n                if cached_hash:\n                    deleted_hashes[cached_hash] = deleted_path\n                    continue\n\n                # If no cached hash, try to read from file if it still exists\n                if deleted_path.exists():\n                    with open(deleted_path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    deleted_hashes[file_hash] = deleted_path\n            except Exception:\n                continue\n\n        # Match created files with deleted files by hash\n        for created_path in created_files:\n            try:\n                with open(created_path, 'rb') as f:\n                    content = f.read()\n                file_hash = hashlib.sha1(content).hexdigest()\n\n                if file_hash in deleted_hashes:\n                    source_path = deleted_hashes[file_hash]\n                    moves.append((source_path, created_path))\n                    # Remove from consideration\n                    del deleted_hashes[file_hash]\n            except Exception:\n                continue\n\n        return moves",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_create_delta_bundle_871": {
      "name": "create_delta_bundle",
      "type": "method",
      "start_line": 871,
      "end_line": 1108,
      "content_hash": "ec3a7d33e368754499c69339a3059c42000d00eb",
      "content": "    def create_delta_bundle(\n        self,\n        changes: Dict[str, List],\n        git_history: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"\n        Create a delta bundle from detected changes.\n\n        Args:\n            changes: Dictionary of file changes by type\n\n        Returns:\n            Tuple of (bundle_path, manifest_metadata)\n        \"\"\"\n        bundle_id = str(uuid.uuid4())\n        # CLI is stateless - server handles sequence numbers\n        created_at = datetime.now().isoformat()\n\n        # Create temporary directory for bundle\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Create directory structure\n            files_dir = temp_path / \"files\"\n            metadata_dir = temp_path / \"metadata\"\n            files_dir.mkdir()\n            metadata_dir.mkdir()\n\n            # Create subdirectories\n            (files_dir / \"created\").mkdir()\n            (files_dir / \"updated\").mkdir()\n            (files_dir / \"moved\").mkdir()\n\n            operations = []\n            total_size = 0\n            file_hashes = {}\n\n            # Process created files\n            for path in changes[\"created\"]:\n                rel_path = path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    with open(path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    content_hash = f\"sha1:{file_hash}\"\n\n                    # Write file to bundle\n                    bundle_file_path = files_dir / \"created\" / rel_path\n                    bundle_file_path.parent.mkdir(parents=True, exist_ok=True)\n                    bundle_file_path.write_bytes(content)\n\n                    # Get file info\n                    stat = path.stat()\n                    language = detect_language(path)\n\n                    operation = {\n                        \"operation\": \"created\",\n                        \"path\": rel_path,\n                        \"relative_path\": rel_path,\n                        \"absolute_path\": str(path.resolve()),\n                        \"size_bytes\": stat.st_size,\n                        \"content_hash\": content_hash,\n                        \"file_hash\": f\"sha1:{hash_id(content.decode('utf-8', errors='ignore'), rel_path, 1, len(content.splitlines()))}\",\n                        \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                        \"language\": language\n                    }\n                    operations.append(operation)\n                    file_hashes[rel_path] = f\"sha1:{file_hash}\"\n                    total_size += stat.st_size\n                    set_cached_file_hash(str(path.resolve()), file_hash, self.repo_name)\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing created file {path}: {e}\")\n                    continue\n\n            # Process updated files\n            for path in changes[\"updated\"]:\n                rel_path = path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    with open(path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    content_hash = f\"sha1:{file_hash}\"\n                    previous_hash = get_cached_file_hash(str(path.resolve()), self.repo_name)\n\n                    # Write file to bundle\n                    bundle_file_path = files_dir / \"updated\" / rel_path\n                    bundle_file_path.parent.mkdir(parents=True, exist_ok=True)\n                    bundle_file_path.write_bytes(content)\n\n                    # Get file info\n                    stat = path.stat()\n                    language = detect_language(path)\n\n                    operation = {\n                        \"operation\": \"updated\",\n                        \"path\": rel_path,\n                        \"relative_path\": rel_path,\n                        \"absolute_path\": str(path.resolve()),\n                        \"size_bytes\": stat.st_size,\n                        \"content_hash\": content_hash,\n                        \"previous_hash\": f\"sha1:{previous_hash}\" if previous_hash else None,\n                        \"file_hash\": f\"sha1:{hash_id(content.decode('utf-8', errors='ignore'), rel_path, 1, len(content.splitlines()))}\",\n                        \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                        \"language\": language\n                    }\n                    operations.append(operation)\n                    file_hashes[rel_path] = f\"sha1:{file_hash}\"\n                    total_size += stat.st_size\n                    set_cached_file_hash(str(path.resolve()), file_hash, self.repo_name)\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing updated file {path}: {e}\")\n                    continue\n\n            # Process moved files\n            for source_path, dest_path in changes[\"moved\"]:\n                dest_rel_path = dest_path.relative_to(Path(self.workspace_path)).as_posix()\n                source_rel_path = source_path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    with open(dest_path, 'rb') as f:\n                        content = f.read()\n                    file_hash = hashlib.sha1(content).hexdigest()\n                    content_hash = f\"sha1:{file_hash}\"\n\n                    # Write file to bundle\n                    bundle_file_path = files_dir / \"moved\" / dest_rel_path\n                    bundle_file_path.parent.mkdir(parents=True, exist_ok=True)\n                    bundle_file_path.write_bytes(content)\n\n                    # Get file info\n                    stat = dest_path.stat()\n                    language = detect_language(dest_path)\n\n                    operation = {\n                        \"operation\": \"moved\",\n                        \"path\": dest_rel_path,\n                        \"relative_path\": dest_rel_path,\n                        \"absolute_path\": str(dest_path.resolve()),\n                        \"source_path\": source_rel_path,\n                        \"source_relative_path\": source_rel_path,\n                        \"source_absolute_path\": str(source_path.resolve()),\n                        \"size_bytes\": stat.st_size,\n                        \"content_hash\": content_hash,\n                        \"file_hash\": f\"sha1:{hash_id(content.decode('utf-8', errors='ignore'), dest_rel_path, 1, len(content.splitlines()))}\",\n                        \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                        \"language\": language\n                    }\n                    operations.append(operation)\n                    file_hashes[dest_rel_path] = f\"sha1:{file_hash}\"\n                    total_size += stat.st_size\n                    set_cached_file_hash(str(dest_path.resolve()), file_hash, self.repo_name)\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing moved file {source_path} -> {dest_path}: {e}\")\n                    continue\n\n            # Process deleted files\n            for path in changes[\"deleted\"]:\n                rel_path = path.relative_to(Path(self.workspace_path)).as_posix()\n                try:\n                    previous_hash = get_cached_file_hash(str(path.resolve()), self.repo_name)\n\n                    operation = {\n                        \"operation\": \"deleted\",\n                        \"path\": rel_path,\n                        \"relative_path\": rel_path,\n                        \"absolute_path\": str(path.resolve()),\n                        \"previous_hash\": f\"sha1:{previous_hash}\" if previous_hash else None,\n                        \"file_hash\": None,\n                        \"modified_time\": datetime.now().isoformat(),\n                        \"language\": detect_language(path)\n                    }\n                    operations.append(operation)\n                    # Once a delete operation has been recorded, drop the cache entry\n                    # so subsequent scans do not keep re-reporting the same deletion.\n                    remove_cached_file(str(path.resolve()), self.repo_name)\n\n                except Exception as e:\n                    print(f\"[bundle_create] Error processing deleted file {path}: {e}\")\n                    continue\n\n            # Create manifest\n            manifest = {\n                \"version\": \"1.0\",\n                \"bundle_id\": bundle_id,\n                \"workspace_path\": self.workspace_path,\n                \"collection_name\": self.collection_name,\n                \"created_at\": created_at,\n                # CLI is stateless - server handles sequence numbers\n                \"sequence_number\": None,  # Server will assign\n                \"parent_sequence\": None,   # Server will determine\n                \"operations\": {\n                    \"created\": len(changes[\"created\"]),\n                    \"updated\": len(changes[\"updated\"]),\n                    \"deleted\": len(changes[\"deleted\"]),\n                    \"moved\": len(changes[\"moved\"])\n                },\n                \"total_files\": len(operations),\n                \"total_size_bytes\": total_size,\n                \"compression\": \"gzip\",\n                \"encoding\": \"utf-8\"\n            }\n\n            # Write manifest\n            (temp_path / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n\n            # Write operations metadata\n            operations_metadata = {\n                \"operations\": operations\n            }\n            (metadata_dir / \"operations.json\").write_text(json.dumps(operations_metadata, indent=2))\n\n            # Write hashes\n            hashes_metadata = {\n                \"workspace_path\": self.workspace_path,\n                \"updated_at\": created_at,\n                \"file_hashes\": file_hashes\n            }\n            (metadata_dir / \"hashes.json\").write_text(json.dumps(hashes_metadata, indent=2))\n\n            try:\n                if git_history is None:\n                    git_history = _collect_git_history_for_workspace(self.workspace_path)\n                if git_history:\n                    (metadata_dir / \"git_history.json\").write_text(\n                        json.dumps(git_history, indent=2)\n                    )\n            except Exception:\n                pass\n\n            # Create tarball in temporary directory\n            temp_bundle_dir = self._get_temp_bundle_dir()\n            bundle_path = temp_bundle_dir / f\"{bundle_id}.tar.gz\"\n            with tarfile.open(bundle_path, \"w:gz\") as tar:\n                tar.add(temp_path, arcname=f\"{bundle_id}\")\n\n            return str(bundle_path), manifest",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_upload_bundle_1110": {
      "name": "upload_bundle",
      "type": "method",
      "start_line": 1110,
      "end_line": 1259,
      "content_hash": "9dfc82b72544b69e33c7054e27b9a627fd732c80",
      "content": "    def upload_bundle(self, bundle_path: str, manifest: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Upload delta bundle to remote server with exponential backoff retry.\n\n        Args:\n            bundle_path: Path to the bundle tarball\n            manifest: Bundle manifest metadata\n\n        Returns:\n            Server response dictionary\n        \"\"\"\n        last_error = None\n\n        for attempt in range(self.max_retries + 1):\n            try:\n                # Simple exponential backoff\n                if attempt > 0:\n                    delay = min(2 ** (attempt - 1), 30)  # 1, 2, 4, 8... capped at 30s\n                    logger.info(f\"[remote_upload] Retry attempt {attempt + 1}/{self.max_retries + 1} after {delay}s delay\")\n                    time.sleep(delay)\n\n                # Verify bundle exists\n                if not os.path.exists(bundle_path):\n                    return {\"success\": False, \"error\": {\"code\": \"BUNDLE_NOT_FOUND\", \"message\": f\"Bundle not found: {bundle_path}\"}}\n\n                # Check bundle size (server-side enforcement)\n                bundle_size = os.path.getsize(bundle_path)\n\n                files = {\n                    \"bundle\": open(bundle_path, \"rb\"),\n                }\n                data = {\n                    \"workspace_path\": self._translate_to_container_path(self.workspace_path),\n                    \"collection_name\": self.collection_name,\n                    \"sequence_number\": manifest.get(\"sequence_number\"),\n                    \"force\": False,\n                    \"source_path\": self.workspace_path,\n                    \"logical_repo_id\": _compute_logical_repo_id(self.workspace_path),\n                }\n\n                sess = get_auth_session(self.upload_endpoint)\n                if sess:\n                    data[\"session\"] = sess\n\n                if getattr(self, \"logical_repo_id\", None):\n                    data['logical_repo_id'] = self.logical_repo_id\n\n                logger.info(f\"[remote_upload] Uploading bundle {manifest['bundle_id']} (size: {bundle_size} bytes)\")\n\n                response = self.session.post(\n                    f\"{self.upload_endpoint}/api/v1/delta/upload\",\n                    files=files,\n                    data=data,\n                    timeout=(10, self.timeout)\n                )\n\n                result = None\n                try:\n                    result = response.json()\n                except Exception:\n                    result = None\n\n                if response.status_code == 200 and isinstance(result, dict) and result.get(\"success\", False):\n                    logger.info(f\"[remote_upload] Successfully uploaded bundle {manifest['bundle_id']}\")\n                    seq = result.get(\"sequence_number\")\n                    if seq is not None:\n                        try:\n                            manifest[\"sequence\"] = seq\n                        except Exception:\n                            pass\n                    return result\n\n                # Handle error\n                error_msg = f\"Upload failed with status {response.status_code}\"\n                try:\n                    error_detail = result if isinstance(result, dict) else response.json()\n                    error_detail_msg = error_detail.get('error', {}).get('message', 'Unknown error')\n                    error_msg += f\": {error_detail_msg}\"\n                    error_code = error_detail.get('error', {}).get('code', 'HTTP_ERROR')\n                except Exception:\n                    error_msg += f\": {response.text[:200]}\"\n                    error_code = \"HTTP_ERROR\"\n\n                # Special-case 401 to make auth issues obvious to users\n                if response.status_code == 401:\n                    if error_code in {None, \"HTTP_ERROR\"}:\n                        error_code = \"UNAUTHORIZED\"\n                    # Always append a clear hint for auth failures\n                    error_msg += \" (unauthorized; please log in with `ctxce auth login` and retry)\"\n\n                last_error = {\"success\": False, \"error\": {\"code\": error_code, \"message\": error_msg, \"status_code\": response.status_code}}\n\n                # Don't retry on client errors (except 429)\n                if 400 <= response.status_code < 500 and response.status_code != 429:\n                    return last_error\n\n                logger.warning(f\"[remote_upload] Upload attempt {attempt + 1} failed: {error_msg}\")\n\n            except requests.exceptions.ConnectTimeout as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"TIMEOUT_ERROR\", \"message\": f\"Upload timeout: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Upload timeout on attempt {attempt + 1}: {e}\")\n\n            except requests.exceptions.ReadTimeout as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"TIMEOUT_ERROR\", \"message\": f\"Upload timeout: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Upload read timeout on attempt {attempt + 1}: {e}\")\n                \n                # After read timeout, poll to check if server processed the bundle\n                logger.info(f\"[remote_upload] Read timeout occurred, polling server to check if bundle was processed...\")\n                poll_result = self._poll_after_timeout(manifest)\n                if poll_result.get(\"success\"):\n                    logger.info(f\"[remote_upload] Server confirmed processing of bundle {manifest['bundle_id']} after timeout\")\n                    return poll_result\n                \n                logger.warning(f\"[remote_upload] Server did not process bundle after timeout, proceeding with failure\")\n                break\n\n            except requests.exceptions.Timeout as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"TIMEOUT_ERROR\", \"message\": f\"Upload timeout: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Upload timeout on attempt {attempt + 1}: {e}\")\n                \n                # For generic timeout, also try polling\n                logger.info(f\"[remote_upload] Timeout occurred, polling server to check if bundle was processed...\")\n                poll_result = self._poll_after_timeout(manifest)\n                if poll_result.get(\"success\"):\n                    logger.info(f\"[remote_upload] Server confirmed processing of bundle {manifest['bundle_id']} after timeout\")\n                    return poll_result\n                \n                logger.warning(f\"[remote_upload] Server did not process bundle after timeout, proceeding with failure\")\n                break\n\n            except requests.exceptions.ConnectionError as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"CONNECTION_ERROR\", \"message\": f\"Connection error: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Connection error on attempt {attempt + 1}: {e}\")\n\n            except requests.exceptions.RequestException as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"NETWORK_ERROR\", \"message\": f\"Network error: {str(e)}\"}}\n                logger.warning(f\"[remote_upload] Network error on attempt {attempt + 1}: {e}\")\n\n            except Exception as e:\n                last_error = {\"success\": False, \"error\": {\"code\": \"UPLOAD_ERROR\", \"message\": f\"Upload error: {str(e)}\"}}\n                logger.error(f\"[remote_upload] Unexpected error on attempt {attempt + 1}: {e}\")\n\n        # All retries exhausted\n        logger.error(f\"[remote_upload] All {self.max_retries + 1} upload attempts failed for bundle {manifest.get('bundle_id', 'unknown')}\")\n        return last_error or {\n            \"success\": False,\n            \"error\": {\n                \"code\": \"MAX_RETRIES_EXCEEDED\",\n                \"message\": f\"Upload failed after {self.max_retries + 1} attempts\"\n            }\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__poll_after_timeout_1261": {
      "name": "_poll_after_timeout",
      "type": "method",
      "start_line": 1261,
      "end_line": 1340,
      "content_hash": "d4d7e40491d76fc3a8b146515b477d5e1fd8d5ca",
      "content": "    def _poll_after_timeout(self, manifest: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Poll server status after a timeout to check if bundle was processed.\n        \n        Args:\n            manifest: Bundle manifest containing sequence information\n            \n        Returns:\n            Dictionary indicating success if bundle was processed\n        \"\"\"\n        try:\n            # Get current server status to know the expected sequence\n            status = self.get_server_status()\n            if not status.get(\"success\"):\n                return {\"success\": False, \"error\": status.get(\"error\", {\"code\": \"UNKNOWN\", \"message\": \"Failed to get status\"})}\n\n            current_sequence = status.get(\"last_sequence\", 0)\n            expected_sequence = manifest.get(\"sequence\", current_sequence + 1)\n\n            logger.info(f\"[remote_upload] Current server sequence: {current_sequence}, expected: {expected_sequence}\")\n\n            # If server is already at expected sequence, bundle was processed\n            if current_sequence >= expected_sequence:\n                return {\n                    \"success\": True,\n                    \"message\": f\"Bundle processed (server at sequence {current_sequence})\",\n                    \"sequence\": current_sequence,\n                }\n\n            # Poll window is configurable via REMOTE_UPLOAD_POLL_MAX_SECS (seconds).\n            # Values <= 0 mean \"no timeout\" (poll until success or process exit).\n            try:\n                max_poll_time = int(os.environ.get(\"REMOTE_UPLOAD_POLL_MAX_SECS\", \"300\"))\n            except Exception:\n                max_poll_time = 300\n            poll_interval = 5\n            start_time = time.time()\n\n            while True:\n                elapsed = time.time() - start_time\n                if max_poll_time > 0 and elapsed >= max_poll_time:\n                    logger.warning(\n                        f\"[remote_upload] Polling timed out after {int(elapsed)}s (limit={max_poll_time}s), bundle was not confirmed as processed\"\n                    )\n                    return {\n                        \"success\": False,\n                        \"error\": {\n                            \"code\": \"POLL_TIMEOUT\",\n                            \"message\": f\"Bundle not confirmed processed after polling for {int(elapsed)}s (limit={max_poll_time}s)\",\n                        },\n                    }\n\n                logger.info(\n                    f\"[remote_upload] Polling server status... (elapsed: {int(elapsed)}s, limit={'no-limit' if max_poll_time <= 0 else max_poll_time}s)\"\n                )\n                time.sleep(poll_interval)\n\n                status = self.get_server_status()\n                if status.get(\"success\"):\n                    new_sequence = status.get(\"last_sequence\", 0)\n                    if new_sequence >= expected_sequence:\n                        logger.info(\n                            f\"[remote_upload] Server sequence advanced to {new_sequence}, bundle was processed!\"\n                        )\n                        return {\n                            \"success\": True,\n                            \"message\": f\"Bundle processed after timeout (server at sequence {new_sequence})\",\n                            \"sequence\": new_sequence,\n                        }\n                    logger.debug(\n                        f\"[remote_upload] Server sequence still at {new_sequence}, continuing to poll...\"\n                    )\n                else:\n                    logger.warning(\n                        f\"[remote_upload] Failed to get server status during poll: {status.get('error', {}).get('message', 'Unknown')}\"\n                    )\n\n        except Exception as e:\n            logger.error(f\"[remote_upload] Error during post-timeout polling: {e}\")\n            return {\"success\": False, \"error\": {\"code\": \"POLL_ERROR\", \"message\": f\"Polling error: {str(e)}\"}}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_server_status_1342": {
      "name": "get_server_status",
      "type": "method",
      "start_line": 1342,
      "end_line": 1373,
      "content_hash": "f7349eb2801ddd26bf8d8a6c508e4a3efb0fb9ee",
      "content": "    def get_server_status(self) -> Dict[str, Any]:\n        \"\"\"Get server status with simplified error handling.\"\"\"\n        try:\n            container_workspace_path = self._translate_to_container_path(self.workspace_path)\n            connect_timeout = min(self.timeout, 10)\n            # Allow slower responses (e.g., cold starts/large collections) before bailing\n            read_timeout = max(self.timeout, 30)\n            response = self.session.get(\n                f\"{self.upload_endpoint}/api/v1/delta/status\",\n                params={'workspace_path': container_workspace_path},\n                timeout=(connect_timeout, read_timeout)\n            )\n\n            if response.status_code == 200:\n                return response.json()\n\n            # Handle error response\n            error_msg = f\"Status check failed with HTTP {response.status_code}\"\n            try:\n                error_detail = response.json()\n                error_msg += f\": {error_detail.get('error', {}).get('message', 'Unknown error')}\"\n            except:\n                error_msg += f\": {response.text[:100]}\"\n\n            return {\"success\": False, \"error\": {\"code\": \"STATUS_ERROR\", \"message\": error_msg}}\n\n        except requests.exceptions.Timeout:\n            return {\"success\": False, \"error\": {\"code\": \"STATUS_TIMEOUT\", \"message\": \"Status check timeout\"}}\n        except requests.exceptions.ConnectionError:\n            return {\"success\": False, \"error\": {\"code\": \"CONNECTION_ERROR\", \"message\": f\"Cannot connect to server\"}}\n        except Exception as e:\n            return {\"success\": False, \"error\": {\"code\": \"STATUS_CHECK_ERROR\", \"message\": f\"Status check error: {str(e)}\"}}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_has_meaningful_changes_1375": {
      "name": "has_meaningful_changes",
      "type": "method",
      "start_line": 1375,
      "end_line": 1378,
      "content_hash": "4cd126933d7477e493544d2dc509141505fd7f6f",
      "content": "    def has_meaningful_changes(self, changes: Dict[str, List]) -> bool:\n        \"\"\"Check if changes warrant a delta upload.\"\"\"\n        total_changes = sum(len(files) for op, files in changes.items() if op != \"unchanged\")\n        return total_changes > 0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_upload_git_history_only_1380": {
      "name": "upload_git_history_only",
      "type": "method",
      "start_line": 1380,
      "end_line": 1405,
      "content_hash": "8079198395bbad97ec354e3818987977298675b0",
      "content": "    def upload_git_history_only(self, git_history: Dict[str, Any]) -> bool:\n        try:\n            empty_changes = {\n                \"created\": [],\n                \"updated\": [],\n                \"deleted\": [],\n                \"moved\": [],\n                \"unchanged\": [],\n            }\n            bundle_path, manifest = self.create_delta_bundle(\n                empty_changes,\n                git_history=git_history,\n            )\n            response = self.upload_bundle(bundle_path, manifest)\n            if response.get(\"success\", False):\n                try:\n                    if os.path.exists(bundle_path):\n                        os.remove(bundle_path)\n                    self.cleanup()\n                except Exception:\n                    pass\n                return True\n            return False\n        except Exception as e:\n            logger.error(f\"[remote_upload] Error uploading git history metadata: {e}\")\n            return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_process_changes_and_upload_1407": {
      "name": "process_changes_and_upload",
      "type": "method",
      "start_line": 1407,
      "end_line": 1484,
      "content_hash": "71f2916625e30a271759111b1b5fe4edb8d30b29",
      "content": "    def process_changes_and_upload(self, changes: Dict[str, List]) -> bool:\n        \"\"\"\n        Process pre-computed changes and upload delta bundle.\n        Includes comprehensive error handling and graceful fallback.\n\n        Args:\n            changes: Dictionary of file changes by type\n\n        Returns:\n            True if upload was successful, False otherwise\n        \"\"\"\n        try:\n            logger.info(f\"[remote_upload] Processing pre-computed changes\")\n\n            # Validate input\n            if not changes:\n                logger.info(\"[remote_upload] No changes provided\")\n                return True\n\n            if not self.has_meaningful_changes(changes):\n                logger.info(\"[remote_upload] No meaningful changes detected, skipping upload\")\n                return True\n\n            # Log change summary\n            total_changes = sum(len(files) for op, files in changes.items() if op != \"unchanged\")\n            logger.info(f\"[remote_upload] Detected {total_changes} meaningful changes: \"\n                       f\"{len(changes['created'])} created, {len(changes['updated'])} updated, \"\n                       f\"{len(changes['deleted'])} deleted, {len(changes['moved'])} moved\")\n\n            # Create delta bundle\n            bundle_path = None\n            try:\n                bundle_path, manifest = self.create_delta_bundle(changes)\n                logger.info(f\"[remote_upload] Created delta bundle: {manifest['bundle_id']} \"\n                           f\"(size: {manifest['total_size_bytes']} bytes)\")\n\n                # Validate bundle was created successfully\n                if not bundle_path or not os.path.exists(bundle_path):\n                    raise RuntimeError(f\"Failed to create bundle at {bundle_path}\")\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error creating delta bundle: {e}\")\n                # Clean up any temporary files on failure\n                self.cleanup()\n                return False\n\n            # Upload bundle with retry logic\n            try:\n                response = self.upload_bundle(bundle_path, manifest)\n\n                if response.get(\"success\", False):\n                    processed_ops = response.get('processed_operations', {})\n                    logger.info(f\"[remote_upload] Successfully uploaded bundle {manifest['bundle_id']}\")\n                    logger.info(f\"[remote_upload] Processed operations: {processed_ops}\")\n\n                    # Clean up temporary bundle after successful upload\n                    try:\n                        if os.path.exists(bundle_path):\n                            os.remove(bundle_path)\n                            logger.debug(f\"[remote_upload] Cleaned up temporary bundle: {bundle_path}\")\n                        # Also clean up the entire temp directory if this is the last bundle\n                        self.cleanup()\n                    except Exception as cleanup_error:\n                        logger.warning(f\"[remote_upload] Failed to cleanup bundle {bundle_path}: {cleanup_error}\")\n\n                    return True\n                else:\n                    error_msg = response.get('error', {}).get('message', 'Unknown upload error')\n                    logger.error(f\"[remote_upload] Upload failed: {error_msg}\")\n                    return False\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error uploading bundle: {e}\")\n                return False\n\n        except Exception as e:\n            logger.error(f\"[remote_upload] Unexpected error in process_changes_and_upload: {e}\")\n            return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_watch_loop_1486": {
      "name": "watch_loop",
      "type": "method",
      "start_line": 1486,
      "end_line": 1558,
      "content_hash": "d1e67b6b410c4653f8813a03bcae070cad085646",
      "content": "    def watch_loop(self, interval: int = 5):\n        \"\"\"Main file watching loop using existing detection and upload methods.\"\"\"\n        logger.info(f\"[watch] Starting file monitoring (interval: {interval}s)\")\n        logger.info(f\"[watch] Monitoring: {self.workspace_path}\")\n        logger.info(f\"[watch] Press Ctrl+C to stop\")\n\n        try:\n            while True:\n                try:\n                    # Use existing change detection over both filesystem and cached registry\n                    fs_files = self.get_all_code_files()\n                    path_map = {}\n                    for p in fs_files:\n                        try:\n                            resolved = p.resolve()\n                        except Exception:\n                            continue\n                        path_map[resolved] = p\n\n                    # Include any paths that are only present in the local cache (deleted files)\n                    for cached_abs in get_all_cached_paths(self.repo_name):\n                        try:\n                            cached_path = Path(cached_abs)\n                            resolved = cached_path.resolve()\n                        except Exception:\n                            continue\n                        if resolved not in path_map:\n                            path_map[resolved] = cached_path\n\n                    all_paths = list(path_map.values())\n                    changes = self.detect_file_changes(all_paths)\n\n                    # Count only meaningful changes (exclude unchanged)\n                    meaningful_changes = len(changes.get(\"created\", [])) + len(changes.get(\"updated\", [])) + len(changes.get(\"deleted\", [])) + len(changes.get(\"moved\", []))\n\n                    if meaningful_changes > 0:\n                        logger.info(f\"[watch] Detected {meaningful_changes} changes: { {k: len(v) for k, v in changes.items() if k != 'unchanged'} }\")\n\n                        success = self.process_changes_and_upload(changes)\n\n                        if success:\n                            logger.info(f\"[watch] Successfully uploaded changes\")\n                        else:\n                            logger.error(f\"[watch] Failed to upload changes\")\n                    else:\n                        git_history = None\n                        try:\n                            git_history = _collect_git_history_for_workspace(self.workspace_path)\n                        except Exception:\n                            git_history = None\n\n                        if git_history:\n                            logger.info(\"[watch] Detected git history update; uploading git history metadata\")\n                            success = self.upload_git_history_only(git_history)\n                            if success:\n                                logger.info(\"[watch] Successfully uploaded git history metadata\")\n                            else:\n                                logger.error(\"[watch] Failed to upload git history metadata\")\n                        else:\n                            logger.debug(f\"[watch] No changes detected\")  # Debug level to avoid spam\n\n                    # Sleep until next check\n                    time.sleep(interval)\n\n                except KeyboardInterrupt:\n                    logger.info(f\"[watch] Received interrupt signal, stopping...\")\n                    break\n                except Exception as e:\n                    logger.error(f\"[watch] Error in watch loop: {e}\")\n                    time.sleep(interval)  # Continue even after errors\n\n        except KeyboardInterrupt:\n            logger.info(f\"[watch] File monitoring stopped by user\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_all_code_files_1560": {
      "name": "get_all_code_files",
      "type": "method",
      "start_line": 1560,
      "end_line": 1604,
      "content_hash": "12061ad1f243bc23521311dcfafb18291ac7ac4a",
      "content": "    def get_all_code_files(self) -> List[Path]:\n        \"\"\"Get all code files in the workspace, excluding heavy/third-party dirs.\"\"\"\n        files: List[Path] = []\n        try:\n            workspace_path = Path(self.workspace_path)\n            if not workspace_path.exists():\n                return files\n\n            # Single walk with early pruning and set-based matching to reduce IO\n            ext_suffixes = {str(ext).lower() for ext in CODE_EXTS if str(ext).startswith('.')}\n            extensionless_names = set(EXTENSIONLESS_FILES.keys())\n            # Always exclude dev-workspace to prevent recursive upload loops\n            # (upload service creates dev-workspace/<collection>/ which would otherwise get re-uploaded)\n            excluded = {\n                \"node_modules\", \"vendor\", \"dist\", \"build\", \"target\", \"out\",\n                \".git\", \".hg\", \".svn\", \".vscode\", \".idea\", \".venv\", \"venv\",\n                \"__pycache__\", \".pytest_cache\", \".mypy_cache\", \".cache\",\n                \".context-engine\", \".context-engine-uploader\", \".codebase\",\n                \"dev-workspace\"\n            }\n\n            seen = set()\n            for root, dirnames, filenames in os.walk(workspace_path):\n                # Prune heavy/hidden directories before descending\n                dirnames[:] = [d for d in dirnames if d not in excluded and not d.startswith('.')]\n\n                for filename in filenames:\n                    # Allow dotfiles that are in EXTENSIONLESS_FILES (e.g., .gitignore)\n                    fname_lower = filename.lower()\n                    if filename.startswith('.') and fname_lower not in extensionless_names:\n                        continue\n                    candidate = Path(root) / filename\n                    suffix = candidate.suffix.lower()\n                    # Match by extension, extensionless name, or Dockerfile.* prefix\n                    if (suffix in ext_suffixes or\n                        fname_lower in extensionless_names or\n                        fname_lower.startswith(\"dockerfile\")):\n                        resolved = candidate.resolve()\n                        if resolved not in seen:\n                            seen.add(resolved)\n                            files.append(candidate)\n        except Exception as e:\n            logger.error(f\"[watch] Error scanning files: {e}\")\n\n        return files",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_process_and_upload_changes_1606": {
      "name": "process_and_upload_changes",
      "type": "method",
      "start_line": 1606,
      "end_line": 1709,
      "content_hash": "0e855b4996c36e5d166566759099f896abbe2989",
      "content": "    def process_and_upload_changes(self, changed_paths: List[Path]) -> bool:\n        \"\"\"\n        Process changed paths and upload delta bundle if meaningful changes exist.\n        Includes comprehensive error handling and graceful fallback.\n\n        Args:\n            changed_paths: List of changed file paths\n\n        Returns:\n            True if upload was successful, False otherwise\n        \"\"\"\n        try:\n            logger.info(f\"[remote_upload] Processing {len(changed_paths)} changed paths\")\n\n            # Validate input\n            if not changed_paths:\n                logger.info(\"[remote_upload] No changed paths provided\")\n                return True\n\n            # Detect changes\n            try:\n                changes = self.detect_file_changes(changed_paths)\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error detecting file changes: {e}\")\n                return False\n\n            if not self.has_meaningful_changes(changes):\n                logger.info(\"[remote_upload] No meaningful changes detected, skipping upload\")\n                return True\n\n            # Log change summary\n            total_changes = sum(len(files) for op, files in changes.items() if op != \"unchanged\")\n            logger.info(f\"[remote_upload] Detected {total_changes} meaningful changes: \"\n                       f\"{len(changes['created'])} created, {len(changes['updated'])} updated, \"\n                       f\"{len(changes['deleted'])} deleted, {len(changes['moved'])} moved\")\n\n            # Create delta bundle\n            bundle_path = None\n            try:\n                bundle_path, manifest = self.create_delta_bundle(changes)\n                logger.info(f\"[remote_upload] Created delta bundle: {manifest['bundle_id']} \"\n                           f\"(size: {manifest['total_size_bytes']} bytes)\")\n\n                # Validate bundle was created successfully\n                if not bundle_path or not os.path.exists(bundle_path):\n                    raise RuntimeError(f\"Failed to create bundle at {bundle_path}\")\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Error creating delta bundle: {e}\")\n                # Clean up any temporary files on failure\n                self.cleanup()\n                return False\n\n            # Upload bundle with retry logic\n            try:\n                response = self.upload_bundle(bundle_path, manifest)\n\n                if response.get(\"success\", False):\n                    processed_ops = response.get('processed_operations', {})\n                    logger.info(f\"[remote_upload] Successfully uploaded bundle {manifest['bundle_id']}\")\n                    logger.info(f\"[remote_upload] Processed operations: {processed_ops}\")\n\n                    # Clean up temporary bundle after successful upload\n                    try:\n                        if os.path.exists(bundle_path):\n                            os.remove(bundle_path)\n                            logger.debug(f\"[remote_upload] Cleaned up temporary bundle: {bundle_path}\")\n                        # Also clean up the entire temp directory if this is the last bundle\n                        self.cleanup()\n                    except Exception as cleanup_error:\n                        logger.warning(f\"[remote_upload] Failed to cleanup bundle {bundle_path}: {cleanup_error}\")\n\n                    return True\n                else:\n                    error = response.get(\"error\", {})\n                    error_code = error.get(\"code\", \"UNKNOWN\")\n                    error_msg = error.get(\"message\", \"Unknown error\")\n\n                    logger.error(f\"[remote_upload] Upload failed: {error_msg}\")\n\n                    # Handle specific error types\n                    # CLI is stateless - server handles sequence management\n                    if error_code in [\"BUNDLE_TOO_LARGE\", \"BUNDLE_NOT_FOUND\"]:\n                        # These are unrecoverable errors\n                        logger.error(f\"[remote_upload] Unrecoverable error ({error_code}): {error_msg}\")\n                        return False\n                    elif error_code in [\"TIMEOUT_ERROR\", \"CONNECTION_ERROR\", \"NETWORK_ERROR\"]:\n                        # These might be temporary, suggest fallback\n                        logger.warning(f\"[remote_upload] Network-related error ({error_code}): {error_msg}\")\n                        logger.warning(\"[remote_upload] Consider falling back to local mode if this persists\")\n                        return False\n                    else:\n                        # Other errors\n                        logger.error(f\"[remote_upload] Upload error ({error_code}): {error_msg}\")\n                        return False\n\n            except Exception as e:\n                logger.error(f\"[remote_upload] Unexpected error during upload: {e}\")\n                return False\n\n        except Exception as e:\n            logger.error(f\"[remote_upload] Critical error in process_and_upload_changes: {e}\")\n            logger.exception(\"[remote_upload] Full traceback:\")\n            return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_remote_config_1711": {
      "name": "get_remote_config",
      "type": "function",
      "start_line": 1711,
      "end_line": 1736,
      "content_hash": "5858a5d4a32b4004798da0dd5b39ec30adec2899",
      "content": "def get_remote_config(cli_path: Optional[str] = None) -> Dict[str, str]:\n    \"\"\"Get remote upload configuration from environment variables and command-line arguments.\"\"\"\n    # Use command-line path if provided, otherwise fall back to environment variables\n    if cli_path:\n        workspace_path = cli_path\n    else:\n        workspace_path = os.environ.get(\"WATCH_ROOT\", os.environ.get(\"WORKSPACE_PATH\", \"/work\"))\n\n    logical_repo_id = _compute_logical_repo_id(workspace_path)\n\n    # Use auto-generated collection name based on repo name\n    repo_name = _extract_repo_name_from_path(workspace_path)\n    # Fallback to directory name if repo detection fails\n    if not repo_name:\n        repo_name = Path(workspace_path).name\n    collection_name = get_collection_name(repo_name)\n\n    return {\n        \"upload_endpoint\": os.environ.get(\"REMOTE_UPLOAD_ENDPOINT\", \"http://localhost:8080\"),\n        \"workspace_path\": workspace_path,\n        \"collection_name\": collection_name,\n        \"logical_repo_id\": logical_repo_id,\n        # Use higher, more robust defaults but still allow env overrides\n        \"max_retries\": int(os.environ.get(\"REMOTE_UPLOAD_MAX_RETRIES\", \"5\")),\n        \"timeout\": int(os.environ.get(\"REMOTE_UPLOAD_TIMEOUT\", \"1800\")),\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_main_1739": {
      "name": "main",
      "type": "function",
      "start_line": 1739,
      "end_line": 1967,
      "content_hash": "b736fdf7aecda9c9df3e3c1d352bde6eae457627",
      "content": "def main():\n    \"\"\"Main entry point for the remote upload client.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Remote upload client for delta bundles in Context-Engine\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # Upload from current directory or environment variables\n  python remote_upload_client.py\n\n  # Upload from specific directory\n  python remote_upload_client.py --path /path/to/repo\n\n  # Upload from specific directory with custom endpoint\n  python remote_upload_client.py --path /path/to/repo --endpoint http://remote-server:8080\n        \"\"\"\n    )\n\n    parser.add_argument(\n        \"--path\",\n        type=str,\n        help=\"Path to the directory to upload (overrides WATCH_ROOT/WORKSPACE_PATH environment variables)\"\n    )\n\n    parser.add_argument(\n        \"--endpoint\",\n        type=str,\n        help=\"Remote upload endpoint (overrides REMOTE_UPLOAD_ENDPOINT environment variable)\"\n    )\n\n    parser.add_argument(\n        \"--max-retries\",\n        type=int,\n        help=\"Maximum number of upload retries (overrides REMOTE_UPLOAD_MAX_RETRIES environment variable)\"\n    )\n\n    parser.add_argument(\n        \"--timeout\",\n        type=int,\n        help=\"Request timeout in seconds (overrides REMOTE_UPLOAD_TIMEOUT environment variable)\"\n    )\n\n    parser.add_argument(\n        \"--force\",\n        action=\"store_true\",\n        help=\"Force upload of all files (ignore cached state and treat all files as new)\"\n    )\n\n    parser.add_argument(\n        \"--show-mapping\",\n        action=\"store_true\",\n        help=\"Print collection\u2194workspace mapping information and exit\"\n    )\n\n    parser.add_argument(\n        \"--watch\", \"-w\",\n        action=\"store_true\",\n        help=\"Watch for file changes and upload automatically (continuous mode)\"\n    )\n\n    parser.add_argument(\n        \"--interval\", \"-i\",\n        type=int,\n        default=5,\n        help=\"Watch interval in seconds (default: 5)\"\n    )\n\n    args = parser.parse_args()\n\n    # Validate path if provided\n    if args.path:\n        if not os.path.exists(args.path):\n            logger.error(f\"Path does not exist: {args.path}\")\n            return 1\n\n        if not os.path.isdir(args.path):\n            logger.error(f\"Path is not a directory: {args.path}\")\n            return 1\n\n        args.path = os.path.abspath(args.path)\n        logger.info(f\"Using specified path: {args.path}\")\n\n    # Get configuration\n    config = get_remote_config(args.path)\n\n    # Override config with command-line arguments if provided\n    if args.endpoint:\n        config[\"upload_endpoint\"] = args.endpoint\n    if args.max_retries is not None:\n        config[\"max_retries\"] = args.max_retries\n    if args.timeout is not None:\n        config[\"timeout\"] = args.timeout\n\n    logger.info(f\"Workspace path: {config['workspace_path']}\")\n    logger.info(f\"Collection name: {config['collection_name']}\")\n    logger.info(f\"Upload endpoint: {config['upload_endpoint']}\")\n\n    if args.show_mapping:\n        with RemoteUploadClient(\n            upload_endpoint=config[\"upload_endpoint\"],\n            workspace_path=config[\"workspace_path\"],\n            collection_name=config[\"collection_name\"],\n            max_retries=config[\"max_retries\"],\n            timeout=config[\"timeout\"],\n            logical_repo_id=config.get(\"logical_repo_id\"),\n        ) as client:\n            client.log_mapping_summary()\n        return 0\n\n    # Handle watch mode\n    if args.watch:\n        logger.info(\"Starting watch mode for continuous file monitoring\")\n        try:\n            with RemoteUploadClient(\n                upload_endpoint=config[\"upload_endpoint\"],\n                workspace_path=config[\"workspace_path\"],\n                collection_name=config[\"collection_name\"],\n                max_retries=config[\"max_retries\"],\n                timeout=config[\"timeout\"],\n                logical_repo_id=config.get(\"logical_repo_id\"),\n            ) as client:\n\n                logger.info(\"Remote upload client initialized successfully\")\n                client.log_mapping_summary()\n\n                # Test server connection first\n                logger.info(\"Checking server status...\")\n                status = client.get_server_status()\n                is_success = (\n                    isinstance(status, dict) and\n                    'workspace_path' in status and\n                    'collection_name' in status and\n                    status.get('status') == 'ready'\n                )\n                if not is_success:\n                    error = status.get(\"error\", {})\n                    logger.error(f\"Cannot connect to server: {error.get('message', 'Unknown error')}\")\n                    return 1\n\n                logger.info(\"Server connection successful\")\n                logger.info(f\"Starting file monitoring with {args.interval}s interval\")\n\n                # Start the watch loop\n                client.watch_loop(interval=args.interval)\n\n            return 0\n\n        except KeyboardInterrupt:\n            logger.info(\"Watch mode stopped by user\")\n            return 0\n        except Exception as e:\n            logger.error(f\"Watch mode failed: {e}\")\n            return 1\n\n    # Single upload mode (original logic)\n    # Initialize client with context manager for cleanup\n    try:\n        with RemoteUploadClient(\n            upload_endpoint=config[\"upload_endpoint\"],\n            workspace_path=config[\"workspace_path\"],\n            collection_name=config[\"collection_name\"],\n            max_retries=config[\"max_retries\"],\n            timeout=config[\"timeout\"],\n            logical_repo_id=config.get(\"logical_repo_id\"),\n        ) as client:\n\n            logger.info(\"Remote upload client initialized successfully\")\n\n            client.log_mapping_summary()\n\n            # Test server connection\n            logger.info(\"Checking server status...\")\n            status = client.get_server_status()\n            # For delta endpoint, success is indicated by having expected fields (not a \"success\" boolean)\n            is_success = (\n                isinstance(status, dict) and\n                'workspace_path' in status and\n                'collection_name' in status and\n                status.get('status') == 'ready'\n            )\n            if not is_success:\n                error = status.get(\"error\", {})\n                logger.error(f\"Cannot connect to server: {error.get('message', 'Unknown error')}\")\n                return 1\n\n            logger.info(\"Server connection successful\")\n\n            # Scan repository and upload files\n            logger.info(\"Scanning repository for files...\")\n            workspace_path = Path(config['workspace_path'])\n\n            # Find code files in the repository (exclude hidden and heavy dirs)\n            all_files = client.get_all_code_files()\n            logger.info(f\"Found {len(all_files)} code files to upload\")\n\n            if not all_files:\n                logger.warning(\"No files found to upload\")\n                return 0\n\n            # Detect changes (treat all files as changes for initial upload)\n            if args.force:\n                # Force mode: treat all files as created\n                changes = {\"created\": all_files, \"updated\": [], \"deleted\": [], \"moved\": [], \"unchanged\": []}\n            else:\n                changes = client.detect_file_changes(all_files)\n\n            if not client.has_meaningful_changes(changes):\n                logger.info(\"No meaningful changes to upload\")\n                return 0\n\n            logger.info(f\"Changes detected: {len(changes.get('created', []))} created, {len(changes.get('updated', []))} updated, {len(changes.get('deleted', []))} deleted\")\n\n            # Process and upload changes\n            logger.info(\"Uploading files to remote server...\")\n            success = client.process_changes_and_upload(changes)\n\n            if success:\n                logger.info(\"Repository upload completed successfully!\")\n                logger.info(f\"Collection name: {config['collection_name']}\")\n                logger.info(f\"Files uploaded: {len(all_files)}\")\n            else:\n                logger.error(\"Repository upload failed!\")\n                return 1\n\n            return 0\n\n    except Exception as e:\n        logger.error(f\"Failed to initialize remote upload client: {e}\")\n        return 1",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}