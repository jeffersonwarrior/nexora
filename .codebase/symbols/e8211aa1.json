{
  "file_path": "/work/context-engine/scripts/deduplication.py",
  "file_hash": "ab7a5d840aa2170c3945afd14897114d27fb1fe7",
  "updated_at": "2025-12-26T17:34:19.948844",
  "symbols": {
    "class_RequestFingerprint_21": {
      "name": "RequestFingerprint",
      "type": "class",
      "start_line": 21,
      "end_line": 100,
      "content_hash": "63eaf4e59c2cd0eefa4b2513394cead5a26edc06",
      "content": "class RequestFingerprint:\n    \"\"\"Represents a unique fingerprint for a request.\"\"\"\n    \n    def __init__(self, request_data: Dict[str, Any]):\n        self.request_data = request_data\n        self.fingerprint = self._generate_fingerprint(request_data)\n        self.created_at = time.time()\n        self.access_count = 1\n        self.last_accessed = self.created_at\n    \n    def _generate_fingerprint(self, request_data: Dict[str, Any]) -> str:\n        \"\"\"Generate a consistent fingerprint from request data.\"\"\"\n        try:\n            # Normalize request data for consistent fingerprinting\n            normalized = {}\n            \n            # Key fields to include in fingerprint (order matters)\n            key_fields = [\n                'queries',\n                'limit', \n                'per_path',\n                'language',\n                'under',\n                'kind',\n                'symbol',\n                'ext',\n                'not',\n                'case',\n                'path_regex',\n                'path_glob',\n                'not_glob',\n                'expand',\n                'collection',\n                'vector_name'\n            ]\n            \n            for field in key_fields:\n                if field in request_data:\n                    value = request_data[field]\n                    \n                    # Normalize different types\n                    if isinstance(value, (list, tuple)):\n                        # Sort lists for consistent ordering\n                        if isinstance(value, list):\n                            normalized[field] = sorted([str(v).lower().strip() for v in value])\n                        else:\n                            normalized[field] = tuple(sorted([str(v).lower().strip() for v in value]))\n                    elif isinstance(value, dict):\n                        # Sort dict keys and normalize values\n                        normalized[field] = {\n                            str(k).lower(): str(v).lower() \n                            for k, v in sorted(value.items())\n                        }\n                    elif isinstance(value, bool):\n                        normalized[field] = value\n                    else:\n                        # Normalize strings\n                        normalized[field] = str(value).lower().strip()\n            \n            # Create fingerprint from normalized data\n            fingerprint_data = json.dumps(normalized, sort_keys=True, separators=(',', ':'))\n            return hashlib.sha256(fingerprint_data.encode('utf-8')).hexdigest()\n            \n        except Exception as e:\n            logger.debug(f\"Error generating fingerprint: {e}\")\n            # Fallback to simple hash of raw data\n            return hashlib.md5(str(request_data).encode()).hexdigest()\n    \n    def access(self) -> None:\n        \"\"\"Record access to this request.\"\"\"\n        self.access_count += 1\n        self.last_accessed = time.time()\n    \n    def is_expired(self, ttl: float) -> bool:\n        \"\"\"Check if request fingerprint has expired.\"\"\"\n        return time.time() - self.created_at > ttl\n    \n    def get_age_seconds(self) -> float:\n        \"\"\"Get age of this request fingerprint in seconds.\"\"\"\n        return time.time() - self.created_at",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___24": {
      "name": "__init__",
      "type": "method",
      "start_line": 24,
      "end_line": 29,
      "content_hash": "b144fb5f86c6b7d71eb898bceb998ef2eef73612",
      "content": "    def __init__(self, request_data: Dict[str, Any]):\n        self.request_data = request_data\n        self.fingerprint = self._generate_fingerprint(request_data)\n        self.created_at = time.time()\n        self.access_count = 1\n        self.last_accessed = self.created_at",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__generate_fingerprint_31": {
      "name": "_generate_fingerprint",
      "type": "method",
      "start_line": 31,
      "end_line": 87,
      "content_hash": "d7f646391342b226aa3fdc2655bdf43a71f4b837",
      "content": "    def _generate_fingerprint(self, request_data: Dict[str, Any]) -> str:\n        \"\"\"Generate a consistent fingerprint from request data.\"\"\"\n        try:\n            # Normalize request data for consistent fingerprinting\n            normalized = {}\n            \n            # Key fields to include in fingerprint (order matters)\n            key_fields = [\n                'queries',\n                'limit', \n                'per_path',\n                'language',\n                'under',\n                'kind',\n                'symbol',\n                'ext',\n                'not',\n                'case',\n                'path_regex',\n                'path_glob',\n                'not_glob',\n                'expand',\n                'collection',\n                'vector_name'\n            ]\n            \n            for field in key_fields:\n                if field in request_data:\n                    value = request_data[field]\n                    \n                    # Normalize different types\n                    if isinstance(value, (list, tuple)):\n                        # Sort lists for consistent ordering\n                        if isinstance(value, list):\n                            normalized[field] = sorted([str(v).lower().strip() for v in value])\n                        else:\n                            normalized[field] = tuple(sorted([str(v).lower().strip() for v in value]))\n                    elif isinstance(value, dict):\n                        # Sort dict keys and normalize values\n                        normalized[field] = {\n                            str(k).lower(): str(v).lower() \n                            for k, v in sorted(value.items())\n                        }\n                    elif isinstance(value, bool):\n                        normalized[field] = value\n                    else:\n                        # Normalize strings\n                        normalized[field] = str(value).lower().strip()\n            \n            # Create fingerprint from normalized data\n            fingerprint_data = json.dumps(normalized, sort_keys=True, separators=(',', ':'))\n            return hashlib.sha256(fingerprint_data.encode('utf-8')).hexdigest()\n            \n        except Exception as e:\n            logger.debug(f\"Error generating fingerprint: {e}\")\n            # Fallback to simple hash of raw data\n            return hashlib.md5(str(request_data).encode()).hexdigest()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_access_89": {
      "name": "access",
      "type": "method",
      "start_line": 89,
      "end_line": 92,
      "content_hash": "4aa7fc64badbe54eefb6c5244128cf48e90d8526",
      "content": "    def access(self) -> None:\n        \"\"\"Record access to this request.\"\"\"\n        self.access_count += 1\n        self.last_accessed = time.time()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_is_expired_94": {
      "name": "is_expired",
      "type": "method",
      "start_line": 94,
      "end_line": 96,
      "content_hash": "1e2d54cc113e0c3a495c0104bad8c29c5d88797d",
      "content": "    def is_expired(self, ttl: float) -> bool:\n        \"\"\"Check if request fingerprint has expired.\"\"\"\n        return time.time() - self.created_at > ttl",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_age_seconds_98": {
      "name": "get_age_seconds",
      "type": "method",
      "start_line": 98,
      "end_line": 100,
      "content_hash": "edfae6ea4bec78abf122d42b9742a02edf446829",
      "content": "    def get_age_seconds(self) -> float:\n        \"\"\"Get age of this request fingerprint in seconds.\"\"\"\n        return time.time() - self.created_at",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_RequestDeduplicator_103": {
      "name": "RequestDeduplicator",
      "type": "class",
      "start_line": 103,
      "end_line": 405,
      "content_hash": "f1d3bd6516a1db70f12a0f5d39c57605a8a2ddc1",
      "content": "class RequestDeduplicator:\n    \"\"\"\n    Intelligent request deduplication system.\n    \n    Features:\n    - Configurable deduplication windows\n    - LRU eviction of old requests\n    - Statistics tracking\n    - Thread-safe operations\n    - Multiple deduplication strategies\n    \"\"\"\n    \n    def __init__(\n        self,\n        name: str = \"default\",\n        dedup_window_seconds: int = 60,\n        max_cache_size: int = 10000,\n        cleanup_interval: float = 30.0,\n        exact_match: bool = True,\n        similarity_threshold: float = 0.9\n    ):\n        self.name = name\n        self.dedup_window_seconds = dedup_window_seconds\n        self.max_cache_size = max_cache_size\n        self.cleanup_interval = cleanup_interval\n        self.exact_match = exact_match\n        self.similarity_threshold = similarity_threshold\n        \n        # Storage for request fingerprints\n        self._fingerprints: Dict[str, RequestFingerprint] = {}\n        self._access_order = deque()  # For LRU tracking\n        \n        # Thread safety\n        self._lock = threading.RLock()\n        \n        # Statistics\n        self._stats = {\n            'total_requests': 0,\n            'deduped_requests': 0,\n            'unique_requests': 0,\n            'cache_hits': 0,\n            'cache_size': 0,\n            'dedup_rate': 0.0\n        }\n        \n        # Start cleanup thread\n        self._cleanup_thread = threading.Thread(target=self._cleanup_worker, daemon=True)\n        self._cleanup_thread.start()\n        \n        logger.debug(f\"Initialized request deduplicator with window={dedup_window_seconds}s, \"\n                    f\"max_size={max_cache_size}, exact_match={exact_match}\")\n    \n    def _normalize_request_data(self, request_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Normalize request data for consistent deduplication (case/whitespace + light stemming).\"\"\"\n        def _stem_token(tok: str) -> str:\n            t = tok.lower().strip()\n            # minimal stemming: queries -> query, classes -> class, etc.\n            if t.endswith(\"ies\") and len(t) > 4:\n                t = t[:-3] + \"y\"\n            elif t.endswith(\"es\") and len(t) > 3:\n                t = t[:-2]\n            elif t.endswith(\"s\") and len(t) > 3:\n                t = t[:-1]\n            return t\n\n        normalized: Dict[str, Any] = {}\n\n        # Handle different input types for queries\n        if 'queries' in request_data:\n            queries = request_data['queries']\n            if isinstance(queries, str):\n                normalized['queries'] = [_stem_token(queries)]\n            elif isinstance(queries, (list, tuple)):\n                normalized['queries'] = [_stem_token(str(q)) for q in queries]\n            else:\n                normalized['queries'] = [_stem_token(str(queries))]\n\n        # Normalize other common fields\n        string_fields = ['language', 'under', 'kind', 'symbol', 'ext', 'not', 'case',\n                        'path_regex', 'collection', 'vector_name']\n        for field in string_fields:\n            if field in request_data:\n                normalized[field] = _stem_token(str(request_data[field]))\n\n        # Normalize list fields\n        list_fields = ['path_glob', 'not_glob']\n        for field in list_fields:\n            if field in request_data:\n                value = request_data[field]\n                if isinstance(value, str):\n                    normalized[field] = [_stem_token(value)]\n                elif isinstance(value, (list, tuple)):\n                    normalized[field] = [_stem_token(str(v)) for v in value]\n                else:\n                    normalized[field] = [_stem_token(str(value))]\n\n        # Copy numeric and boolean fields as-is\n        numeric_fields = ['limit', 'per_path']\n        for field in numeric_fields:\n            if field in request_data:\n                try:\n                    normalized[field] = int(request_data[field])\n                except (ValueError, TypeError):\n                    normalized[field] = request_data[field]\n\n        bool_fields = ['expand']\n        for field in bool_fields:\n            if field in request_data:\n                normalized[field] = bool(request_data[field])\n\n        return normalized\n    \n    def _calculate_similarity(self, cand_norm: Dict[str, Any], exist_norm: Dict[str, Any]) -> float:\n        \"\"\"Calculate similarity between two normalized request dicts.\n        Uses Jaccard over flattened key:value tokens. For exact_match, only exact equality returns 1.0.\n        \"\"\"\n        if self.exact_match:\n            return 1.0 if cand_norm == exist_norm else 0.0\n\n        def _flatten(d: Dict[str, Any]) -> set:\n            toks = set()\n            for k, v in d.items():\n                if isinstance(v, list):\n                    toks.update(f\"{k}:{str(it)}\" for it in v)\n                else:\n                    toks.add(f\"{k}:{str(v)}\")\n            return toks\n        try:\n            s1, s2 = _flatten(cand_norm), _flatten(exist_norm)\n            if not s1 and not s2:\n                return 1.0\n            if not s1 or not s2:\n                return 0.0\n            inter = len(s1 & s2)\n            union = len(s1 | s2)\n            return inter / union if union else 0.0\n        except Exception:\n            return 0.0\n    \n    def _find_similar_requests(self, candidate_fp: str, candidate_norm: Dict[str, Any]) -> List[str]:\n        \"\"\"Find requests similar to the given normalized candidate.\"\"\"\n        if self.exact_match:\n            return [candidate_fp] if candidate_fp in self._fingerprints else []\n\n        similar: List[str] = []\n        for existing_fp, obj in self._fingerprints.items():\n            try:\n                sim = self._calculate_similarity(candidate_norm, obj.request_data)\n            except Exception:\n                sim = 0.0\n            if sim >= self.similarity_threshold:\n                similar.append(existing_fp)\n\n        return similar\n    \n    def _cleanup_worker(self) -> None:\n        \"\"\"Background worker for periodic cleanup.\"\"\"\n        while True:\n            try:\n                time.sleep(self.cleanup_interval)\n                self._cleanup_expired()\n            except Exception as e:\n                logger.error(f\"Deduplication cleanup error: {e}\")\n    \n    def _cleanup_expired(self) -> None:\n        \"\"\"Clean up expired request fingerprints.\"\"\"\n        current_time = time.time()\n        expired_keys = []\n        \n        for key, fp in self._fingerprints.items():\n            if fp.is_expired(self.dedup_window_seconds):\n                expired_keys.append(key)\n        \n        with self._lock:\n            for key in expired_keys:\n                del self._fingerprints[key]\n                # Remove from access order\n                try:\n                    self._access_order.remove(key)\n                except ValueError:\n                    pass\n        \n        if expired_keys:\n            logger.debug(f\"Cleaned up {len(expired_keys)} expired request fingerprints\")\n    \n    def _evict_if_needed(self) -> None:\n        \"\"\"Evict oldest entries if cache is full.\"\"\"\n        with self._lock:\n            while len(self._fingerprints) >= self.max_cache_size:\n                try:\n                    oldest_key = self._access_order.popleft()\n                    if oldest_key in self._fingerprints:\n                        del self._fingerprints[oldest_key]\n                except (IndexError, KeyError):\n                    break\n    \n    def is_duplicate(self, request_data: Dict[str, Any]) -> Tuple[bool, Optional[str]]:\n        \"\"\"\n        Check if request is a duplicate.\n\n        Returns:\n            Tuple of (is_duplicate, similar_fingerprint)\n        \"\"\"\n        # Normalize request data (no lock needed)\n        normalized_data = self._normalize_request_data(request_data)\n\n        # Generate candidate fingerprint (not inserted yet)\n        candidate_obj = RequestFingerprint(normalized_data)\n        candidate_fp = candidate_obj.fingerprint\n\n        with self._lock:\n            # Synchronous TTL cleanup to avoid stale matches in tight loops\n            expired_keys = [k for k, fp in self._fingerprints.items() if fp.is_expired(self.dedup_window_seconds)]\n            for k in expired_keys:\n                try:\n                    del self._fingerprints[k]\n                    try:\n                        self._access_order.remove(k)\n                    except ValueError:\n                        pass\n                except Exception:\n                    pass\n            if expired_keys:\n                # Keep cache_size accurate after purge\n                self._stats['cache_size'] = len(self._fingerprints)\n\n            self._stats['total_requests'] += 1\n\n            # Find similar requests using normalized representation\n            similar_fingerprints = self._find_similar_requests(candidate_fp, normalized_data)\n\n            if similar_fingerprints:\n                # Found similar request(s)\n                similar_fp = similar_fingerprints[0]  # Use first match\n\n                # Update access statistics\n                self._fingerprints[similar_fp].access()\n\n                # Update access order for LRU\n                try:\n                    self._access_order.remove(similar_fp)\n                    self._access_order.append(similar_fp)\n                except ValueError:\n                    self._access_order.append(similar_fp)\n\n                self._stats['deduped_requests'] += 1\n                self._stats['cache_hits'] += 1\n\n                logger.debug(f\"Request deduplicated: {candidate_fp[:8]}... matches {similar_fp[:8]}...\")\n                return True, similar_fp\n            else:\n                # New unique request\n                self._evict_if_needed()\n\n                # Store new fingerprint\n                self._fingerprints[candidate_fp] = candidate_obj\n                self._access_order.append(candidate_fp)\n\n                self._stats['unique_requests'] += 1\n                self._stats['cache_size'] = len(self._fingerprints)\n\n                logger.debug(f\"New unique request: {candidate_fp[:8]}...\")\n                return False, None\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get deduplication statistics.\"\"\"\n        with self._lock:\n            stats = self._stats.copy()\n            total = stats['total_requests']\n            if total > 0:\n                stats['dedup_rate'] = round((stats['deduped_requests'] / total) * 100, 2)\n            else:\n                stats['dedup_rate'] = 0.0\n\n            stats['cache_utilization'] = (stats['cache_size'] / max(1, self.max_cache_size)) * 100\n\n            return stats\n    \n    def clear_cache(self) -> None:\n        \"\"\"Clear all request fingerprints and reset statistics.\"\"\"\n        with self._lock:\n            self._fingerprints.clear()\n            self._access_order.clear()\n            # Reset statistics\n            self._stats = {\n                'total_requests': 0,\n                'deduped_requests': 0,\n                'unique_requests': 0,\n                'cache_hits': 0,\n                'cache_size': 0,\n                'dedup_rate': 0.0\n            }\n\n        logger.debug(\"Cleared request deduplication cache\")\n    \n    def get_cache_size(self) -> int:\n        \"\"\"Get current cache size.\"\"\"\n        with self._lock:\n            return len(self._fingerprints)\n    \n    def __len__(self) -> int:\n        \"\"\"Get current cache size.\"\"\"\n        return self.get_cache_size()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___115": {
      "name": "__init__",
      "type": "method",
      "start_line": 115,
      "end_line": 153,
      "content_hash": "25a0989168fee0f2d45a83ed36f599cef4831e21",
      "content": "    def __init__(\n        self,\n        name: str = \"default\",\n        dedup_window_seconds: int = 60,\n        max_cache_size: int = 10000,\n        cleanup_interval: float = 30.0,\n        exact_match: bool = True,\n        similarity_threshold: float = 0.9\n    ):\n        self.name = name\n        self.dedup_window_seconds = dedup_window_seconds\n        self.max_cache_size = max_cache_size\n        self.cleanup_interval = cleanup_interval\n        self.exact_match = exact_match\n        self.similarity_threshold = similarity_threshold\n        \n        # Storage for request fingerprints\n        self._fingerprints: Dict[str, RequestFingerprint] = {}\n        self._access_order = deque()  # For LRU tracking\n        \n        # Thread safety\n        self._lock = threading.RLock()\n        \n        # Statistics\n        self._stats = {\n            'total_requests': 0,\n            'deduped_requests': 0,\n            'unique_requests': 0,\n            'cache_hits': 0,\n            'cache_size': 0,\n            'dedup_rate': 0.0\n        }\n        \n        # Start cleanup thread\n        self._cleanup_thread = threading.Thread(target=self._cleanup_worker, daemon=True)\n        self._cleanup_thread.start()\n        \n        logger.debug(f\"Initialized request deduplicator with window={dedup_window_seconds}s, \"\n                    f\"max_size={max_cache_size}, exact_match={exact_match}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__normalize_request_data_155": {
      "name": "_normalize_request_data",
      "type": "method",
      "start_line": 155,
      "end_line": 213,
      "content_hash": "cdaa07c86b60b1004ad18119c224a6d2c3427655",
      "content": "    def _normalize_request_data(self, request_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Normalize request data for consistent deduplication (case/whitespace + light stemming).\"\"\"\n        def _stem_token(tok: str) -> str:\n            t = tok.lower().strip()\n            # minimal stemming: queries -> query, classes -> class, etc.\n            if t.endswith(\"ies\") and len(t) > 4:\n                t = t[:-3] + \"y\"\n            elif t.endswith(\"es\") and len(t) > 3:\n                t = t[:-2]\n            elif t.endswith(\"s\") and len(t) > 3:\n                t = t[:-1]\n            return t\n\n        normalized: Dict[str, Any] = {}\n\n        # Handle different input types for queries\n        if 'queries' in request_data:\n            queries = request_data['queries']\n            if isinstance(queries, str):\n                normalized['queries'] = [_stem_token(queries)]\n            elif isinstance(queries, (list, tuple)):\n                normalized['queries'] = [_stem_token(str(q)) for q in queries]\n            else:\n                normalized['queries'] = [_stem_token(str(queries))]\n\n        # Normalize other common fields\n        string_fields = ['language', 'under', 'kind', 'symbol', 'ext', 'not', 'case',\n                        'path_regex', 'collection', 'vector_name']\n        for field in string_fields:\n            if field in request_data:\n                normalized[field] = _stem_token(str(request_data[field]))\n\n        # Normalize list fields\n        list_fields = ['path_glob', 'not_glob']\n        for field in list_fields:\n            if field in request_data:\n                value = request_data[field]\n                if isinstance(value, str):\n                    normalized[field] = [_stem_token(value)]\n                elif isinstance(value, (list, tuple)):\n                    normalized[field] = [_stem_token(str(v)) for v in value]\n                else:\n                    normalized[field] = [_stem_token(str(value))]\n\n        # Copy numeric and boolean fields as-is\n        numeric_fields = ['limit', 'per_path']\n        for field in numeric_fields:\n            if field in request_data:\n                try:\n                    normalized[field] = int(request_data[field])\n                except (ValueError, TypeError):\n                    normalized[field] = request_data[field]\n\n        bool_fields = ['expand']\n        for field in bool_fields:\n            if field in request_data:\n                normalized[field] = bool(request_data[field])\n\n        return normalized",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__stem_token_157": {
      "name": "_stem_token",
      "type": "method",
      "start_line": 157,
      "end_line": 166,
      "content_hash": "99ced8b89de01ba55b7bb66ec2cb91baa8cd9f4d",
      "content": "        def _stem_token(tok: str) -> str:\n            t = tok.lower().strip()\n            # minimal stemming: queries -> query, classes -> class, etc.\n            if t.endswith(\"ies\") and len(t) > 4:\n                t = t[:-3] + \"y\"\n            elif t.endswith(\"es\") and len(t) > 3:\n                t = t[:-2]\n            elif t.endswith(\"s\") and len(t) > 3:\n                t = t[:-1]\n            return t",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__calculate_similarity_215": {
      "name": "_calculate_similarity",
      "type": "method",
      "start_line": 215,
      "end_line": 240,
      "content_hash": "703795a004f7a62e5d00f3dc1307e908b43d4bd8",
      "content": "    def _calculate_similarity(self, cand_norm: Dict[str, Any], exist_norm: Dict[str, Any]) -> float:\n        \"\"\"Calculate similarity between two normalized request dicts.\n        Uses Jaccard over flattened key:value tokens. For exact_match, only exact equality returns 1.0.\n        \"\"\"\n        if self.exact_match:\n            return 1.0 if cand_norm == exist_norm else 0.0\n\n        def _flatten(d: Dict[str, Any]) -> set:\n            toks = set()\n            for k, v in d.items():\n                if isinstance(v, list):\n                    toks.update(f\"{k}:{str(it)}\" for it in v)\n                else:\n                    toks.add(f\"{k}:{str(v)}\")\n            return toks\n        try:\n            s1, s2 = _flatten(cand_norm), _flatten(exist_norm)\n            if not s1 and not s2:\n                return 1.0\n            if not s1 or not s2:\n                return 0.0\n            inter = len(s1 & s2)\n            union = len(s1 | s2)\n            return inter / union if union else 0.0\n        except Exception:\n            return 0.0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__flatten_222": {
      "name": "_flatten",
      "type": "method",
      "start_line": 222,
      "end_line": 229,
      "content_hash": "5cae2f78a6d9e7764c159fdaf0ab08af27cd6384",
      "content": "        def _flatten(d: Dict[str, Any]) -> set:\n            toks = set()\n            for k, v in d.items():\n                if isinstance(v, list):\n                    toks.update(f\"{k}:{str(it)}\" for it in v)\n                else:\n                    toks.add(f\"{k}:{str(v)}\")\n            return toks",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__find_similar_requests_242": {
      "name": "_find_similar_requests",
      "type": "method",
      "start_line": 242,
      "end_line": 256,
      "content_hash": "bd5d8b340a41f1406b55dd762b1ed82c490a4d61",
      "content": "    def _find_similar_requests(self, candidate_fp: str, candidate_norm: Dict[str, Any]) -> List[str]:\n        \"\"\"Find requests similar to the given normalized candidate.\"\"\"\n        if self.exact_match:\n            return [candidate_fp] if candidate_fp in self._fingerprints else []\n\n        similar: List[str] = []\n        for existing_fp, obj in self._fingerprints.items():\n            try:\n                sim = self._calculate_similarity(candidate_norm, obj.request_data)\n            except Exception:\n                sim = 0.0\n            if sim >= self.similarity_threshold:\n                similar.append(existing_fp)\n\n        return similar",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__cleanup_worker_258": {
      "name": "_cleanup_worker",
      "type": "method",
      "start_line": 258,
      "end_line": 265,
      "content_hash": "dacf8d5d73e96da26675c93cf164894c5b56d9b7",
      "content": "    def _cleanup_worker(self) -> None:\n        \"\"\"Background worker for periodic cleanup.\"\"\"\n        while True:\n            try:\n                time.sleep(self.cleanup_interval)\n                self._cleanup_expired()\n            except Exception as e:\n                logger.error(f\"Deduplication cleanup error: {e}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__cleanup_expired_267": {
      "name": "_cleanup_expired",
      "type": "method",
      "start_line": 267,
      "end_line": 286,
      "content_hash": "892a157667569d879e9145bd1ec9dcdd1c4a116d",
      "content": "    def _cleanup_expired(self) -> None:\n        \"\"\"Clean up expired request fingerprints.\"\"\"\n        current_time = time.time()\n        expired_keys = []\n        \n        for key, fp in self._fingerprints.items():\n            if fp.is_expired(self.dedup_window_seconds):\n                expired_keys.append(key)\n        \n        with self._lock:\n            for key in expired_keys:\n                del self._fingerprints[key]\n                # Remove from access order\n                try:\n                    self._access_order.remove(key)\n                except ValueError:\n                    pass\n        \n        if expired_keys:\n            logger.debug(f\"Cleaned up {len(expired_keys)} expired request fingerprints\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__evict_if_needed_288": {
      "name": "_evict_if_needed",
      "type": "method",
      "start_line": 288,
      "end_line": 297,
      "content_hash": "c83d3c81250068b5e47776526fc56b91679b4e90",
      "content": "    def _evict_if_needed(self) -> None:\n        \"\"\"Evict oldest entries if cache is full.\"\"\"\n        with self._lock:\n            while len(self._fingerprints) >= self.max_cache_size:\n                try:\n                    oldest_key = self._access_order.popleft()\n                    if oldest_key in self._fingerprints:\n                        del self._fingerprints[oldest_key]\n                except (IndexError, KeyError):\n                    break",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_is_duplicate_299": {
      "name": "is_duplicate",
      "type": "method",
      "start_line": 299,
      "end_line": 365,
      "content_hash": "16e07ab02e2fd9c30d37484d610ab0b4728c8dc7",
      "content": "    def is_duplicate(self, request_data: Dict[str, Any]) -> Tuple[bool, Optional[str]]:\n        \"\"\"\n        Check if request is a duplicate.\n\n        Returns:\n            Tuple of (is_duplicate, similar_fingerprint)\n        \"\"\"\n        # Normalize request data (no lock needed)\n        normalized_data = self._normalize_request_data(request_data)\n\n        # Generate candidate fingerprint (not inserted yet)\n        candidate_obj = RequestFingerprint(normalized_data)\n        candidate_fp = candidate_obj.fingerprint\n\n        with self._lock:\n            # Synchronous TTL cleanup to avoid stale matches in tight loops\n            expired_keys = [k for k, fp in self._fingerprints.items() if fp.is_expired(self.dedup_window_seconds)]\n            for k in expired_keys:\n                try:\n                    del self._fingerprints[k]\n                    try:\n                        self._access_order.remove(k)\n                    except ValueError:\n                        pass\n                except Exception:\n                    pass\n            if expired_keys:\n                # Keep cache_size accurate after purge\n                self._stats['cache_size'] = len(self._fingerprints)\n\n            self._stats['total_requests'] += 1\n\n            # Find similar requests using normalized representation\n            similar_fingerprints = self._find_similar_requests(candidate_fp, normalized_data)\n\n            if similar_fingerprints:\n                # Found similar request(s)\n                similar_fp = similar_fingerprints[0]  # Use first match\n\n                # Update access statistics\n                self._fingerprints[similar_fp].access()\n\n                # Update access order for LRU\n                try:\n                    self._access_order.remove(similar_fp)\n                    self._access_order.append(similar_fp)\n                except ValueError:\n                    self._access_order.append(similar_fp)\n\n                self._stats['deduped_requests'] += 1\n                self._stats['cache_hits'] += 1\n\n                logger.debug(f\"Request deduplicated: {candidate_fp[:8]}... matches {similar_fp[:8]}...\")\n                return True, similar_fp\n            else:\n                # New unique request\n                self._evict_if_needed()\n\n                # Store new fingerprint\n                self._fingerprints[candidate_fp] = candidate_obj\n                self._access_order.append(candidate_fp)\n\n                self._stats['unique_requests'] += 1\n                self._stats['cache_size'] = len(self._fingerprints)\n\n                logger.debug(f\"New unique request: {candidate_fp[:8]}...\")\n                return False, None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_stats_367": {
      "name": "get_stats",
      "type": "method",
      "start_line": 367,
      "end_line": 379,
      "content_hash": "97b6c69e840836723c9c6c7f0fb9065648ab99d8",
      "content": "    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get deduplication statistics.\"\"\"\n        with self._lock:\n            stats = self._stats.copy()\n            total = stats['total_requests']\n            if total > 0:\n                stats['dedup_rate'] = round((stats['deduped_requests'] / total) * 100, 2)\n            else:\n                stats['dedup_rate'] = 0.0\n\n            stats['cache_utilization'] = (stats['cache_size'] / max(1, self.max_cache_size)) * 100\n\n            return stats",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_clear_cache_381": {
      "name": "clear_cache",
      "type": "method",
      "start_line": 381,
      "end_line": 396,
      "content_hash": "3de34f0bd2b62d8ef8bd785a1e2806d0cd24a335",
      "content": "    def clear_cache(self) -> None:\n        \"\"\"Clear all request fingerprints and reset statistics.\"\"\"\n        with self._lock:\n            self._fingerprints.clear()\n            self._access_order.clear()\n            # Reset statistics\n            self._stats = {\n                'total_requests': 0,\n                'deduped_requests': 0,\n                'unique_requests': 0,\n                'cache_hits': 0,\n                'cache_size': 0,\n                'dedup_rate': 0.0\n            }\n\n        logger.debug(\"Cleared request deduplication cache\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_cache_size_398": {
      "name": "get_cache_size",
      "type": "method",
      "start_line": 398,
      "end_line": 401,
      "content_hash": "811a29bbc61e01a4fe1bf949a575d13b26fea6cc",
      "content": "    def get_cache_size(self) -> int:\n        \"\"\"Get current cache size.\"\"\"\n        with self._lock:\n            return len(self._fingerprints)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___len___403": {
      "name": "__len__",
      "type": "method",
      "start_line": 403,
      "end_line": 405,
      "content_hash": "8d8309b357fe4fc7a04ebaff41d58fe3ade38e16",
      "content": "    def __len__(self) -> int:\n        \"\"\"Get current cache size.\"\"\"\n        return self.get_cache_size()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_deduplicator_413": {
      "name": "get_deduplicator",
      "type": "function",
      "start_line": 413,
      "end_line": 434,
      "content_hash": "ec0bf8d5eefd88edbca2380045912d5e14fbce83",
      "content": "def get_deduplicator() -> RequestDeduplicator:\n    \"\"\"Get or create the global request deduplicator.\"\"\"\n    global _deduplicator\n    \n    with _deduplicator_lock:\n        if _deduplicator is None:\n            # Configure from environment\n            dedup_window = int(os.environ.get(\"DEDUP_WINDOW_SECONDS\", \"60\"))\n            max_cache_size = int(os.environ.get(\"DEDUP_MAX_CACHE_SIZE\", \"10000\"))\n            cleanup_interval = float(os.environ.get(\"DEDUP_CLEANUP_INTERVAL\", \"30\"))\n            exact_match = os.environ.get(\"DEDUP_EXACT_MATCH\", \"1\").lower() in {\"1\", \"true\", \"yes\"}\n            similarity_threshold = float(os.environ.get(\"DEDUP_SIMILARITY_THRESHOLD\", \"0.9\"))\n            \n            _deduplicator = RequestDeduplicator(\n                dedup_window_seconds=dedup_window,\n                max_cache_size=max_cache_size,\n                cleanup_interval=cleanup_interval,\n                exact_match=exact_match,\n                similarity_threshold=similarity_threshold\n            )\n    \n    return _deduplicator",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_is_duplicate_request_437": {
      "name": "is_duplicate_request",
      "type": "function",
      "start_line": 437,
      "end_line": 448,
      "content_hash": "dbd1f6532a4d1bce9fa116456017e628dbea3247",
      "content": "def is_duplicate_request(request_data: Dict[str, Any]) -> Tuple[bool, Optional[str]]:\n    \"\"\"\n    Check if a request is a duplicate using the global deduplicator.\n    \n    Args:\n        request_data: Dictionary containing request parameters\n        \n    Returns:\n        Tuple of (is_duplicate, similar_fingerprint)\n    \"\"\"\n    deduplicator = get_deduplicator()\n    return deduplicator.is_duplicate(request_data)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_deduplication_stats_451": {
      "name": "get_deduplication_stats",
      "type": "function",
      "start_line": 451,
      "end_line": 454,
      "content_hash": "dc85971e1bf4456500d322a360460bbead86aec8",
      "content": "def get_deduplication_stats() -> Dict[str, Any]:\n    \"\"\"Get deduplication statistics.\"\"\"\n    deduplicator = get_deduplicator()\n    return deduplicator.get_stats()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_clear_deduplication_cache_457": {
      "name": "clear_deduplication_cache",
      "type": "function",
      "start_line": 457,
      "end_line": 460,
      "content_hash": "73d3175b2239feaf9adc9945194d7c57a69287fe",
      "content": "def clear_deduplication_cache() -> None:\n    \"\"\"Clear the deduplication cache.\"\"\"\n    deduplicator = get_deduplicator()\n    deduplicator.clear_cache()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_deduplicate_request_464": {
      "name": "deduplicate_request",
      "type": "function",
      "start_line": 464,
      "end_line": 497,
      "content_hash": "1461af1b5a3910a0b9a3488ee2777e11ce08bcdc",
      "content": "def deduplicate_request(ttl: Optional[float] = None):\n    \"\"\"\n    Decorator to automatically deduplicate function calls.\n\n    Args:\n        ttl: Time-to-live for deduplication in seconds (scoped to this function)\n    \"\"\"\n    def decorator(func):\n        # Per-function deduplicator so TTL applies locally and predictably in tests/usages\n        window = int(ttl) if ttl is not None else int(os.environ.get(\"DEDUP_WINDOW_SECONDS\", \"60\"))\n        local_dedup = RequestDeduplicator(name=f\"func:{func.__name__}\", dedup_window_seconds=window)\n\n        def wrapper(*args, **kwargs):\n            # Build request data from function arguments\n            request_data = {\n                'function': func.__name__,\n                'args': list(args),\n                'kwargs': {k: v for k, v in kwargs.items()}\n            }\n\n            # Check for duplicates using the local deduplicator\n            is_dup, _ = local_dedup.is_duplicate(request_data)\n            if is_dup:\n                logger.debug(f\"Function call deduplicated: {func.__name__}\")\n                # Return None for duplicates (callers can treat as no-op)\n                return None\n\n            # Execute function\n            return func(*args, **kwargs)\n\n        wrapper._deduplicated = True\n        return wrapper\n\n    return decorator",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_decorator_471": {
      "name": "decorator",
      "type": "function",
      "start_line": 471,
      "end_line": 495,
      "content_hash": "398eda1dfa7cf978b21f74c710cf937ad8c927e4",
      "content": "    def decorator(func):\n        # Per-function deduplicator so TTL applies locally and predictably in tests/usages\n        window = int(ttl) if ttl is not None else int(os.environ.get(\"DEDUP_WINDOW_SECONDS\", \"60\"))\n        local_dedup = RequestDeduplicator(name=f\"func:{func.__name__}\", dedup_window_seconds=window)\n\n        def wrapper(*args, **kwargs):\n            # Build request data from function arguments\n            request_data = {\n                'function': func.__name__,\n                'args': list(args),\n                'kwargs': {k: v for k, v in kwargs.items()}\n            }\n\n            # Check for duplicates using the local deduplicator\n            is_dup, _ = local_dedup.is_duplicate(request_data)\n            if is_dup:\n                logger.debug(f\"Function call deduplicated: {func.__name__}\")\n                # Return None for duplicates (callers can treat as no-op)\n                return None\n\n            # Execute function\n            return func(*args, **kwargs)\n\n        wrapper._deduplicated = True\n        return wrapper",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_wrapper_476": {
      "name": "wrapper",
      "type": "function",
      "start_line": 476,
      "end_line": 492,
      "content_hash": "ce0016b50e941c6bafbcc808191cee589b53c987",
      "content": "        def wrapper(*args, **kwargs):\n            # Build request data from function arguments\n            request_data = {\n                'function': func.__name__,\n                'args': list(args),\n                'kwargs': {k: v for k, v in kwargs.items()}\n            }\n\n            # Check for duplicates using the local deduplicator\n            is_dup, _ = local_dedup.is_duplicate(request_data)\n            if is_dup:\n                logger.debug(f\"Function call deduplicated: {func.__name__}\")\n                # Return None for duplicates (callers can treat as no-op)\n                return None\n\n            # Execute function\n            return func(*args, **kwargs)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}