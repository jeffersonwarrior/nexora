{
  "file_path": "/work/external-deps/Context-Engine/scripts/memory_backup.py",
  "file_hash": "2410732648ae9679968e22db5eeac79210990001",
  "updated_at": "2025-12-26T17:34:20.773491",
  "symbols": {
    "function_get_qdrant_client_35": {
      "name": "get_qdrant_client",
      "type": "function",
      "start_line": 35,
      "end_line": 40,
      "content_hash": "879aa7743f523b2a5158e0e5a71abe2b7992363f",
      "content": "def get_qdrant_client() -> QdrantClient:\n    \"\"\"Initialize Qdrant client with environment configuration.\"\"\"\n    qdrant_url = os.environ.get(\"QDRANT_URL\", \"http://localhost:6333\")\n    api_key = os.environ.get(\"QDRANT_API_KEY\")\n\n    return QdrantClient(url=qdrant_url, api_key=api_key or None)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_is_memory_point_43": {
      "name": "is_memory_point",
      "type": "function",
      "start_line": 43,
      "end_line": 77,
      "content_hash": "91e25a6a0edf0db2b0a67002bb9ea8434ae7802e",
      "content": "def is_memory_point(payload: Dict[str, Any]) -> bool:\n    \"\"\"\n    Determine if a point is a memory (user-added) rather than code-indexed content.\n\n    Memory points typically:\n    - Have no 'path' in metadata (not tied to a file)\n    - May have 'source' set to 'memory'\n    - Have 'content' field that's not extracted from code\n\n    Args:\n        payload: Point payload from Qdrant\n\n    Returns:\n        True if this appears to be a memory point, False if it's code content\n    \"\"\"\n    if not payload:\n        return False\n\n    metadata = payload.get(\"metadata\", {})\n\n    # Primary indicator: no file path means it's likely a memory\n    if not metadata.get(\"path\"):\n        return True\n\n    # Secondary indicator: explicit source marking\n    if metadata.get(\"source\") == \"memory\":\n        return True\n\n    # Tertiary: content-based heuristics\n    content = payload.get(\"information\", \"\")\n    if content and not metadata.get(\"language\") and not metadata.get(\"kind\"):\n        # Content without language/kind metadata is likely user-added\n        return True\n\n    return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_export_memories_80": {
      "name": "export_memories",
      "type": "function",
      "start_line": 80,
      "end_line": 223,
      "content_hash": "2808278dfdba81fef2f0eb398d5f0d7145d78758",
      "content": "def export_memories(\n    collection_name: str,\n    output_file: str,\n    client: Optional[QdrantClient] = None,\n    include_vectors: bool = True,\n    batch_size: int = 1000\n) -> Dict[str, Any]:\n    \"\"\"\n    Export memories from a Qdrant collection to JSON.\n\n    Args:\n        collection_name: Qdrant collection name\n        output_file: Output JSON file path\n        client: Qdrant client instance (will create if None)\n        include_vectors: Whether to include vector embeddings in backup\n        batch_size: Number of points to fetch per request\n\n    Returns:\n        Dict with backup statistics\n    \"\"\"\n    if client is None:\n        client = get_qdrant_client()\n\n    # Verify collection exists\n    try:\n        collections = client.get_collections().collections\n        if collection_name not in [c.name for c in collections]:\n            raise ValueError(f\"Collection '{collection_name}' not found\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to access Qdrant: {e}\")\n\n    print(f\"Exporting memories from collection: {collection_name}\")\n    print(f\"Output file: {output_file}\")\n\n    # Get all points from collection\n    all_points = []\n    total_count = 0\n    memory_count = 0\n\n    # Use scroll to get all points efficiently\n    next_page_offset = None\n    while True:\n        points, next_page_offset = client.scroll(\n            collection_name=collection_name,\n            offset=next_page_offset,\n            limit=batch_size,\n            with_payload=True,\n            with_vectors=include_vectors\n        )\n\n        if not points:\n            break\n\n        all_points.extend(points)\n        total_count += len(points)\n\n        # Filter for memory points\n        memory_points = []\n        for point in points:\n            if is_memory_point(point.payload or {}):\n                memory_points.append(point)\n                memory_count += 1\n\n        print(f\"Fetched {len(points)} points (total: {total_count}), found {len(memory_points)} memories (total: {memory_count})\")\n\n        if next_page_offset is None:\n            break\n\n    if memory_count == 0:\n        print(\"No memories found in collection!\")\n        return {\n            \"collection\": collection_name,\n            \"total_points\": total_count,\n            \"memory_count\": 0,\n            \"backup_file\": output_file,\n            \"success\": True\n        }\n\n    # Prepare backup data\n    backup_data = {\n        \"backup_info\": {\n            \"collection_name\": collection_name,\n            \"export_date\": datetime.now().isoformat(),\n            \"total_points_exported\": total_count,\n            \"memory_points_found\": memory_count,\n            \"include_vectors\": include_vectors,\n            \"vector_dimension\": None  # Will be set if vectors included\n        },\n        \"memories\": []\n    }\n\n    # Process memory points\n    for point in all_points:\n        if not is_memory_point(point.payload or {}):\n            continue\n\n        payload = point.payload or {}\n        memory_entry = {\n            \"id\": str(point.id),\n            \"content\": payload.get(\"information\", \"\"),\n            \"metadata\": payload.get(\"metadata\", {}),\n        }\n\n        # Include vector if requested\n        if include_vectors and point.vector:\n            if hasattr(point.vector, 'tolist'):\n                memory_entry[\"vector\"] = point.vector.tolist()\n            else:\n                memory_entry[\"vector\"] = point.vector\n\n            # Set vector dimension from first memory\n            if backup_data[\"backup_info\"][\"vector_dimension\"] is None:\n                vector_data = memory_entry[\"vector\"]\n                if isinstance(vector_data, dict):\n                    # Named vector format: {\"memory\": [values]}\n                    first_vector = next(iter(vector_data.values()))\n                    backup_data[\"backup_info\"][\"vector_dimension\"] = len(first_vector)\n                else:\n                    # Direct vector list format\n                    backup_data[\"backup_info\"][\"vector_dimension\"] = len(vector_data)\n\n        backup_data[\"memories\"].append(memory_entry)\n\n    # Write backup file\n    output_path = Path(output_file)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(output_path, 'w') as f:\n        json.dump(backup_data, f, indent=2)\n\n    print(f\"\u2705 Backup completed successfully!\")\n    print(f\"   Total points processed: {total_count}\")\n    print(f\"   Memory points exported: {memory_count}\")\n    print(f\"   Backup file: {output_path}\")\n    print(f\"   File size: {output_path.stat().st_size / 1024:.1f} KB\")\n\n    return {\n        \"collection\": collection_name,\n        \"total_points\": total_count,\n        \"memory_count\": memory_count,\n        \"backup_file\": str(output_path),\n        \"file_size\": output_path.stat().st_size,\n        \"success\": True\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_list_collections_226": {
      "name": "list_collections",
      "type": "function",
      "start_line": 226,
      "end_line": 238,
      "content_hash": "e588caa92e0b45ab095b377415c5753e7ccca501",
      "content": "def list_collections() -> None:\n    \"\"\"List all available Qdrant collections.\"\"\"\n    client = get_qdrant_client()\n\n    try:\n        collections = client.get_collections().collections\n        print(\"Available collections:\")\n        for collection in collections:\n            info = client.get_collection(collection.name)\n            point_count = info.points_count\n            print(f\"  - {collection.name} ({point_count:,} points)\")\n    except Exception as e:\n        print(f\"Error listing collections: {e}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_main_241": {
      "name": "main",
      "type": "function",
      "start_line": 241,
      "end_line": 315,
      "content_hash": "59fcd2942df7b6bb50a67443dfffb75c86f39b0e",
      "content": "def main():\n    parser = argparse.ArgumentParser(\n        description=\"Backup memories (non-code points) from Qdrant collections\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  %(prog)s --collection test-repo-58ecbbc8 --output memories_backup.json\n  %(prog)s --list-collections\n  %(prog)s --collection test-repo-58ecbbc8 --output backup_$(date +%Y%m%d_%H%M%S).json --no-vectors\n        \"\"\"\n    )\n\n    parser.add_argument(\n        \"--collection\", \"-c\",\n        required=False,\n        help=\"Qdrant collection name to backup memories from\"\n    )\n\n    parser.add_argument(\n        \"--output\", \"-o\",\n        help=\"Output JSON file path for backup\"\n    )\n\n    parser.add_argument(\n        \"--list-collections\", \"-l\",\n        action=\"store_true\",\n        help=\"List all available collections\"\n    )\n\n    parser.add_argument(\n        \"--no-vectors\",\n        action=\"store_true\",\n        help=\"Don't include vector embeddings in backup (smaller file, requires re-embedding)\"\n    )\n\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=1000,\n        help=\"Number of points to fetch per request (default: 1000)\"\n    )\n\n    args = parser.parse_args()\n\n    if args.list_collections:\n        list_collections()\n        return\n\n    if not args.collection:\n        parser.error(\"--collection required unless using --list-collections\")\n\n    if not args.output:\n        # Generate default filename with timestamp\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        args.output = f\"{args.collection}_memories_{timestamp}.json\"\n\n    try:\n        result = export_memories(\n            collection_name=args.collection,\n            output_file=args.output,\n            include_vectors=not args.no_vectors,\n            batch_size=args.batch_size\n        )\n\n        if result[\"success\"]:\n            print(f\"\\n\ud83c\udf89 Memory backup completed successfully!\")\n            if result[\"memory_count\"] == 0:\n                print(\"   (No memories found to backup)\")\n        else:\n            print(f\"\\n\u274c Memory backup failed!\")\n            sys.exit(1)\n\n    except Exception as e:\n        print(f\"\\n\u274c Error during backup: {e}\")\n        sys.exit(1)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}