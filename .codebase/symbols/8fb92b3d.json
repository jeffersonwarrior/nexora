{
  "file_path": "/work/external-deps/Context-Engine/scripts/ingest_code.py",
  "file_hash": "bb00b8ed7a7b506f134a4248941a8b8ec68b2444",
  "updated_at": "2025-12-26T17:34:19.888814",
  "symbols": {
    "function__detect_repo_name_from_path_5": {
      "name": "_detect_repo_name_from_path",
      "type": "function",
      "start_line": 5,
      "end_line": 12,
      "content_hash": "611e92aa6fad54c78bbb754958caefbd17a89b9e",
      "content": "def _detect_repo_name_from_path(path: Path) -> str:\n    \"\"\"Wrapper function to use workspace_state repository detection.\"\"\"\n    try:\n        from scripts.workspace_state import _extract_repo_name_from_path as _ws_detect\n        return _ws_detect(str(path))\n    except ImportError:\n        # Fallback for when workspace_state is not available\n        return path.name if path.is_dir() else path.parent.name",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_logical_repo_reuse_enabled_71": {
      "name": "logical_repo_reuse_enabled",
      "type": "function",
      "start_line": 71,
      "end_line": 72,
      "content_hash": "09002e2eb032a9c2f1c9d162354d2cf63ff84751",
      "content": "    def logical_repo_reuse_enabled() -> bool:  # type: ignore[no-redef]\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__set_optional_ws_func_110": {
      "name": "_set_optional_ws_func",
      "type": "function",
      "start_line": 110,
      "end_line": 111,
      "content_hash": "11cd2dcdfd819858b6bbed59f465adb525d926f2",
      "content": "    def _set_optional_ws_func(name: str) -> None:\n        globals()[name] = None  # type: ignore[attr-defined]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__clear_indexing_caches_for_run_147": {
      "name": "_clear_indexing_caches_for_run",
      "type": "function",
      "start_line": 147,
      "end_line": 186,
      "content_hash": "5a531e27eaf3b77ce01b07c7c44fb207ecd4f535",
      "content": "def _clear_indexing_caches_for_run(\n    workspace_path: str,\n    repo_name: Optional[str],\n) -> None:\n    \"\"\"Best-effort removal of file-hash + symbol caches before a rebuild.\"\"\"\n    actions: list[str] = []\n    if not workspace_path:\n        workspace_path = os.environ.get(\"WORKSPACE_PATH\") or os.environ.get(\"WATCH_ROOT\") or \"/work\"\n\n    multi_repo = False\n    try:\n        if is_multi_repo_mode:\n            multi_repo = bool(is_multi_repo_mode())\n    except Exception:\n        multi_repo = False\n\n    # Clear file-hash cache\n    try:\n        if multi_repo and repo_name and clear_repo_cache:\n            if clear_repo_cache(repo_name):\n                actions.append(f\"file-hash cache (repo={repo_name})\")\n        elif clear_cache:\n            if clear_cache(workspace_path):\n                actions.append(\"file-hash cache\")\n    except Exception as e:\n        print(f\"[clear_caches] Failed to clear file hash cache: {e}\")\n\n    # Clear symbol cache\n    try:\n        if clear_symbol_cache:\n            cleared_dirs = clear_symbol_cache(workspace_path, repo_name)\n            if cleared_dirs:\n                actions.append(f\"symbol cache ({cleared_dirs} dirs)\")\n    except Exception as e:\n        print(f\"[clear_caches] Failed to clear symbol cache: {e}\")\n\n    if actions:\n        print(f\"[clear_caches] Cleared {'; '.join(actions)}\")\n    else:\n        print(\"[clear_caches] No caches were cleared (helpers unavailable or already empty)\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__mark_staging_idle_190": {
      "name": "_mark_staging_idle",
      "type": "function",
      "start_line": 190,
      "end_line": 216,
      "content_hash": "e65ce7130f7057afbc743ecddce842c0bf5637d8",
      "content": "def _mark_staging_idle(target_ws: str, repo_name: str, files_indexed: int) -> None:\n    if not (get_workspace_state and update_workspace_state):\n        return\n    try:\n        st = get_workspace_state(target_ws, repo_name) or {}\n        staging = st.get(\"staging\") or {}\n        if not (isinstance(staging, dict) and staging.get(\"collection\")):\n            return\n        status = staging.get(\"status\") or {}\n        if not isinstance(status, dict):\n            status = {}\n        status.setdefault(\"started_at\", status.get(\"started_at\") or datetime.now().isoformat())\n        status[\"state\"] = \"idle\"\n        status[\"progress\"] = {\n            \"files_processed\": files_indexed,\n            \"total_files\": None,\n            \"current_file\": None,\n        }\n        staging[\"status\"] = status\n        staging[\"updated_at\"] = datetime.now().isoformat()\n        update_workspace_state(\n            workspace_path=target_ws,\n            repo_name=repo_name,\n            updates={\"staging\": staging},\n        )\n    except Exception:\n        pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__load_ts_language_224": {
      "name": "_load_ts_language",
      "type": "function",
      "start_line": 224,
      "end_line": 252,
      "content_hash": "05e09322b907a239e1a325d52e62f64ce8e3518b",
      "content": "    def _load_ts_language(mod: Any, *, preferred: list[str] | None = None) -> Any | None:\n        \"\"\"Return a tree-sitter Language instance from a per-language package.\n\n        Different packages expose different entrypoints (e.g. language(),\n        language_typescript(), language_tsx()).\n        \"\"\"\n        preferred = preferred or []\n        candidates: list[Any] = []\n        if getattr(mod, \"language\", None) is not None and callable(getattr(mod, \"language\")):\n            candidates.append(getattr(mod, \"language\"))\n        for name in preferred:\n            fn = getattr(mod, name, None)\n            if fn is not None and callable(fn):\n                candidates.append(fn)\n        # Last resort: scan for any callable language* attribute\n        for name in dir(mod):\n            if not name.startswith(\"language\"):\n                continue\n            fn = getattr(mod, name, None)\n            if fn is not None and callable(fn):\n                candidates.append(fn)\n\n        for fn in candidates:\n            try:\n                raw_lang = fn()\n                return raw_lang if isinstance(raw_lang, Language) else Language(raw_lang)\n            except Exception:\n                continue\n        return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__use_tree_sitter_318": {
      "name": "_use_tree_sitter",
      "type": "function",
      "start_line": 318,
      "end_line": 331,
      "content_hash": "66a999c6a9012115b6de0dd9df47bdd707eb39d4",
      "content": "def _use_tree_sitter() -> bool:\n    global _TS_WARNED\n    val = os.environ.get(\"USE_TREE_SITTER\")\n    # Default ON when libs are available; allow explicit disable via 0/false\n    if val is None or str(val).strip() == \"\":\n        want = True\n    else:\n        want = str(val).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n    if want and not _TS_AVAILABLE and not _TS_WARNED:\n        print(\n            \"[WARN] USE_TREE_SITTER=1 but tree-sitter libs not available; falling back to regex heuristics\"\n        )\n        _TS_WARNED = True\n    return _TS_AVAILABLE and want",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__safe_int_env_432": {
      "name": "_safe_int_env",
      "type": "function",
      "start_line": 432,
      "end_line": 437,
      "content_hash": "98d3b01322dc15c9412cacf67296412ecc9f74c9",
      "content": "def _safe_int_env(key: str, default: int) -> int:\n    try:\n        val = os.environ.get(key)\n        return int(val) if val else default\n    except (ValueError, TypeError):\n        return default",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_mini_proj_472": {
      "name": "_get_mini_proj",
      "type": "function",
      "start_line": 472,
      "end_line": 489,
      "content_hash": "55186d7b91aaa36d72d1f962d0d2160f6ef40d6e",
      "content": "def _get_mini_proj(\n    in_dim: int, out_dim: int, seed: int | None = None\n) -> list[list[float]]:\n    import math, random\n\n    s = int(os.environ.get(\"MINI_VEC_SEED\", \"1337\")) if seed is None else int(seed)\n    key = (in_dim, out_dim, s)\n    M = _MINI_PROJ_CACHE.get(key)\n    if M is None:\n        rnd = random.Random(s)\n        scale = 1.0 / math.sqrt(out_dim)\n        # Dense Rademacher matrix (+/-1) scaled; good enough for fast gating\n        M = [\n            [scale * (1.0 if rnd.random() < 0.5 else -1.0) for _ in range(out_dim)]\n            for _ in range(in_dim)\n        ]\n        _MINI_PROJ_CACHE[key] = M\n    return M",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_project_mini_492": {
      "name": "project_mini",
      "type": "function",
      "start_line": 492,
      "end_line": 509,
      "content_hash": "41ef0bb0065a9bdadb7a36a3b24d707678fbdc68",
      "content": "def project_mini(vec: list[float], out_dim: int | None = None) -> list[float]:\n    import math\n\n    if not vec:\n        return [0.0] * (int(out_dim or MINI_VEC_DIM))\n    od = int(out_dim or MINI_VEC_DIM)\n    M = _get_mini_proj(len(vec), od)\n    out = [0.0] * od\n    # y = x @ M\n    for i, val in enumerate(vec):\n        if val == 0.0:\n            continue\n        row = M[i]\n        for j in range(od):\n            out[j] += val * row[j]\n    # L2 normalize to keep scale consistent\n    norm = (sum(x * x for x in out) or 0.0) ** 0.5 or 1.0\n    return [x / norm for x in out]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__split_ident_lex_512": {
      "name": "_split_ident_lex",
      "type": "function",
      "start_line": 512,
      "end_line": 520,
      "content_hash": "4c4b18faabd1d1968d2ad9c37af56d76c2d06e3f",
      "content": "def _split_ident_lex(s: str):\n    parts = re.split(r\"[^A-Za-z0-9]+\", s)\n    out = []\n    for p in parts:\n        if not p:\n            continue\n        segs = re.findall(r\"[A-Z]?[a-z]+|[A-Z]+(?![a-z])|\\d+\", p)\n        out.extend([x for x in segs if x])\n    return [x.lower() for x in out if x and x.lower() not in _STOP]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__lex_hash_vector_523": {
      "name": "_lex_hash_vector",
      "type": "function",
      "start_line": 523,
      "end_line": 539,
      "content_hash": "a4ee746900bbcf037f1f16d7e927116ea57937d3",
      "content": "def _lex_hash_vector(text: str, dim: int = LEX_VECTOR_DIM) -> list[float]:\n    if not text:\n        return [0.0] * dim\n    vec = [0.0] * dim\n    # Tokenize identifiers & words\n    toks = _split_ident_lex(text)\n    if not toks:\n        return vec\n    for t in toks:\n        h = int(hashlib.md5(t.encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:8], 16)\n        idx = h % dim\n        vec[idx] += 1.0\n    # L2 normalize (avoid huge magnitudes)\n    import math\n\n    norm = math.sqrt(sum(v * v for v in vec)) or 1.0\n    return [v / norm for v in vec]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__git_metadata_542": {
      "name": "_git_metadata",
      "type": "function",
      "start_line": 542,
      "end_line": 579,
      "content_hash": "ae8812ea41aa03c0806ba51569b25c3284f4bcfd",
      "content": "def _git_metadata(file_path: Path) -> tuple[int, int, int]:\n    \"\"\"Return (last_modified_at, churn_count, author_count) using git when available.\n    Fallbacks to fs mtime and zeros when not in a repo.\n    \"\"\"\n    try:\n        import subprocess\n\n        fp = str(file_path)\n        # last commit unix timestamp (%ct)\n        ts = subprocess.run(\n            [\"git\", \"log\", \"-1\", \"--format=%ct\", \"--\", fp],\n            capture_output=True,\n            text=True,\n            cwd=file_path.parent,\n        ).stdout.strip()\n        last_ts = int(ts) if ts.isdigit() else int(file_path.stat().st_mtime)\n        # churn: number of commits touching this file (bounded)\n        churn_s = subprocess.run(\n            [\"git\", \"rev-list\", \"--count\", \"HEAD\", \"--\", fp],\n            capture_output=True,\n            text=True,\n            cwd=file_path.parent,\n        ).stdout.strip()\n        churn = int(churn_s) if churn_s.isdigit() else 0\n        # author count\n        authors = subprocess.run(\n            [\"git\", \"shortlog\", \"-s\", \"--\", fp],\n            capture_output=True,\n            text=True,\n            cwd=file_path.parent,\n        ).stdout\n        author_count = len([ln for ln in authors.splitlines() if ln.strip()])\n        return last_ts, churn, author_count\n    except Exception:\n        try:\n            return int(file_path.stat().st_mtime), 0, 0\n        except Exception:\n            return int(time.time()), 0, 0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__should_skip_explicit_file_by_excluder_624": {
      "name": "_should_skip_explicit_file_by_excluder",
      "type": "function",
      "start_line": 624,
      "end_line": 688,
      "content_hash": "ce9730db3c20e9bb69b100a4e311dbf341dec54c",
      "content": "def _should_skip_explicit_file_by_excluder(file_path: Path) -> bool:\n    try:\n        p = file_path if isinstance(file_path, Path) else Path(str(file_path))\n    except Exception:\n        return False\n\n    root = None\n    try:\n        parts = list(p.parts)\n        if \".remote-git\" in parts:\n            i = parts.index(\".remote-git\")\n            root = Path(*parts[:i]) if i > 0 else Path(\"/\")\n    except Exception:\n        root = None\n\n    if root is None:\n        try:\n            s = str(p)\n            if s.startswith(\"/work/\"):\n                slug = s[len(\"/work/\") :].split(\"/\", 1)[0]\n                root = (Path(\"/work\") / slug) if slug else None\n        except Exception:\n            root = None\n\n    if root is None:\n        try:\n            ws = (os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"\").strip()\n            if ws:\n                ws_path = Path(ws).resolve()\n                pr = p.resolve()\n                if pr == ws_path or ws_path in pr.parents:\n                    root = ws_path\n        except Exception:\n            root = None\n\n    if root is None:\n        try:\n            pr = p.resolve()\n            for anc in [pr.parent] + list(pr.parents):\n                if (anc / \".codebase\").exists():\n                    root = anc\n                    break\n        except Exception:\n            root = None\n\n    if not root or str(root) == \"/\":\n        return False\n\n    try:\n        rel = p.resolve().relative_to(root.resolve()).as_posix().lstrip(\"/\")\n    except Exception:\n        return False\n    if not rel:\n        return False\n\n    try:\n        excl = _Excluder(root)\n        cur = \"\"\n        for seg in [x for x in rel.split(\"/\") if x][:-1]:\n            cur = cur + \"/\" + seg\n            if excl.exclude_dir(cur):\n                return True\n        return excl.exclude_file(rel)\n    except Exception:\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__env_truthy_691": {
      "name": "_env_truthy",
      "type": "function",
      "start_line": 691,
      "end_line": 694,
      "content_hash": "4ba8b911ad3c1e8506b914f7510f211af28fa4bb",
      "content": "def _env_truthy(val: str | None, default: bool) -> bool:\n    if val is None:\n        return default\n    return val.strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_hash_id_697": {
      "name": "hash_id",
      "type": "function",
      "start_line": 697,
      "end_line": 701,
      "content_hash": "c433b1fc4b098f4649cb79aac76f6aafaf3fd6f2",
      "content": "def hash_id(text: str, path: str, start: int, end: int) -> int:\n    h = hashlib.sha1(\n        f\"{path}:{start}-{end}\\n{text}\".encode(\"utf-8\", errors=\"ignore\")\n    ).hexdigest()\n    return int(h[:16], 16)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class__Excluder_704": {
      "name": "_Excluder",
      "type": "class",
      "start_line": 704,
      "end_line": 785,
      "content_hash": "b2990a0d6b79dbe9566c37a0675957aa03756152",
      "content": "class _Excluder:\n    def __init__(self, root: Path):\n        self.root = root\n        self.dir_prefixes = []  # absolute like /path/sub\n        self.dir_globs = []  # fnmatch patterns for directory names\n        self.file_globs = []  # fnmatch patterns\n\n        # Defaults\n        use_defaults = _env_truthy(os.environ.get(\"QDRANT_DEFAULT_EXCLUDES\"), True)\n        if use_defaults:\n            self.dir_prefixes.extend(_DEFAULT_EXCLUDE_DIRS)\n            self.dir_globs.extend(_DEFAULT_EXCLUDE_DIR_GLOBS)\n            self.file_globs.extend(_DEFAULT_EXCLUDE_FILES)\n\n        # .qdrantignore\n        ignore_file = os.environ.get(\"QDRANT_IGNORE_FILE\", \".qdrantignore\")\n        ig_path = root / ignore_file\n        if ig_path.exists():\n            for raw in ig_path.read_text(\n                encoding=\"utf-8\", errors=\"ignore\"\n            ).splitlines():\n                line = raw.strip()\n                if not line or line.startswith(\"#\"):\n                    continue\n                self._add_pattern(line)\n\n        # Extra excludes via env (comma separated)\n        extra = os.environ.get(\"QDRANT_EXCLUDES\", \"\").strip()\n        if extra:\n            for pat in [p.strip() for p in extra.split(\",\") if p.strip()]:\n                self._add_pattern(pat)\n\n    def _add_pattern(self, pat: str):\n        # Normalize to leading-slash for prefixes\n        has_wild = any(ch in pat for ch in \"*?[\")\n        if pat.startswith(\"/\") and not has_wild:\n            # Treat as directory prefix if no wildcard\n            self.dir_prefixes.append(pat.rstrip(\"/\"))\n        else:\n            # Treat as file glob (match against relpath and basename)\n            self.file_globs.append(pat.lstrip(\"/\"))\n\n    def exclude_dir(self, rel: str) -> bool:\n        import fnmatch\n\n        # rel like /a/b\n        for pref in self.dir_prefixes:\n            if rel == pref or rel.startswith(pref + \"/\"):\n                return True\n\n        base = rel.rsplit(\"/\", 1)[-1]\n\n        # Match directory name against dir_globs (e.g., .venv*)\n        for g in self.dir_globs:\n            if fnmatch.fnmatch(base, g):\n                return True\n\n        # Treat single-segment dir prefixes (e.g. \"/.git\", \"/node_modules\") as\n        # \"exclude this directory name anywhere\". This matters when indexing a\n        # workspace root that contains multiple repos, e.g. /work/<repo>/.git.\n        try:\n            if base in _ANY_DEPTH_EXCLUDE_DIR_NAMES and (\"/\" + base) in self.dir_prefixes:\n                return True\n        except Exception:\n            pass\n\n        # Also allow dir name-only patterns in file_globs (e.g., node_modules)\n        for g in self.file_globs:\n            # Match bare dir names without wildcards\n            if g and all(ch not in g for ch in \"*?[\") and base == g:\n                return True\n        return False\n\n    def exclude_file(self, rel: str) -> bool:\n        import fnmatch\n\n        # Try matching whole rel path and basename\n        base = rel.rsplit(\"/\", 1)[-1]\n        for g in self.file_globs:\n            if fnmatch.fnmatch(rel.lstrip(\"/\"), g) or fnmatch.fnmatch(base, g):\n                return True\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___705": {
      "name": "__init__",
      "type": "method",
      "start_line": 705,
      "end_line": 734,
      "content_hash": "880b462d85ac6372667d5043bf58151d6a40338c",
      "content": "    def __init__(self, root: Path):\n        self.root = root\n        self.dir_prefixes = []  # absolute like /path/sub\n        self.dir_globs = []  # fnmatch patterns for directory names\n        self.file_globs = []  # fnmatch patterns\n\n        # Defaults\n        use_defaults = _env_truthy(os.environ.get(\"QDRANT_DEFAULT_EXCLUDES\"), True)\n        if use_defaults:\n            self.dir_prefixes.extend(_DEFAULT_EXCLUDE_DIRS)\n            self.dir_globs.extend(_DEFAULT_EXCLUDE_DIR_GLOBS)\n            self.file_globs.extend(_DEFAULT_EXCLUDE_FILES)\n\n        # .qdrantignore\n        ignore_file = os.environ.get(\"QDRANT_IGNORE_FILE\", \".qdrantignore\")\n        ig_path = root / ignore_file\n        if ig_path.exists():\n            for raw in ig_path.read_text(\n                encoding=\"utf-8\", errors=\"ignore\"\n            ).splitlines():\n                line = raw.strip()\n                if not line or line.startswith(\"#\"):\n                    continue\n                self._add_pattern(line)\n\n        # Extra excludes via env (comma separated)\n        extra = os.environ.get(\"QDRANT_EXCLUDES\", \"\").strip()\n        if extra:\n            for pat in [p.strip() for p in extra.split(\",\") if p.strip()]:\n                self._add_pattern(pat)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__add_pattern_736": {
      "name": "_add_pattern",
      "type": "method",
      "start_line": 736,
      "end_line": 744,
      "content_hash": "50cdaa3dc4e8a0d73ae38e44b207551380857702",
      "content": "    def _add_pattern(self, pat: str):\n        # Normalize to leading-slash for prefixes\n        has_wild = any(ch in pat for ch in \"*?[\")\n        if pat.startswith(\"/\") and not has_wild:\n            # Treat as directory prefix if no wildcard\n            self.dir_prefixes.append(pat.rstrip(\"/\"))\n        else:\n            # Treat as file glob (match against relpath and basename)\n            self.file_globs.append(pat.lstrip(\"/\"))",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_exclude_dir_746": {
      "name": "exclude_dir",
      "type": "method",
      "start_line": 746,
      "end_line": 775,
      "content_hash": "cc7440c4de7c8abd4d8d64acfb31f8bff5ffddfa",
      "content": "    def exclude_dir(self, rel: str) -> bool:\n        import fnmatch\n\n        # rel like /a/b\n        for pref in self.dir_prefixes:\n            if rel == pref or rel.startswith(pref + \"/\"):\n                return True\n\n        base = rel.rsplit(\"/\", 1)[-1]\n\n        # Match directory name against dir_globs (e.g., .venv*)\n        for g in self.dir_globs:\n            if fnmatch.fnmatch(base, g):\n                return True\n\n        # Treat single-segment dir prefixes (e.g. \"/.git\", \"/node_modules\") as\n        # \"exclude this directory name anywhere\". This matters when indexing a\n        # workspace root that contains multiple repos, e.g. /work/<repo>/.git.\n        try:\n            if base in _ANY_DEPTH_EXCLUDE_DIR_NAMES and (\"/\" + base) in self.dir_prefixes:\n                return True\n        except Exception:\n            pass\n\n        # Also allow dir name-only patterns in file_globs (e.g., node_modules)\n        for g in self.file_globs:\n            # Match bare dir names without wildcards\n            if g and all(ch not in g for ch in \"*?[\") and base == g:\n                return True\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_exclude_file_777": {
      "name": "exclude_file",
      "type": "method",
      "start_line": 777,
      "end_line": 785,
      "content_hash": "6aa4b3e49c5c6a05f12dafe51ec41f155fb21d2c",
      "content": "    def exclude_file(self, rel: str) -> bool:\n        import fnmatch\n\n        # Try matching whole rel path and basename\n        base = rel.rsplit(\"/\", 1)[-1]\n        for g in self.file_globs:\n            if fnmatch.fnmatch(rel.lstrip(\"/\"), g) or fnmatch.fnmatch(base, g):\n                return True\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_is_indexable_file_788": {
      "name": "is_indexable_file",
      "type": "function",
      "start_line": 788,
      "end_line": 803,
      "content_hash": "2783cc1c62f08ec164332b557a348110f62571ac",
      "content": "def is_indexable_file(p: Path) -> bool:\n    \"\"\"Check if a file should be indexed (by extension or name pattern).\n\n    Public API for use by watch_index and other modules.\n    \"\"\"\n    # Check by extension first\n    if p.suffix.lower() in CODE_EXTS:\n        return True\n    # Check by filename (for Dockerfile, Makefile, etc.)\n    fname_lower = p.name.lower()\n    if fname_lower in EXTENSIONLESS_FILES:\n        return True\n    # Check for Dockerfile.* pattern (e.g., Dockerfile.dev, Dockerfile.prod)\n    if fname_lower.startswith(\"dockerfile\"):\n        return True\n    return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_iter_files_810": {
      "name": "iter_files",
      "type": "function",
      "start_line": 810,
      "end_line": 852,
      "content_hash": "10d6dd9a1b59fdb163ddb2f100ac49e7d5e15a90",
      "content": "def iter_files(root: Path) -> Iterable[Path]:\n    # Allow passing a single file\n    if root.is_file():\n        if _is_indexable_file(root) and not _should_skip_explicit_file_by_excluder(root):\n            yield root\n        return\n\n    excl = _Excluder(root)\n    # Use os.walk to prune directories for performance\n    # NOTE: avoid Path.resolve()/realpath here; on network filesystems (e.g. CephFS)\n    # it can trigger expensive metadata calls during large unchanged indexing runs.\n    try:\n        root_abs = os.path.abspath(str(root))\n    except Exception:\n        root_abs = str(root)\n\n    for dirpath, dirnames, filenames in os.walk(root_abs):\n        # Compute rel path like /a/b from root without resolving symlinks\n        try:\n            rel = os.path.relpath(dirpath, root_abs)\n        except Exception:\n            rel = \".\"\n        if rel in (\".\", \"\"):\n            rel_dir = \"/\"\n        else:\n            rel_dir = \"/\" + rel.replace(os.sep, \"/\")\n        # Prune excluded directories in-place\n        keep = []\n        for d in dirnames:\n            rel = (rel_dir.rstrip(\"/\") + \"/\" + d).replace(\"//\", \"/\")\n            if excl.exclude_dir(rel):\n                continue\n            keep.append(d)\n        dirnames[:] = keep\n\n        for f in filenames:\n            p = Path(dirpath) / f\n            if not _is_indexable_file(p):\n                continue\n            relf = (rel_dir.rstrip(\"/\") + \"/\" + f).replace(\"//\", \"/\")\n            if excl.exclude_file(relf):\n                continue\n            yield p",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_chunk_lines_855": {
      "name": "chunk_lines",
      "type": "function",
      "start_line": 855,
      "end_line": 868,
      "content_hash": "4fb89806647b0cfa372341d86f196fd53ecf21af",
      "content": "def chunk_lines(text: str, max_lines: int = 120, overlap: int = 20) -> List[Dict]:\n    lines = text.splitlines()\n    chunks = []\n    i = 0\n    n = len(lines)\n    while i < n:\n        j = min(n, i + max_lines)\n        chunk = \"\\n\".join(lines[i:j])\n        chunks.append({\"text\": chunk, \"start\": i + 1, \"end\": j})\n        if j == n:\n            break\n\n        i = max(j - overlap, i + 1)\n    return chunks",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_chunk_semantic_871": {
      "name": "chunk_semantic",
      "type": "function",
      "start_line": 871,
      "end_line": 949,
      "content_hash": "1bcb61124487194aa88d0aa398c42689914f9666",
      "content": "def chunk_semantic(\n    text: str, language: str, max_lines: int = 120, overlap: int = 20\n) -> List[Dict]:\n    \"\"\"AST-aware chunking that tries to keep complete functions/classes together.\"\"\"\n    # Try enhanced AST analyzer first (if available)\n    # Note: ast_analyzer can use Python's built-in ast module even without tree-sitter\n    use_enhanced = os.environ.get(\"INDEX_USE_ENHANCED_AST\", \"1\").lower() in {\"1\", \"true\", \"yes\", \"on\"}\n    _ast_supported = language in _TS_LANGUAGES or language == \"python\"  # Python has built-in ast fallback\n    if use_enhanced and _AST_ANALYZER_AVAILABLE and _ast_supported:\n        try:\n            chunks = chunk_code_semantically(text, language, max_lines, overlap)\n            # Convert to expected format\n            return [\n                {\n                    \"text\": c[\"text\"],\n                    \"start\": c[\"start\"],\n                    \"end\": c[\"end\"],\n                    \"is_semantic\": c.get(\"is_semantic\", True)\n                }\n                for c in chunks\n            ]\n        except Exception as e:\n            if os.environ.get(\"DEBUG_INDEXING\"):\n                print(f\"[DEBUG] Enhanced AST chunking failed, falling back: {e}\")\n    \n    if not _use_tree_sitter() or language not in _TS_LANGUAGES:\n        # Fallback to line-based chunking\n        return chunk_lines(text, max_lines, overlap)\n\n    lines = text.splitlines()\n    n = len(lines)\n\n    # Extract symbols with line ranges\n    symbols = _extract_symbols(language, text)\n    if not symbols:\n        return chunk_lines(text, max_lines, overlap)\n\n    # Sort symbols by start line\n    symbols.sort(key=lambda s: s.start)\n\n    chunks = []\n    i = 0  # Current line index (0-based)\n\n    while i < n:\n        chunk_start = i + 1  # 1-based for output\n        chunk_end = min(n, i + max_lines)  # 1-based\n\n        # Try to find a symbol that starts within our current window\n        best_symbol = None\n        for sym in symbols:\n            if sym.start >= chunk_start and sym.start <= chunk_end:\n                # Check if the entire symbol fits within max_lines from current position\n                symbol_size = sym.end - sym.start + 1\n                if symbol_size <= max_lines and sym.end <= i + max_lines:\n                    best_symbol = sym\n                    break\n\n        if best_symbol:\n            # Chunk this complete symbol\n            chunk_text = \"\\n\".join(lines[best_symbol.start - 1 : best_symbol.end])\n            chunks.append(\n                {\n                    \"text\": chunk_text,\n                    \"start\": best_symbol.start,\n                    \"end\": best_symbol.end,\n                    \"symbol\": best_symbol.name,\n                    \"kind\": best_symbol.kind,\n                }\n            )\n            # Move past this symbol with minimal overlap\n            i = max(best_symbol.end - overlap, i + 1)\n        else:\n            # No suitable symbol found, fall back to line-based chunking\n            chunk_text = \"\\n\".join(lines[i : i + max_lines])\n            actual_end = min(n, i + max_lines)\n            chunks.append({\"text\": chunk_text, \"start\": i + 1, \"end\": actual_end})\n            i = max(actual_end - overlap, i + 1)\n\n    return chunks",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_chunk_by_tokens_954": {
      "name": "chunk_by_tokens",
      "type": "function",
      "start_line": 954,
      "end_line": 1063,
      "content_hash": "bfb938e43431e58af52c2dd7c2796bceea475c71",
      "content": "def chunk_by_tokens(\n    text: str, k_tokens: int = None, stride_tokens: int = None\n) -> List[Dict]:\n    try:\n        from tokenizers import Tokenizer  # lightweight, already in requirements\n    except Exception:\n        Tokenizer = None  # type: ignore\n\n    # Prefer explicit function arguments when provided; fall back to env/defaults.\n    # This lets dynamic resizing callers override MICRO_CHUNK_TOKENS/STRIDE correctly.\n    try:\n        if k_tokens is not None:\n            k = int(k_tokens)\n        else:\n            k = int(os.environ.get(\"MICRO_CHUNK_TOKENS\", \"16\") or 16)\n    except Exception:\n        k = 16\n    try:\n        if stride_tokens is not None:\n            s = int(stride_tokens)\n        else:\n            s = int(os.environ.get(\"MICRO_CHUNK_STRIDE\", \"\") or max(1, k // 2))\n    except Exception:\n        s = max(1, k // 2)\n\n    # Helper: simple regex-based token offsets when HF tokenizer JSON is unavailable\n    def _simple_offsets(txt: str):\n        import re\n        offs = []\n        for m in re.finditer(r\"\\S+\", txt):\n            offs.append((m.start(), m.end()))\n        return offs\n\n    offsets = []\n    # Load tokenizer; default to local model file if present\n    tok_path = os.environ.get(\n        \"TOKENIZER_JSON\", str((ROOT_DIR / \"models\" / \"tokenizer.json\"))\n    )\n    if Tokenizer is not None:\n        try:\n            tokenizer = Tokenizer.from_file(tok_path)\n            try:\n                enc = tokenizer.encode(text)\n                offsets = getattr(enc, \"offsets\", None) or []\n            except Exception:\n                offsets = []\n        except Exception:\n            offsets = []\n\n    if not offsets:\n        # Fallback to simple regex tokenization; avoids degrading to 120-line chunks\n        if os.environ.get(\"DEBUG_CHUNKING\"):\n            print(\"[ingest] tokenizers missing/unusable -> using simple regex tokenization\")\n        offsets = _simple_offsets(text)\n\n    if not offsets:\n        return chunk_lines(text, max_lines=120, overlap=20)\n\n    # Precompute line starts for fast char->line mapping\n    lines = text.splitlines(keepends=True)\n    line_starts = []\n    pos = 0\n    for ln in lines:\n        line_starts.append(pos)\n        pos += len(ln)\n    total_chars = len(text)\n\n    def char_to_line(c: int) -> int:\n        # Binary search line_starts to find 1-based line number\n        lo, hi = 0, len(line_starts) - 1\n        if c <= 0:\n            return 1\n        if c >= total_chars:\n            return len(lines)\n        ans = 0\n        while lo <= hi:\n            mid = (lo + hi) // 2\n            if line_starts[mid] <= c:\n                ans = mid\n                lo = mid + 1\n            else:\n                hi = mid - 1\n        return ans + 1  # 1-based\n\n    chunks: List[Dict] = []\n    i = 0\n    n = len(offsets)\n    while i < n:\n        j = min(n, i + k)\n        start_char = offsets[i][0]\n        end_char = offsets[j - 1][1] if j - 1 < n else offsets[-1][1]\n        start_char = max(0, start_char)\n        end_char = min(total_chars, max(start_char, end_char))\n        chunk_text = text[start_char:end_char]\n        if chunk_text:\n            start_line = char_to_line(start_char)\n            end_line = (\n                char_to_line(end_char - 1) if end_char > start_char else start_line\n            )\n            chunks.append(\n                {\n                    \"text\": chunk_text,\n                    \"start\": start_line,\n                    \"end\": end_line,\n                }\n            )\n        if j == n:\n            break\n        i = i + s if s > 0 else i + 1\n    return chunks",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__simple_offsets_980": {
      "name": "_simple_offsets",
      "type": "function",
      "start_line": 980,
      "end_line": 985,
      "content_hash": "89afb2635541acd20aaf764d980875a6ca176b01",
      "content": "    def _simple_offsets(txt: str):\n        import re\n        offs = []\n        for m in re.finditer(r\"\\S+\", txt):\n            offs.append((m.start(), m.end()))\n        return offs",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_char_to_line_1021": {
      "name": "char_to_line",
      "type": "function",
      "start_line": 1021,
      "end_line": 1036,
      "content_hash": "5456894d6c9f162288a477ca0d79508b47525fcf",
      "content": "    def char_to_line(c: int) -> int:\n        # Binary search line_starts to find 1-based line number\n        lo, hi = 0, len(line_starts) - 1\n        if c <= 0:\n            return 1\n        if c >= total_chars:\n            return len(lines)\n        ans = 0\n        while lo <= hi:\n            mid = (lo + hi) // 2\n            if line_starts[mid] <= c:\n                ans = mid\n                lo = mid + 1\n            else:\n                hi = mid - 1\n        return ans + 1  # 1-based",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__pseudo_describe_enabled_1074": {
      "name": "_pseudo_describe_enabled",
      "type": "function",
      "start_line": 1074,
      "end_line": 1083,
      "content_hash": "b7085718a040bed5e81d18c46c4d2c44778db7a1",
      "content": "def _pseudo_describe_enabled() -> bool:\n    try:\n        return str(os.environ.get(\"REFRAG_PSEUDO_DESCRIBE\", \"0\")).strip().lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }\n    except Exception:\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__smart_symbol_reindexing_enabled_1088": {
      "name": "_smart_symbol_reindexing_enabled",
      "type": "function",
      "start_line": 1088,
      "end_line": 1098,
      "content_hash": "e49964338c13519bbd97c9cd8fc802ef125b80b8",
      "content": "def _smart_symbol_reindexing_enabled() -> bool:\n    \"\"\"Check if symbol-aware reindexing is enabled.\"\"\"\n    try:\n        return str(os.environ.get(\"SMART_SYMBOL_REINDEXING\", \"0\")).strip().lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }\n    except Exception:\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_extract_symbols_with_tree_sitter_1101": {
      "name": "extract_symbols_with_tree_sitter",
      "type": "function",
      "start_line": 1101,
      "end_line": 1142,
      "content_hash": "e94f802b649f280187ec5752661b4edd60d52563",
      "content": "def extract_symbols_with_tree_sitter(file_path: str) -> dict:\n    \"\"\"Extract functions, classes, methods from file using tree-sitter or fallback.\n\n    Returns:\n        dict: {symbol_id: {name, type, start_line, end_line, content_hash, pseudo, tags}}\n    \"\"\"\n    try:\n        # Read file content\n        text = Path(file_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n        language = detect_language(Path(file_path))\n\n        # Use existing symbol extraction infrastructure\n        symbols_list = _extract_symbols(language, text)\n\n        # Convert to our expected dict format\n        symbols = {}\n        for sym in symbols_list:\n            symbol_id = f\"{sym['kind']}_{sym['name']}_{sym['start']}\"\n\n            # Extract actual content for hashing\n            content_lines = text.split(\"\\n\")[sym[\"start\"] - 1 : sym[\"end\"]]\n            content = \"\\n\".join(content_lines)\n            content_hash = hashlib.sha1(content.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n\n            symbols[symbol_id] = {\n                \"name\": sym[\"name\"],\n                \"type\": sym[\"kind\"],\n                \"start_line\": sym[\"start\"],\n                \"end_line\": sym[\"end\"],\n                \"content_hash\": content_hash,\n                \"content\": content,\n                # These will be populated during processing\n                \"pseudo\": \"\",\n                \"tags\": [],\n                \"qdrant_ids\": [],  # Will store Qdrant point IDs for this symbol\n            }\n\n        return symbols\n\n    except Exception as e:\n        print(f\"[SYMBOL_EXTRACTION] Failed to extract symbols from {file_path}: {e}\")\n        return {}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_should_use_smart_reindexing_1145": {
      "name": "should_use_smart_reindexing",
      "type": "function",
      "start_line": 1145,
      "end_line": 1179,
      "content_hash": "91b4e2c207c4b2957c1520a5c839758b2e2c92f0",
      "content": "def should_use_smart_reindexing(file_path: str, file_hash: str) -> tuple[bool, str]:\n    \"\"\"Determine if smart reindexing should be used for a file.\n\n    Returns:\n        (use_smart, reason)\n    \"\"\"\n    if not _smart_symbol_reindexing_enabled():\n        return False, \"smart_reindexing_disabled\"\n\n    if not get_cached_symbols or not set_cached_symbols:\n        return False, \"symbol_cache_unavailable\"\n\n    # Load cached symbols\n    cached_symbols = get_cached_symbols(file_path)\n    if not cached_symbols:\n        return False, \"no_cached_symbols\"\n\n    # Extract current symbols\n    current_symbols = extract_symbols_with_tree_sitter(file_path)\n    if not current_symbols:\n        return False, \"no_current_symbols\"\n\n    # Compare symbols\n    unchanged_symbols, changed_symbols = compare_symbol_changes(cached_symbols, current_symbols)\n\n    total_symbols = len(current_symbols)\n    changed_ratio = len(changed_symbols) / max(total_symbols, 1)\n\n    # Use thresholds to decide strategy\n    max_changed_ratio = float(os.environ.get(\"MAX_CHANGED_SYMBOLS_RATIO\", \"0.3\"))\n    if changed_ratio > max_changed_ratio:\n        return False, f\"too_many_changes_{changed_ratio:.2f}\"\n\n    print(f\"[SMART_REINDEX] {file_path}: {len(unchanged_symbols)} unchanged, {len(changed_symbols)} changed\")\n    return True, f\"smart_reindex_{len(changed_symbols)}/{total_symbols}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_generate_pseudo_tags_1182": {
      "name": "generate_pseudo_tags",
      "type": "function",
      "start_line": 1182,
      "end_line": 1247,
      "content_hash": "9787b0dba3e444fb713d6dce1982595b1dda9220",
      "content": "def generate_pseudo_tags(text: str) -> tuple[str, list[str]]:\n    \"\"\"Best-effort: ask local decoder to produce a short label and 3-6 tags.\n    Returns (pseudo, tags). On failure returns (\"\", []).\"\"\"\n    pseudo: str = \"\"\n    tags: list[str] = []\n    if not _pseudo_describe_enabled() or not text.strip():\n        return pseudo, tags\n    try:\n        from scripts.refrag_llamacpp import (  # type: ignore\n            LlamaCppRefragClient,\n            is_decoder_enabled,\n            get_runtime_kind,\n        )\n        if not is_decoder_enabled():\n            return \"\", []\n        runtime = get_runtime_kind()\n        # Keep decoding tight/fast \u2013 this is only enrichment for retrieval.\n        # Preserve original llama.cpp prompt semantics, and use a stricter\n        # JSON-only prompt only for the GLM runtime.\n        if runtime == \"glm\":\n            prompt = (\n                \"You are a JSON-only function that labels code spans for search enrichment.\\n\"\n                \"Respond with a single JSON object and nothing else (no prose, no markdown).\\n\"\n                \"Exact format: {\\\"pseudo\\\": string (<=20 tokens), \\\"tags\\\": [3-6 short strings]}.\\n\"\n                \"Code:\\n\" + text[:2000]\n            )\n            from scripts.refrag_glm import GLMRefragClient  # type: ignore\n            client = GLMRefragClient()\n            out = client.generate_with_soft_embeddings(\n                prompt=prompt,\n                max_tokens=int(os.environ.get(\"PSEUDO_MAX_TOKENS\", \"96\") or 96),\n                temperature=float(os.environ.get(\"PSEUDO_TEMPERATURE\", \"0.10\") or 0.10),\n                top_p=float(os.environ.get(\"PSEUDO_TOP_P\", \"0.9\") or 0.9),\n                stop=[\"\\n\\n\"],\n                force_json=True,\n            )\n        else:\n            prompt = (\n                \"You label code spans for search enrichment.\\n\"\n                \"Return strictly JSON: {\\\"pseudo\\\": string (<=20 tokens), \\\"tags\\\": [3-6 short strings]}.\\n\"\n                \"Code:\\n\" + text[:2000]\n            )\n            client = LlamaCppRefragClient()\n            out = client.generate_with_soft_embeddings(\n                prompt=prompt,\n                max_tokens=int(os.environ.get(\"PSEUDO_MAX_TOKENS\", \"96\") or 96),\n                temperature=float(os.environ.get(\"PSEUDO_TEMPERATURE\", \"0.10\") or 0.10),\n                top_k=int(os.environ.get(\"PSEUDO_TOP_K\", \"30\") or 30),\n                top_p=float(os.environ.get(\"PSEUDO_TOP_P\", \"0.9\") or 0.9),\n                stop=[\"\\n\\n\"],\n            )\n        import json as _json\n        try:\n            obj = _json.loads(out)\n            if isinstance(obj, dict):\n                p = obj.get(\"pseudo\")\n                t = obj.get(\"tags\")\n                if isinstance(p, str):\n                    pseudo = p.strip()[:256]\n                if isinstance(t, list):\n                    tags = [str(x).strip() for x in t if str(x).strip()][:6]\n        except Exception:\n            pass\n    except Exception:\n        return \"\", []\n    return pseudo, tags",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_should_process_pseudo_for_chunk_1250": {
      "name": "should_process_pseudo_for_chunk",
      "type": "function",
      "start_line": 1250,
      "end_line": 1298,
      "content_hash": "b2dae7f5e023c73eb2aca277a68ac94924b34056",
      "content": "def should_process_pseudo_for_chunk(\n    file_path: str, chunk: dict, changed_symbols: set\n) -> tuple[bool, str, list[str]]:\n    \"\"\"Determine if a chunk needs pseudo processing based on symbol changes AND pseudo cache.\n\n    Uses existing symbol change detection and pseudo cache lookup for optimal performance.\n\n    Args:\n        file_path: Path to the file containing this chunk\n        chunk: Chunk dict with symbol information\n        changed_symbols: Set of symbol IDs that changed (from compare_symbol_changes)\n\n    Returns:\n        (needs_processing, cached_pseudo, cached_tags)\n    \"\"\"\n    # For chunks without symbol info, process them (fallback - no symbol to reuse from)\n    symbol_name = chunk.get(\"symbol\", \"\")\n    if not symbol_name:\n        return True, \"\", []\n\n    # Create symbol ID matching the format used in symbol cache\n    kind = chunk.get(\"kind\", \"unknown\")\n    start_line = chunk.get(\"start\", 0)\n    symbol_id = f\"{kind}_{symbol_name}_{start_line}\"\n\n    # If we don't have any change information, best effort: try reusing cached pseudo when present\n    if not changed_symbols and get_cached_pseudo:\n        try:\n            cached_pseudo, cached_tags = get_cached_pseudo(file_path, symbol_id)\n            if cached_pseudo or cached_tags:\n                return False, cached_pseudo, cached_tags\n        except Exception:\n            pass\n        return True, \"\", []\n\n    # Unchanged symbol: prefer reuse when cached pseudo/tags exist\n    if symbol_id not in changed_symbols:\n        if get_cached_pseudo:\n            try:\n                cached_pseudo, cached_tags = get_cached_pseudo(file_path, symbol_id)\n                if cached_pseudo or cached_tags:\n                    return False, cached_pseudo, cached_tags\n            except Exception:\n                pass\n        # Unchanged but no cached data yet \u2013 process once\n        return True, \"\", []\n\n    # Symbol content changed: always re-run pseudo; do not reuse stale cached values\n    return True, \"\", []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_CollectionNeedsRecreateError_1301": {
      "name": "CollectionNeedsRecreateError",
      "type": "class",
      "start_line": 1301,
      "end_line": 1303,
      "content_hash": "2af613fa7c113b08f293cd82d30a993442ead2ed",
      "content": "class CollectionNeedsRecreateError(Exception):\n    \"\"\"Raised when a collection needs to be recreated to add new vector types.\"\"\"\n    pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_ensure_collection_1306": {
      "name": "ensure_collection",
      "type": "function",
      "start_line": 1306,
      "end_line": 1546,
      "content_hash": "2cab6a2ff3ad5abb72bcff9bf5289f25d36b2194",
      "content": "def ensure_collection(client: QdrantClient, name: str, dim: int, vector_name: str):\n    \"\"\"Ensure collection exists with named vectors.\n    Always includes dense (vector_name) and lexical (LEX_VECTOR_NAME).\n    When REFRAG_MODE=1, also includes a compact mini vector (MINI_VECTOR_NAME).\n    \"\"\"\n    # Track backup file path for this ensure_collection call (per-collection, per-process)\n    backup_file = None\n    try:\n        info = client.get_collection(name)\n        # Prevent I/O storm - only update vectors if they actually don't exist\n        try:\n            cfg = getattr(info.config.params, \"vectors\", None)\n            sparse_cfg = getattr(info.config.params, \"sparse_vectors\", None)\n            if isinstance(cfg, dict):\n                # Check if collection already has required vectors before updating\n                has_lex = LEX_VECTOR_NAME in cfg\n                has_mini = MINI_VECTOR_NAME in cfg\n                # Check if sparse vectors are configured when LEX_SPARSE_MODE is enabled\n                has_sparse = sparse_cfg and LEX_SPARSE_NAME in (sparse_cfg if isinstance(sparse_cfg, dict) else {})\n\n                # If LEX_SPARSE_MODE enabled but collection lacks sparse vectors, must recreate\n                if LEX_SPARSE_MODE and not has_sparse:\n                    print(f\"[COLLECTION_INFO] Collection {name} lacks sparse vector '{LEX_SPARSE_NAME}' - recreating...\")\n                    # Backup memories before recreating\n                    try:\n                        import tempfile\n                        import subprocess\n                        import sys\n                        with tempfile.NamedTemporaryFile(mode='w', suffix='_memories_backup.json', delete=False) as f:\n                            backup_file = f.name\n                        print(f\"[MEMORY_BACKUP] Backing up memories from {name} to {backup_file}\")\n                        backup_script = Path(__file__).parent / \"memory_backup.py\"\n                        result = subprocess.run([\n                            sys.executable, str(backup_script),\n                            \"--collection\", name,\n                            \"--output\", backup_file\n                        ], capture_output=True, text=True, cwd=Path(__file__).parent.parent)\n                        if result.returncode != 0:\n                            print(f\"[MEMORY_BACKUP_WARNING] Backup script failed: {result.stderr}\")\n                            backup_file = None\n                    except Exception as backup_e:\n                        print(f\"[MEMORY_BACKUP_WARNING] Failed to backup memories: {backup_e}\")\n                        backup_file = None\n                    try:\n                        client.delete_collection(name)\n                        print(f\"[COLLECTION_INFO] Deleted existing collection {name}\")\n                    except Exception:\n                        pass\n                    raise CollectionNeedsRecreateError(f\"Collection {name} needs sparse vectors\")\n\n                # Only add to missing if vector doesn't already exist\n                missing = {}\n                if not has_lex:\n                    missing[LEX_VECTOR_NAME] = models.VectorParams(\n                        size=LEX_VECTOR_DIM, distance=models.Distance.COSINE\n                    )\n\n                try:\n                    refrag_on = os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\n                        \"1\",\n                        \"true\",\n                        \"yes\",\n                        \"on\",\n                    }\n                except Exception:\n                    refrag_on = False\n\n                if refrag_on and not has_mini:\n                    missing[MINI_VECTOR_NAME] = models.VectorParams(\n                        size=int(\n                            os.environ.get(\"MINI_VEC_DIM\", MINI_VEC_DIM) or MINI_VEC_DIM\n                        ),\n                        distance=models.Distance.COSINE,\n                    )\n\n                # Only update collection if vectors are actually missing\n                # Previous behavior: always called update_collection() causing I/O storms\n                if missing:\n                    try:\n                        client.update_collection(\n                            collection_name=name, vectors_config=missing\n                        )\n                        print(f\"[COLLECTION_SUCCESS] Successfully updated collection {name} with missing vectors\")\n                    except Exception as update_e:\n                        # Qdrant doesn't support adding new vector names to existing collections\n                        # Fall back to recreating the collection with the correct vector configuration\n                        print(f\"[COLLECTION_WARNING] Cannot add missing vectors to {name} ({update_e}). Recreating collection...\")\n\n                        # Backup memories before recreating collection using dedicated backup script\n                        backup_file = None\n                        try:\n                            import tempfile\n                            import subprocess\n                            import sys\n\n                            # Create temporary backup file\n                            with tempfile.NamedTemporaryFile(mode='w', suffix='_memories_backup.json', delete=False) as f:\n                                backup_file = f.name\n\n                            print(f\"[MEMORY_BACKUP] Backing up memories from {name} to {backup_file}\")\n\n                            # Use battle-tested backup script\n                            backup_script = Path(__file__).parent / \"memory_backup.py\"\n                            result = subprocess.run([\n                                sys.executable, str(backup_script),\n                                \"--collection\", name,\n                                \"--output\", backup_file\n                            ], capture_output=True, text=True, cwd=Path(__file__).parent.parent)\n\n                            if result.returncode == 0:\n                                print(f\"[MEMORY_BACKUP] Successfully backed up memories using {backup_script.name}\")\n                            else:\n                                print(f\"[MEMORY_BACKUP_WARNING] Backup script failed: {result.stderr}\")\n                                backup_file = None\n\n                        except Exception as backup_e:\n                            print(f\"[MEMORY_BACKUP_WARNING] Failed to backup memories: {backup_e}\")\n                            backup_file = None\n\n                        try:\n                            client.delete_collection(name)\n                            print(f\"[COLLECTION_INFO] Deleted existing collection {name}\")\n                        except Exception:\n                            pass\n\n                        # Store backup info for restoration\n                        # backup_file remains bound for this function call; used after collection creation\n\n                        # Proceed to recreate with full vector configuration\n                        raise CollectionNeedsRecreateError(f\"Collection {name} needs recreation for new vectors\")\n        except CollectionNeedsRecreateError:\n            # Let this fall through to collection creation logic\n            print(f\"[COLLECTION_INFO] Collection {name} needs recreation - proceeding...\")\n            raise\n        except Exception as e:\n            print(f\"[COLLECTION_ERROR] Failed to update collection {name}: {e}\")\n            pass\n        return\n    except Exception as e:\n        # Collection doesn't exist - proceed to create it\n        print(f\"[COLLECTION_INFO] Creating new collection {name}: {type(e).__name__}\")\n        pass\n    vectors_cfg = {\n        vector_name: models.VectorParams(size=dim, distance=models.Distance.COSINE),\n        LEX_VECTOR_NAME: models.VectorParams(\n            size=LEX_VECTOR_DIM, distance=models.Distance.COSINE\n        ),\n    }\n    # Conditionally add mini vector for ReFRAG gating\n    try:\n        if os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }:\n            vectors_cfg[MINI_VECTOR_NAME] = models.VectorParams(\n                size=int(os.environ.get(\"MINI_VEC_DIM\", MINI_VEC_DIM) or MINI_VEC_DIM),\n                distance=models.Distance.COSINE,\n            )\n    except Exception:\n        pass\n    # Sparse vectors config for lossless lexical matching\n    sparse_cfg = None\n    if LEX_SPARSE_MODE:\n        sparse_cfg = {\n            LEX_SPARSE_NAME: models.SparseVectorParams(\n                index=models.SparseIndexParams(full_scan_threshold=5000)\n            )\n        }\n    client.create_collection(\n        collection_name=name,\n        vectors_config=vectors_cfg,\n        sparse_vectors_config=sparse_cfg,\n        hnsw_config=models.HnswConfigDiff(m=16, ef_construct=256),\n    )\n    sparse_info = f\", sparse: [{LEX_SPARSE_NAME}]\" if sparse_cfg else \"\"\n    print(f\"[COLLECTION_INFO] Successfully created new collection {name} with vectors: {list(vectors_cfg.keys())}{sparse_info}\")\n\n    # Restore memories if we have a backup from recreation using dedicated restore script\n    strict_restore = False\n    try:\n        val = os.environ.get(\"STRICT_MEMORY_RESTORE\", \"\")\n        strict_restore = str(val or \"\").strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n    except Exception:\n        strict_restore = False\n\n    try:\n        if backup_file and os.path.exists(backup_file):\n            print(f\"[MEMORY_RESTORE] Restoring memories from {backup_file}\")\n            import subprocess\n            import sys\n\n            # Use battle-tested restore script (skip collection creation since ingest_code.py already handles it)\n            restore_script = Path(__file__).parent / \"memory_restore.py\"\n            result = subprocess.run(\n                [\n                    sys.executable,\n                    str(restore_script),\n                    \"--backup\",\n                    backup_file,\n                    \"--collection\",\n                    name,\n                    \"--skip-collection-creation\",\n                ],\n                capture_output=True,\n                text=True,\n                cwd=Path(__file__).parent.parent,\n            )\n\n            if result.returncode == 0:\n                print(f\"[MEMORY_RESTORE] Successfully restored memories using {restore_script.name}\")\n            else:\n                # Log full output for debugging\n                print(f\"[MEMORY_RESTORE_WARNING] Restore script failed (exit {result.returncode})\")\n                if result.stdout:\n                    print(f\"[MEMORY_RESTORE_STDOUT] {result.stdout}\")\n                if result.stderr:\n                    print(f\"[MEMORY_RESTORE_STDERR] {result.stderr}\")\n                if strict_restore:\n                    msg = result.stderr or result.stdout or f\"exit code {result.returncode}\"\n                    raise RuntimeError(f\"Memory restore failed for collection {name}: {msg}\")\n\n            # Clean up backup file once we've attempted restore\n            try:\n                os.unlink(backup_file)\n                print(f\"[MEMORY_RESTORE] Cleaned up backup file {backup_file}\")\n            except Exception:\n                pass\n            finally:\n                backup_file = None\n\n        elif backup_file:\n            print(f\"[MEMORY_RESTORE_WARNING] Backup file {backup_file} not found\")\n            backup_file = None\n\n    except Exception as restore_e:\n        print(f\"[MEMORY_RESTORE_ERROR] Failed to restore memories: {restore_e}\")\n        # Optionally fail hard when STRICT_MEMORY_RESTORE is enabled\n        if strict_restore:\n            raise",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_recreate_collection_1549": {
      "name": "recreate_collection",
      "type": "function",
      "start_line": 1549,
      "end_line": 1587,
      "content_hash": "4d6d3d4f45c7a86ed41372a24c4b6084db6f30ee",
      "content": "def recreate_collection(client: QdrantClient, name: str, dim: int, vector_name: str):\n    \"\"\"Drop and recreate collection with named vectors (dense + lex [+ mini when REFRAG_MODE=1] [+ sparse when LEX_SPARSE_MODE=1]).\"\"\"\n    try:\n        client.delete_collection(name)\n    except Exception:\n        pass\n    vectors_cfg = {\n        vector_name: models.VectorParams(size=dim, distance=models.Distance.COSINE),\n        LEX_VECTOR_NAME: models.VectorParams(\n            size=LEX_VECTOR_DIM, distance=models.Distance.COSINE\n        ),\n    }\n    try:\n        if os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }:\n            vectors_cfg[MINI_VECTOR_NAME] = models.VectorParams(\n                size=int(os.environ.get(\"MINI_VEC_DIM\", MINI_VEC_DIM) or MINI_VEC_DIM),\n                distance=models.Distance.COSINE,\n            )\n    except Exception:\n        pass\n    # Sparse vectors config for lossless lexical matching\n    sparse_cfg = None\n    if LEX_SPARSE_MODE:\n        sparse_cfg = {\n            LEX_SPARSE_NAME: models.SparseVectorParams(\n                index=models.SparseIndexParams(full_scan_threshold=5000)\n            )\n        }\n    client.create_collection(\n        collection_name=name,\n        vectors_config=vectors_cfg,\n        sparse_vectors_config=sparse_cfg,\n        hnsw_config=models.HnswConfigDiff(m=16, ef_construct=256),\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_ensure_payload_indexes_1590": {
      "name": "ensure_payload_indexes",
      "type": "function",
      "start_line": 1590,
      "end_line": 1617,
      "content_hash": "e8837a339536e05bf6c30a2daf3c67cc634d32db",
      "content": "def ensure_payload_indexes(client: QdrantClient, collection: str):\n    \"\"\"Create helpful payload indexes if they don't exist (idempotent).\"\"\"\n    for field in (\n        \"metadata.language\",\n        \"metadata.path_prefix\",\n        \"metadata.repo_id\",\n        \"metadata.repo_rel_path\",\n        \"metadata.repo\",\n        \"metadata.kind\",\n        \"metadata.symbol\",\n        \"metadata.symbol_path\",\n        \"metadata.imports\",\n        \"metadata.calls\",\n        \"metadata.file_hash\",\n        \"metadata.ingested_at\",\n        \"metadata.last_modified_at\",\n        \"metadata.churn_count\",\n        \"metadata.author_count\",\n        \"pid_str\",\n    ):\n        try:\n            client.create_payload_index(\n                collection_name=collection,\n                field_name=field,\n                field_schema=models.PayloadSchemaType.KEYWORD,\n            )\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_pseudo_backfill_tick_1623": {
      "name": "pseudo_backfill_tick",
      "type": "function",
      "start_line": 1623,
      "end_line": 1836,
      "content_hash": "c1b751def3d63dc8651245a7f3dfd3f5023c4621",
      "content": "def pseudo_backfill_tick(\n    client: QdrantClient,\n    collection: str,\n    repo_name: str | None = None,\n    *,\n    max_points: int = 256,\n    dim: int | None = None,\n    vector_name: str | None = None,\n) -> int:\n    \"\"\"Best-effort pseudo/tag backfill for a collection.\n\n    Scans up to max_points points for a given repo (when provided) that have not yet\n    been marked as pseudo-enriched and updates them in-place with pseudo/tags and\n    refreshed lexical vectors. Does not touch cache.json or hash-based skip logic;\n    operates purely on Qdrant payloads/vectors.\n    \"\"\"\n\n    if not collection or max_points <= 0:\n        return 0\n\n    try:\n        from qdrant_client import models as _models\n    except Exception:\n        return 0\n\n    must_conditions: list[Any] = []\n    if repo_name:\n        try:\n            must_conditions.append(\n                _models.FieldCondition(\n                    key=\"metadata.repo\",\n                    match=_models.MatchValue(value=repo_name),\n                )\n            )\n        except Exception:\n            pass\n\n    flt = None\n    try:\n        # Prefer server-side filtering for points missing pseudo/tags when supported\n        null_cond = getattr(_models, \"IsNullCondition\", None)\n        empty_cond = getattr(_models, \"IsEmptyCondition\", None)\n        if null_cond is not None:\n            should_conditions = []\n            try:\n                should_conditions.append(null_cond(is_null=\"pseudo\"))\n            except Exception:\n                pass\n            try:\n                should_conditions.append(null_cond(is_null=\"tags\"))\n            except Exception:\n                pass\n            if empty_cond is not None:\n                try:\n                    should_conditions.append(empty_cond(is_empty=\"tags\"))\n                except Exception:\n                    pass\n            flt = _models.Filter(\n                must=must_conditions or None,\n                should=should_conditions or None,\n            )\n        else:\n            # Fallback: only scope by repo, rely on Python-side pseudo/tags checks\n            flt = _models.Filter(must=must_conditions or None)\n    except Exception:\n        flt = None\n\n    processed = 0\n    debug_enabled = (os.environ.get(\"PSEUDO_BACKFILL_DEBUG\") or \"\").strip().lower() in {\n        \"1\",\n        \"true\",\n        \"yes\",\n        \"on\",\n    }\n    debug_stats = {\n        \"scanned\": 0,\n        \"glm_calls\": 0,\n        \"glm_success\": 0,\n        \"filled_new\": 0,\n        \"updated_existing\": 0,\n        \"skipped_no_code\": 0,\n        \"skipped_after_glm\": 0,\n    }\n    next_offset = None\n\n    def _maybe_ensure_collection() -> bool:\n        if not dim or not vector_name:\n            return False\n        try:\n            ensure_collection_and_indexes_once(client, collection, int(dim), vector_name)\n            return True\n        except Exception:\n            return False\n\n    while processed < max_points:\n        batch_limit = max(1, min(64, max_points - processed))\n        try:\n            points, next_offset = client.scroll(\n                collection_name=collection,\n                scroll_filter=flt,\n                limit=batch_limit,\n                with_payload=True,\n                with_vectors=True,\n                offset=next_offset,\n            )\n        except Exception:\n            if _maybe_ensure_collection():\n                try:\n                    points, next_offset = client.scroll(\n                        collection_name=collection,\n                        scroll_filter=flt,\n                        limit=batch_limit,\n                        with_payload=True,\n                        with_vectors=True,\n                        offset=next_offset,\n                    )\n                except Exception:\n                    break\n            else:\n                break\n\n        if not points:\n            break\n\n        new_points: list[Any] = []\n        for rec in points:\n            try:\n                if debug_enabled:\n                    debug_stats[\"scanned\"] += 1\n                payload = rec.payload or {}\n                md = payload.get(\"metadata\") or {}\n                code = md.get(\"code\") or \"\"\n                if not code:\n                    if debug_enabled:\n                        debug_stats[\"skipped_no_code\"] += 1\n                    continue\n\n                pseudo = payload.get(\"pseudo\") or \"\"\n                tags_val = payload.get(\"tags\") or []\n                tags: list[str] = list(tags_val) if isinstance(tags_val, list) else []\n                had_existing = bool(pseudo or tags)\n\n                # If pseudo/tags are missing, generate them once\n                if not pseudo and not tags:\n                    try:\n                        if debug_enabled:\n                            debug_stats[\"glm_calls\"] += 1\n                        pseudo, tags = generate_pseudo_tags(code)\n                        if debug_enabled and (pseudo or tags):\n                            debug_stats[\"glm_success\"] += 1\n                    except Exception:\n                        pseudo, tags = \"\", []\n\n                if not pseudo and not tags:\n                    if debug_enabled:\n                        debug_stats[\"skipped_after_glm\"] += 1\n                    continue\n\n                # Update payload and lexical vector with pseudo/tags\n                payload[\"pseudo\"] = pseudo\n                payload[\"tags\"] = tags\n                if debug_enabled:\n                    if had_existing:\n                        debug_stats[\"updated_existing\"] += 1\n                    else:\n                        debug_stats[\"filled_new\"] += 1\n\n                aug_text = f\"{code} {pseudo} {' '.join(tags)}\".strip()\n                lex_vec = _lex_hash_vector_text(aug_text)\n\n                vec = rec.vector\n                if isinstance(vec, dict):\n                    vecs = dict(vec)\n                    vecs[LEX_VECTOR_NAME] = lex_vec\n                    new_vec = vecs\n                else:\n                    # Fallback: collections without named vectors - leave dense vector as-is\n                    new_vec = vec\n\n                # Add sparse vector to vecs dict if LEX_SPARSE_MODE enabled (new qdrant-client API)\n                if LEX_SPARSE_MODE and aug_text and isinstance(new_vec, dict):\n                    sparse_vec = _lex_sparse_vector_text(aug_text)\n                    if sparse_vec.get(\"indices\"):\n                        new_vec[LEX_SPARSE_NAME] = models.SparseVector(**sparse_vec)\n\n                new_points.append(\n                    models.PointStruct(\n                        id=rec.id,\n                        vector=new_vec,\n                        payload=payload,\n                    )\n                )\n                processed += 1\n            except Exception:\n                continue\n\n        if new_points:\n            try:\n                upsert_points(client, collection, new_points)\n            except Exception:\n                if _maybe_ensure_collection():\n                    try:\n                        upsert_points(client, collection, new_points)\n                    except Exception:\n                        # Best-effort: on failure, stop this tick\n                        break\n                else:\n                    # Best-effort: on failure, stop this tick\n                    break\n\n        if next_offset is None:\n            break\n\n    return processed",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__maybe_ensure_collection_1708": {
      "name": "_maybe_ensure_collection",
      "type": "function",
      "start_line": 1708,
      "end_line": 1715,
      "content_hash": "5f3ec858b1082975d5977ec60e2678ef7a5b098d",
      "content": "    def _maybe_ensure_collection() -> bool:\n        if not dim or not vector_name:\n            return False\n        try:\n            ensure_collection_and_indexes_once(client, collection, int(dim), vector_name)\n            return True\n        except Exception:\n            return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_ensure_collection_and_indexes_once_1839": {
      "name": "ensure_collection_and_indexes_once",
      "type": "function",
      "start_line": 1839,
      "end_line": 1882,
      "content_hash": "6d173c4f1e268265878f2a7457d847bf9070b990",
      "content": "def ensure_collection_and_indexes_once(\n    client: QdrantClient,\n    collection: str,\n    dim: int,\n    vector_name: str | None,\n) -> None:\n    if not collection:\n        return\n    if collection in ENSURED_COLLECTIONS:\n        # By default we do NOT ping Qdrant repeatedly for an already-ensured collection.\n        # This avoids a steady stream of GET /collections/<name> requests during large runs.\n        # Opt-in: set ENSURED_COLLECTION_PING_SECONDS to a positive float (e.g. 60).\n        try:\n            ping_seconds = float(os.environ.get(\"ENSURED_COLLECTION_PING_SECONDS\", \"0\") or 0)\n        except Exception:\n            ping_seconds = 0.0\n\n        if ping_seconds <= 0:\n            return\n\n        try:\n            now = time.time()\n            last = ENSURED_COLLECTIONS_LAST_CHECK.get(collection, 0.0)\n            if (now - last) < ping_seconds:\n                return\n            client.get_collection(collection)\n            ENSURED_COLLECTIONS_LAST_CHECK[collection] = now\n            return\n        except Exception:\n            try:\n                ENSURED_COLLECTIONS.discard(collection)\n            except Exception:\n                pass\n            try:\n                ENSURED_COLLECTIONS_LAST_CHECK.pop(collection, None)\n            except Exception:\n                pass\n    ensure_collection(client, collection, dim, vector_name)\n    ensure_payload_indexes(client, collection)\n    ENSURED_COLLECTIONS.add(collection)\n    try:\n        ENSURED_COLLECTIONS_LAST_CHECK[collection] = time.time()\n    except Exception:\n        pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_imports_1886": {
      "name": "_extract_imports",
      "type": "function",
      "start_line": 1886,
      "end_line": 1987,
      "content_hash": "841bbcfa2e0a21dc00855e5affd012560f688a9a",
      "content": "def _extract_imports(language: str, text: str) -> list:\n    lines = text.splitlines()\n    imps = []\n    if language == \"python\":\n        for ln in lines:\n            m = re.match(r\"^\\s*import\\s+([\\w\\.]+)\", ln)\n            if m:\n                imps.append(m.group(1))\n                continue\n            m = re.match(r\"^\\s*from\\s+([\\w\\.]+)\\s+import\\s+\", ln)\n            if m:\n                imps.append(m.group(1))\n                continue\n    elif language in (\"javascript\", \"typescript\"):\n        for ln in lines:\n            m = re.match(r\"^\\s*import\\s+.*?from\\s+['\\\"]([^'\\\"]+)['\\\"]\", ln)\n            if m:\n                imps.append(m.group(1))\n                continue\n            m = re.match(r\"^\\s*require\\(\\s*['\\\"]([^'\\\"]+)['\\\"]\\s*\\)\", ln)\n            if m:\n                imps.append(m.group(1))\n                continue\n    elif language == \"go\":\n        block = False\n        for ln in lines:\n            if re.match(r\"^\\s*import\\s*\\(\", ln):\n                block = True\n                continue\n            if block:\n                if \")\" in ln:\n                    block = False\n                    continue\n                m = re.match(r\"^\\s*\\\"([^\\\"]+)\\\"\", ln)\n                if m:\n                    imps.append(m.group(1))\n                    continue\n            m = re.match(r\"^\\s*import\\s+\\\"([^\\\"]+)\\\"\", ln)\n            if m:\n                imps.append(m.group(1))\n                continue\n    elif language == \"java\":\n        for ln in lines:\n            m = re.match(r\"^\\s*import\\s+([\\w\\.\\*]+);\", ln)\n            if m:\n                imps.append(m.group(1))\n                continue\n    elif language == \"csharp\":\n        for ln in lines:\n            # using Namespace.Sub; using static System.Math; using Alias = Namespace.Type;\n            m = re.match(r\"^\\s*using\\s+(?:static\\s+)?([A-Za-z_][\\w\\._]*)(?:\\s*;|\\s*=)\", ln)\n            if m:\n                imps.append(m.group(1))\n                continue\n    elif language == \"php\":\n        for ln in lines:\n            # Namespaced uses: use Foo\\Bar; use function Foo\\bar; use const Foo\\BAR;\n            m = re.match(r\"^\\s*use\\s+(?:function\\s+|const\\s+)?([A-Za-z_][A-Za-z0-9_\\\\\\\\]*)\\s*;\", ln)\n            if m:\n                imps.append(m.group(1).replace(\"\\\\\\\\\", \"\\\\\"))\n                continue\n        for ln in lines:\n            # include/require path-like imports\n            m = re.match(r\"^\\s*(?:include|include_once|require|require_once)\\s*\\(?\\s*['\\\"]([^'\\\"]+)['\\\"]\\s*\\)?\\s*;\", ln)\n            if m:\n                imps.append(m.group(1))\n                continue\n\n    elif language == \"rust\":\n        for ln in lines:\n            m = re.match(r\"^\\s*use\\s+([^;]+);\", ln)\n            if m:\n                imps.append(m.group(1).strip())\n                continue\n    elif language == \"terraform\":\n        # modules/providers are most relevant cross-file references\n        for ln in lines:\n            m = re.match(r\"^\\s*source\\s*=\\s*['\\\"]([^'\\\"]+)['\\\"]\", ln)\n            if m:\n                imps.append(m.group(1))\n                continue\n            m = re.match(r\"^\\s*provider\\s*=\\s*['\\\"]([^'\\\"]+)['\\\"]\", ln)\n            if m:\n                imps.append(m.group(1))\n                continue\n    elif language == \"powershell\":\n        for ln in lines:\n            m = re.match(\n                r\"^\\s*Import-Module\\s+([A-Za-z0-9_.\\-]+)\", ln, flags=re.IGNORECASE\n            )\n            if m:\n                imps.append(m.group(1))\n                continue\n            m = re.match(r\"^\\s*using\\s+module\\s+([^\\s;]+)\", ln, flags=re.IGNORECASE)\n            if m:\n                imps.append(m.group(1))\n                continue\n            m = re.match(r\"^\\s*using\\s+namespace\\s+([^\\s;]+)\", ln, flags=re.IGNORECASE)\n            if m:\n                imps.append(m.group(1))\n                continue\n    return imps[:200]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_calls_1991": {
      "name": "_extract_calls",
      "type": "function",
      "start_line": 1991,
      "end_line": 2020,
      "content_hash": "b75cc75f976ba7303d3efc13b0e09c6afc510e94",
      "content": "def _extract_calls(language: str, text: str) -> list:\n    names = []\n    # Simple heuristic: word followed by '(' that isn't a keyword\n    kw = set(\n        [\n            \"if\",\n            \"for\",\n            \"while\",\n            \"switch\",\n            \"return\",\n            \"new\",\n            \"catch\",\n            \"func\",\n            \"def\",\n            \"class\",\n            \"match\",\n        ]\n    )\n    for m in re.finditer(r\"\\b([A-Za-z_][A-Za-z0-9_]*)\\s*\\(\", text):\n        name = m.group(1)\n        if name not in kw:\n            names.append(name)\n    # Deduplicate preserving order\n    out = []\n    seen = set()\n    for n in names:\n        if n not in seen:\n            out.append(n)\n            seen.add(n)\n    return out[:200]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_indexed_file_hash_2023": {
      "name": "get_indexed_file_hash",
      "type": "function",
      "start_line": 2023,
      "end_line": 2088,
      "content_hash": "79a84e4402977da6585cb1557502fe41644d049a",
      "content": "def get_indexed_file_hash(\n    client: QdrantClient,\n    collection: str,\n    file_path: str,\n    *,\n    repo_id: str | None = None,\n    repo_rel_path: str | None = None,\n) -> str:\n    \"\"\"Return previously indexed file hash for this logical path, or empty string.\n\n    Prefers logical identity (repo_id + repo_rel_path) when available so that\n    worktrees sharing a logical repo can reuse existing index state, but falls\n    back to metadata.path for backwards compatibility.\n    \"\"\"\n    # Prefer logical identity when both repo_id and repo_rel_path are provided\n    if logical_repo_reuse_enabled() and repo_id and repo_rel_path:\n        try:\n            filt = models.Filter(\n                must=[\n                    models.FieldCondition(\n                        key=\"metadata.repo_id\", match=models.MatchValue(value=repo_id)\n                    ),\n                    models.FieldCondition(\n                        key=\"metadata.repo_rel_path\",\n                        match=models.MatchValue(value=repo_rel_path),\n                    ),\n                ]\n            )\n            points, _ = client.scroll(\n                collection_name=collection,\n                scroll_filter=filt,\n                with_payload=True,\n                limit=1,\n            )\n            if points:\n                md = (points[0].payload or {}).get(\"metadata\") or {}\n                fh = md.get(\"file_hash\")\n                if fh:\n                    return str(fh)\n        except Exception:\n            # Fall back to path-based lookup below\n            pass\n\n    # Backwards-compatible path-based lookup\n    try:\n        filt = models.Filter(\n            must=[\n                models.FieldCondition(\n                    key=\"metadata.path\", match=models.MatchValue(value=file_path)\n                )\n            ]\n        )\n        points, _ = client.scroll(\n            collection_name=collection,\n            scroll_filter=filt,\n            with_payload=True,\n            limit=1,\n        )\n        if points:\n            md = (points[0].payload or {}).get(\"metadata\") or {}\n            fh = md.get(\"file_hash\")\n            if fh:\n                return str(fh)\n    except Exception:\n        return \"\"\n    return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_delete_points_by_path_2091": {
      "name": "delete_points_by_path",
      "type": "function",
      "start_line": 2091,
      "end_line": 2106,
      "content_hash": "16c62d9382bcddafc21c7c49b8ce58a060cdf644",
      "content": "def delete_points_by_path(client: QdrantClient, collection: str, file_path: str):\n    try:\n        filt = models.Filter(\n            must=[\n                models.FieldCondition(\n                    key=\"metadata.path\", match=models.MatchValue(value=file_path)\n                )\n            ]\n        )\n        client.delete(\n            collection_name=collection,\n            points_selector=models.FilterSelector(filter=filt),\n            wait=True,\n        )\n    except Exception:\n        pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_embed_batch_2109": {
      "name": "embed_batch",
      "type": "function",
      "start_line": 2109,
      "end_line": 2111,
      "content_hash": "81f84618ab77335d5f6b090f6edc97892f063ffc",
      "content": "def embed_batch(model: \"TextEmbedding\", texts: List[str]) -> List[List[float]]:\n    # fastembed returns a generator of numpy arrays\n    return [vec.tolist() for vec in model.embed(texts)]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_upsert_points_2114": {
      "name": "upsert_points",
      "type": "function",
      "start_line": 2114,
      "end_line": 2159,
      "content_hash": "3b5081ef2d87da6c1bf94982c0f6624737dc715e",
      "content": "def upsert_points(\n    client: QdrantClient, collection: str, points: List[models.PointStruct]\n):\n    if not points:\n        return\n    # Safer upsert for large payloads: chunk + retry with backoff\n    try:\n        bsz = int(os.environ.get(\"INDEX_UPSERT_BATCH\", \"256\") or 256)\n    except Exception:\n        bsz = 256\n    try:\n        retries = int(os.environ.get(\"INDEX_UPSERT_RETRIES\", \"3\") or 3)\n    except Exception:\n        retries = 3\n    try:\n        backoff = float(os.environ.get(\"INDEX_UPSERT_BACKOFF\", \"0.5\") or 0.5)\n    except Exception:\n        backoff = 0.5\n\n    for i in range(0, len(points), max(1, bsz)):\n        batch = points[i : i + max(1, bsz)]\n        attempt = 0\n        while True:\n            try:\n                client.upsert(collection_name=collection, points=batch, wait=True)\n                break\n            except Exception:\n                attempt += 1\n                if attempt >= retries:\n                    # Last-resort: try smaller sub-batches to avoid dropping updates entirely\n                    sub_size = max(1, bsz // 4)\n                    for j in range(0, len(batch), sub_size):\n                        sub = batch[j : j + sub_size]\n                        try:\n                            client.upsert(\n                                collection_name=collection, points=sub, wait=True\n                            )\n                        except Exception:\n                            # Give up on this tiny sub-batch; continue with the rest\n                            pass\n                    break\n                else:\n                    try:\n                        time.sleep(backoff * attempt)\n                    except Exception:\n                        pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_detect_language_2162": {
      "name": "detect_language",
      "type": "function",
      "start_line": 2162,
      "end_line": 2175,
      "content_hash": "4bad376e2769687ddff05543b02031df2109663f",
      "content": "def detect_language(path: Path) -> str:\n    \"\"\"Detect language from file extension or name pattern.\"\"\"\n    # Check extension first\n    ext_lang = CODE_EXTS.get(path.suffix.lower())\n    if ext_lang:\n        return ext_lang\n    # Check extensionless files by name\n    fname_lower = path.name.lower()\n    if fname_lower in EXTENSIONLESS_FILES:\n        return EXTENSIONLESS_FILES[fname_lower]\n    # Handle Dockerfile.* pattern\n    if fname_lower.startswith(\"dockerfile\"):\n        return \"dockerfile\"\n    return \"unknown\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_build_information_2178": {
      "name": "build_information",
      "type": "function",
      "start_line": 2178,
      "end_line": 2184,
      "content_hash": "e21e15b94540da7a53b18d0fe46f51aa46979bc2",
      "content": "def build_information(\n    language: str, path: Path, start: int, end: int, first_line: str\n) -> str:\n    first_line = (first_line or \"\").strip()\n    if len(first_line) > 160:\n        first_line = first_line[:160] + \"\u2026\"\n    return f\"{language} code from {path} lines {start}-{end}. {first_line}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class__Sym_2188": {
      "name": "_Sym",
      "type": "class",
      "start_line": 2188,
      "end_line": 2189,
      "content_hash": "21af50b69e9cdbb26bd3500ed654254db2e38f4e",
      "content": "class _Sym(dict):\n    __getattr__ = dict.get",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_symbols_python_2192": {
      "name": "_extract_symbols_python",
      "type": "function",
      "start_line": 2192,
      "end_line": 2218,
      "content_hash": "928e0d3c48904507680e258bf38f5498622564e0",
      "content": "def _extract_symbols_python(text: str) -> List[_Sym]:\n    try:\n        tree = ast.parse(text)\n    except Exception:\n        return []\n    out: List[_Sym] = []\n    for node in ast.walk(tree):\n        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n            out.append(\n                _Sym(\n                    kind=\"function\",\n                    name=node.name,\n                    start=getattr(node, \"lineno\", 0),\n                    end=getattr(node, \"end_lineno\", getattr(node, \"lineno\", 0)),\n                )\n            )\n        elif isinstance(node, ast.ClassDef):\n            out.append(\n                _Sym(\n                    kind=\"class\",\n                    name=node.name,\n                    start=getattr(node, \"lineno\", 0),\n                    end=getattr(node, \"end_lineno\", getattr(node, \"lineno\", 0)),\n                )\n            )\n    # Filter invalid\n    return [s for s in out if s.start and s.end and s.end >= s.start]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_symbols_js_like_2230": {
      "name": "_extract_symbols_js_like",
      "type": "function",
      "start_line": 2230,
      "end_line": 2251,
      "content_hash": "8f5f555e9a46cd1f4c64c442496277ba2f6a3a96",
      "content": "def _extract_symbols_js_like(text: str) -> List[_Sym]:\n    lines = text.splitlines()\n    syms: List[_Sym] = []\n    for idx, line in enumerate(lines, 1):\n        for pat in _JS_CLASS_PATTERNS:\n            m = re.match(pat, line)\n            if m:\n                syms.append(_Sym(kind=\"class\", name=m.group(1), start=idx, end=idx))\n                break\n        for pat in _JS_FUNC_PATTERNS:\n            m = re.match(pat, line)\n            if m:\n                syms.append(_Sym(kind=\"function\", name=m.group(1), start=idx, end=idx))\n                break\n    # Approximate end by next symbol start-1\n    syms.sort(key=lambda s: s.start)\n    for i in range(len(syms)):\n        if i + 1 < len(syms):\n            syms[i][\"end\"] = max(syms[i].start, syms[i + 1].start - 1)\n        else:\n            syms[i][\"end\"] = max(syms[i].start, len(lines))\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_symbols_go_2254": {
      "name": "_extract_symbols_go",
      "type": "function",
      "start_line": 2254,
      "end_line": 2288,
      "content_hash": "628c91e6927ac5a1851b8537c6475f2fe3688766",
      "content": "def _extract_symbols_go(text: str) -> List[_Sym]:\n    lines = text.splitlines()\n    syms: List[_Sym] = []\n    for idx, line in enumerate(lines, 1):\n        m = re.match(r\"^\\s*type\\s+([A-Za-z_][\\w]*)\\s+struct\\b\", line)\n        if m:\n            syms.append(_Sym(kind=\"struct\", name=m.group(1), start=idx, end=idx))\n            continue\n        m = re.match(r\"^\\s*type\\s+([A-Za-z_][\\w]*)\\s+interface\\b\", line)\n        if m:\n            syms.append(_Sym(kind=\"interface\", name=m.group(1), start=idx, end=idx))\n            continue\n        m = re.match(\n            r\"^\\s*func\\s*\\(\\s*[^)]+\\s+\\*?([A-Za-z_][\\w]*)\\s*\\)\\s*([A-Za-z_][\\w]*)\\s*\\(\",\n            line,\n        )\n        if m:\n            syms.append(\n                _Sym(\n                    kind=\"method\",\n                    name=m.group(2),\n                    path=f\"{m.group(1)}.{m.group(2)}\",\n                    start=idx,\n                    end=idx,\n                )\n            )\n            continue\n        m = re.match(r\"^\\s*func\\s+([A-Za-z_][\\w]*)\\s*\\(\", line)\n        if m:\n            syms.append(_Sym(kind=\"function\", name=m.group(1), start=idx, end=idx))\n            continue\n    syms.sort(key=lambda s: s.start)\n    for i in range(len(syms)):\n        syms[i][\"end\"] = (syms[i + 1].start - 1) if (i + 1 < len(syms)) else len(lines)\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_symbols_java_2291": {
      "name": "_extract_symbols_java",
      "type": "function",
      "start_line": 2291,
      "end_line": 2316,
      "content_hash": "1a7e557b2809f8c303621a77164da0393f1bdd76",
      "content": "def _extract_symbols_java(text: str) -> List[_Sym]:\n    lines = text.splitlines()\n    syms: List[_Sym] = []\n    current_class = None\n    for idx, line in enumerate(lines, 1):\n        m = re.match(\n            r\"^\\s*(?:public|protected|private)?\\s*(?:final\\s+|abstract\\s+)?class\\s+([A-Za-z_][\\w]*)\\b\",\n            line,\n        )\n        if m:\n            current_class = m.group(1)\n            syms.append(_Sym(kind=\"class\", name=current_class, start=idx, end=idx))\n            continue\n        m = re.match(\n            r\"^\\s*(?:public|protected|private)?\\s*(?:static\\s+)?[A-Za-z_<>,\\[\\]]+\\s+([A-Za-z_][\\w]*)\\s*\\(\",\n            line,\n        )\n        if m:\n            name = m.group(1)\n            path = f\"{current_class}.{name}\" if current_class else name\n            syms.append(_Sym(kind=\"method\", name=name, path=path, start=idx, end=idx))\n            continue\n    syms.sort(key=lambda s: s.start)\n    for i in range(len(syms)):\n        syms[i][\"end\"] = (syms[i + 1].start - 1) if (i + 1 < len(syms)) else len(lines)\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_symbols_csharp_2319": {
      "name": "_extract_symbols_csharp",
      "type": "function",
      "start_line": 2319,
      "end_line": 2342,
      "content_hash": "c8e3f1ce8d155e79f5b98e69dc8c56c0446d6818",
      "content": "def _extract_symbols_csharp(text: str) -> List[_Sym]:\n    lines = text.splitlines()\n    syms: List[_Sym] = []\n    current_type = None\n    for idx, line in enumerate(lines, 1):\n        # class / interface / struct / enum\n        m = re.match(r\"^\\s*(?:public|protected|private|internal)?\\s*(?:abstract\\s+|sealed\\s+|static\\s+)?(class|interface|struct|enum)\\s+([A-Za-z_][\\w]*)\\b\", line)\n        if m:\n            kind, name = m.group(1), m.group(2)\n            current_type = name\n            kind_map = {\"class\": \"class\", \"interface\": \"interface\", \"struct\": \"struct\", \"enum\": \"enum\"}\n            syms.append(_Sym(kind=kind_map.get(kind, \"type\"), name=name, start=idx, end=idx))\n            continue\n        # method (very heuristic)\n        m = re.match(r\"^\\s*(?:public|protected|private|internal)?\\s*(?:static\\s+|virtual\\s+|override\\s+|async\\s+)?[A-Za-z_<>,\\[\\]\\.]+\\s+([A-Za-z_][\\w]*)\\s*\\(\", line)\n        if m:\n            name = m.group(1)\n            path = f\"{current_type}.{name}\" if current_type else name\n            syms.append(_Sym(kind=\"method\", name=name, path=path, start=idx, end=idx))\n            continue\n    syms.sort(key=lambda s: s.start)\n    for i in range(len(syms)):\n        syms[i][\"end\"] = (syms[i + 1].start - 1) if (i + 1 < len(syms)) else len(lines)\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_symbols_php_2345": {
      "name": "_extract_symbols_php",
      "type": "function",
      "start_line": 2345,
      "end_line": 2379,
      "content_hash": "384174e278b85d1e9986a6c4cbe3eb47912201f9",
      "content": "def _extract_symbols_php(text: str) -> List[_Sym]:\n    lines = text.splitlines()\n    syms: List[_Sym] = []\n    current_type = None\n    depth = 0\n    for idx, line in enumerate(lines, 1):\n        # track simple brace depth to reset current_type when leaving class\n        depth += line.count(\"{\")\n        depth -= line.count(\"}\")\n        if depth <= 0:\n            current_type = None\n        # namespace declaration (optional informational anchor)\n        m = re.match(r\"^\\s*namespace\\s+([A-Za-z_][A-Za-z0-9_\\\\\\\\]*)\\s*;\", line)\n        if m:\n            ns = m.group(1).replace(\"\\\\\\\\\", \"\\\\\")\n            syms.append(_Sym(kind=\"namespace\", name=ns, start=idx, end=idx))\n            continue\n        # class/interface/trait\n        m = re.match(r\"^\\s*(?:final\\s+|abstract\\s+)?(class|interface|trait)\\s+([A-Za-z_][\\w]*)\\b\", line)\n        if m:\n            kind, name = m.group(1), m.group(2)\n            current_type = name\n            syms.append(_Sym(kind=kind, name=name, start=idx, end=idx))\n            continue\n        # methods or functions\n        m = re.match(r\"^\\s*(?:public|private|protected)?\\s*(?:static\\s+)?function\\s+([A-Za-z_][\\w]*)\\s*\\(\", line)\n        if m:\n            name = m.group(1)\n            path = f\"{current_type}.{name}\" if current_type else name\n            syms.append(_Sym(kind=\"method\" if current_type else \"function\", name=name, path=path, start=idx, end=idx))\n            continue\n    syms.sort(key=lambda s: s.start)\n    for i in range(len(syms)):\n        syms[i][\"end\"] = (syms[i + 1].start - 1) if (i + 1 < len(syms)) else len(lines)\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_symbols_shell_2382": {
      "name": "_extract_symbols_shell",
      "type": "function",
      "start_line": 2382,
      "end_line": 2394,
      "content_hash": "9d5985fb222b84038d765cc8e68b46226ecf3ae8",
      "content": "def _extract_symbols_shell(text: str) -> List[_Sym]:\n    lines = text.splitlines()\n    syms: List[_Sym] = []\n    for idx, line in enumerate(lines, 1):\n        m = re.match(r\"^\\s*([A-Za-z_][\\w]*)\\s*\\(\\)\\s*\\{\", line)\n        if m:\n            syms.append(_Sym(kind=\"function\", name=m.group(1), start=idx, end=idx))\n            continue\n        m = re.match(r\"^\\s*function\\s+([A-Za-z_][\\w]*)\\s*\\{\", line)\n        if m:\n            syms.append(_Sym(kind=\"function\", name=m.group(1), start=idx, end=idx))\n            continue\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_symbols_yaml_2397": {
      "name": "_extract_symbols_yaml",
      "type": "function",
      "start_line": 2397,
      "end_line": 2407,
      "content_hash": "12f3140ff92a5f22858edb87f79a795a123e4bf9",
      "content": "def _extract_symbols_yaml(text: str) -> List[_Sym]:\n    lines = text.splitlines()\n    syms: List[_Sym] = []\n    for idx, line in enumerate(lines, 1):\n        # treat Markdown-style headings in YAML files as anchors\n        m = re.match(r\"^#\\s+(.+)$\", line)\n        if m:\n            syms.append(\n                _Sym(kind=\"heading\", name=m.group(1).strip(), start=idx, end=idx)\n            )\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_symbols_powershell_2410": {
      "name": "_extract_symbols_powershell",
      "type": "function",
      "start_line": 2410,
      "end_line": 2428,
      "content_hash": "044dca7f59b0f11311ee0879702b80d4bf129802",
      "content": "def _extract_symbols_powershell(text: str) -> List[_Sym]:\n    lines = text.splitlines()\n    syms: List[_Sym] = []\n    for idx, line in enumerate(lines, 1):\n        if re.match(\n            r\"^\\s*function\\s+([A-Za-z_][\\w-]*)\\s*\\{\", line, flags=re.IGNORECASE\n        ):\n            name = (\n                re.sub(r\"^\\s*function\\s+\", \"\", line, flags=re.IGNORECASE)\n                .split(\"{\")[0]\n                .strip()\n            )\n            syms.append(_Sym(kind=\"function\", name=name, start=idx, end=idx))\n            continue\n        m = re.match(r\"^\\s*class\\s+([A-Za-z_][\\w-]*)\\s*\\{\", line, flags=re.IGNORECASE)\n        if m:\n            syms.append(_Sym(kind=\"class\", name=m.group(1), start=idx, end=idx))\n            continue\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_symbols_rust_2431": {
      "name": "_extract_symbols_rust",
      "type": "function",
      "start_line": 2431,
      "end_line": 2463,
      "content_hash": "c126811f621e796d2758a43b93544ad754e2dde4",
      "content": "def _extract_symbols_rust(text: str) -> List[_Sym]:\n    lines = text.splitlines()\n    syms: List[_Sym] = []\n    current_impl = None\n    for idx, line in enumerate(lines, 1):\n        m = re.match(r\"^\\s*impl(?:\\s*<[^>]+>)?\\s*([A-Za-z_][\\w:]*)\", line)\n        if m:\n            current_impl = m.group(1)\n            syms.append(_Sym(kind=\"impl\", name=current_impl, start=idx, end=idx))\n            continue\n        m = re.match(r\"^\\s*(?:pub\\s+)?struct\\s+([A-Za-z_][\\w]*)\\b\", line)\n        if m:\n            syms.append(_Sym(kind=\"struct\", name=m.group(1), start=idx, end=idx))\n            continue\n        m = re.match(r\"^\\s*(?:pub\\s+)?enum\\s+([A-Za-z_][\\w]*)\\b\", line)\n        if m:\n            syms.append(_Sym(kind=\"enum\", name=m.group(1), start=idx, end=idx))\n            continue\n        m = re.match(r\"^\\s*(?:pub\\s+)?trait\\s+([A-Za-z_][\\w]*)\\b\", line)\n        if m:\n            syms.append(_Sym(kind=\"trait\", name=m.group(1), start=idx, end=idx))\n            continue\n        m = re.match(r\"^\\s*(?:pub\\s+)?fn\\s+([A-Za-z_][\\w]*)\\s*\\(\", line)\n        if m:\n            name = m.group(1)\n            path = f\"{current_impl}::{name}\" if current_impl else name\n            kind = \"method\" if current_impl else \"function\"\n            syms.append(_Sym(kind=kind, name=name, path=path, start=idx, end=idx))\n            continue\n    syms.sort(key=lambda s: s.start)\n    for i in range(len(syms)):\n        syms[i][\"end\"] = (syms[i + 1].start - 1) if (i + 1 < len(syms)) else len(lines)\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ts_parser_2467": {
      "name": "_ts_parser",
      "type": "function",
      "start_line": 2467,
      "end_line": 2482,
      "content_hash": "4ec125e431e137c6fc932226ed0723794ff6b4c7",
      "content": "def _ts_parser(lang_key: str):\n    \"\"\"Return a tree-sitter Parser for the given language key.\n\n    Uses tree-sitter 0.25+ API with pre-loaded Language objects.\n    \"\"\"\n    if not _use_tree_sitter():\n        return None\n\n    if Parser is None or lang_key not in _TS_LANGUAGES:\n        return None\n\n    try:\n        lang = _TS_LANGUAGES[lang_key]\n        return Parser(lang)\n    except Exception:\n        return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ts_extract_symbols_python_2485": {
      "name": "_ts_extract_symbols_python",
      "type": "function",
      "start_line": 2485,
      "end_line": 2537,
      "content_hash": "254c5da437873474dca328812d6a58f5a0c92484",
      "content": "def _ts_extract_symbols_python(text: str) -> List[_Sym]:\n    parser = _ts_parser(\"python\")\n    if not parser:\n        return []\n    try:\n        tree = parser.parse(text.encode(\"utf-8\"))\n        if tree is None:\n            return []\n        root = tree.root_node\n    except (ValueError, Exception) as e:\n        # Parsing can fail on malformed code - fallback to empty symbols\n        print(f\"[WARN] Tree-sitter parse failed for Python: {e}\")\n        return []\n    syms: List[_Sym] = []\n\n    def node_text(n):\n        return text.encode(\"utf-8\")[n.start_byte : n.end_byte].decode(\n            \"utf-8\", errors=\"ignore\"\n        )\n\n    class_stack: List[str] = []\n\n    def walk(n):\n        t = n.type\n        if t == \"class_definition\":\n            name_node = n.child_by_field_name(\"name\")\n            cls = node_text(name_node) if name_node else \"\"\n            start = n.start_point[0] + 1\n            end = n.end_point[0] + 1\n            syms.append(_Sym(kind=\"class\", name=cls, start=start, end=end))\n            class_stack.append(cls)\n            # Walk body\n            for c in n.children:\n                walk(c)\n            class_stack.pop()\n            return\n        if t == \"function_definition\":\n            name_node = n.child_by_field_name(\"name\")\n            fn = node_text(name_node) if name_node else \"\"\n            start = n.start_point[0] + 1\n            end = n.end_point[0] + 1\n            if class_stack:\n                path = f\"{class_stack[-1]}.{fn}\"\n                syms.append(\n                    _Sym(kind=\"method\", name=fn, path=path, start=start, end=end)\n                )\n            else:\n                syms.append(_Sym(kind=\"function\", name=fn, start=start, end=end))\n        for c in n.children:\n            walk(c)\n\n    walk(root)\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_node_text_2500": {
      "name": "node_text",
      "type": "function",
      "start_line": 2500,
      "end_line": 2503,
      "content_hash": "fc04ec458a2ec8e2c52d6baedfb13251921fe0e6",
      "content": "    def node_text(n):\n        return text.encode(\"utf-8\")[n.start_byte : n.end_byte].decode(\n            \"utf-8\", errors=\"ignore\"\n        )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_walk_2507": {
      "name": "walk",
      "type": "function",
      "start_line": 2507,
      "end_line": 2534,
      "content_hash": "1361eb21e7f7df0462324b24bc17cb4db1ffd3a5",
      "content": "    def walk(n):\n        t = n.type\n        if t == \"class_definition\":\n            name_node = n.child_by_field_name(\"name\")\n            cls = node_text(name_node) if name_node else \"\"\n            start = n.start_point[0] + 1\n            end = n.end_point[0] + 1\n            syms.append(_Sym(kind=\"class\", name=cls, start=start, end=end))\n            class_stack.append(cls)\n            # Walk body\n            for c in n.children:\n                walk(c)\n            class_stack.pop()\n            return\n        if t == \"function_definition\":\n            name_node = n.child_by_field_name(\"name\")\n            fn = node_text(name_node) if name_node else \"\"\n            start = n.start_point[0] + 1\n            end = n.end_point[0] + 1\n            if class_stack:\n                path = f\"{class_stack[-1]}.{fn}\"\n                syms.append(\n                    _Sym(kind=\"method\", name=fn, path=path, start=start, end=end)\n                )\n            else:\n                syms.append(_Sym(kind=\"function\", name=fn, start=start, end=end))\n        for c in n.children:\n            walk(c)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ts_extract_symbols_js_2540": {
      "name": "_ts_extract_symbols_js",
      "type": "function",
      "start_line": 2540,
      "end_line": 2611,
      "content_hash": "edc7a419d5d78edeadbead4c2b79b795c19118a0",
      "content": "def _ts_extract_symbols_js(text: str) -> List[_Sym]:\n    # Works for javascript/typescript using the most-specific available grammar\n    parser = _ts_parser(\"javascript\")\n    if not parser:\n        return []\n    try:\n        tree = parser.parse(text.encode(\"utf-8\"))\n        if tree is None:\n            return []\n        root = tree.root_node\n    except (ValueError, Exception) as e:\n        # Parsing can fail on malformed code - fallback to empty symbols\n        print(f\"[WARN] Tree-sitter parse failed for JavaScript/TypeScript: {e}\")\n        return []\n    syms: List[_Sym] = []\n\n    def node_text(n):\n        return text.encode(\"utf-8\")[n.start_byte : n.end_byte].decode(\n            \"utf-8\", errors=\"ignore\"\n        )\n\n    class_stack: List[str] = []\n\n    def walk(n):\n        t = n.type\n        if t == \"class_declaration\":\n            name_node = n.child_by_field_name(\"name\")\n            cls = node_text(name_node) if name_node else \"\"\n            start = n.start_point[0] + 1\n            end = n.end_point[0] + 1\n            syms.append(_Sym(kind=\"class\", name=cls, start=start, end=end))\n            class_stack.append(cls)\n            for c in n.children:\n                walk(c)\n            class_stack.pop()\n            return\n        if t in (\"function_declaration\",):\n            name_node = n.child_by_field_name(\"name\")\n            fn = node_text(name_node) if name_node else \"\"\n            start = n.start_point[0] + 1\n            end = n.end_point[0] + 1\n            syms.append(_Sym(kind=\"function\", name=fn, start=start, end=end))\n        if t == \"method_definition\":\n            name_node = n.child_by_field_name(\"name\")\n            m = node_text(name_node) if name_node else \"\"\n            start = n.start_point[0] + 1\n            end = n.end_point[0] + 1\n            path = f\"{class_stack[-1]}.{m}\" if class_stack else m\n            syms.append(_Sym(kind=\"method\", name=m, path=path, start=start, end=end))\n        # Handle variable declarations with function expressions or arrow functions\n        # e.g., const g = function() {}, const h = () => {}, var j = function() {}\n        if t == \"variable_declarator\":\n            # Check if the value is a function expression or arrow function\n            name_node = None\n            value_node = None\n            for c in n.children:\n                if c.type == \"identifier\" and name_node is None:\n                    name_node = c\n                elif c.type in (\"function_expression\", \"arrow_function\"):\n                    value_node = c\n            if name_node and value_node:\n                fn = node_text(name_node)\n                start = n.start_point[0] + 1\n                end = n.end_point[0] + 1\n                syms.append(_Sym(kind=\"function\", name=fn, start=start, end=end))\n                # Don't recurse into the function expression to avoid duplicates\n                return\n        for c in n.children:\n            walk(c)\n\n    walk(root)\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_node_text_2556": {
      "name": "node_text",
      "type": "function",
      "start_line": 2556,
      "end_line": 2559,
      "content_hash": "fc04ec458a2ec8e2c52d6baedfb13251921fe0e6",
      "content": "    def node_text(n):\n        return text.encode(\"utf-8\")[n.start_byte : n.end_byte].decode(\n            \"utf-8\", errors=\"ignore\"\n        )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_walk_2563": {
      "name": "walk",
      "type": "function",
      "start_line": 2563,
      "end_line": 2608,
      "content_hash": "253d6730f3d662628214e605b80a0682b67e709a",
      "content": "    def walk(n):\n        t = n.type\n        if t == \"class_declaration\":\n            name_node = n.child_by_field_name(\"name\")\n            cls = node_text(name_node) if name_node else \"\"\n            start = n.start_point[0] + 1\n            end = n.end_point[0] + 1\n            syms.append(_Sym(kind=\"class\", name=cls, start=start, end=end))\n            class_stack.append(cls)\n            for c in n.children:\n                walk(c)\n            class_stack.pop()\n            return\n        if t in (\"function_declaration\",):\n            name_node = n.child_by_field_name(\"name\")\n            fn = node_text(name_node) if name_node else \"\"\n            start = n.start_point[0] + 1\n            end = n.end_point[0] + 1\n            syms.append(_Sym(kind=\"function\", name=fn, start=start, end=end))\n        if t == \"method_definition\":\n            name_node = n.child_by_field_name(\"name\")\n            m = node_text(name_node) if name_node else \"\"\n            start = n.start_point[0] + 1\n            end = n.end_point[0] + 1\n            path = f\"{class_stack[-1]}.{m}\" if class_stack else m\n            syms.append(_Sym(kind=\"method\", name=m, path=path, start=start, end=end))\n        # Handle variable declarations with function expressions or arrow functions\n        # e.g., const g = function() {}, const h = () => {}, var j = function() {}\n        if t == \"variable_declarator\":\n            # Check if the value is a function expression or arrow function\n            name_node = None\n            value_node = None\n            for c in n.children:\n                if c.type == \"identifier\" and name_node is None:\n                    name_node = c\n                elif c.type in (\"function_expression\", \"arrow_function\"):\n                    value_node = c\n            if name_node and value_node:\n                fn = node_text(name_node)\n                start = n.start_point[0] + 1\n                end = n.end_point[0] + 1\n                syms.append(_Sym(kind=\"function\", name=fn, start=start, end=end))\n                # Don't recurse into the function expression to avoid duplicates\n                return\n        for c in n.children:\n            walk(c)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ts_extract_symbols_2614": {
      "name": "_ts_extract_symbols",
      "type": "function",
      "start_line": 2614,
      "end_line": 2697,
      "content_hash": "f62be41236e0415ef1a2d5f66a298e0dd2a9342f",
      "content": "def _ts_extract_symbols(language: str, text: str) -> List[_Sym]:\n    if language == \"python\":\n        return _ts_extract_symbols_python(text)\n    if language == \"javascript\":\n        return _ts_extract_symbols_js(text)\n    if language == \"typescript\":\n        # Prefer TypeScript grammar when available; otherwise fall back to JS grammar.\n        if \"typescript\" in _TS_LANGUAGES:\n            parser = _ts_parser(\"typescript\")\n            if parser:\n                try:\n                    tree = parser.parse(text.encode(\"utf-8\"))\n                    if tree is None:\n                        return []\n                    root = tree.root_node\n                except (ValueError, Exception) as e:\n                    print(f\"[WARN] Tree-sitter parse failed for TypeScript: {e}\")\n                    return []\n\n                syms: List[_Sym] = []\n\n                def node_text(n):\n                    return text.encode(\"utf-8\")[n.start_byte : n.end_byte].decode(\n                        \"utf-8\", errors=\"ignore\"\n                    )\n\n                class_stack: List[str] = []\n\n                def walk(n):\n                    t = n.type\n                    if t == \"class_declaration\":\n                        name_node = n.child_by_field_name(\"name\")\n                        cls = node_text(name_node) if name_node else \"\"\n                        start = n.start_point[0] + 1\n                        end = n.end_point[0] + 1\n                        syms.append(_Sym(kind=\"class\", name=cls, start=start, end=end))\n                        class_stack.append(cls)\n                        for c in n.children:\n                            walk(c)\n                        class_stack.pop()\n                        return\n                    if t in (\"function_declaration\",):\n                        name_node = n.child_by_field_name(\"name\")\n                        fn = node_text(name_node) if name_node else \"\"\n                        start = n.start_point[0] + 1\n                        end = n.end_point[0] + 1\n                        syms.append(_Sym(kind=\"function\", name=fn, start=start, end=end))\n                    if t == \"method_definition\":\n                        name_node = n.child_by_field_name(\"name\")\n                        m = node_text(name_node) if name_node else \"\"\n                        start = n.start_point[0] + 1\n                        end = n.end_point[0] + 1\n                        path = f\"{class_stack[-1]}.{m}\" if class_stack else m\n                        syms.append(_Sym(kind=\"method\", name=m, path=path, start=start, end=end))\n                    # Handle variable declarations with function expressions or arrow functions\n                    # e.g., const g = function() {}, const h = () => {}, var j = function() {}\n                    if t == \"variable_declarator\":\n                        # Check if the value is a function expression or arrow function\n                        name_node = None\n                        value_node = None\n                        for c in n.children:\n                            if c.type == \"identifier\" and name_node is None:\n                                name_node = c\n                            elif c.type in (\"function_expression\", \"arrow_function\"):\n                                value_node = c\n                        if name_node and value_node:\n                            fn = node_text(name_node)\n                            start = n.start_point[0] + 1\n                            end = n.end_point[0] + 1\n                            syms.append(_Sym(kind=\"function\", name=fn, start=start, end=end))\n                            # Don't recurse into the function expression to avoid duplicates\n                            return\n                    for c in n.children:\n                        walk(c)\n\n                walk(root)\n                return syms\n\n        return _ts_extract_symbols_js(text)\n\n    if language == \"yaml\":\n        return _ts_extract_symbols_yaml(text)\n\n    return []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_node_text_2635": {
      "name": "node_text",
      "type": "function",
      "start_line": 2635,
      "end_line": 2638,
      "content_hash": "07b0d2be5afe9a080334b3852e65fd0ef14b73e3",
      "content": "                def node_text(n):\n                    return text.encode(\"utf-8\")[n.start_byte : n.end_byte].decode(\n                        \"utf-8\", errors=\"ignore\"\n                    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_walk_2642": {
      "name": "walk",
      "type": "function",
      "start_line": 2642,
      "end_line": 2687,
      "content_hash": "0434ffb0cbce3ffa0e7d7683595b76e279b5d9bc",
      "content": "                def walk(n):\n                    t = n.type\n                    if t == \"class_declaration\":\n                        name_node = n.child_by_field_name(\"name\")\n                        cls = node_text(name_node) if name_node else \"\"\n                        start = n.start_point[0] + 1\n                        end = n.end_point[0] + 1\n                        syms.append(_Sym(kind=\"class\", name=cls, start=start, end=end))\n                        class_stack.append(cls)\n                        for c in n.children:\n                            walk(c)\n                        class_stack.pop()\n                        return\n                    if t in (\"function_declaration\",):\n                        name_node = n.child_by_field_name(\"name\")\n                        fn = node_text(name_node) if name_node else \"\"\n                        start = n.start_point[0] + 1\n                        end = n.end_point[0] + 1\n                        syms.append(_Sym(kind=\"function\", name=fn, start=start, end=end))\n                    if t == \"method_definition\":\n                        name_node = n.child_by_field_name(\"name\")\n                        m = node_text(name_node) if name_node else \"\"\n                        start = n.start_point[0] + 1\n                        end = n.end_point[0] + 1\n                        path = f\"{class_stack[-1]}.{m}\" if class_stack else m\n                        syms.append(_Sym(kind=\"method\", name=m, path=path, start=start, end=end))\n                    # Handle variable declarations with function expressions or arrow functions\n                    # e.g., const g = function() {}, const h = () => {}, var j = function() {}\n                    if t == \"variable_declarator\":\n                        # Check if the value is a function expression or arrow function\n                        name_node = None\n                        value_node = None\n                        for c in n.children:\n                            if c.type == \"identifier\" and name_node is None:\n                                name_node = c\n                            elif c.type in (\"function_expression\", \"arrow_function\"):\n                                value_node = c\n                        if name_node and value_node:\n                            fn = node_text(name_node)\n                            start = n.start_point[0] + 1\n                            end = n.end_point[0] + 1\n                            syms.append(_Sym(kind=\"function\", name=fn, start=start, end=end))\n                            # Don't recurse into the function expression to avoid duplicates\n                            return\n                    for c in n.children:\n                        walk(c)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ts_extract_symbols_yaml_2700": {
      "name": "_ts_extract_symbols_yaml",
      "type": "function",
      "start_line": 2700,
      "end_line": 2777,
      "content_hash": "8d73ca19484e72ad37aae39de38872289ddcd3d8",
      "content": "def _ts_extract_symbols_yaml(text: str) -> List[_Sym]:\n    \"\"\"Tree-sitter based YAML symbol extraction.\n\n    Extracts top-level keys, anchors, and nested structure.\n    \"\"\"\n    parser = _ts_parser(\"yaml\")\n    if not parser:\n        return []\n    try:\n        tree = parser.parse(text.encode(\"utf-8\"))\n        if tree is None:\n            return []\n        root = tree.root_node\n    except (ValueError, Exception):\n        return []\n\n    syms: List[_Sym] = []\n    text_bytes = text.encode(\"utf-8\")\n    lines = text_bytes.split(b\"\\n\")\n\n    def _node_text(node) -> str:\n        # Use byte offsets on the byte string, then decode, to handle non-ASCII correctly\n        return text_bytes[node.start_byte:node.end_byte].decode(\"utf-8\", errors=\"replace\")\n\n    def walk(node, path: list[str] | None = None):\n        path = path or []\n        ntype = node.type if hasattr(node, \"type\") else \"\"\n\n        # block_mapping_pair is a key-value pair in YAML\n        if ntype == \"block_mapping_pair\":\n            # First child is typically the key\n            key_node = None\n            for child in node.children:\n                if hasattr(child, \"type\") and child.type == \"flow_node\":\n                    key_node = child\n                    break\n                # Sometimes key is direct (plain_scalar, etc.)\n                if hasattr(child, \"type\") and child.type in (\n                    \"plain_scalar\",\n                    \"double_quote_scalar\",\n                    \"single_quote_scalar\",\n                ):\n                    key_node = child\n                    break\n            if key_node:\n                key = _node_text(key_node).strip().strip('\"').strip(\"'\")\n                if key:\n                    full_path = \".\".join(path + [key])\n                    syms.append(\n                        _Sym(\n                            kind=\"key\",\n                            name=full_path,\n                            start=node.start_point[0] + 1,\n                            end=node.end_point[0] + 1,\n                        )\n                    )\n                    # Recurse with updated path\n                    for child in node.children:\n                        walk(child, path + [key])\n                    return\n\n        # anchor_name or alias_name\n        if ntype in (\"anchor\", \"alias\"):\n            name = _node_text(node)\n            syms.append(\n                _Sym(\n                    kind=\"anchor\" if ntype == \"anchor\" else \"alias\",\n                    name=name,\n                    start=node.start_point[0] + 1,\n                    end=node.end_point[0] + 1,\n                )\n            )\n\n        for child in node.children:\n            walk(child, path)\n\n    walk(root)\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__node_text_2720": {
      "name": "_node_text",
      "type": "function",
      "start_line": 2720,
      "end_line": 2722,
      "content_hash": "b9935fa1fb95cbb9a211182b9c39591e8369755e",
      "content": "    def _node_text(node) -> str:\n        # Use byte offsets on the byte string, then decode, to handle non-ASCII correctly\n        return text_bytes[node.start_byte:node.end_byte].decode(\"utf-8\", errors=\"replace\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_walk_2724": {
      "name": "walk",
      "type": "function",
      "start_line": 2724,
      "end_line": 2774,
      "content_hash": "b74eeccc49d3448aa91717741051046472379932",
      "content": "    def walk(node, path: list[str] | None = None):\n        path = path or []\n        ntype = node.type if hasattr(node, \"type\") else \"\"\n\n        # block_mapping_pair is a key-value pair in YAML\n        if ntype == \"block_mapping_pair\":\n            # First child is typically the key\n            key_node = None\n            for child in node.children:\n                if hasattr(child, \"type\") and child.type == \"flow_node\":\n                    key_node = child\n                    break\n                # Sometimes key is direct (plain_scalar, etc.)\n                if hasattr(child, \"type\") and child.type in (\n                    \"plain_scalar\",\n                    \"double_quote_scalar\",\n                    \"single_quote_scalar\",\n                ):\n                    key_node = child\n                    break\n            if key_node:\n                key = _node_text(key_node).strip().strip('\"').strip(\"'\")\n                if key:\n                    full_path = \".\".join(path + [key])\n                    syms.append(\n                        _Sym(\n                            kind=\"key\",\n                            name=full_path,\n                            start=node.start_point[0] + 1,\n                            end=node.end_point[0] + 1,\n                        )\n                    )\n                    # Recurse with updated path\n                    for child in node.children:\n                        walk(child, path + [key])\n                    return\n\n        # anchor_name or alias_name\n        if ntype in (\"anchor\", \"alias\"):\n            name = _node_text(node)\n            syms.append(\n                _Sym(\n                    kind=\"anchor\" if ntype == \"anchor\" else \"alias\",\n                    name=name,\n                    start=node.start_point[0] + 1,\n                    end=node.end_point[0] + 1,\n                )\n            )\n\n        for child in node.children:\n            walk(child, path)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ts_extract_imports_calls_python_2780": {
      "name": "_ts_extract_imports_calls_python",
      "type": "function",
      "start_line": 2780,
      "end_line": 2830,
      "content_hash": "99f0918c5fcb6b47e692b8f700c2f8c06f3a7386",
      "content": "def _ts_extract_imports_calls_python(text: str):\n    parser = _ts_parser(\"python\")\n    if not parser:\n        return [], []\n    data = text.encode(\"utf-8\")\n    try:\n        tree = parser.parse(data)\n        if tree is None:\n            return [], []\n        root = tree.root_node\n    except (ValueError, Exception):\n        return [], []\n\n    def node_text(n):\n        return data[n.start_byte : n.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\n    imports: List[str] = []\n    calls: List[str] = []\n\n    def walk(n):\n        t = n.type\n        if t == \"import_statement\":\n            s = node_text(n)\n            m = re.search(r\"\\bimport\\s+([\\w\\.]+)\", s)\n            if m:\n                imports.append(m.group(1))\n        elif t == \"import_from_statement\":\n            s = node_text(n)\n            m = re.search(r\"\\bfrom\\s+([\\w\\.]+)\\s+import\\b\", s)\n            if m:\n                imports.append(m.group(1))\n        elif t == \"call\":\n            func = n.child_by_field_name(\"function\")\n            if func:\n                name = node_text(func)\n                # Take the last attribute part if dotted\n                base = re.split(r\"[\\.:]\", name)[-1]\n                if re.match(r\"^[A-Za-z_][A-Za-z0-9_]*$\", base):\n                    calls.append(base)\n        for c in n.children:\n            walk(c)\n\n    walk(root)\n    # Deduplicate preserving order\n    seen = set()\n    calls_dedup = []\n    for x in calls:\n        if x not in seen:\n            calls_dedup.append(x)\n            seen.add(x)\n    return imports[:200], calls_dedup[:200]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_node_text_2793": {
      "name": "node_text",
      "type": "function",
      "start_line": 2793,
      "end_line": 2794,
      "content_hash": "9613be95103394eefbe61850c1b35819d4554f5e",
      "content": "    def node_text(n):\n        return data[n.start_byte : n.end_byte].decode(\"utf-8\", errors=\"ignore\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_walk_2799": {
      "name": "walk",
      "type": "function",
      "start_line": 2799,
      "end_line": 2820,
      "content_hash": "fee0620fb4e212e52678c268d702b56ae8178451",
      "content": "    def walk(n):\n        t = n.type\n        if t == \"import_statement\":\n            s = node_text(n)\n            m = re.search(r\"\\bimport\\s+([\\w\\.]+)\", s)\n            if m:\n                imports.append(m.group(1))\n        elif t == \"import_from_statement\":\n            s = node_text(n)\n            m = re.search(r\"\\bfrom\\s+([\\w\\.]+)\\s+import\\b\", s)\n            if m:\n                imports.append(m.group(1))\n        elif t == \"call\":\n            func = n.child_by_field_name(\"function\")\n            if func:\n                name = node_text(func)\n                # Take the last attribute part if dotted\n                base = re.split(r\"[\\.:]\", name)[-1]\n                if re.match(r\"^[A-Za-z_][A-Za-z0-9_]*$\", base):\n                    calls.append(base)\n        for c in n.children:\n            walk(c)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_imports_calls_2833": {
      "name": "_get_imports_calls",
      "type": "function",
      "start_line": 2833,
      "end_line": 2836,
      "content_hash": "3ca0ad71ece8a19dfca2f49ef0fe563d71808864",
      "content": "def _get_imports_calls(language: str, text: str):\n    if _use_tree_sitter() and language == \"python\":\n        return _ts_extract_imports_calls_python(text)\n    return _extract_imports(language, text), _extract_calls(language, text)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_symbols_terraform_2839": {
      "name": "_extract_symbols_terraform",
      "type": "function",
      "start_line": 2839,
      "end_line": 2906,
      "content_hash": "232d16e3ac1653907f4cc79db31b2b2dea42d02f",
      "content": "def _extract_symbols_terraform(text: str) -> List[_Sym]:\n    lines = text.splitlines()\n    syms: List[_Sym] = []\n    for idx, line in enumerate(lines, 1):\n        m = re.match(r\"^\\s*(resource)\\s+\\\"([^\\\"]+)\\\"\\s+\\\"([^\\\"]+)\\\"\\s*\\{\", line)\n        if m:\n            t, name = m.group(2), m.group(3)\n            syms.append(\n                _Sym(kind=\"resource\", name=name, path=f\"{t}.{name}\", start=idx, end=idx)\n            )\n            continue\n        m = re.match(r\"^\\s*(data)\\s+\\\"([^\\\"]+)\\\"\\s+\\\"([^\\\"]+)\\\"\\s*\\{\", line)\n        if m:\n            t, name = m.group(2), m.group(3)\n            syms.append(\n                _Sym(\n                    kind=\"data\", name=name, path=f\"data.{t}.{name}\", start=idx, end=idx\n                )\n            )\n            continue\n        m = re.match(r\"^\\s*(module)\\s+\\\"([^\\\"]+)\\\"\\s*\\{\", line)\n        if m:\n            name = m.group(2)\n            syms.append(\n                _Sym(\n                    kind=\"module\", name=name, path=f\"module.{name}\", start=idx, end=idx\n                )\n            )\n            continue\n        m = re.match(r\"^\\s*(variable)\\s+\\\"([^\\\"]+)\\\"\\s*\\{\", line)\n        if m:\n            name = m.group(2)\n            syms.append(\n                _Sym(kind=\"variable\", name=name, path=f\"var.{name}\", start=idx, end=idx)\n            )\n            continue\n        m = re.match(r\"^\\s*(output)\\s+\\\"([^\\\"]+)\\\"\\s*\\{\", line)\n        if m:\n            name = m.group(2)\n            syms.append(\n                _Sym(\n                    kind=\"output\", name=name, path=f\"output.{name}\", start=idx, end=idx\n                )\n            )\n            continue\n        m = re.match(r\"^\\s*(provider)\\s+\\\"([^\\\"]+)\\\"\\s*\\{\", line)\n        if m:\n            name = m.group(2)\n            syms.append(\n                _Sym(\n                    kind=\"provider\",\n                    name=name,\n                    path=f\"provider.{name}\",\n                    start=idx,\n                    end=idx,\n                )\n            )\n            continue\n        m = re.match(r\"^\\s*(locals)\\s*\\{\", line)\n        if m:\n            syms.append(\n                _Sym(kind=\"locals\", name=\"locals\", path=\"locals\", start=idx, end=idx)\n            )\n            continue\n    syms.sort(key=lambda s: s.start)\n    for i in range(len(syms)):\n        syms[i][\"end\"] = (syms[i + 1].start - 1) if (i + 1 < len(syms)) else len(lines)\n    return syms",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_symbols_2909": {
      "name": "_extract_symbols",
      "type": "function",
      "start_line": 2909,
      "end_line": 2937,
      "content_hash": "5f09ad284cb0492905eca2dee6fae9a3cf796c3d",
      "content": "def _extract_symbols(language: str, text: str) -> List[_Sym]:\n    # Prefer tree-sitter when enabled and supported; fallback to existing extractors\n    if _use_tree_sitter():\n        ts_syms = _ts_extract_symbols(language, text)\n        if ts_syms:\n            return ts_syms\n    if language == \"python\":\n        return _extract_symbols_python(text)\n    if language in (\"javascript\", \"typescript\"):\n        return _extract_symbols_js_like(text)\n    if language == \"go\":\n        return _extract_symbols_go(text)\n    if language == \"java\":\n        return _extract_symbols_java(text)\n    if language == \"rust\":\n        return _extract_symbols_rust(text)\n    if language == \"terraform\":\n        return _extract_symbols_terraform(text)\n    if language == \"shell\":\n        return _extract_symbols_shell(text)\n    if language == \"yaml\":\n        return _extract_symbols_yaml(text)\n    if language == \"powershell\":\n        return _extract_symbols_powershell(text)\n    if language == \"csharp\":\n        return _extract_symbols_csharp(text)\n    if language == \"php\":\n        return _extract_symbols_php(text)\n    return []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__choose_symbol_for_chunk_2940": {
      "name": "_choose_symbol_for_chunk",
      "type": "function",
      "start_line": 2940,
      "end_line": 2957,
      "content_hash": "4d92e4ec764bd721241778889a3f7adb99a064a0",
      "content": "def _choose_symbol_for_chunk(start: int, end: int, symbols: List[_Sym]):\n    if not symbols:\n        return \"\", \"\", \"\"\n    overlaps = [s for s in symbols if s.start <= end and s.end >= start]\n\n    def pick(sym):\n        name = sym.get(\"name\") or \"\"\n        path = sym.get(\"path\") or name\n        return sym.get(\"kind\") or \"\", name, path\n\n    if overlaps:\n        overlaps.sort(key=lambda s: (-(s.start), (s.end - s.start)))\n        return pick(overlaps[0])\n    preceding = [s for s in symbols if s.start <= end]\n    if preceding:\n        s = max(preceding, key=lambda x: x.start)\n        return pick(s)\n    return \"\", \"\", \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_pick_2945": {
      "name": "pick",
      "type": "function",
      "start_line": 2945,
      "end_line": 2948,
      "content_hash": "4e562c8e3f0ef453074bb05b3abe941e21c3708f",
      "content": "    def pick(sym):\n        name = sym.get(\"name\") or \"\"\n        path = sym.get(\"path\") or name\n        return sym.get(\"kind\") or \"\", name, path",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_host_path_from_origin_2960": {
      "name": "_get_host_path_from_origin",
      "type": "function",
      "start_line": 2960,
      "end_line": 2969,
      "content_hash": "15426ae970fbb8e88ce7c680d8a6e7a612f7db93",
      "content": "def _get_host_path_from_origin(workspace_path: str, repo_name: str = None) -> Optional[str]:\n    \"\"\"Get client host_path from origin source_path in workspace state.\"\"\"\n    try:\n        from scripts.workspace_state import get_workspace_state\n        state = get_workspace_state(workspace_path, repo_name)\n        if state and state.get(\"origin\", {}).get(\"source_path\"):\n            return state[\"origin\"][\"source_path\"]\n    except Exception:\n        pass\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__compute_host_and_container_paths_2972": {
      "name": "_compute_host_and_container_paths",
      "type": "function",
      "start_line": 2972,
      "end_line": 3034,
      "content_hash": "d0d3d763d6be5646bbbb89be6a5cea3dd6ac0d9d",
      "content": "def _compute_host_and_container_paths(cur_path: str) -> tuple[Optional[str], Optional[str]]:\n    \"\"\"Compute host_path and container_path for a given absolute path.\n\n    Behavior:\n    - path field in metadata continues to use cur_path as-is (container view).\n    - host_path prefers origin.source_path (client workspace root) when available.\n    - When indexing under /work/<slug>/..., drop the slug when mapping back to host.\n    - HOST_INDEX_PATH is used only as a best-effort fallback and ignored when it\n      looks like a Windows path (contains a drive letter).\n    \"\"\"\n    _host_root = str(os.environ.get(\"HOST_INDEX_PATH\") or \"\").strip().rstrip(\"/\")\n    if \":\" in _host_root:\n        _host_root = \"\"\n    _host_path: Optional[str] = None\n    _container_path: Optional[str] = None\n    _origin_client_path: Optional[str] = None\n\n    # Try to get client workspace root from origin metadata first.\n    try:\n        if cur_path.startswith(\"/work/\"):\n            # Extract workspace from container path\n            _parts = cur_path[6:].split(\"/\")  # Remove \"/work/\" prefix\n            if len(_parts) >= 2:\n                _repo_name = _parts[0]  # First part is repo name / slug\n                _workspace_path = f\"/work/{_repo_name}\"\n                _origin_client_path = _get_host_path_from_origin(\n                    _workspace_path, _repo_name\n                )\n    except Exception:\n        _origin_client_path = None\n\n    try:\n        if cur_path.startswith(\"/work/\") and (_host_root or _origin_client_path):\n            _rel = cur_path[len(\"/work/\") :]\n            # Prioritize client path from origin metadata over HOST_INDEX_PATH.\n            if _origin_client_path:\n                # Drop the leading repo slug (e.g. Context-Engine-<hash>) when mapping\n                # /work paths back to the client workspace root, so host_path is\n                # /home/.../Context-Engine/<rel-path-inside-repo> instead of including\n                # the slug directory.\n                _parts = _rel.split(\"/\", 1)\n                _tail = _parts[1] if len(_parts) > 1 else \"\"\n                _base = _origin_client_path.rstrip(\"/\")\n                _host_path = (\n                    os.path.realpath(os.path.join(_base, _tail)) if _tail else _base\n                )\n            else:\n                _host_path = os.path.realpath(os.path.join(_host_root, _rel))\n            _container_path = cur_path\n        else:\n            # Likely indexing on the host directly\n            _host_path = cur_path\n            if (\n                (_host_root or _origin_client_path)\n                and cur_path.startswith(((_origin_client_path or _host_root) + \"/\"))\n            ):\n                _rel = cur_path[len((_origin_client_path or _host_root)) + 1 :]\n                _container_path = \"/work/\" + _rel\n    except Exception:\n        _host_path = cur_path\n        _container_path = cur_path if cur_path.startswith(\"/work/\") else None\n\n    return _host_path, _container_path",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_index_single_file_3037": {
      "name": "index_single_file",
      "type": "function",
      "start_line": 3037,
      "end_line": 3093,
      "content_hash": "197055a00a54afff032a8fdfbfcc261120429747",
      "content": "def index_single_file(\n    client: QdrantClient,\n    model: \"TextEmbedding\",\n    collection: str,\n    vector_name: str,\n    file_path: Path,\n    *,\n    dedupe: bool = True,\n    skip_unchanged: bool = True,\n    pseudo_mode: str = \"full\",\n    trust_cache: bool | None = None,\n    repo_name_for_cache: str | None = None,\n) -> bool:\n    \"\"\"Index a single file path. Returns True if indexed, False if skipped.\n\n    When trust_cache is enabled (via argument or INDEX_TRUST_CACHE=1), rely solely on the\n    local .codebase/cache.json for unchanged detection and skip Qdrant per-file hash checks.\n    This is a debug-only escape hatch and is unsafe for normal operation: enabling it may\n    hide index/cache drift, especially with git worktree reuse or collection rebuilds.\n    \"\"\"\n\n    try:\n        if _should_skip_explicit_file_by_excluder(file_path):\n            try:\n                delete_points_by_path(client, collection, str(file_path))\n            except Exception:\n                pass\n            print(f\"Skipping excluded file: {file_path}\")\n            return False\n    except Exception:\n        return False\n\n    # Try to acquire per-file lock - skip if another process is indexing this file\n    _file_lock_ctx = None\n    if file_indexing_lock is not None:\n        try:\n            _file_lock_ctx = file_indexing_lock(str(file_path))\n            _file_lock_ctx.__enter__()\n        except FileExistsError:\n            print(f\"[FILE_LOCKED] Skipping {file_path} - another process is indexing it\")\n            return False\n        except Exception:\n            pass  # Continue without lock if lock acquisition fails\n\n    # Wrap the rest in try/finally to ensure lock release\n    try:\n        return _index_single_file_inner(\n            client, model, collection, vector_name, file_path,\n            dedupe=dedupe, skip_unchanged=skip_unchanged, pseudo_mode=pseudo_mode,\n            trust_cache=trust_cache, repo_name_for_cache=repo_name_for_cache,\n        )\n    finally:\n        if _file_lock_ctx is not None:\n            try:\n                _file_lock_ctx.__exit__(None, None, None)\n            except Exception:\n                pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__index_single_file_inner_3096": {
      "name": "_index_single_file_inner",
      "type": "function",
      "start_line": 3096,
      "end_line": 3492,
      "content_hash": "fe25b05c29be7660eced25698511e2f394b677ce",
      "content": "def _index_single_file_inner(\n    client: QdrantClient,\n    model: \"TextEmbedding\",\n    collection: str,\n    vector_name: str,\n    file_path: Path,\n    *,\n    dedupe: bool = True,\n    skip_unchanged: bool = True,\n    pseudo_mode: str = \"full\",\n    trust_cache: bool | None = None,\n    repo_name_for_cache: str | None = None,\n) -> bool:\n    \"\"\"Inner implementation of index_single_file (after lock is acquired).\"\"\"\n\n    # Resolve trust_cache from env when not explicitly provided. INDEX_TRUST_CACHE is intended\n    # for debugging only and should not be enabled in normal indexing runs.\n    if trust_cache is None:\n        try:\n            trust_cache = os.environ.get(\"INDEX_TRUST_CACHE\", \"\").strip().lower() in {\n                \"1\",\n                \"true\",\n                \"yes\",\n                \"on\",\n            }\n        except Exception:\n            trust_cache = False\n\n    fast_fs = _env_truthy(os.environ.get(\"INDEX_FS_FASTPATH\"), False)\n    if skip_unchanged and fast_fs and get_cached_file_meta is not None:\n        try:\n            repo_for_cache = repo_name_for_cache or _detect_repo_name_from_path(file_path)\n            meta = get_cached_file_meta(str(file_path), repo_for_cache) or {}\n            size = meta.get(\"size\")\n            mtime = meta.get(\"mtime\")\n            if size is not None and mtime is not None:\n                st = file_path.stat()\n                if int(getattr(st, \"st_size\", 0)) == int(size) and int(\n                    getattr(st, \"st_mtime\", 0)\n                ) == int(mtime):\n                    print(f\"Skipping unchanged file (fs-meta): {file_path}\")\n                    return False\n        except Exception:\n            pass\n\n    try:\n        text = file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n    except Exception as e:\n        print(f\"Skipping {file_path}: {e}\")\n        return False\n\n    language = detect_language(file_path)\n    file_hash = hashlib.sha1(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n\n    repo_tag = repo_name_for_cache or _detect_repo_name_from_path(file_path)\n\n    # Derive logical repo identity and repo-relative path for cross-worktree reuse.\n    repo_id: str | None = None\n    repo_rel_path: str | None = None\n    if logical_repo_reuse_enabled() and get_workspace_state is not None:\n        try:\n            ws_root = os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"/work\"\n            # Resolve workspace state for this repo to read logical_repo_id\n            state = get_workspace_state(ws_root, repo_tag)\n            lrid = state.get(\"logical_repo_id\") if isinstance(state, dict) else None\n            if isinstance(lrid, str) and lrid:\n                repo_id = lrid\n            # Compute repo-relative path within the current workspace tree\n            try:\n                fp = file_path.resolve()\n            except Exception:\n                fp = file_path\n            try:\n                ws_base = Path(os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"/work\").resolve()\n                repo_root = ws_base\n                if repo_tag:\n                    # In multi-repo scenarios, repos live under /work/<repo_tag>\n                    candidate = ws_base / repo_tag\n                    if candidate.exists():\n                        repo_root = candidate\n                rel = fp.relative_to(repo_root)\n                repo_rel_path = rel.as_posix()\n            except Exception:\n                repo_rel_path = None\n        except Exception as e:\n            print(f\"[logical_repo] Failed to derive logical identity for {file_path}: {e}\")\n\n    # Get changed symbols for pseudo processing optimization\n    changed_symbols = set()\n    if get_cached_symbols and set_cached_symbols:\n        cached_symbols = get_cached_symbols(str(file_path))\n        if cached_symbols:\n            current_symbols = extract_symbols_with_tree_sitter(str(file_path))\n            _, changed = compare_symbol_changes(cached_symbols, current_symbols)\n            # Convert symbol names to IDs for lookup\n            for symbol_data in current_symbols.values():\n                symbol_id = f\"{symbol_data['type']}_{symbol_data['name']}_{symbol_data['start_line']}\"\n                if symbol_id in changed:\n                    changed_symbols.add(symbol_id)\n\n    if skip_unchanged:\n        # Prefer local workspace cache to avoid Qdrant lookups\n        ws_path = os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"/work\"\n        try:\n            if get_cached_file_hash:\n                prev_local = get_cached_file_hash(str(file_path), repo_tag)\n                if prev_local and file_hash and prev_local == file_hash:\n                    # When fs fast-path is enabled, refresh cache entry with size/mtime\n                    if fast_fs and set_cached_file_hash:\n                        try:\n                            set_cached_file_hash(str(file_path), file_hash, repo_tag)\n                        except Exception:\n                            pass\n                    print(f\"Skipping unchanged file (cache): {file_path}\")\n                    return False\n        except Exception:\n            pass\n\n        # Optional Qdrant-backed unchanged detection; disabled when trust_cache is enabled\n        if not trust_cache:\n            prev = get_indexed_file_hash(\n                client,\n                collection,\n                str(file_path),\n                repo_id=repo_id,\n                repo_rel_path=repo_rel_path,\n            )\n            if prev and prev == file_hash:\n                # When fs fast-path is enabled, refresh cache entry with size/mtime\n                if fast_fs and set_cached_file_hash:\n                    try:\n                        set_cached_file_hash(str(file_path), file_hash, repo_tag)\n                    except Exception:\n                        pass\n                print(f\"Skipping unchanged file: {file_path}\")\n                return False\n\n    if dedupe:\n        delete_points_by_path(client, collection, str(file_path))\n\n    symbols = _extract_symbols(language, text)\n    imports, calls = _get_imports_calls(language, text)\n    last_mod, churn_count, author_count = _git_metadata(file_path)\n\n    CHUNK_LINES = int(os.environ.get(\"INDEX_CHUNK_LINES\", \"120\") or 120)\n    CHUNK_OVERLAP = int(os.environ.get(\"INDEX_CHUNK_OVERLAP\", \"20\") or 20)\n    # Micro-chunking (token-based) takes precedence; else semantic; else line-based\n    use_micro = os.environ.get(\"INDEX_MICRO_CHUNKS\", \"0\").lower() in {\n        \"1\",\n        \"true\",\n        \"yes\",\n        \"on\",\n    }\n    use_semantic = os.environ.get(\"INDEX_SEMANTIC_CHUNKS\", \"1\").lower() in {\n        \"1\",\n        \"true\",\n        \"yes\",\n        \"on\",\n    }\n    if use_micro:\n        # Dynamic chunk sizing: adjust tokens/stride to fit within MAX_MICRO_CHUNKS_PER_FILE\n        # Never truncate - instead increase chunk size to cover entire file\n        try:\n            _cap = int(os.environ.get(\"MAX_MICRO_CHUNKS_PER_FILE\", \"200\") or 200)\n            _base_tokens = int(os.environ.get(\"MICRO_CHUNK_TOKENS\", \"128\") or 128)\n            _base_stride = int(os.environ.get(\"MICRO_CHUNK_STRIDE\", \"64\") or 64)\n            \n            # First pass with base settings\n            chunks = chunk_by_tokens(text, k_tokens=_base_tokens, stride_tokens=_base_stride)\n            \n            # If exceeds cap, recalculate chunk size to fit\n            if _cap > 0 and len(chunks) > _cap:\n                _before = len(chunks)\n                # Estimate total tokens and calculate required chunk size\n                # Scale up tokens proportionally to fit within cap\n                _scale = (len(chunks) / _cap) * 1.1  # 10% buffer\n                _new_tokens = max(_base_tokens, int(_base_tokens * _scale))\n                _new_stride = max(_base_stride, int(_base_stride * _scale))\n                \n                # Re-chunk with larger window\n                chunks = chunk_by_tokens(text, k_tokens=_new_tokens, stride_tokens=_new_stride)\n                try:\n                    print(\n                        f\"[ingest] micro-chunks resized path={file_path} count={_before}->{len(chunks)} \"\n                        f\"tokens={_base_tokens}->{_new_tokens} stride={_base_stride}->{_new_stride}\"\n                    )\n                except Exception:\n                    pass\n        except Exception:\n            chunks = chunk_by_tokens(text)\n    elif use_semantic:\n        chunks = chunk_semantic(text, language, CHUNK_LINES, CHUNK_OVERLAP)\n    else:\n        chunks = chunk_lines(text, CHUNK_LINES, CHUNK_OVERLAP)\n    batch_texts: List[str] = []\n    batch_meta: List[Dict] = []\n    batch_ids: List[int] = []\n    batch_lex: List[list[float]] = []\n    batch_lex_text: List[str] = []  # Raw text for sparse vector generation\n\n    def make_point(pid, dense_vec, lex_vec, payload, lex_text: str = \"\"):\n        if vector_name:\n            vecs = {vector_name: dense_vec, LEX_VECTOR_NAME: lex_vec}\n            try:\n                if os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\n                    \"1\",\n                    \"true\",\n                    \"yes\",\n                    \"on\",\n                }:\n                    vecs[MINI_VECTOR_NAME] = project_mini(list(dense_vec), MINI_VEC_DIM)\n            except Exception:\n                pass\n            # Add sparse vector to vecs dict if LEX_SPARSE_MODE enabled (new qdrant-client API)\n            if LEX_SPARSE_MODE and lex_text:\n                sparse_vec = _lex_sparse_vector_text(lex_text)\n                if sparse_vec.get(\"indices\"):\n                    vecs[LEX_SPARSE_NAME] = models.SparseVector(**sparse_vec)\n            return models.PointStruct(id=pid, vector=vecs, payload=payload)\n        else:\n            # unnamed collection: store dense only\n            return models.PointStruct(id=pid, vector=dense_vec, payload=payload)\n\n    # Feature flag: PSEUDO_BATCH_CONCURRENCY > 1 enables parallel GLM calls for pseudo-tags\n    # Default 1 = sequential (existing behavior), 4 = 4x parallel speedup\n    pseudo_batch_concurrency = int(os.environ.get(\"PSEUDO_BATCH_CONCURRENCY\", \"1\") or 1)\n    use_batch_pseudo = pseudo_batch_concurrency > 1 and pseudo_mode == \"full\"\n\n    # Pre-process chunks to collect metadata and identify which need pseudo generation\n    chunk_data: list[dict] = []  # Stores per-chunk info, payload, needs_pseudo flag, cached values\n    for ch in chunks:\n        info = build_information(\n            language,\n            file_path,\n            ch[\"start\"],\n            ch[\"end\"],\n            ch[\"text\"].splitlines()[0] if ch[\"text\"] else \"\",\n        )\n        kind, sym, sym_path = _choose_symbol_for_chunk(ch[\"start\"], ch[\"end\"], symbols)\n        # Prefer embedded symbol metadata from semantic chunker when present\n        if \"kind\" in ch and ch.get(\"kind\"):\n            kind = ch.get(\"kind\") or kind\n        if \"symbol\" in ch and ch.get(\"symbol\"):\n            sym = ch.get(\"symbol\") or sym\n        if \"symbol_path\" in ch and ch.get(\"symbol_path\"):\n            sym_path = ch.get(\"symbol_path\") or sym_path\n        # Ensure chunks always carry symbol metadata so pseudo gating can work for all chunking modes\n        if not ch.get(\"kind\") and kind:\n            ch[\"kind\"] = kind\n        if not ch.get(\"symbol\") and sym:\n            ch[\"symbol\"] = sym\n        if not ch.get(\"symbol_path\") and sym_path:\n            ch[\"symbol_path\"] = sym_path\n        # Track both container path (/work mirror) and original host path for clarity across environments\n        _cur_path = str(file_path)\n        # upload_service writes origin.source_path from the client --path flag so we can\n        # reconstruct host paths even when indexing inside a slugged /work/<repo-hash> tree.\n        _host_path, _container_path = _compute_host_and_container_paths(_cur_path)\n\n        payload = {\n            \"document\": info,\n            \"information\": info,\n            \"metadata\": {\n                \"path\": str(file_path),\n                \"path_prefix\": str(file_path.parent),\n                \"ext\": str(file_path.suffix).lstrip(\".\").lower(),\n                \"language\": language,\n                \"kind\": kind,\n                \"symbol\": sym,\n                \"symbol_path\": sym_path,\n                \"repo\": repo_tag,\n                \"start_line\": ch[\"start\"],\n                \"end_line\": ch[\"end\"],\n                \"code\": ch[\"text\"],\n                \"file_hash\": file_hash,\n                \"imports\": imports,\n                \"calls\": calls,\n                \"ingested_at\": int(time.time()),\n                \"last_modified_at\": int(last_mod),\n                \"churn_count\": int(churn_count),\n                \"author_count\": int(author_count),\n                # Logical identity for cross-worktree reuse\n                \"repo_id\": repo_id,\n                \"repo_rel_path\": repo_rel_path,\n                # New: explicit dual-path tracking\n                \"host_path\": _host_path,\n                \"container_path\": _container_path,\n            },\n        }\n\n        # Check pseudo cache status\n        needs_pseudo_gen = False\n        cached_pseudo, cached_tags = \"\", []\n        if pseudo_mode != \"off\":\n            needs_pseudo_gen, cached_pseudo, cached_tags = should_process_pseudo_for_chunk(\n                str(file_path), ch, changed_symbols\n            )\n\n        chunk_data.append({\n            \"chunk\": ch,\n            \"info\": info,\n            \"payload\": payload,\n            \"kind\": kind,\n            \"needs_pseudo\": needs_pseudo_gen and pseudo_mode == \"full\",\n            \"cached_pseudo\": cached_pseudo,\n            \"cached_tags\": cached_tags,\n        })\n\n    # Batch pseudo generation (feature-flagged)\n    if use_batch_pseudo:\n        # Collect chunks that need pseudo generation\n        pending_indices = [i for i, cd in enumerate(chunk_data) if cd[\"needs_pseudo\"]]\n        pending_texts = [chunk_data[i][\"chunk\"].get(\"text\") or \"\" for i in pending_indices]\n\n        if pending_texts:\n            try:\n                from scripts.refrag_glm import generate_pseudo_tags_batch\n                batch_results = generate_pseudo_tags_batch(pending_texts, concurrency=pseudo_batch_concurrency)\n                # Attach results back to chunk_data\n                for idx, (pseudo, tags) in zip(pending_indices, batch_results):\n                    chunk_data[idx][\"cached_pseudo\"] = pseudo\n                    chunk_data[idx][\"cached_tags\"] = tags\n                    chunk_data[idx][\"needs_pseudo\"] = False  # Mark as processed\n                    # Cache the result\n                    if pseudo or tags:\n                        ch = chunk_data[idx][\"chunk\"]\n                        symbol_name = ch.get(\"symbol\", \"\")\n                        if symbol_name and set_cached_pseudo:\n                            k = ch.get(\"kind\", \"unknown\")\n                            start_line = ch.get(\"start\", 0)\n                            symbol_id = f\"{k}_{symbol_name}_{start_line}\"\n                            set_cached_pseudo(str(file_path), symbol_id, pseudo, tags, file_hash)\n            except Exception as e:\n                # Fall back to sequential on batch failure\n                print(f\"[PSEUDO_BATCH] Batch failed, falling back to sequential: {e}\")\n                use_batch_pseudo = False\n\n    # Build final payloads\n    for cd in chunk_data:\n        ch = cd[\"chunk\"]\n        payload = cd[\"payload\"]\n        pseudo = cd[\"cached_pseudo\"]\n        tags = cd[\"cached_tags\"]\n\n        # Sequential fallback: generate pseudo if still needed (batch disabled or failed)\n        if not use_batch_pseudo and cd[\"needs_pseudo\"]:\n            try:\n                pseudo, tags = generate_pseudo_tags(ch.get(\"text\") or \"\")\n                if pseudo or tags:\n                    # Cache the pseudo data for this symbol\n                    symbol_name = ch.get(\"symbol\", \"\")\n                    if symbol_name:\n                        kind = ch.get(\"kind\", \"unknown\")\n                        start_line = ch.get(\"start\", 0)\n                        symbol_id = f\"{kind}_{symbol_name}_{start_line}\"\n                        if set_cached_pseudo:\n                            set_cached_pseudo(str(file_path), symbol_id, pseudo, tags, file_hash)\n            except Exception:\n                # Fall back to cached values (if any) or empty pseudo/tags\n                pass\n\n        # Attach whichever pseudo/tags we ended up with (cached or freshly generated)\n        if pseudo:\n            payload[\"pseudo\"] = pseudo\n        if tags:\n            payload[\"tags\"] = tags\n        batch_texts.append(cd[\"info\"])\n        batch_meta.append(payload)\n        batch_ids.append(hash_id(ch[\"text\"], str(file_path), ch[\"start\"], ch[\"end\"]))\n        aug_lex_text = (ch.get(\"text\") or \"\") + (\" \" + pseudo if pseudo else \"\") + (\" \" + \" \".join(tags) if tags else \"\")\n        batch_lex.append(_lex_hash_vector_text(aug_lex_text))\n        batch_lex_text.append(aug_lex_text)\n\n    if batch_texts:\n        vectors = embed_batch(model, batch_texts)\n        # Inject pid_str into payloads for server-side gating\n\n        for _idx, _m in enumerate(batch_meta):\n            try:\n                _m[\"pid_str\"] = str(batch_ids[_idx])\n            except Exception:\n                pass\n        points = [\n            make_point(i, v, lx, m, lt)\n            for i, v, lx, m, lt in zip(batch_ids, vectors, batch_lex, batch_meta, batch_lex_text)\n        ]\n        upsert_points(client, collection, points)\n        # Update local file-hash cache only after successful upsert\n        try:\n            ws = os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"/work\"\n            if set_cached_file_hash:\n                file_repo_tag = repo_tag\n                set_cached_file_hash(str(file_path), file_hash, file_repo_tag)\n        except Exception:\n            pass\n        return True\n    return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_make_point_3296": {
      "name": "make_point",
      "type": "function",
      "start_line": 3296,
      "end_line": 3317,
      "content_hash": "e2bf2a241ab1cd998d581309bb3cce8fb5d99e98",
      "content": "    def make_point(pid, dense_vec, lex_vec, payload, lex_text: str = \"\"):\n        if vector_name:\n            vecs = {vector_name: dense_vec, LEX_VECTOR_NAME: lex_vec}\n            try:\n                if os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\n                    \"1\",\n                    \"true\",\n                    \"yes\",\n                    \"on\",\n                }:\n                    vecs[MINI_VECTOR_NAME] = project_mini(list(dense_vec), MINI_VEC_DIM)\n            except Exception:\n                pass\n            # Add sparse vector to vecs dict if LEX_SPARSE_MODE enabled (new qdrant-client API)\n            if LEX_SPARSE_MODE and lex_text:\n                sparse_vec = _lex_sparse_vector_text(lex_text)\n                if sparse_vec.get(\"indices\"):\n                    vecs[LEX_SPARSE_NAME] = models.SparseVector(**sparse_vec)\n            return models.PointStruct(id=pid, vector=vecs, payload=payload)\n        else:\n            # unnamed collection: store dense only\n            return models.PointStruct(id=pid, vector=dense_vec, payload=payload)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_index_repo_3494": {
      "name": "index_repo",
      "type": "function",
      "start_line": 3494,
      "end_line": 4379,
      "content_hash": "1a9794977e1151b00f199d550e7a4c23d9f68563",
      "content": "def index_repo(\n    root: Path,\n    qdrant_url: str,\n    api_key: Optional[str],\n    collection: Optional[str],\n    model_name: str,\n    recreate: bool,\n    *,\n    dedupe: bool = True,\n    skip_unchanged: bool = True,\n    pseudo_mode: str = \"full\",\n    clear_caches: bool = False,\n):\n    \"\"\"Index a repository into Qdrant.\n\n    Note: Caller is responsible for acquiring indexing_lock() if coordination\n    with watcher is required. See workspace_state.indexing_lock() for details.\n    \"\"\"\n    ws_path = str(root)\n    repo_tag: Optional[str] = None\n    try:\n        repo_tag = _detect_repo_name_from_path(root) if _detect_repo_name_from_path else None\n    except Exception:\n        repo_tag = None\n\n    if clear_caches:\n        try:\n            _clear_indexing_caches_for_run(ws_path, repo_tag)\n        except Exception as e:\n            import traceback\n\n            print(f\"[ERROR] Failed to clear caches before indexing: {e}\")\n            print(f\"[ERROR] Traceback: {traceback.format_exc()}\")\n    # Optional fast no-change precheck: when INDEX_FS_FASTPATH is enabled, use\n    # fs metadata + cache.json to exit early before model/Qdrant setup when all\n    # files are unchanged.\n    fast_fs = _env_truthy(os.environ.get(\"INDEX_FS_FASTPATH\"), False)\n    if skip_unchanged and not recreate and fast_fs and get_cached_file_meta is not None:\n        try:\n            is_multi_repo = bool(is_multi_repo_mode and is_multi_repo_mode())\n            root_repo_for_cache = (\n                _detect_repo_name_from_path(root)\n                if (not is_multi_repo and _detect_repo_name_from_path)\n                else None\n            )\n            all_unchanged = True\n            for file_path in iter_files(root):\n                per_file_repo_for_cache = (\n                    root_repo_for_cache\n                    if root_repo_for_cache is not None\n                    else (\n                        _detect_repo_name_from_path(file_path)\n                        if _detect_repo_name_from_path\n                        else None\n                    )\n                )\n                meta = get_cached_file_meta(str(file_path), per_file_repo_for_cache) or {}\n                size = meta.get(\"size\")\n                mtime = meta.get(\"mtime\")\n                if size is None or mtime is None:\n                    all_unchanged = False\n                    break\n                st = file_path.stat()\n                if int(getattr(st, \"st_size\", 0)) != int(size) or int(getattr(st, \"st_mtime\", 0)) != int(mtime):\n                    all_unchanged = False\n                    break\n            if all_unchanged:\n                try:\n                    print(\"[fast_index] No changes detected via fs metadata; skipping model and Qdrant setup\")\n                except Exception:\n                    pass\n                return\n        except Exception:\n            pass\n\n    # Use centralized embedder factory if available (supports Qwen3 feature flag)\n    try:\n        from scripts.embedder import get_embedding_model, get_model_dimension\n        model = get_embedding_model(model_name)\n        dim = get_model_dimension(model_name)\n    except ImportError:\n        # Fallback to direct fastembed initialization\n        model = TextEmbedding(model_name=model_name)\n        dim = len(next(model.embed([\"dimension probe\"])))\n\n    client = QdrantClient(\n        url=qdrant_url,\n        api_key=api_key or None,\n        timeout=int(os.environ.get(\"QDRANT_TIMEOUT\", \"20\") or 20),\n    )\n\n    # Determine vector name\n    if recreate:\n        vector_name = _sanitize_vector_name(model_name)\n    else:\n        vector_name = None\n        try:\n            info = client.get_collection(collection)\n            cfg = info.config.params.vectors\n            if isinstance(cfg, dict) and cfg:\n                # Prefer named vector whose size matches current embedding dim\n                for name, params in cfg.items():\n                    psize = getattr(params, \"size\", None) or getattr(\n                        params, \"dim\", None\n                    )\n                    if psize and int(psize) == int(dim):\n                        vector_name = name\n                        break\n                # Otherwise, if a LEX vector exists, pick a different name as dense\n                if vector_name is None and LEX_VECTOR_NAME in cfg:\n                    for name in cfg.keys():\n                        if name != LEX_VECTOR_NAME:\n                            vector_name = name\n                            break\n        except Exception:\n            pass\n        if vector_name is None:\n            vector_name = _sanitize_vector_name(model_name)\n\n    use_per_repo_collections = False\n\n    # Workspace state: derive collection and persist metadata\n    try:\n        force_collection = False\n        try:\n            # CTXCE_FORCE_COLLECTION_NAME forces ingest_code to honor the explicit COLLECTION_NAME\n            # env var even in multi-repo mode (disables per-repo collection routing). This is\n            # primarily used by admin-driven subprocess runs (e.g. staging rebuild) that supply\n            # per-repo indexing_env overrides.\n            force_collection = str(os.environ.get(\"CTXCE_FORCE_COLLECTION_NAME\", \"\")).strip().lower() in {\n                \"1\",\n                \"true\",\n                \"yes\",\n                \"on\",\n            }\n        except Exception:\n            force_collection = False\n\n        is_multi_repo = bool(is_multi_repo_mode and is_multi_repo_mode())\n        use_per_repo_collections = bool(is_multi_repo and _get_collection_for_file)\n        if force_collection and collection:\n            use_per_repo_collections = False\n\n        if use_per_repo_collections:\n            collection = None  # Determined per file later\n            print(\"[multi_repo] Using per-repo collections for root\")\n        else:\n            if 'get_collection_name' in globals() and get_collection_name:\n                try:\n                    # get_collection_name expects a repo identifier/slug, not a filesystem path.\n                    resolved = get_collection_name(repo_tag or ws_path)\n                    placeholders = {\"\", \"default-collection\", \"my-collection\", \"codebase\"}\n                    if resolved and collection in placeholders:\n                        collection = resolved\n                except Exception:\n                    pass\n\n        if update_workspace_state and not use_per_repo_collections:\n            updates = {\"qdrant_collection\": collection}\n            try:\n                if get_indexing_config_snapshot and compute_indexing_config_hash:\n                    cfg = get_indexing_config_snapshot()\n                    updates[\"indexing_config\"] = cfg\n                    updates[\"indexing_config_hash\"] = compute_indexing_config_hash(cfg)\n            except Exception:\n                pass\n            update_workspace_state(\n                workspace_path=ws_path,\n                updates=updates,\n                repo_name=repo_tag,\n            )\n        if update_indexing_status and repo_tag:\n            update_indexing_status(\n                workspace_path=ws_path,\n                status={\n                    \"state\": \"indexing\",\n                    \"started_at\": datetime.now().isoformat(),\n                    \"progress\": {\"files_processed\": 0, \"total_files\": None},\n                },\n                repo_name=repo_tag,\n            )\n    except Exception as e:\n        # Log state update errors instead of silent failure\n        import traceback\n        print(f\"[ERROR] Failed to update workspace state during indexing: {e}\")\n        print(f\"[ERROR] Traceback: {traceback.format_exc()}\")\n\n\n    print(\n        f\"Indexing root={root} -> {qdrant_url} collection={collection} model={model_name} recreate={recreate}\"\n    )\n\n    # Health check: detect cache/collection sync issues before indexing (single-collection mode only)\n    # TODO: In future, consider a dedicated \"health-check-only\" mode/command that runs these\n    # expensive Qdrant probes without doing a full index pass, so that \"nothing changed\" runs\n    # can stay as cheap as possible while still offering an explicit way to validate collections.\n    # Skip with SKIP_HEALTH_CHECK=1 for large collections where scroll is slow\n    _skip_health = os.environ.get(\"SKIP_HEALTH_CHECK\", \"\").strip().lower() in {\"1\", \"true\", \"yes\"}\n    if not _skip_health and not recreate and skip_unchanged and not use_per_repo_collections and collection:\n        try:\n            from scripts.collection_health import auto_heal_if_needed\n\n            print(\"[health_check] Checking collection health...\")\n            heal_result = auto_heal_if_needed(str(root), collection, qdrant_url, dry_run=False)\n            if heal_result[\"action_taken\"] == \"cleared_cache\":\n                print(\"[health_check] Cache cleared due to sync issue - forcing full reindex\")\n            elif not heal_result[\"health_check\"][\"healthy\"]:\n                print(f\"[health_check] Issue detected: {heal_result['health_check']['issue']}\")\n            else:\n                print(\"[health_check] Collection health OK\")\n        except Exception as e:\n            print(f\"[health_check] Warning: health check failed: {e}\")\n    elif _skip_health:\n        print(\"[health_check] Skipped (SKIP_HEALTH_CHECK=1)\")\n\n    # Skip single collection setup in multi-repo mode\n    if not use_per_repo_collections:\n        if recreate:\n            recreate_collection(client, collection, dim, vector_name)\n        # Ensure useful payload indexes exist (idempotent)\n        ensure_collection_and_indexes_once(client, collection, dim, vector_name)\n    else:\n        print(\"[multi_repo] Skipping single collection setup - will create per-repo collections during indexing\")\n    # Repo tag for filtering: auto-detect from git or folder name\n    repo_tag = _detect_repo_name_from_path(root)\n    # TODO: Long-term, in upload-server mode repo identity should come from workspace metadata\n    # (state.json/origin), but keep bindmount/git fallbacks for back-compat.\n    workspace_root = os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"/work\"\n    touched_repos: set[str] = set()\n    repo_roots: dict[str, str] = {}\n\n    # Batch and scaling config (env/CLI overridable)\n    batch_texts: list[str] = []\n    batch_meta: list[dict] = []\n    batch_ids: list[int] = []\n    batch_lex: list[list[float]] = []\n    batch_lex_text: list[str] = []  # Raw text for sparse vector generation\n    BATCH_SIZE = int(os.environ.get(\"INDEX_BATCH_SIZE\", \"256\") or 256)\n    CHUNK_LINES = int(os.environ.get(\"INDEX_CHUNK_LINES\", \"120\") or 120)\n    CHUNK_OVERLAP = int(os.environ.get(\"INDEX_CHUNK_OVERLAP\", \"20\") or 20)\n    PROGRESS_EVERY = int(os.environ.get(\"INDEX_PROGRESS_EVERY\", \"200\") or 200)\n    # Trust-cache mode: skip Qdrant hash lookups when local cache says unchanged\n    _trust_cache = os.environ.get(\"INDEX_TRUST_CACHE\", \"\").strip().lower() in {\n        \"1\",\n        \"true\",\n        \"yes\",\n        \"on\",\n    }\n    if _trust_cache:\n        print(\"[trust_cache] INDEX_TRUST_CACHE enabled - skipping Qdrant per-file hash checks\")\n    # Semantic chunking toggle\n    use_semantic = os.environ.get(\"INDEX_SEMANTIC_CHUNKS\", \"1\").lower() in {\n        \"1\",\n        \"true\",\n        \"yes\",\n        \"on\",\n    }\n    # Debug chunking mode\n    if os.environ.get(\"DEBUG_CHUNKING\"):\n        print(f\"[DEBUG] INDEX_SEMANTIC_CHUNKS={os.environ.get('INDEX_SEMANTIC_CHUNKS', 'NOT_SET')} -> use_semantic={use_semantic}\")\n        print(f\"[DEBUG] INDEX_MICRO_CHUNKS={os.environ.get('INDEX_MICRO_CHUNKS', 'NOT_SET')}\")\n\n    files_seen = 0\n    files_indexed = 0\n    points_indexed = 0\n\n    skipped_fsmeta = 0\n    skipped_cache = 0\n    skipped_qdrant = 0\n\n    def make_point(pid, dense_vec, lex_vec, payload, lex_text: str = \"\"):\n        # Use named vectors if collection has names: store dense + lexical (+ mini if REFRAG_MODE) (+ sparse if LEX_SPARSE_MODE)\n        if vector_name:\n            vecs = {vector_name: dense_vec, LEX_VECTOR_NAME: lex_vec}\n            try:\n                if os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\n                    \"1\",\n                    \"true\",\n                    \"yes\",\n                    \"on\",\n                }:\n                    vecs[MINI_VECTOR_NAME] = project_mini(list(dense_vec), MINI_VEC_DIM)\n            except Exception:\n                pass\n            # Add sparse vector to vecs dict if LEX_SPARSE_MODE enabled (new qdrant-client API)\n            if LEX_SPARSE_MODE and lex_text:\n                sparse_vec = _lex_sparse_vector_text(lex_text)\n                if sparse_vec.get(\"indices\"):\n                    vecs[LEX_SPARSE_NAME] = models.SparseVector(**sparse_vec)\n            return models.PointStruct(id=pid, vector=vecs, payload=payload)\n        else:\n            # unnamed collection: store dense only\n            return models.PointStruct(id=pid, vector=dense_vec, payload=payload)\n\n    # Track per-file hashes across the entire run for cache updates on any flush\n    batch_file_hashes = {}\n\n    fast_fs = _env_truthy(os.environ.get(\"INDEX_FS_FASTPATH\"), False)\n\n    # Collect files for progress bar (fast: just list paths, no I/O)\n    all_files = list(iter_files(root))\n    total_files = len(all_files)\n    print(f\"Found {total_files} files to process\")\n\n    # Use tqdm progress bar if available, otherwise simple iteration\n    # When progress bar is active, suppress per-file skip messages\n    _use_progress_bar = tqdm is not None\n    if _use_progress_bar:\n        file_iter = tqdm(\n            all_files,\n            desc=\"Indexing\",\n            unit=\"file\",\n            ncols=100,\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {postfix}\",\n        )\n    else:\n        file_iter = all_files\n\n    def _maybe_update_postfix() -> None:\n        if not _use_progress_bar:\n            return\n        try:\n            if files_seen % 25 != 0:\n                return\n        except Exception:\n            return\n        try:\n            file_iter.set_postfix_str(\n                f\"fsmeta={skipped_fsmeta} cache={skipped_cache} qdrant={skipped_qdrant}\",\n                refresh=False,\n            )\n        except Exception:\n            pass\n\n    for file_path in file_iter:\n        files_seen += 1\n\n        # Determine collection per-file in multi-repo mode (use watcher's exact logic)\n        current_collection = collection\n        if use_per_repo_collections:\n            if _get_collection_for_file:\n                current_collection = _get_collection_for_file(file_path)\n                # Ensure collection exists on first use\n                ensure_collection_and_indexes_once(client, current_collection, dim, vector_name)\n            else:\n                current_collection = get_collection_name(ws_path) if get_collection_name else \"default-collection\"\n\n        # Optional fs-metadata fast-path: skip files whose size/mtime match cache\n        if skip_unchanged and fast_fs and get_cached_file_meta is not None:\n            try:\n                per_file_repo_for_cache = (\n                    _detect_repo_name_from_path(file_path)\n                    if use_per_repo_collections\n                    else repo_tag\n                )\n                meta = get_cached_file_meta(str(file_path), per_file_repo_for_cache) or {}\n                size = meta.get(\"size\")\n                mtime = meta.get(\"mtime\")\n                if size is not None and mtime is not None:\n                    st = file_path.stat()\n                    if int(getattr(st, \"st_size\", 0)) == int(size) and int(\n                        getattr(st, \"st_mtime\", 0)\n                    ) == int(mtime):\n                        if not _use_progress_bar:\n                            print(f\"Skipping unchanged file (fs-meta): {file_path}\")\n                        else:\n                            skipped_fsmeta += 1\n                            _maybe_update_postfix()\n                        continue\n            except Exception:\n                pass\n\n        try:\n            text = file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n        except Exception as e:\n            print(f\"Skipping {file_path}: {e}\")\n            continue\n\n        # Skip empty files\n        if not text or not text.strip():\n            continue\n\n        language = detect_language(file_path)\n        file_hash = hashlib.sha1(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n\n        per_file_repo = repo_tag\n        if use_per_repo_collections:\n            per_file_repo = (\n                _detect_repo_name_from_path(file_path)\n                if _detect_repo_name_from_path\n                else repo_tag\n            )\n        if per_file_repo:\n            touched_repos.add(per_file_repo)\n            repo_roots.setdefault(\n                per_file_repo,\n                str(Path(workspace_root).resolve() / per_file_repo),\n            )\n\n        # Derive logical repo identity and repo-relative path for cross-worktree reuse.\n        repo_id: str | None = None\n        repo_rel_path: str | None = None\n        try:\n            if get_workspace_state is not None:\n                ws_root = os.environ.get(\"WATCH_ROOT\") or os.environ.get(\"WORKSPACE_PATH\") or \"/work\"\n                state = get_workspace_state(ws_root, per_file_repo)\n                lrid = state.get(\"logical_repo_id\") if isinstance(state, dict) else None\n                if isinstance(lrid, str) and lrid:\n                    repo_id = lrid\n            try:\n                fp_resolved = file_path.resolve()\n            except Exception:\n                fp_resolved = file_path\n            try:\n                ws_base = Path(workspace_root).resolve()\n                repo_root = ws_base\n                if per_file_repo:\n                    candidate = ws_base / per_file_repo\n                    if candidate.exists():\n                        repo_root = candidate\n                rel = fp_resolved.relative_to(repo_root)\n                repo_rel_path = rel.as_posix()\n            except Exception:\n                repo_rel_path = None\n        except Exception:\n            repo_id = None\n            repo_rel_path = None\n\n        # Skip unchanged files if enabled (default)\n        if skip_unchanged:\n            # Prefer local workspace cache to avoid Qdrant lookups\n            try:\n                if get_cached_file_hash:\n                    prev_local = get_cached_file_hash(str(file_path), per_file_repo)\n                    if prev_local and file_hash and prev_local == file_hash:\n                        # When fs fast-path is enabled, refresh cache entry with size/mtime\n                        if fast_fs and set_cached_file_hash:\n                            try:\n                                need_refresh = True\n                                try:\n                                    if get_cached_file_meta:\n                                        _m = get_cached_file_meta(str(file_path), per_file_repo) or {}\n                                        if _m.get(\"size\") is not None and _m.get(\"mtime\") is not None:\n                                            need_refresh = False\n                                except Exception:\n                                    need_refresh = True\n                                if need_refresh:\n                                    set_cached_file_hash(str(file_path), file_hash, per_file_repo)\n                            except Exception:\n                                pass\n                        # Only print skip messages if no progress bar\n                        if not _use_progress_bar:\n                            if PROGRESS_EVERY <= 0 and files_seen % 50 == 0:\n                                print(f\"... processed {files_seen} files (skipping unchanged, cache)\")\n                                try:\n                                    if update_indexing_status:\n                                        target_workspace = (\n                                            ws_path if not use_per_repo_collections else str(file_path.parent)\n                                        )\n                                        target_repo = (\n                                            repo_tag if not use_per_repo_collections else per_file_repo\n                                        )\n                                        update_indexing_status(\n                                            workspace_path=target_workspace,\n                                            status={\n                                                \"state\": \"indexing\",\n                                                \"progress\": {\n                                                    \"files_processed\": files_seen,\n                                                    \"total_files\": None,\n                                                    \"current_file\": str(file_path),\n                                                },\n                                            },\n                                            repo_name=target_repo,\n                                        )\n                                except Exception:\n                                    pass\n                            else:\n                                print(f\"Skipping unchanged file (cache): {file_path}\")\n                        else:\n                            skipped_cache += 1\n                            _maybe_update_postfix()\n                        continue\n            except Exception:\n                pass\n\n            # Check existing indexed hash in Qdrant (logical identity when available)\n            # Skip this when INDEX_TRUST_CACHE is enabled - rely solely on local cache\n            if not _trust_cache:\n                prev = get_indexed_file_hash(\n                    client,\n                    current_collection,\n                    str(file_path),\n                    repo_id=repo_id,\n                    repo_rel_path=repo_rel_path,\n                )\n                if prev and file_hash and prev == file_hash:\n                    # File exists in Qdrant with same hash - cache it locally for next time\n                    try:\n                        if set_cached_file_hash:\n                            set_cached_file_hash(str(file_path), file_hash, per_file_repo)\n                    except Exception:\n                        pass\n                    # Only print skip messages if no progress bar\n                    if not _use_progress_bar:\n                        if PROGRESS_EVERY <= 0 and files_seen % 50 == 0:\n                            print(f\"... processed {files_seen} files (skipping unchanged)\")\n                        else:\n                            print(f\"Skipping unchanged file: {file_path}\")\n                    else:\n                        skipped_qdrant += 1\n                        _maybe_update_postfix()\n                    continue\n\n            # At this point, file content has changed vs previous index; attempt smart reindex when enabled\n            if _smart_symbol_reindexing_enabled():\n                try:\n                    use_smart, smart_reason = should_use_smart_reindexing(str(file_path), file_hash)\n                    if use_smart:\n                        print(f\"[SMART_REINDEX] Using smart reindexing for {file_path} ({smart_reason})\")\n                        status = process_file_with_smart_reindexing(\n                            file_path,\n                            text,\n                            language,\n                            client,\n                            current_collection,\n                            per_file_repo,\n                            model,\n                            vector_name,\n                        )\n                        if status == \"success\":\n                            files_indexed += 1\n                            # Smart path handles point counts internally; skip full reindex for this file\n                            continue\n                        else:\n                            print(\n                                f\"[SMART_REINDEX] Smart reindex failed for {file_path} (status={status}), falling back to full reindex\"\n                            )\n                    else:\n                        print(f\"[SMART_REINDEX] Using full reindexing for {file_path} ({smart_reason})\")\n                except Exception as e:\n                    print(f\"[SMART_REINDEX] Smart reindexing failed, falling back to full reindex: {e}\")\n\n        # Dedupe per-file by deleting previous points for this path (default)\n        if dedupe:\n            delete_points_by_path(client, current_collection, str(file_path))\n\n        files_indexed += 1\n        # Progress: show each file being indexed\n        print(f\"Indexing [{files_indexed}]: {file_path}\")\n        symbols = _extract_symbols(language, text)\n        imports, calls = _get_imports_calls(language, text)\n        last_mod, churn_count, author_count = _git_metadata(file_path)\n\n        # Get changed symbols for pseudo processing optimization (reuse existing pattern)\n        changed_symbols = set()\n        if get_cached_symbols and set_cached_symbols:\n            cached_symbols = get_cached_symbols(str(file_path))\n            if cached_symbols:\n                current_symbols = extract_symbols_with_tree_sitter(str(file_path))\n                _, changed = compare_symbol_changes(cached_symbols, current_symbols)\n                # Convert symbol names to IDs for lookup\n                for symbol_data in current_symbols.values():\n                    symbol_id = f\"{symbol_data['type']}_{symbol_data['name']}_{symbol_data['start_line']}\"\n                    if symbol_id in changed:\n                        changed_symbols.add(symbol_id)\n\n        # Micro-chunking (token-based) takes precedence; else semantic; else line-based\n        use_micro = os.environ.get(\"INDEX_MICRO_CHUNKS\", \"0\").lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }\n        if use_micro:\n            # Dynamic chunk sizing: adjust tokens/stride to fit within MAX_MICRO_CHUNKS_PER_FILE\n            # Never truncate - instead increase chunk size to cover entire file\n            try:\n                _cap = int(os.environ.get(\"MAX_MICRO_CHUNKS_PER_FILE\", \"200\") or 200)\n                _base_tokens = int(os.environ.get(\"MICRO_CHUNK_TOKENS\", \"128\") or 128)\n                _base_stride = int(os.environ.get(\"MICRO_CHUNK_STRIDE\", \"64\") or 64)\n                \n                # First pass with base settings\n                chunks = chunk_by_tokens(text, k_tokens=_base_tokens, stride_tokens=_base_stride)\n                \n                # If exceeds cap, recalculate chunk size to fit\n                if _cap > 0 and len(chunks) > _cap:\n                    _before = len(chunks)\n                    # Scale up tokens proportionally to fit within cap\n                    _scale = (len(chunks) / _cap) * 1.1  # 10% buffer\n                    _new_tokens = max(_base_tokens, int(_base_tokens * _scale))\n                    _new_stride = max(_base_stride, int(_base_stride * _scale))\n                    \n                    # Re-chunk with larger window\n                    chunks = chunk_by_tokens(text, k_tokens=_new_tokens, stride_tokens=_new_stride)\n                    try:\n                        print(\n                            f\"[ingest] micro-chunks resized path={file_path} count={_before}->{len(chunks)} \"\n                            f\"tokens={_base_tokens}->{_new_tokens} stride={_base_stride}->{_new_stride}\"\n                        )\n                    except Exception:\n                        pass\n            except Exception:\n                chunks = chunk_by_tokens(text)\n        elif use_semantic:\n            chunks = chunk_semantic(text, language, CHUNK_LINES, CHUNK_OVERLAP)\n        else:\n            chunks = chunk_lines(text, CHUNK_LINES, CHUNK_OVERLAP)\n        for ch in chunks:\n            info = build_information(\n                language,\n                file_path,\n                ch[\"start\"],\n                ch[\"end\"],\n                ch[\"text\"].splitlines()[0] if ch[\"text\"] else \"\",\n            )\n            kind, sym, sym_path = _choose_symbol_for_chunk(\n                ch[\"start\"], ch[\"end\"], symbols\n            )\n            # If chunk_semantic returned embedded symbol/kind, prefer it\n            if \"kind\" in ch and ch.get(\"kind\"):\n                kind = ch.get(\"kind\") or kind\n            if \"symbol\" in ch and ch.get(\"symbol\"):\n                sym = ch.get(\"symbol\") or sym\n            if \"symbol_path\" in ch and ch.get(\"symbol_path\"):\n                sym_path = ch.get(\"symbol_path\") or sym_path\n            # Ensure chunks carry symbol metadata so pseudo gating works across all chunking modes\n            if not ch.get(\"kind\") and kind:\n                ch[\"kind\"] = kind\n            if not ch.get(\"symbol\") and sym:\n                ch[\"symbol\"] = sym\n            if not ch.get(\"symbol_path\") and sym_path:\n                ch[\"symbol_path\"] = sym_path\n            # Track both container path (/work mirror) and original host path\n            _cur_path = str(file_path)\n            _host_path, _container_path = _compute_host_and_container_paths(_cur_path)\n\n            payload = {\n                \"document\": info,\n                \"information\": info,\n                \"metadata\": {\n                    \"path\": str(file_path),\n                    \"path_prefix\": str(file_path.parent),\n                    \"ext\": str(file_path.suffix).lstrip(\".\").lower(),\n                    \"language\": language,\n                    \"kind\": kind,\n                    \"symbol\": sym,\n                    \"symbol_path\": sym_path or \"\",\n                    \"repo\": per_file_repo,\n                    \"start_line\": ch[\"start\"],\n                    \"end_line\": ch[\"end\"],\n                    \"code\": ch[\"text\"],\n                    \"file_hash\": file_hash,\n                    \"imports\": imports,\n                    \"calls\": calls,\n                    \"ingested_at\": int(time.time()),\n                    \"last_modified_at\": int(last_mod),\n                    \"churn_count\": int(churn_count),\n                    \"author_count\": int(author_count),\n                    # Logical identity for cross-worktree reuse\n                    \"repo_id\": repo_id,\n                    \"repo_rel_path\": repo_rel_path,\n                    # New: dual-path tracking\n                    \"host_path\": _host_path,\n                    \"container_path\": _container_path,\n                },\n            }\n            # Optional LLM enrichment for lexical retrieval: pseudo + tags per micro-chunk\n            # Use symbol-aware gating and cached pseudo/tags where possible\n            pseudo = \"\"\n            tags: list[str] = []\n            if pseudo_mode != \"off\":\n                needs_pseudo, cached_pseudo, cached_tags = should_process_pseudo_for_chunk(\n                    str(file_path), ch, changed_symbols\n                )\n                pseudo, tags = cached_pseudo, cached_tags\n                if pseudo_mode == \"full\" and needs_pseudo:\n                    try:\n                        pseudo, tags = generate_pseudo_tags(ch.get(\"text\") or \"\")\n                        if pseudo or tags:\n                            symbol_name = ch.get(\"symbol\", \"\")\n                            if symbol_name:\n                                kind = ch.get(\"kind\", \"unknown\")\n                                start_line = ch.get(\"start\", 0)\n                                symbol_id = f\"{kind}_{symbol_name}_{start_line}\"\n                                if set_cached_pseudo:\n                                    set_cached_pseudo(str(file_path), symbol_id, pseudo, tags, file_hash)\n                    except Exception:\n                        pass\n            if pseudo:\n                payload[\"pseudo\"] = pseudo\n            if tags:\n                payload[\"tags\"] = tags\n            batch_texts.append(info)\n            batch_meta.append(payload)\n            # Track per-file latest hash once we add the first chunk to any batch\n            try:\n                batch_file_hashes[str(file_path)] = file_hash\n            except Exception:\n                pass\n\n            batch_ids.append(\n                hash_id(ch[\"text\"], str(file_path), ch[\"start\"], ch[\"end\"])\n            )\n            aug_lex_text = (ch.get(\"text\") or \"\") + (\" \" + pseudo if pseudo else \"\") + (\" \" + \" \".join(tags) if tags else \"\")\n            batch_lex.append(_lex_hash_vector_text(aug_lex_text))\n            batch_lex_text.append(aug_lex_text)\n            points_indexed += 1\n            if len(batch_texts) >= BATCH_SIZE:\n                vectors = embed_batch(model, batch_texts)\n                # Inject pid_str into payloads for server-side gating\n                for _idx, _m in enumerate(batch_meta):\n                    try:\n                        _m[\"pid_str\"] = str(batch_ids[_idx])\n                    except Exception:\n                        pass\n                points = [\n                    make_point(i, v, lx, m, lt)\n                    for i, v, lx, m, lt in zip(batch_ids, vectors, batch_lex, batch_meta, batch_lex_text)\n                ]\n                upsert_points(client, current_collection, points)\n                # Update local file-hash cache for any files that had chunks in this flush\n                try:\n                    if set_cached_file_hash:\n                        for _p, _h in list(batch_file_hashes.items()):\n                            try:\n                                if _p and _h:\n                                    file_repo_tag = (\n                                        _detect_repo_name_from_path(Path(_p))\n                                        if use_per_repo_collections\n                                        else repo_tag\n                                    )\n                                    repos_touched_name = file_repo_tag or per_file_repo\n                                    if repos_touched_name:\n                                        touched_repos.add(repos_touched_name)\n                                        repo_roots.setdefault(\n                                            repos_touched_name,\n                                            str(Path(workspace_root).resolve() / repos_touched_name),\n                                        )\n                                    set_cached_file_hash(_p, _h, file_repo_tag)\n                            except Exception:\n                                continue\n                except Exception:\n                    pass\n\n                batch_texts, batch_meta, batch_ids, batch_lex, batch_lex_text = [], [], [], [], []\n\n        if PROGRESS_EVERY > 0 and files_seen % PROGRESS_EVERY == 0:\n            print(\n                f\"Progress: files_seen={files_seen}, files_indexed={files_indexed}, chunks_indexed={points_indexed}\"\n            )\n            try:\n                if update_indexing_status:\n                    per_file_repo = repo_tag\n                    if use_per_repo_collections:\n                        per_file_repo = (\n                            _detect_repo_name_from_path(file_path)\n                            if _detect_repo_name_from_path\n                            else repo_tag\n                        )\n                    if per_file_repo:\n                        update_indexing_status(\n                            workspace_path=str(file_path.parent),\n                            status={\n                                \"state\": \"indexing\",\n                                \"progress\": {\n                                    \"files_processed\": files_indexed,\n                                    \"total_files\": files_seen,\n                                    \"current_file\": str(file_path),\n                                },\n                            },\n                            repo_name=per_file_repo,\n                        )\n            except Exception as e:\n                # Log progress update errors instead of silent failure\n                import traceback\n                print(f\"[ERROR] Failed to update indexing progress: {e}\")\n                print(f\"[ERROR] Traceback: {traceback.format_exc()}\")\n\n    if batch_texts:\n        vectors = embed_batch(model, batch_texts)\n        # Inject pid_str into payloads for server-side gating (final batch)\n        for _idx, _m in enumerate(batch_meta):\n            try:\n                _m[\"pid_str\"] = str(batch_ids[_idx])\n            except Exception:\n                pass\n        points = [\n            make_point(i, v, lx, m, lt)\n            for i, v, lx, m, lt in zip(batch_ids, vectors, batch_lex, batch_meta, batch_lex_text)\n        ]\n        upsert_points(client, current_collection, points)\n        # Update local file-hash cache for any files that had chunks during this run (final flush)\n        try:\n            if set_cached_file_hash:\n                for _p, _h in list(batch_file_hashes.items()):\n                    try:\n                        if _p and _h:\n                            per_file_repo = (\n                                _detect_repo_name_from_path(Path(_p))\n                                if use_per_repo_collections\n                                else repo_tag\n                            )\n                            if per_file_repo:\n                                set_cached_file_hash(_p, _h, per_file_repo)\n                    except Exception:\n                        continue\n\n            # NEW: Update symbol cache for files that were processed\n            if set_cached_symbols and _smart_symbol_reindexing_enabled():\n                try:\n                    # Process files that had chunks and extract/update their symbol cache\n                    processed_files = set(str(Path(_p).resolve()) for _p in batch_file_hashes.keys())\n\n                    for file_path_str in processed_files:\n                        try:\n                            # Extract current symbols for this file\n                            current_symbols = extract_symbols_with_tree_sitter(file_path_str)\n                            if current_symbols:\n                                # Generate file hash for this file\n                                with open(file_path_str, 'r', encoding='utf-8') as f:\n                                    content = f.read()\n                                file_hash = hashlib.sha1(content.encode('utf-8', errors='ignore')).hexdigest()\n\n                                # Save symbol cache\n                                set_cached_symbols(file_path_str, current_symbols, file_hash)\n                                print(f\"[SYMBOL_CACHE] Updated symbols for {Path(file_path_str).name}: {len(current_symbols)} symbols\")\n                        except Exception as e:\n                            print(f\"[SYMBOL_CACHE] Failed to update symbols for {Path(_p).name}: {e}\")\n                except Exception as e:\n                    print(f\"[SYMBOL_CACHE] Symbol cache update failed: {e}\")\n        except Exception:\n            pass\n\n    print(\n        f\"Indexing complete. files_seen={files_seen}, files_indexed={files_indexed}, chunks_indexed={points_indexed}\"\n    )\n\n    # Workspace state: mark completion\n    try:\n        if log_activity:\n            # Extract repo name from workspace path for log_activity\n            repo_name = None\n            if use_per_repo_collections:\n                # In multi-repo mode, we need to determine which repo this activity belongs to\n                # For scan completion, we use the workspace path as the repo identifier\n                repo_name = _detect_repo_name_from_path(Path(ws_path))\n\n            log_activity(\n                repo_name=repo_name,\n                action=\"scan-completed\",\n                file_path=\"\",\n                details={\n                    \"files_seen\": files_seen,\n                    \"files_indexed\": files_indexed,\n                    \"chunks_indexed\": points_indexed,\n                },\n            )\n        if update_indexing_status:\n            for repo_name in touched_repos or ({repo_tag} if repo_tag else set()):\n                try:\n                    target_ws = repo_roots.get(repo_name) or ws_path\n                    try:\n                        if promote_pending_indexing_config:\n                            promote_pending_indexing_config(workspace_path=target_ws, repo_name=repo_name)\n                        elif persist_indexing_config:\n                            persist_indexing_config(workspace_path=target_ws, repo_name=repo_name)\n                    except Exception:\n                        pass\n                    update_indexing_status(\n                        workspace_path=target_ws,\n                        status={\n                            \"state\": \"idle\",\n                            \"progress\": {\"files_processed\": files_indexed, \"total_files\": None},\n                        },\n                        repo_name=repo_name,\n                    )\n\n                    # If staging is active, advance staging status out of \"initializing\".\n                    _mark_staging_idle(target_ws, repo_name, files_indexed)\n                except Exception:\n                    continue\n    except Exception as e:\n        # Log the error instead of silently swallowing it\n        import traceback\n        print(f\"[ERROR] Failed to update workspace state after indexing completion: {e}\")\n        print(f\"[ERROR] Traceback: {traceback.format_exc()}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_make_point_3764": {
      "name": "make_point",
      "type": "function",
      "start_line": 3764,
      "end_line": 3786,
      "content_hash": "9a1a5a541465bb0247488ab78a9b0c2833c498c5",
      "content": "    def make_point(pid, dense_vec, lex_vec, payload, lex_text: str = \"\"):\n        # Use named vectors if collection has names: store dense + lexical (+ mini if REFRAG_MODE) (+ sparse if LEX_SPARSE_MODE)\n        if vector_name:\n            vecs = {vector_name: dense_vec, LEX_VECTOR_NAME: lex_vec}\n            try:\n                if os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\n                    \"1\",\n                    \"true\",\n                    \"yes\",\n                    \"on\",\n                }:\n                    vecs[MINI_VECTOR_NAME] = project_mini(list(dense_vec), MINI_VEC_DIM)\n            except Exception:\n                pass\n            # Add sparse vector to vecs dict if LEX_SPARSE_MODE enabled (new qdrant-client API)\n            if LEX_SPARSE_MODE and lex_text:\n                sparse_vec = _lex_sparse_vector_text(lex_text)\n                if sparse_vec.get(\"indices\"):\n                    vecs[LEX_SPARSE_NAME] = models.SparseVector(**sparse_vec)\n            return models.PointStruct(id=pid, vector=vecs, payload=payload)\n        else:\n            # unnamed collection: store dense only\n            return models.PointStruct(id=pid, vector=dense_vec, payload=payload)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__maybe_update_postfix_3812": {
      "name": "_maybe_update_postfix",
      "type": "function",
      "start_line": 3812,
      "end_line": 3826,
      "content_hash": "07a568499e7d9032bf0b81b5c04b12b14fae8d6c",
      "content": "    def _maybe_update_postfix() -> None:\n        if not _use_progress_bar:\n            return\n        try:\n            if files_seen % 25 != 0:\n                return\n        except Exception:\n            return\n        try:\n            file_iter.set_postfix_str(\n                f\"fsmeta={skipped_fsmeta} cache={skipped_cache} qdrant={skipped_qdrant}\",\n                refresh=False,\n            )\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_process_file_with_smart_reindexing_4382": {
      "name": "process_file_with_smart_reindexing",
      "type": "function",
      "start_line": 4382,
      "end_line": 4855,
      "content_hash": "3562c6ef54ee84cdc71dce9ee9af209148487094",
      "content": "def process_file_with_smart_reindexing(\n    file_path,\n    text: str,\n    language: str,\n    client: QdrantClient,\n    current_collection: str,\n    per_file_repo,\n    model: \"TextEmbedding\",\n    vector_name: str | None,\n) -> str:\n    \"\"\"Smart, chunk-level reindexing for a single file.\n\n    Rebuilds all points for the file with *accurate* line numbers while:\n    - Reusing existing embeddings/lexical vectors for unchanged chunks (by code content), and\n    - Re-embedding only for changed chunks.\n\n    Symbol cache is used to gate pseudo/tag generation, but embedding reuse is decided\n    at the chunk level by matching previous chunk code.\n\n    TODO(logical_repo): consider loading existing points by logical identity\n    (repo_id + repo_rel_path) instead of metadata.path so worktrees/branches\n    sharing a repo can reuse embeddings across slugs, not just per-path.\n    \"\"\"\n    try:\n        try:\n            p = Path(str(file_path))\n            if _should_skip_explicit_file_by_excluder(p):\n                try:\n                    delete_points_by_path(client, current_collection, str(p))\n                except Exception:\n                    pass\n                print(f\"[SMART_REINDEX] Skipping excluded file: {file_path}\")\n                return \"skipped\"\n        except Exception:\n            return \"skipped\"\n\n        print(f\"[SMART_REINDEX] Processing {file_path} with chunk-level reindexing\")\n\n        # Normalize path / types\n        try:\n            fp = str(file_path)\n        except Exception:\n            fp = str(file_path)\n        try:\n            if not isinstance(file_path, Path):\n                file_path = Path(fp)\n        except Exception:\n            file_path = Path(fp)\n\n        # Compute current file hash\n        file_hash = hashlib.sha1(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n\n        # Extract current symbols for diffing (dict) and for chunk mapping (List[_Sym])\n        symbol_meta = extract_symbols_with_tree_sitter(fp)\n        if not symbol_meta:\n            print(f\"[SMART_REINDEX] No symbols found in {file_path}, falling back to full reindex\")\n            return \"failed\"\n\n        # Use the dict-style symbol_meta for cache diffing\n        cached_symbols = get_cached_symbols(fp) if get_cached_symbols else {}\n        unchanged_symbols: list[str] = []\n        changed_symbols: list[str] = []\n        if cached_symbols and compare_symbol_changes:\n            try:\n                unchanged_symbols, changed_symbols = compare_symbol_changes(\n                    cached_symbols, symbol_meta\n                )\n            except Exception:\n                # On failure, treat everything as changed\n                unchanged_symbols = []\n                changed_symbols = list(symbol_meta.keys())\n        else:\n            changed_symbols = list(symbol_meta.keys())\n        changed_set = set(changed_symbols)\n\n        # Short-circuit: if nothing changed and we have cached symbols, skip entirely\n        if len(changed_symbols) == 0 and cached_symbols:\n            print(f\"[SMART_REINDEX] {file_path}: 0 changes detected, skipping\")\n            return \"skipped\"\n\n        # Load existing points for this file (for embedding reuse)\n        existing_points = []\n        try:\n            filt = models.Filter(\n                must=[\n                    models.FieldCondition(\n                        key=\"metadata.path\", match=models.MatchValue(value=fp)\n                    )\n                ]\n            )\n            next_offset = None\n            while True:\n                pts, next_offset = client.scroll(\n                    collection_name=current_collection,\n                    scroll_filter=filt,\n                    with_payload=True,\n                    with_vectors=True,\n                    limit=256,\n                    offset=next_offset,\n                )\n                if not pts:\n                    break\n                existing_points.extend(pts)\n                if next_offset is None:\n                    break\n        except Exception as e:\n            print(f\"[SMART_REINDEX] Failed to load existing points for {file_path}: {e}\")\n            existing_points = []\n\n        # Index existing points by (symbol_id, code, embedding_text) for reuse.\n        # Important: the dense embedding is computed from `info` (payload['information']).\n        # If line ranges change, `info` changes; reusing an old dense vector would be wrong.\n        points_by_code: dict[tuple[str, str, str], list[models.Record]] = {}\n        try:\n            for rec in existing_points:\n                payload = rec.payload or {}\n                md = payload.get(\"metadata\") or {}\n                code_text = md.get(\"code\") or \"\"\n                embed_text = payload.get(\"information\") or payload.get(\"document\") or \"\"\n                kind = md.get(\"kind\") or \"\"\n                sym_name = md.get(\"symbol\") or \"\"\n                start_line = md.get(\"start_line\") or 0\n                symbol_id = (\n                    f\"{kind}_{sym_name}_{start_line}\"\n                    if kind and sym_name and start_line\n                    else \"\"\n                )\n                key = (symbol_id, code_text, embed_text) if symbol_id else (\"\", code_text, embed_text)\n                points_by_code.setdefault(key, []).append(rec)\n        except Exception:\n            points_by_code = {}\n\n        # Chunk current file using the same strategy as normal indexing\n        CHUNK_LINES = int(os.environ.get(\"INDEX_CHUNK_LINES\", \"120\") or 120)\n        CHUNK_OVERLAP = int(os.environ.get(\"INDEX_CHUNK_OVERLAP\", \"20\") or 20)\n        use_micro = os.environ.get(\"INDEX_MICRO_CHUNKS\", \"0\").lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }\n        use_semantic = os.environ.get(\"INDEX_SEMANTIC_CHUNKS\", \"1\").lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }\n\n        if use_micro:\n            chunks = chunk_by_tokens(text)\n            symbol_spans: list[_Sym] = _extract_symbols(language, text)\n        elif use_semantic:\n            chunks = chunk_semantic(text, language, CHUNK_LINES, CHUNK_OVERLAP)\n            symbol_spans = _extract_symbols(language, text)\n        else:\n            chunks = chunk_lines(text, CHUNK_LINES, CHUNK_OVERLAP)\n            symbol_spans = _extract_symbols(language, text)\n\n        # Prepare collections for reused vs newly embedded points\n        reused_points: list[models.PointStruct] = []\n        embed_texts: list[str] = []\n        embed_payloads: list[dict] = []\n        embed_ids: list[int] = []\n        embed_lex: list[list[float]] = []\n        embed_lex_text: list[str] = []  # Raw text for sparse vectors\n\n        imports, calls = _get_imports_calls(language, text)\n        last_mod, churn_count, author_count = _git_metadata(file_path)\n\n        # Feature flag: PSEUDO_BATCH_CONCURRENCY > 1 enables parallel GLM calls\n        pseudo_batch_concurrency = int(os.environ.get(\"PSEUDO_BATCH_CONCURRENCY\", \"1\") or 1)\n        use_batch_pseudo = pseudo_batch_concurrency > 1\n\n        # Pre-process chunks: collect metadata and identify which need pseudo\n        chunk_data_sr: list[dict] = []\n        for ch in chunks:\n            info = build_information(\n                language,\n                file_path,\n                ch[\"start\"],\n                ch[\"end\"],\n                ch[\"text\"].splitlines()[0] if ch[\"text\"] else \"\",\n            )\n            # Use span-style symbols for mapping chunks to symbols\n            kind, sym, sym_path = _choose_symbol_for_chunk(\n                ch[\"start\"], ch[\"end\"], symbol_spans\n            )\n            # Prefer embedded symbol metadata from semantic chunker when present\n            if \"kind\" in ch and ch.get(\"kind\"):\n                kind = ch.get(\"kind\") or kind\n            if \"symbol\" in ch and ch.get(\"symbol\"):\n                sym = ch.get(\"symbol\") or sym\n            if \"symbol_path\" in ch and ch.get(\"symbol_path\"):\n                sym_path = ch.get(\"symbol_path\") or sym_path\n            # Ensure chunks carry symbol metadata so pseudo gating works\n            if not ch.get(\"kind\") and kind:\n                ch[\"kind\"] = kind\n            if not ch.get(\"symbol\") and sym:\n                ch[\"symbol\"] = sym\n            if not ch.get(\"symbol_path\") and sym_path:\n                ch[\"symbol_path\"] = sym_path\n\n            # Basic metadata payload\n            _cur_path = str(file_path)\n            _host_path, _container_path = _compute_host_and_container_paths(_cur_path)\n\n            payload = {\n                \"document\": info,\n                \"information\": info,\n                \"metadata\": {\n                    \"path\": str(file_path),\n                    \"path_prefix\": str(file_path.parent),\n                    \"ext\": str(file_path.suffix).lstrip(\".\").lower(),\n                    \"language\": language,\n                    \"kind\": kind,\n                    \"symbol\": sym,\n                    \"symbol_path\": sym_path or \"\",\n                    \"repo\": per_file_repo,\n                    \"start_line\": ch[\"start\"],\n                    \"end_line\": ch[\"end\"],\n                    \"code\": ch[\"text\"],\n                    \"file_hash\": file_hash,\n                    \"imports\": imports,\n                    \"calls\": calls,\n                    \"ingested_at\": int(time.time()),\n                    \"last_modified_at\": int(last_mod),\n                    \"churn_count\": int(churn_count),\n                    \"author_count\": int(author_count),\n                    \"host_path\": _host_path,\n                    \"container_path\": _container_path,\n                },\n            }\n\n            # Check pseudo cache status\n            needs_pseudo_gen, cached_pseudo, cached_tags = should_process_pseudo_for_chunk(\n                fp, ch, changed_set\n            )\n\n            chunk_data_sr.append({\n                \"chunk\": ch,\n                \"info\": info,\n                \"payload\": payload,\n                \"kind\": kind,\n                \"sym\": sym,\n                \"sym_path\": sym_path,\n                \"needs_pseudo\": needs_pseudo_gen,\n                \"cached_pseudo\": cached_pseudo,\n                \"cached_tags\": cached_tags,\n            })\n\n        # Batch pseudo generation (feature-flagged)\n        if use_batch_pseudo:\n            pending_indices = [i for i, cd in enumerate(chunk_data_sr) if cd[\"needs_pseudo\"]]\n            pending_texts = [chunk_data_sr[i][\"chunk\"].get(\"text\") or \"\" for i in pending_indices]\n\n            if pending_texts:\n                try:\n                    from scripts.refrag_glm import generate_pseudo_tags_batch\n                    batch_results = generate_pseudo_tags_batch(pending_texts, concurrency=pseudo_batch_concurrency)\n                    for idx, (pseudo, tags) in zip(pending_indices, batch_results):\n                        chunk_data_sr[idx][\"cached_pseudo\"] = pseudo\n                        chunk_data_sr[idx][\"cached_tags\"] = tags\n                        chunk_data_sr[idx][\"needs_pseudo\"] = False\n                        if pseudo or tags:\n                            ch = chunk_data_sr[idx][\"chunk\"]\n                            symbol_name = ch.get(\"symbol\", \"\")\n                            if symbol_name and set_cached_pseudo:\n                                k = ch.get(\"kind\", \"unknown\")\n                                start_line = ch.get(\"start\", 0)\n                                sid = f\"{k}_{symbol_name}_{start_line}\"\n                                set_cached_pseudo(fp, sid, pseudo, tags, file_hash)\n                except Exception as e:\n                    print(f\"[PSEUDO_BATCH] Smart reindex batch failed, falling back: {e}\")\n                    use_batch_pseudo = False\n\n        # Finalize payloads with pseudo/tags\n        for cd in chunk_data_sr:\n            ch = cd[\"chunk\"]\n            payload = cd[\"payload\"]\n            pseudo = cd[\"cached_pseudo\"]\n            tags = cd[\"cached_tags\"]\n\n            # Sequential fallback\n            if not use_batch_pseudo and cd[\"needs_pseudo\"]:\n                try:\n                    pseudo, tags = generate_pseudo_tags(ch.get(\"text\") or \"\")\n                    if pseudo or tags:\n                        symbol_name = ch.get(\"symbol\", \"\")\n                        if symbol_name:\n                            k = ch.get(\"kind\", \"unknown\")\n                            start_line = ch.get(\"start\", 0)\n                            sid = f\"{k}_{symbol_name}_{start_line}\"\n                            if set_cached_pseudo:\n                                set_cached_pseudo(fp, sid, pseudo, tags, file_hash)\n                except Exception:\n                    pass\n\n            if pseudo:\n                payload[\"pseudo\"] = pseudo\n            if tags:\n                payload[\"tags\"] = tags\n\n            # Extract variables for embedding reuse logic\n            info = cd[\"info\"]\n            kind = cd[\"kind\"]\n            sym = cd[\"sym\"]\n\n            # Decide whether we can reuse an existing embedding for this chunk\n            code_text = ch.get(\"text\") or \"\"\n            chunk_symbol_id = \"\"\n            if sym and kind:\n                chunk_symbol_id = f\"{kind}_{sym}_{ch['start']}\"\n\n            reuse_key = (chunk_symbol_id, code_text, info)\n            fallback_key = (\"\", code_text, info)\n            reused_rec = None\n            used_key = None\n            bucket = points_by_code.get(reuse_key)\n            if bucket is not None:\n                used_key = reuse_key\n            else:\n                bucket = points_by_code.get(fallback_key)\n                if bucket is not None:\n                    used_key = fallback_key\n            if bucket:\n                try:\n                    reused_rec = bucket.pop()\n                    if not bucket:\n                        # Clean up empty bucket\n                        if used_key is not None:\n                            points_by_code.pop(used_key, None)\n                except Exception:\n                    reused_rec = None\n\n            if reused_rec is not None:\n                try:\n                    vec = reused_rec.vector\n                    # Validate vector shape before reuse.\n                    if vector_name and isinstance(vec, dict) and vector_name not in vec:\n                        raise ValueError(\"reused vector missing dense key\")\n                    # If we're reusing an existing embedding, we still need to refresh\n                    # the lexical vector because it depends on pseudo/tags (and can drift).\n                    aug_lex_text = (code_text or \"\") + (\" \" + pseudo if pseudo else \"\") + (\n                        \" \" + \" \".join(tags) if tags else \"\"\n                    )\n                    refreshed_lex = _lex_hash_vector_text(aug_lex_text)\n                    if vector_name:\n                        if isinstance(vec, dict):\n                            # Named vectors: keep dense/mini as-is, overwrite lex.\n                            vec = dict(vec)\n                            vec[LEX_VECTOR_NAME] = refreshed_lex\n                        else:\n                            # Unexpected shape: treat as dense and rebuild named vectors.\n                            vecs = {vector_name: vec, LEX_VECTOR_NAME: refreshed_lex}\n                            try:\n                                if os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\n                                    \"1\",\n                                    \"true\",\n                                    \"yes\",\n                                    \"on\",\n                                }:\n                                    vecs[MINI_VECTOR_NAME] = project_mini(\n                                        list(vec), MINI_VEC_DIM\n                                    )\n                            except Exception:\n                                pass\n                            vec = vecs\n                    else:\n                        # Unnamed vectors collection: ensure we pass dense-only vector.\n                        if isinstance(vec, dict):\n                            # Prefer any non-lex/non-mini vector as dense.\n                            dense = None\n                            try:\n                                for k, v in vec.items():\n                                    if k not in {LEX_VECTOR_NAME, MINI_VECTOR_NAME}:\n                                        dense = v\n                                        break\n                            except Exception:\n                                dense = None\n                            if dense is None:\n                                raise ValueError(\"reused vector has no dense component\")\n                            vec = dense\n                    pid = hash_id(code_text, fp, ch[\"start\"], ch[\"end\"])\n                    reused_points.append(\n                        models.PointStruct(id=pid, vector=vec, payload=payload)\n                    )\n                    continue\n                except Exception:\n                    # Fall through to re-embedding path\n                    pass\n\n            # Need to embed this chunk\n            embed_texts.append(info)\n            embed_payloads.append(payload)\n            embed_ids.append(\n                hash_id(code_text, fp, ch[\"start\"], ch[\"end\"])\n            )\n            aug_lex_text = (code_text or \"\") + (\n                \" \" + pseudo if pseudo else \"\"\n            ) + (\" \" + \" \".join(tags) if tags else \"\")\n            embed_lex.append(_lex_hash_vector_text(aug_lex_text))\n            embed_lex_text.append(aug_lex_text)\n\n        # Embed changed/new chunks and build final point set\n        new_points: list[models.PointStruct] = []\n        if embed_texts:\n            vectors = embed_batch(model, embed_texts)\n            for pid, v, lx, pl, lt in zip(\n                embed_ids,\n                vectors,\n                embed_lex,\n                embed_payloads,\n                embed_lex_text,\n            ):\n                if vector_name:\n                    vecs = {vector_name: v, LEX_VECTOR_NAME: lx}\n                    try:\n                        if os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\n                            \"1\",\n                            \"true\",\n                            \"yes\",\n                            \"on\",\n                        }:\n                            vecs[MINI_VECTOR_NAME] = project_mini(\n                                list(v), MINI_VEC_DIM\n                            )\n                    except Exception:\n                        pass\n                    # Add sparse vector to vecs dict if LEX_SPARSE_MODE enabled (new qdrant-client API)\n                    if LEX_SPARSE_MODE and lt:\n                        sparse_vec = _lex_sparse_vector_text(lt)\n                        if sparse_vec.get(\"indices\"):\n                            vecs[LEX_SPARSE_NAME] = models.SparseVector(**sparse_vec)\n                    new_points.append(\n                        models.PointStruct(id=pid, vector=vecs, payload=pl)\n                    )\n                else:\n                    new_points.append(\n                        models.PointStruct(id=pid, vector=v, payload=pl)\n                    )\n\n        all_points = reused_points + new_points\n\n        # Replace existing points for this file with the new set\n        try:\n            delete_points_by_path(client, current_collection, fp)\n        except Exception as e:\n            print(f\"[SMART_REINDEX] Failed to delete old points for {file_path}: {e}\")\n\n        if all_points:\n            upsert_points(client, current_collection, all_points)\n\n        # Update caches with the new state\n        try:\n            if set_cached_symbols:\n                set_cached_symbols(fp, symbol_meta, file_hash)\n        except Exception as e:\n            print(f\"[SMART_REINDEX] Failed to update symbol cache for {file_path}: {e}\")\n        try:\n            if set_cached_file_hash:\n                set_cached_file_hash(fp, file_hash, per_file_repo)\n        except Exception:\n            pass\n\n        print(\n            f\"[SMART_REINDEX] Completed {file_path}: chunks={len(chunks)}, reused_points={len(reused_points)}, embedded_points={len(new_points)}\"\n        )\n        return \"success\"\n\n    except Exception as e:\n        print(f\"[SMART_REINDEX] Failed to process {file_path}: {e}\")\n        import traceback\n        print(f\"[SMART_REINDEX] Traceback: {traceback.format_exc()}\")\n        return \"failed\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_main_4857": {
      "name": "main",
      "type": "function",
      "start_line": 4857,
      "end_line": 5067,
      "content_hash": "f8bcd32cf5184a1bebe7e6cb4a69368a719affd8",
      "content": "def main():\n    parser = argparse.ArgumentParser(\n        description=\"Index code into Qdrant with metadata for MCP code search.\"\n    )\n    parser.add_argument(\"--root\", type=str, default=\".\", help=\"Root directory to index\")\n    parser.add_argument(\n        \"--recreate\",\n        action=\"store_true\",\n        help=\"Recreate the collection before indexing\",\n    )\n    parser.add_argument(\n        \"--no-dedupe\",\n        action=\"store_true\",\n        help=\"Do not delete existing points for each file before inserting\",\n    )\n    parser.add_argument(\n        \"--no-skip-unchanged\",\n        action=\"store_true\",\n        help=\"Do not skip files whose content hash matches existing index\",\n    )\n    parser.add_argument(\n        \"--clear-indexing-caches\",\n        action=\"store_true\",\n        help=\"Clear local file hash + symbol caches before indexing\",\n    )\n    # Exclusion controls\n    parser.add_argument(\n        \"--ignore-file\",\n        type=str,\n        default=None,\n        help=\"Path to a .qdrantignore-style file of patterns to exclude\",\n    )\n    parser.add_argument(\n        \"--no-default-excludes\",\n        action=\"store_true\",\n        help=\"Disable default exclusions (models, node_modules, build, venv, .git, etc.)\",\n    )\n    parser.add_argument(\n        \"--exclude\",\n        action=\"append\",\n        default=None,\n        help=\"Additional exclude pattern(s); can be used multiple times or comma-separated\",\n    )\n    # Scaling controls\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=None,\n        help=\"Embedding/upsert batch size (default 64)\",\n    )\n    parser.add_argument(\n        \"--chunk-lines\",\n        type=int,\n        default=None,\n        help=\"Max lines per chunk (default 120)\",\n    )\n    parser.add_argument(\n        \"--chunk-overlap\",\n        type=int,\n        default=None,\n        help=\"Overlap lines between chunks (default 20)\",\n    )\n    parser.add_argument(\n        \"--progress-every\",\n        type=int,\n        default=None,\n        help=\"Print progress every N files (default 200; 0 disables)\",\n    )\n    # GLM psueo tag test - # TODO: Remove GLM psuedo tag test harness after confirming 100% stable and not needed\n    parser.add_argument(\n        \"--test-pseudo\",\n        type=str,\n        default=None,\n        help=\"Test generate_pseudo_tags on the given code snippet and print result, then exit\",\n    )\n    parser.add_argument(\n        \"--test-pseudo-file\",\n        type=str,\n        default=None,\n        help=\"Test generate_pseudo_tags on the contents of the given file and print result, then exit\",\n    )\n    # End\n\n    args = parser.parse_args()\n\n    # Map CLI overrides to env so downstream helpers pick them up\n    if args.ignore_file:\n        os.environ[\"QDRANT_IGNORE_FILE\"] = args.ignore_file\n    if args.no_default_excludes:\n        os.environ[\"QDRANT_DEFAULT_EXCLUDES\"] = \"0\"\n    if args.exclude:\n        # allow comma-separated and repeated flags\n        parts = []\n        for e in args.exclude:\n            parts.extend([p.strip() for p in str(e).split(\",\") if p.strip()])\n        if parts:\n            os.environ[\"QDRANT_EXCLUDES\"] = \",\".join(parts)\n    if args.batch_size is not None:\n        os.environ[\"INDEX_BATCH_SIZE\"] = str(args.batch_size)\n    if args.chunk_lines is not None:\n        os.environ[\"INDEX_CHUNK_LINES\"] = str(args.chunk_lines)\n    if args.chunk_overlap is not None:\n        os.environ[\"INDEX_CHUNK_OVERLAP\"] = str(args.chunk_overlap)\n    if args.progress_every is not None:\n        os.environ[\"INDEX_PROGRESS_EVERY\"] = str(args.progress_every)\n\n    # TODO: Remove GLM psuedo tag test harness after confirming 100% stable and not needed\n    # # Optional test mode: exercise generate_pseudo_tags (including GLM runtime) and exit\n    if args.test_pseudo or args.test_pseudo_file:\n        import json as _json\n\n        code_text = \"\"\n        if args.test_pseudo:\n            code_text = args.test_pseudo\n        if args.test_pseudo_file:\n            try:\n                code_text = Path(args.test_pseudo_file).read_text(\n                    encoding=\"utf-8\", errors=\"ignore\"\n                )\n            except Exception as e:\n                print(f\"[TEST_PSEUDO] Failed to read file {args.test_pseudo_file}: {e}\")\n                return\n        if not code_text.strip():\n            print(\"[TEST_PSEUDO] No code text provided\")\n            return\n\n        # Use the normal generate_pseudo_tags path so behavior matches indexing.\n        try:\n            from scripts.refrag_llamacpp import get_runtime_kind  # type: ignore\n\n            runtime = get_runtime_kind()\n        except Exception:\n            runtime = \"unknown\"\n\n        pseudo, tags = \"\", []\n        try:\n            pseudo, tags = generate_pseudo_tags(code_text)\n        except Exception as e:\n            print(f\"[TEST_PSEUDO] Error while generating pseudo tags: {e}\")\n\n        print(\n            _json.dumps(\n                {\n                    \"runtime\": runtime,\n                    \"pseudo\": pseudo,\n                    \"tags\": tags,\n                },\n                ensure_ascii=False,\n                indent=2,\n            )\n        )\n        return\n\n    qdrant_url = os.environ.get(\"QDRANT_URL\", \"http://localhost:6333\")\n    api_key = os.environ.get(\"QDRANT_API_KEY\")\n    collection = os.environ.get(\"COLLECTION_NAME\") or os.environ.get(\"DEFAULT_COLLECTION\") or \"codebase\"\n    model_name = os.environ.get(\"EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\")\n\n    force_collection = False\n    try:\n        force_collection = str(os.environ.get(\"CTXCE_FORCE_COLLECTION_NAME\", \"\")).strip().lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }\n    except Exception:\n        force_collection = False\n\n    # Resolve collection name based on multi-repo mode\n    multi_repo = bool(is_multi_repo_mode and is_multi_repo_mode())\n    if multi_repo and not force_collection:\n        # Multi-repo mode: pass collection=None to trigger per-repo collection resolution\n        collection = None\n        print(\"[multi_repo] Multi-repo mode enabled - will create separate collections per repository\")\n    else:\n        # Single-repo mode: use environment variable\n        if 'get_collection_name' in globals() and get_collection_name:\n            try:\n                # get_collection_name expects a repo identifier/slug, not a filesystem path.\n                root_repo = None\n                try:\n                    root_repo = _detect_repo_name_from_path(Path(args.root).resolve())\n                except Exception:\n                    root_repo = None\n                resolved = get_collection_name(root_repo or str(Path(args.root).resolve()))\n                placeholders = {\"\", \"default-collection\", \"my-collection\", \"codebase\"}\n                if resolved and collection in placeholders:\n                    collection = resolved\n            except Exception:\n                pass\n        if not collection:\n            collection = os.environ.get(\"COLLECTION_NAME\", \"codebase\")\n        print(f\"[single_repo] Single-repo mode enabled - using collection: {collection}\")\n\n    flag = (os.environ.get(\"PSEUDO_BACKFILL_ENABLED\") or \"\").strip().lower()\n    pseudo_mode = \"off\" if flag in {\"1\", \"true\", \"yes\", \"on\"} else \"full\"\n\n    # Per-file locking in index_single_file handles indexer/watcher coordination\n    index_repo(\n        Path(args.root).resolve(),\n        qdrant_url,\n        api_key,\n        collection,\n        model_name,\n        args.recreate,\n        dedupe=(not args.no_dedupe),\n        skip_unchanged=(not args.no_skip_unchanged),\n        pseudo_mode=pseudo_mode,\n        clear_caches=args.clear_indexing_caches,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}