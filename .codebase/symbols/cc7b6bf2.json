{
  "file_path": "/work/external-deps/helix-db/helix-db/src/helix_gateway/tests/worker_pool_concurrency_tests.rs",
  "file_hash": "38730a286ca9a6818f6f932a0ee94ebcc72705a9",
  "updated_at": "2025-12-26T17:34:20.169557",
  "symbols": {
    "function_test_handler_38": {
      "name": "test_handler",
      "type": "function",
      "start_line": 38,
      "end_line": 44,
      "content_hash": "ef3c05cd18bc3f4b9de82bbb7c47e455539963c4",
      "content": "fn test_handler(_input: HandlerInput) -> Result<Response, GraphError> {\n    Ok(Response {\n        body: b\"test response\".to_vec(),\n        fmt: Format::Json,\n    })\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_create_test_graph_45": {
      "name": "create_test_graph",
      "type": "function",
      "start_line": 45,
      "end_line": 55,
      "content_hash": "3d67913393cebfedc195b65cc1e8760f8087f85b",
      "content": "fn create_test_graph() -> (Arc<HelixGraphEngine>, TempDir) {\n    let temp_dir = TempDir::new().unwrap();\n    let opts = HelixGraphEngineOpts {\n        path: temp_dir.path().to_str().unwrap().to_string(),\n        config: Config::default(),\n        version_info: Default::default(),\n    };\n    let graph = Arc::new(HelixGraphEngine::new(opts).unwrap());\n    (graph, temp_dir)\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_create_request_56": {
      "name": "create_request",
      "type": "function",
      "start_line": 56,
      "end_line": 66,
      "content_hash": "51334e4fe4836080e139cd605848783c403c50f7",
      "content": "fn create_request(name: &str) -> Request {\n    Request {\n        name: name.to_string(),\n        req_type: RequestType::Query,\n        api_key: None,\n        body: Bytes::new(),\n        in_fmt: Format::Json,\n        out_fmt: Format::Json,\n    }\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_create_test_pool_67": {
      "name": "create_test_pool",
      "type": "function",
      "start_line": 67,
      "end_line": 444,
      "content_hash": "4fa91d47a8f1521438c442aed56cabf19bc38210",
      "content": "fn create_test_pool(\n    num_cores: usize,\n    threads_per_core: usize,\n    routes: Option<\n        HashMap<String, Arc<dyn Fn(HandlerInput) -> Result<Response, GraphError> + Send + Sync>>,\n    >,\n) -> (WorkerPool, Arc<HelixGraphEngine>, TempDir) {\n    let (graph, temp_dir) = create_test_graph();\n    let router = Arc::new(HelixRouter::new(routes, None, None));\n    let rt = Arc::new(\n        tokio::runtime::Builder::new_multi_thread()\n            .worker_threads(num_cores)\n            .enable_all()\n            .build()\n            .unwrap(),\n    );\n\n    let cores: Vec<_> = (0..num_cores)\n        .map(|id| core_affinity::CoreId { id })\n        .collect();\n    let core_setter = Arc::new(CoreSetter::new(cores, threads_per_core));\n\n    let pool = WorkerPool::new(core_setter, Arc::clone(&graph), router, rt);\n    (pool, graph, temp_dir)\n}\n\n#[tokio::test]\nasync fn test_concurrent_requests_high_load() {\n    // Tests many concurrent requests to validate worker pool handles high load\n    //\n    // EXPECTED: All requests complete successfully, no deadlocks\n\n    let (pool, _graph, _temp_dir) = create_test_pool(2, 2, None); // 4 workers\n    let pool = Arc::new(pool);\n\n    let num_concurrent = 100;\n    let mut handles = vec![];\n\n    for i in 0..num_concurrent {\n        let pool = Arc::clone(&pool);\n        let handle = tokio::spawn(async move {\n            let req = create_request(&format!(\"request_{}\", i));\n            pool.process(req).await\n        });\n        handles.push(handle);\n    }\n\n    let mut completed = 0;\n    for handle in handles {\n        // Count all requests that complete (regardless of success/error)\n        if handle.await.is_ok() {\n            completed += 1;\n        }\n    }\n\n    // All should complete (no panics or hangs)\n    assert_eq!(\n        completed, num_concurrent,\n        \"All requests should complete, got {}/{}\",\n        completed, num_concurrent\n    );\n    println!(\"High load test: {} requests completed\", num_concurrent);\n}\n\n#[tokio::test(flavor = \"multi_thread\")]\nasync fn test_concurrent_burst_requests() {\n    // Tests burst of concurrent requests\n    //\n    // EXPECTED: Pool handles bursts without deadlock\n\n    // Register handlers for all burst request names\n    let mut routes = HashMap::new();\n    for burst in 0..5 {\n        for req in 0..20 {\n            routes.insert(\n                format!(\"burst_{}_req_{}\", burst, req),\n                Arc::new(test_handler) as Arc<_>,\n            );\n        }\n    }\n\n    let (pool, _graph, _temp_dir) = create_test_pool(2, 2, Some(routes));\n    let pool = Arc::new(pool);\n\n    // Send multiple bursts\n    for burst in 0..5 {\n        let mut handles = vec![];\n\n        for i in 0..20 {\n            let pool = Arc::clone(&pool);\n            let handle = tokio::spawn(async move {\n                let req = create_request(&format!(\"burst_{}_req_{}\", burst, i));\n                pool.process(req).await\n            });\n            handles.push(handle);\n        }\n\n        for handle in handles {\n            handle.await.unwrap().unwrap();\n        }\n    }\n\n    // If we reach here, all bursts completed\n    println!(\"Burst test: 5 bursts of 20 requests completed\");\n}\n\n#[tokio::test]\nasync fn test_channel_backpressure() {\n    // Tests bounded channel behavior (1000 capacity)\n    //\n    // EXPECTED: Requests block when channel is full but don't fail\n\n    let (pool, _graph, _temp_dir) = create_test_pool(1, 2, None); // 2 workers\n    let pool = Arc::new(pool);\n\n    // Send requests that should stress channel capacity\n    let num_requests = 100;\n    let mut handles = vec![];\n\n    for i in 0..num_requests {\n        let pool = Arc::clone(&pool);\n        let handle = tokio::spawn(async move {\n            let req = create_request(&format!(\"backpressure_{}\", i));\n            pool.process(req).await\n        });\n        handles.push(handle);\n    }\n\n    // All should complete even with channel pressure\n    let mut completed = 0;\n    for handle in handles {\n        if handle.await.is_ok() {\n            completed += 1;\n        }\n    }\n\n    assert_eq!(completed, num_requests);\n    println!(\"Backpressure test: {} requests completed\", completed);\n}\n\n#[tokio::test]\nasync fn test_request_timeout_handling() {\n    // Tests that requests don't hang indefinitely\n    //\n    // EXPECTED: Timeout mechanism works correctly\n\n    let (pool, _graph, _temp_dir) = create_test_pool(1, 2, None);\n    let pool = Arc::new(pool);\n\n    let req = create_request(\"timeout_test\");\n\n    // Should complete quickly (either success or error, not hang)\n    let result = timeout(Duration::from_secs(5), pool.process(req)).await;\n\n    assert!(result.is_ok(), \"Request should not timeout\");\n}\n\n#[tokio::test]\nasync fn test_parity_mechanism_both_workers() {\n    // Tests that parity mechanism allows both even and odd workers to process\n    //\n    // This is indirect - we validate many requests complete successfully\n\n    let (pool, _graph, _temp_dir) = create_test_pool(2, 2, None); // 4 workers (even number for parity)\n    let pool = Arc::new(pool);\n\n    // Send many requests - with proper parity, both types of workers participate\n    let num_requests = 100;\n    let mut handles = vec![];\n\n    for i in 0..num_requests {\n        let pool = Arc::clone(&pool);\n        handles.push(tokio::spawn(async move {\n            let req = create_request(&format!(\"parity_test_{}\", i));\n            pool.process(req).await\n        }));\n    }\n\n    let mut completed = 0;\n    for handle in handles {\n        if handle.await.is_ok() {\n            completed += 1;\n        }\n    }\n\n    // All should complete if parity mechanism allows all workers to participate\n    assert_eq!(completed, num_requests);\n    println!(\n        \"Parity test: {} requests completed across even/odd workers\",\n        completed\n    );\n}\n\n#[tokio::test]\nasync fn test_worker_pool_drop_graceful() {\n    // Tests that dropping the pool doesn't cause panics\n    //\n    // EXPECTED: No panics or hangs when pool is dropped\n\n    {\n        let (pool, _graph, _temp_dir) = create_test_pool(1, 2, None);\n        let pool = Arc::new(pool);\n\n        // Process a few requests\n        for i in 0..5 {\n            let req = create_request(&format!(\"drop_test_{}\", i));\n            let _ = pool.process(req).await;\n        }\n    } // Pool dropped here\n\n    // If we reach this point, drop was graceful\n    println!(\"Drop test: Pool dropped gracefully\");\n}\n\n#[tokio::test(flavor = \"multi_thread\")]\nasync fn test_stress_sustained_load() {\n    // Stress test: sustained high load over time\n    //\n    // EXPECTED: No degradation, memory leaks, or panics\n\n    let (pool, _graph, _temp_dir) = create_test_pool(2, 2, None);\n    let pool = Arc::new(pool);\n\n    let total_requests = Arc::new(AtomicUsize::new(0));\n    let duration = Duration::from_secs(2); // 2 second stress test\n    let start = std::time::Instant::now();\n\n    let mut handles = vec![];\n\n    // Spawn multiple concurrent request generators\n    for gen_id in 0..4 {\n        let pool = Arc::clone(&pool);\n        let total = Arc::clone(&total_requests);\n\n        handles.push(tokio::spawn(async move {\n            let mut local_count = 0;\n            while start.elapsed() < duration {\n                let req = create_request(&format!(\"stress_gen_{}_req_{}\", gen_id, local_count));\n                let _ = pool.process(req).await;\n                total.fetch_add(1, Ordering::Relaxed);\n                local_count += 1;\n            }\n            local_count\n        }));\n    }\n\n    let mut per_gen = vec![];\n    for handle in handles {\n        per_gen.push(handle.await.unwrap());\n    }\n\n    let total = total_requests.load(Ordering::Relaxed);\n    println!(\"Stress test: {} total requests in {:?}\", total, duration);\n    println!(\"Per generator: {:?}\", per_gen);\n\n    // Should process many requests (validates no deadlocks or severe contention)\n    assert!(total > 100, \"Should process many requests, got {}\", total);\n}\n\n#[tokio::test]\nasync fn test_concurrent_different_request_types() {\n    // Tests concurrent requests of different types\n    //\n    // EXPECTED: All request types handled concurrently\n\n    let (pool, _graph, _temp_dir) = create_test_pool(2, 2, None);\n    let pool = Arc::new(pool);\n\n    let mut handles = vec![];\n\n    // Mix of different request names\n    let request_types = vec![\"query_a\", \"query_b\", \"mutation_c\", \"read_d\"];\n\n    for _ in 0..25 {\n        for req_type in &request_types {\n            let pool = Arc::clone(&pool);\n            let req_type = req_type.to_string();\n            handles.push(tokio::spawn(async move {\n                let req = create_request(&req_type);\n                pool.process(req).await\n            }));\n        }\n    }\n\n    let mut completed = 0;\n    for handle in handles {\n        if handle.await.is_ok() {\n            completed += 1;\n        }\n    }\n\n    let expected = 25 * request_types.len();\n    assert_eq!(completed, expected);\n    println!(\n        \"Different request types: {}/{} completed\",\n        completed, expected\n    );\n}\n\n#[tokio::test]\nasync fn test_sequential_then_concurrent() {\n    // Tests transitioning from sequential to concurrent load\n    //\n    // EXPECTED: No issues transitioning between load patterns\n\n    let (pool, _graph, _temp_dir) = create_test_pool(2, 2, None);\n    let pool = Arc::new(pool);\n\n    // Sequential requests\n    for i in 0..10 {\n        let req = create_request(&format!(\"sequential_{}\", i));\n        pool.process(req).await.ok();\n    }\n\n    // Then concurrent burst\n    let mut handles = vec![];\n    for i in 0..50 {\n        let pool = Arc::clone(&pool);\n        handles.push(tokio::spawn(async move {\n            let req = create_request(&format!(\"concurrent_{}\", i));\n            pool.process(req).await\n        }));\n    }\n\n    let mut completed = 0;\n    for handle in handles {\n        if handle.await.is_ok() {\n            completed += 1;\n        }\n    }\n\n    assert_eq!(completed, 50);\n    println!(\"Sequential->Concurrent test: 10 sequential + 50 concurrent completed\");\n}\n\n#[tokio::test]\nasync fn test_worker_distribution_fairness() {\n    // Tests that requests are distributed across workers\n    //\n    // With 4 workers and 100 requests, work should be distributed\n\n    // Register handlers for all fairness request names\n    let mut routes = HashMap::new();\n    for i in 0..100 {\n        routes.insert(format!(\"fairness_{}\", i), Arc::new(test_handler) as Arc<_>);\n    }\n\n    let (pool, _graph, _temp_dir) = create_test_pool(2, 2, Some(routes)); // 4 workers\n    let pool = Arc::new(pool);\n\n    let start = std::time::Instant::now();\n    let mut handles = vec![];\n\n    for i in 0..100 {\n        let pool = Arc::clone(&pool);\n        handles.push(tokio::spawn(async move {\n            let req = create_request(&format!(\"fairness_{}\", i));\n            pool.process(req).await\n        }));\n    }\n\n    for handle in handles {\n        handle.await.unwrap().unwrap();\n    }\n\n    let elapsed = start.elapsed();\n\n    // With good distribution across 4 workers, should be relatively fast\n    // (Not strictly deterministic, but gives us a signal)\n    println!(\"Fairness test: 100 requests completed in {:?}\", elapsed);\n\n    // Basic sanity: should complete in reasonable time\n    assert!(\n        elapsed < Duration::from_secs(10),\n        \"Requests took {:?}, may indicate poor distribution\",\n        elapsed\n    );\n}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}