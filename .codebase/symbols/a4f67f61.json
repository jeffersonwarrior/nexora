{
  "file_path": "/work/context-engine/scripts/rerank_tools/eval.py",
  "file_hash": "350283c620eb22abe7a28ef823e97a08f764d3f9",
  "updated_at": "2025-12-26T17:34:23.403966",
  "symbols": {
    "class_EvalResult_49": {
      "name": "EvalResult",
      "type": "class",
      "start_line": 49,
      "end_line": 58,
      "content_hash": "53206f2c2c3e463f0074490941f00ed93c5d19e7",
      "content": "class EvalResult:\n    \"\"\"Evaluation result for a single query.\"\"\"\n    query: str\n    mode: str\n    latency_ms: float\n    top_k_paths: List[str]\n    top_k_scores: List[float]\n    mrr: float = 0.0\n    recall_at_5: float = 0.0\n    recall_at_10: float = 0.0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_EvalSummary_62": {
      "name": "EvalSummary",
      "type": "class",
      "start_line": 62,
      "end_line": 72,
      "content_hash": "d114a1a2b96ac5e0a35cd51f80acd1a3955728d0",
      "content": "class EvalSummary:\n    \"\"\"Aggregated evaluation summary.\"\"\"\n    mode: str\n    num_queries: int\n    mrr_mean: float\n    recall_at_5_mean: float\n    recall_at_10_mean: float\n    latency_p50_ms: float\n    latency_p95_ms: float\n    latency_p99_ms: float\n    results: List[EvalResult] = field(default_factory=list)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_candidates_75": {
      "name": "get_candidates",
      "type": "function",
      "start_line": 75,
      "end_line": 105,
      "content_hash": "1c60e1ffc566b805516ef24d93271eaa9ade565b",
      "content": "def get_candidates(query: str, limit: int = 30) -> List[Dict[str, Any]]:\n    \"\"\"Get candidates from hybrid search.\"\"\"\n    try:\n        from scripts.hybrid_search import run_hybrid_search\n        from scripts.embedder import get_embedding_model\n\n        # Use BAAI/bge-base-en-v1.5 which is supported by fastembed\n        model_name = os.environ.get(\"EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\")\n        model = get_embedding_model(model_name)\n\n        results = run_hybrid_search(\n            queries=[query],\n            limit=limit,\n            per_path=3,\n            model=model,\n        )\n\n        candidates = []\n        for r in results:\n            candidates.append({\n                \"path\": r.get(\"path\", \"\"),\n                \"symbol\": r.get(\"symbol\", \"\"),\n                \"start_line\": r.get(\"start_line\", 0),\n                \"end_line\": r.get(\"end_line\", 0),\n                \"score\": float(r.get(\"score\", 0)),\n                \"snippet\": r.get(\"snippet\", \"\")[:500] if r.get(\"snippet\") else \"\",\n            })\n        return candidates\n    except Exception as e:\n        print(f\"Warning: Could not get candidates: {e}\", file=sys.stderr)\n        return []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_onnx_scores_108": {
      "name": "get_onnx_scores",
      "type": "function",
      "start_line": 108,
      "end_line": 126,
      "content_hash": "dbfffcb8a7664f9c38bcedede319a0dc4e6426ba",
      "content": "def get_onnx_scores(query: str, candidates: List[Dict[str, Any]]) -> Optional[List[float]]:\n    \"\"\"Get ONNX reranker scores (ground truth).\"\"\"\n    try:\n        from scripts.rerank_local import rerank_local\n        pairs = []\n        for c in candidates:\n            doc_parts = []\n            if c.get(\"symbol\"):\n                doc_parts.append(str(c[\"symbol\"]))\n            if c.get(\"path\"):\n                doc_parts.append(str(c[\"path\"]))\n            code = c.get(\"code\") or c.get(\"snippet\") or \"\"\n            if code:\n                doc_parts.append(code[:500])\n            doc = \" \".join(doc_parts) if doc_parts else \"empty\"\n            pairs.append((query, doc))\n        return rerank_local(pairs)\n    except Exception:\n        return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_rerank_baseline_129": {
      "name": "rerank_baseline",
      "type": "function",
      "start_line": 129,
      "end_line": 131,
      "content_hash": "84d42f332a433c3cf3be6a89c86bd594aba5431e",
      "content": "def rerank_baseline(query: str, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Baseline: no reranking, just return as-is.\"\"\"\n    return candidates",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_rerank_recursive_134": {
      "name": "rerank_recursive",
      "type": "function",
      "start_line": 134,
      "end_line": 147,
      "content_hash": "de531598930724b23041021f0489b181be88e977",
      "content": "def rerank_recursive(\n    query: str,\n    candidates: List[Dict[str, Any]],\n    n_iterations: int = 3,\n) -> List[Dict[str, Any]]:\n    \"\"\"Recursive reranker (no learning).\"\"\"\n    try:\n        from scripts.rerank_recursive import RecursiveReranker\n        reranker = RecursiveReranker(n_iterations=n_iterations, dim=256)\n        initial_scores = [c.get(\"score\", 0) for c in candidates]\n        return reranker.rerank(query, candidates, initial_scores)\n    except Exception as e:\n        print(f\"Warning: Recursive rerank failed: {e}\", file=sys.stderr)\n        return candidates",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_rerank_learning_150": {
      "name": "rerank_learning",
      "type": "function",
      "start_line": 150,
      "end_line": 168,
      "content_hash": "f762e2dc1b25e06a35c417af686cc974a4d162c6",
      "content": "def rerank_learning(\n    query: str,\n    candidates: List[Dict[str, Any]],\n    collection: str = \"eval\",\n) -> List[Dict[str, Any]]:\n    \"\"\"Learning reranker (uses trained weights).\"\"\"\n    try:\n        from scripts.rerank_recursive import rerank_with_learning\n        return rerank_with_learning(\n            query=query,\n            candidates=candidates,\n            limit=len(candidates),\n            n_iterations=3,\n            learn_from_onnx=False,  # Eval mode: no training\n            collection=collection,\n        )\n    except Exception as e:\n        print(f\"Warning: Learning rerank failed: {e}\", file=sys.stderr)\n        return candidates",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_rerank_onnx_171": {
      "name": "rerank_onnx",
      "type": "function",
      "start_line": 171,
      "end_line": 178,
      "content_hash": "f07b4ee7797378c4b4e3b1a96104e092e517f461",
      "content": "def rerank_onnx(query: str, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"ONNX reranker (teacher/ground truth).\"\"\"\n    scores = get_onnx_scores(query, candidates)\n    if scores is None:\n        return candidates\n    for c, s in zip(candidates, scores):\n        c[\"score\"] = s\n    return sorted(candidates, key=lambda x: x.get(\"score\", 0), reverse=True)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_compute_mrr_181": {
      "name": "compute_mrr",
      "type": "function",
      "start_line": 181,
      "end_line": 201,
      "content_hash": "ae914e085443913128785d67f51e4e3c915f02b8",
      "content": "def compute_mrr(ranked_paths: List[str], relevant_paths: List[str], k: int = 10) -> float:\n    \"\"\"Compute Mean Reciprocal Rank.\"\"\"\n    # Deduplicate while preserving order (paths can repeat due to multi-span retrieval)\n    rel_set = {p for p in relevant_paths if p}\n    if not rel_set:\n        return 0.0\n\n    seen = set()\n    uniq_ranked: List[str] = []\n    for p in ranked_paths:\n        if not p or p in seen:\n            continue\n        seen.add(p)\n        uniq_ranked.append(p)\n        if len(uniq_ranked) >= k:\n            break\n\n    for i, path in enumerate(uniq_ranked):\n        if path in rel_set:\n            return 1.0 / (i + 1)\n    return 0.0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_compute_recall_at_k_204": {
      "name": "compute_recall_at_k",
      "type": "function",
      "start_line": 204,
      "end_line": 221,
      "content_hash": "63b11c534e46389b55e6e6bbcfa50920811da830",
      "content": "def compute_recall_at_k(ranked_paths: List[str], relevant_paths: List[str], k: int) -> float:\n    \"\"\"Compute Recall@k.\"\"\"\n    rel_set = {p for p in relevant_paths if p}\n    if not rel_set:\n        return 0.0\n\n    seen = set()\n    uniq_ranked: List[str] = []\n    for p in ranked_paths:\n        if not p or p in seen:\n            continue\n        seen.add(p)\n        uniq_ranked.append(p)\n        if len(uniq_ranked) >= k:\n            break\n\n    found = len(set(uniq_ranked) & rel_set)\n    return found / len(rel_set)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_eval_single_query_224": {
      "name": "eval_single_query",
      "type": "function",
      "start_line": 224,
      "end_line": 273,
      "content_hash": "4176e2314c17d9f5544f502a8938af1f5e68396f",
      "content": "def eval_single_query(\n    query: str,\n    mode: str,\n    candidates: List[Dict[str, Any]],\n    reference_paths: List[str],\n) -> EvalResult:\n    \"\"\"Evaluate a single query with a specific reranking mode.\"\"\"\n    start = time.perf_counter()\n\n    if mode == \"baseline\":\n        reranked = rerank_baseline(query, candidates)\n    elif mode == \"recursive\":\n        reranked = rerank_recursive(query, candidates)\n    elif mode == \"learning\":\n        reranked = rerank_learning(query, candidates)\n    elif mode == \"onnx\":\n        reranked = rerank_onnx(query, candidates)\n    else:\n        reranked = candidates\n\n    latency_ms = (time.perf_counter() - start) * 1000\n\n    # Export unique paths (avoid duplicates from multi-span retrieval)\n    ranked_paths: List[str] = []\n    ranked_scores: List[float] = []\n    seen_paths = set()\n    for c in reranked:\n        path = str(c.get(\"path\", \"\") or \"\")\n        if not path or path in seen_paths:\n            continue\n        seen_paths.add(path)\n        ranked_paths.append(path)\n        ranked_scores.append(float(c.get(\"score\", 0)))\n        if len(ranked_paths) >= 10:\n            break\n\n    mrr = compute_mrr(ranked_paths, reference_paths)\n    recall_5 = compute_recall_at_k(ranked_paths, reference_paths, 5)\n    recall_10 = compute_recall_at_k(ranked_paths, reference_paths, 10)\n\n    return EvalResult(\n        query=query,\n        mode=mode,\n        latency_ms=latency_ms,\n        top_k_paths=ranked_paths,\n        top_k_scores=ranked_scores,\n        mrr=mrr,\n        recall_at_5=recall_5,\n        recall_at_10=recall_10,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_run_eval_276": {
      "name": "run_eval",
      "type": "function",
      "start_line": 276,
      "end_line": 329,
      "content_hash": "00fc84ca97d030816105cba5b4dfbaf02d62af4e",
      "content": "def run_eval(\n    queries: List[str],\n    modes: List[str],\n    use_onnx_reference: bool = True,\n) -> Dict[str, EvalSummary]:\n    \"\"\"Run evaluation across all queries and modes.\"\"\"\n    summaries: Dict[str, EvalSummary] = {}\n\n    # Warmup: pre-cache embeddings for all queries (cold start is not representative)\n    print(\"Warming up embedding cache...\")\n    for query in queries:\n        candidates = get_candidates(query)\n        if candidates and \"learning\" in modes:\n            # Run once to cache embeddings\n            rerank_learning(query, copy.deepcopy(candidates))\n    print(\"Warmup complete.\")\n\n    for mode in modes:\n        results = []\n        latencies = []\n\n        for query in queries:\n            candidates = get_candidates(query)\n            if not candidates:\n                continue\n\n            # Use ONNX top-5 as \"relevant\" ground truth\n            # Deep copy to prevent score mutation from leaking between modes\n            reference_paths = []\n            if use_onnx_reference:\n                onnx_ranked = rerank_onnx(query, copy.deepcopy(candidates))\n                reference_paths = [c.get(\"path\", \"\") for c in onnx_ranked[:5]]\n\n            result = eval_single_query(query, mode, copy.deepcopy(candidates), reference_paths)\n            results.append(result)\n            latencies.append(result.latency_ms)\n\n        if not results:\n            continue\n\n        latencies_arr = np.array(latencies)\n        summaries[mode] = EvalSummary(\n            mode=mode,\n            num_queries=len(results),\n            mrr_mean=np.mean([r.mrr for r in results]),\n            recall_at_5_mean=np.mean([r.recall_at_5 for r in results]),\n            recall_at_10_mean=np.mean([r.recall_at_10 for r in results]),\n            latency_p50_ms=float(np.percentile(latencies_arr, 50)),\n            latency_p95_ms=float(np.percentile(latencies_arr, 95)),\n            latency_p99_ms=float(np.percentile(latencies_arr, 99)),\n            results=results,\n        )\n\n    return summaries",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_print_summary_332": {
      "name": "print_summary",
      "type": "function",
      "start_line": 332,
      "end_line": 375,
      "content_hash": "0d86831ab9302b34d1b7685c4067d475ea39a99b",
      "content": "def print_summary(summaries: Dict[str, EvalSummary]):\n    \"\"\"Print evaluation summary table.\"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"RERANKER EVALUATION SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"{'Mode':<12} {'MRR':<8} {'R@5':<8} {'R@10':<8} {'p50ms':<8} {'p95ms':<8} {'p99ms':<8}\")\n    print(\"-\" * 80)\n\n    # Track for comparison\n    baseline_mrr = summaries.get(\"baseline\", EvalSummary(\"baseline\", 0, 0, 0, 0, 0, 0, 0)).mrr_mean\n    onnx_mrr = summaries.get(\"onnx\", EvalSummary(\"onnx\", 0, 0, 0, 0, 0, 0, 0)).mrr_mean\n    onnx_p50 = summaries.get(\"onnx\", EvalSummary(\"onnx\", 0, 0, 0, 0, 0, 0, 0)).latency_p50_ms\n    learning_mrr = summaries.get(\"learning\", EvalSummary(\"learning\", 0, 0, 0, 0, 0, 0, 0)).mrr_mean\n    learning_p50 = summaries.get(\"learning\", EvalSummary(\"learning\", 0, 0, 0, 0, 0, 0, 0)).latency_p50_ms\n\n    for mode, summary in summaries.items():\n        print(\n            f\"{mode:<12} \"\n            f\"{summary.mrr_mean:<8.3f} \"\n            f\"{summary.recall_at_5_mean:<8.3f} \"\n            f\"{summary.recall_at_10_mean:<8.3f} \"\n            f\"{summary.latency_p50_ms:<8.1f} \"\n            f\"{summary.latency_p95_ms:<8.1f} \"\n            f\"{summary.latency_p99_ms:<8.1f}\"\n        )\n    print(\"=\" * 80)\n\n    # Self-improving search analysis\n    print(\"\\nSELF-IMPROVING SEARCH ANALYSIS:\")\n    print(\"-\" * 50)\n\n    if baseline_mrr > 0:\n        learning_vs_baseline = ((learning_mrr - baseline_mrr) / baseline_mrr) * 100\n        print(f\"Learning vs Baseline: {learning_vs_baseline:+.1f}% MRR improvement\")\n\n    if onnx_p50 > 0 and learning_p50 > 0:\n        speedup = onnx_p50 / learning_p50\n        print(f\"Learning vs ONNX:     {speedup:.1f}x faster ({learning_p50:.1f}ms vs {onnx_p50:.1f}ms)\")\n\n    if onnx_mrr > 0:\n        distill_quality = (learning_mrr / onnx_mrr) * 100\n        print(f\"Distillation quality: {distill_quality:.1f}% of ONNX MRR\")\n\n    print(\"-\" * 50)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_main_378": {
      "name": "main",
      "type": "function",
      "start_line": 378,
      "end_line": 404,
      "content_hash": "09d7be1595e1a72568c6998fc7310ab31b6180a8",
      "content": "def main():\n    parser = argparse.ArgumentParser(description=\"Offline reranker evaluation\")\n    parser.add_argument(\"--queries\", type=str, help=\"JSON file with query list\")\n    parser.add_argument(\"--output\", type=str, help=\"Output JSON file for results\")\n    parser.add_argument(\"--ablations\", action=\"store_true\", help=\"Run all ablation modes\")\n    parser.add_argument(\"--modes\", type=str, default=\"baseline,recursive,learning\",\n                        help=\"Comma-separated modes to evaluate\")\n    args = parser.parse_args()\n\n    queries = DEFAULT_EVAL_QUERIES\n    if args.queries:\n        with open(args.queries) as f:\n            queries = json.load(f)\n\n    modes = args.modes.split(\",\")\n    if args.ablations:\n        modes = [\"baseline\", \"recursive\", \"learning\", \"onnx\"]\n\n    print(f\"Running evaluation: {len(queries)} queries, modes: {modes}\")\n    summaries = run_eval(queries, modes)\n    print_summary(summaries)\n\n    if args.output:\n        output_data = {mode: asdict(s) for mode, s in summaries.items()}\n        with open(args.output, \"w\") as f:\n            json.dump(output_data, f, indent=2)\n        print(f\"Results saved to {args.output}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}