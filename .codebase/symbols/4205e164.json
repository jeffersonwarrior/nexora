{
  "file_path": "/work/context-engine/tests/test_context_answer_path_mention.py",
  "file_hash": "c7a7008e4c136304b9b3bfd38fc843725856a9e9",
  "updated_at": "2025-12-26T17:34:20.136600",
  "symbols": {
    "function_test_context_answer_path_mention_fallback_8": {
      "name": "test_context_answer_path_mention_fallback",
      "type": "function",
      "start_line": 8,
      "end_line": 42,
      "content_hash": "8d253f7917c461201911f28bbd61e67d7fe9022f",
      "content": "def test_context_answer_path_mention_fallback(monkeypatch):\n    # Mock embedding model to avoid loading real model\n    monkeypatch.setattr(srv, \"_get_embedding_model\", lambda *a, **k: None)\n\n    # Force retrieval to return nothing so path-mention fallback engages\n    import scripts.hybrid_search as hs\n    monkeypatch.setattr(hs, \"run_hybrid_search\", lambda **k: [])\n\n    import scripts.refrag_llamacpp as ref\n\n    class FakeLlama:\n        def __init__(self, *a, **k):\n            pass\n\n        def generate_with_soft_embeddings(self, prompt: str, max_tokens: int = 64, **kw):\n            # Should still include Sources and [1] with the mentioned file\n            assert \"Sources:\" in prompt\n            assert \"[1]\" in prompt\n            return \"ok [1]\"\n\n    monkeypatch.setattr(ref, \"LlamaCppRefragClient\", FakeLlama)\n    monkeypatch.setattr(ref, \"is_decoder_enabled\", lambda: True)\n\n    # Mention an actual file in this repo so fallback can find it\n    q = \"explain something in scripts/hybrid_search.py\"\n    out = srv.asyncio.get_event_loop().run_until_complete(\n        srv.context_answer(query=q, limit=3, per_path=2)\n    )\n    assert isinstance(out, dict)\n    cits = out.get(\"citations\") or []\n    assert len(cits) >= 1\n    # Either path or rel_path should indicate the file\n    p = cits[0].get(\"path\") or \"\"\n    rp = cits[0].get(\"rel_path\") or \"\"\n    assert p.endswith(\"scripts/hybrid_search.py\") or rp.endswith(\"scripts/hybrid_search.py\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_FakeLlama_18": {
      "name": "FakeLlama",
      "type": "class",
      "start_line": 18,
      "end_line": 26,
      "content_hash": "9f0a244ffd16765d49c4a49871d447065fdeca7e",
      "content": "    class FakeLlama:\n        def __init__(self, *a, **k):\n            pass\n\n        def generate_with_soft_embeddings(self, prompt: str, max_tokens: int = 64, **kw):\n            # Should still include Sources and [1] with the mentioned file\n            assert \"Sources:\" in prompt\n            assert \"[1]\" in prompt\n            return \"ok [1]\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___19": {
      "name": "__init__",
      "type": "method",
      "start_line": 19,
      "end_line": 20,
      "content_hash": "2f01f3eeec882714bac9a504b0450cf9884e2078",
      "content": "        def __init__(self, *a, **k):\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_generate_with_soft_embeddings_22": {
      "name": "generate_with_soft_embeddings",
      "type": "method",
      "start_line": 22,
      "end_line": 26,
      "content_hash": "c4747361ee65fe0ce6f54e029835691183b927b8",
      "content": "        def generate_with_soft_embeddings(self, prompt: str, max_tokens: int = 64, **kw):\n            # Should still include Sources and [1] with the mentioned file\n            assert \"Sources:\" in prompt\n            assert \"[1]\" in prompt\n            return \"ok [1]\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}