{
  "file_path": "/work/external-deps/Context-Engine/scripts/semantic_expansion.py",
  "file_hash": "591c1a966a7104b8daef4f099f7fdd43b542b359",
  "updated_at": "2025-12-26T17:34:20.162362",
  "symbols": {
    "function__cosine_similarity_77": {
      "name": "_cosine_similarity",
      "type": "function",
      "start_line": 77,
      "end_line": 92,
      "content_hash": "c968db4e07d4c3bf16ac30d13bcd42280b918615",
      "content": "def _cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    if not vec1 or not vec2 or len(vec1) != len(vec2):\n        return 0.0\n    \n    try:\n        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n        norm1 = math.sqrt(sum(a * a for a in vec1))\n        norm2 = math.sqrt(sum(b * b for b in vec2))\n        \n        if norm1 == 0 or norm2 == 0:\n            return 0.0\n        \n        return dot_product / (norm1 * norm2)\n    except Exception:\n        return 0.0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__coerce_embedding_vector_95": {
      "name": "_coerce_embedding_vector",
      "type": "function",
      "start_line": 95,
      "end_line": 107,
      "content_hash": "d766addee5b9ac4cfa021fd1ac9dd3d4eb0f35f4",
      "content": "def _coerce_embedding_vector(raw: Any) -> Optional[List[float]]:\n    \"\"\"Best-effort conversion of embedding output into a flat float list.\"\"\"\n    try:\n        if raw is None:\n            return None\n        vec = raw.tolist() if hasattr(raw, \"tolist\") else raw\n        if isinstance(vec, (list, tuple)):\n            if vec and isinstance(vec[0], (list, tuple)):\n                vec = vec[0]\n            return [float(x) for x in vec]\n        return [float(x) for x in list(vec)]\n    except Exception:\n        return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_expansion_cache_key_110": {
      "name": "_get_expansion_cache_key",
      "type": "function",
      "start_line": 110,
      "end_line": 115,
      "content_hash": "70196218a8f88fed6bcb0be5595e9745e24f7357",
      "content": "def _get_expansion_cache_key(queries: List[str], language: Optional[str] = None) -> str:\n    \"\"\"Generate a cache key for query expansion.\"\"\"\n    # Normalize queries for consistent caching\n    normalized = [q.lower().strip() for q in queries if q.strip()]\n    lang_part = f\"lang:{language}\" if language else \"\"\n    return \"|\".join(sorted(normalized)) + f\"#{lang_part}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_cached_expansion_118": {
      "name": "_get_cached_expansion",
      "type": "function",
      "start_line": 118,
      "end_line": 131,
      "content_hash": "5255efae0b832a547c8e39cce8dffbe00fe06c8d",
      "content": "def _get_cached_expansion(cache_key: str) -> Optional[List[str]]:\n    \"\"\"Get cached expansion results.\"\"\"\n    global _cache_hits\n    if _UNIFIED_CACHE:\n        result = _expansion_cache.get(cache_key)\n        if result is not None:\n            _cache_hits += 1\n            return result.copy() if isinstance(result, list) else result\n        return None\n    else:\n        if cache_key in _expansion_cache:\n            _cache_hits += 1\n            return _expansion_cache[cache_key].copy()\n        return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cache_expansion_134": {
      "name": "_cache_expansion",
      "type": "function",
      "start_line": 134,
      "end_line": 146,
      "content_hash": "afd3582bdfff3e73160a4e0429c99196922b6d6d",
      "content": "def _cache_expansion(cache_key: str, expansions: List[str]) -> None:\n    \"\"\"Cache expansion results with LRU eviction.\"\"\"\n    global _cache_misses\n    _cache_misses += 1\n    \n    if _UNIFIED_CACHE:\n        _expansion_cache.set(cache_key, expansions.copy())\n    else:\n        _expansion_cache[cache_key] = expansions.copy()\n        if len(_expansion_cache) > SEMANTIC_EXPANSION_CACHE_SIZE:\n            keys_to_remove = list(_expansion_cache.keys())[:len(_expansion_cache) - SEMANTIC_EXPANSION_CACHE_SIZE]\n            for key in keys_to_remove:\n                del _expansion_cache[key]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_code_tokens_149": {
      "name": "_extract_code_tokens",
      "type": "function",
      "start_line": 149,
      "end_line": 163,
      "content_hash": "3eb43a47c59f93b2039fbe1bf3ff3babf959184e",
      "content": "def _extract_code_tokens(text: str) -> List[str]:\n    \"\"\"Extract code-relevant tokens from text.\"\"\"\n    # Split on common delimiters and filter\n    tokens = re.split(r'[^A-Za-z0-9_]+', text)\n    \n    # Filter and normalize tokens\n    filtered = []\n    for token in tokens:\n        token = token.strip().lower()\n        if (len(token) >= 3 and \n            not token.isdigit() and \n            token not in {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'way', 'who', 'boy', 'did', 'does', 'let', 'put', 'say', 'she', 'too', 'use'}):\n            filtered.append(token)\n    \n    return filtered",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_terms_from_results_166": {
      "name": "_extract_terms_from_results",
      "type": "function",
      "start_line": 166,
      "end_line": 205,
      "content_hash": "c6dd4f9171583a9eb11fa70ef6789712d220cd94",
      "content": "def _extract_terms_from_results(results: List[Any], max_terms: int = 20) -> List[str]:\n    \"\"\"Extract relevant terms from search results for expansion.\"\"\"\n    if not results:\n        return []\n    \n    term_freq = defaultdict(int)\n    \n    for result in results[:10]:  # Limit to top 10 results for performance\n        try:\n            # Extract metadata\n            if hasattr(result, 'payload') and result.payload:\n                metadata = result.payload.get('metadata', {})\n            else:\n                metadata = {}\n            \n            # Extract text from various fields\n            text_fields = [\n                metadata.get('text', ''),\n                metadata.get('code', ''),\n                metadata.get('symbol', ''),\n                metadata.get('symbol_path', ''),\n                metadata.get('path', '')\n            ]\n            \n            combined_text = ' '.join(str(field) for field in text_fields if field)\n            \n            # Extract tokens\n            tokens = _extract_code_tokens(combined_text)\n            \n            # Count frequency\n            for token in tokens:\n                term_freq[token] += 1\n                \n        except Exception as e:\n            logger.debug(f\"Error extracting terms from result: {e}\")\n            continue\n    \n    # Sort by frequency and return top terms\n    sorted_terms = sorted(term_freq.items(), key=lambda x: x[1], reverse=True)\n    return [term for term, freq in sorted_terms[:max_terms]]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__expand_with_lexical_similarity_208": {
      "name": "_expand_with_lexical_similarity",
      "type": "function",
      "start_line": 208,
      "end_line": 227,
      "content_hash": "67111462af127287bc8acc5b1213505f08f6b2e7",
      "content": "def _expand_with_lexical_similarity(queries: List[str], candidate_terms: List[str]) -> List[str]:\n    \"\"\"Expand queries using lexical similarity when embeddings aren't available.\"\"\"\n    expansions = []\n    \n    for query in queries:\n        query_tokens = set(_extract_code_tokens(query))\n        \n        for term in candidate_terms:\n            term_tokens = set(_extract_code_tokens(term))\n            \n            # Calculate Jaccard similarity\n            intersection = query_tokens.intersection(term_tokens)\n            union = query_tokens.union(term_tokens)\n            \n            if union:\n                similarity = len(intersection) / len(union)\n                if similarity >= 0.3:  # Threshold for lexical similarity\n                    expansions.append(term)\n    \n    return expansions[:SEMANTIC_EXPANSION_MAX_TERMS]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_expand_queries_semantically_230": {
      "name": "expand_queries_semantically",
      "type": "function",
      "start_line": 230,
      "end_line": 372,
      "content_hash": "85242cf4f655e2c3c098aab3c4ef9871a4f53e51",
      "content": "def expand_queries_semantically(\n    queries: List[str], \n    language: Optional[str] = None,\n    client: Optional['QdrantClient'] = None,\n    model: Optional['TextEmbedding'] = None,\n    collection: Optional[str] = None,\n    max_expansions: int = None\n) -> List[str]:\n    \"\"\"\n    Expand queries using semantic similarity to improve search relevance.\n    \n    Args:\n        queries: Original query strings\n        language: Optional programming language hint\n        client: QdrantClient instance (optional, will create if None)\n        model: TextEmbedding instance (optional, will create if None)\n        collection: Collection name to search in\n        max_expansions: Maximum number of expansion terms to return\n        \n    Returns:\n        List of semantically related expansion terms\n    \"\"\"\n    if not SEMANTIC_EXPANSION_ENABLED or not queries:\n        return []\n    \n    max_expansions = max_expansions or SEMANTIC_EXPANSION_MAX_TERMS\n    \n    # Check cache first\n    cache_key = _get_expansion_cache_key(queries, language)\n    cached_result = _get_cached_expansion(cache_key)\n    if cached_result:\n        return cached_result[:max_expansions]\n    \n    try:\n        # Initialize components if not provided\n        if client is None and QDRANT_AVAILABLE:\n            qdrant_url = os.environ.get(\"QDRANT_URL\", \"http://localhost:6333\")\n            api_key = os.environ.get(\"QDRANT_API_KEY\")\n            client = QdrantClient(url=qdrant_url, api_key=api_key)\n        \n        if model is None and FASTEMBED_AVAILABLE:\n            model_name = os.environ.get(\"EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\")\n            if _EMBEDDER_FACTORY:\n                model = _get_embedding_model(model_name)\n            else:\n                model = TextEmbedding(model_name=model_name)\n        else:\n            # When caller injects a model, prefer its name for vector selection if available\n            model_name = getattr(model, \"model_name\", os.environ.get(\"EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\"))\n        \n        # Qdrant collections with multiple vectors require the vector name\n        vector_name = None\n        try:\n            vector_name = _sanitize_vector_name(model_name) if _sanitize_vector_name else None\n        except Exception:\n            vector_name = None\n        if not vector_name and model_name:\n            vector_name = str(model_name).replace(\"/\", \"-\").replace(\"_\", \"-\")[:64]\n        \n        if collection is None:\n            collection = os.environ.get(\"COLLECTION_NAME\", \"codebase\")\n        \n        # If we don't have the required components, fall back to lexical expansion\n        if not (client and model):\n            logger.debug(\"Semantic expansion unavailable: missing client or model\")\n            return []\n        \n        # Get initial search results to extract terms from\n        # Use a hybrid approach: combine original queries for initial search\n        combined_query = \" \".join(queries)\n        \n        # Get embeddings for the query\n        query_embeddings = list(model.embed([combined_query]))\n        if not query_embeddings:\n            return []\n\n        # Accept either vector objects with tolist() or plain (nested) lists\n        try:\n            query_vector = _coerce_embedding_vector(query_embeddings[0])\n        except Exception:\n            query_vector = None\n        if not query_vector:\n            return []\n\n        # Search for similar documents\n        try:\n            search_vector = query_vector\n            if vector_name:\n                try:\n                    search_vector = models.NamedVector(name=vector_name, vector=query_vector)\n                except Exception:\n                    search_vector = query_vector\n\n            search_results = client.search(\n                collection_name=collection,\n                query_vector=search_vector,\n                limit=SEMANTIC_EXPANSION_TOP_K,\n                with_payload=True,\n                with_vectors=False  # We don't need vectors for term extraction\n            )\n        except Exception as e:\n            logger.debug(f\"Search failed during semantic expansion: {e}\")\n            return []\n        \n        # Extract candidate terms from search results\n        candidate_terms = _extract_terms_from_results(search_results)\n        \n        if not candidate_terms:\n            return []\n        \n        # Calculate semantic similarity between query and candidates\n        # Get embeddings for candidate terms\n        candidate_embeddings = list(model.embed(candidate_terms))\n        if not candidate_embeddings:\n            return []\n\n        # Calculate similarities and filter by threshold\n        similar_terms = []\n        for i, term in enumerate(candidate_terms):\n            if i < len(candidate_embeddings):\n                try:\n                    candidate_vector = _coerce_embedding_vector(candidate_embeddings[i])\n                except Exception:\n                    continue\n                if not candidate_vector:\n                    continue\n                similarity = _cosine_similarity(query_vector, candidate_vector)\n\n                if similarity >= SEMANTIC_EXPANSION_SIMILARITY_THRESHOLD:\n                    similar_terms.append((term, similarity))\n\n        # Sort by similarity and return top terms\n        similar_terms.sort(key=lambda x: x[1], reverse=True)\n        expansions = [term for term, _ in similar_terms[:max_expansions]]\n\n        # Cache the result\n        _cache_expansion(cache_key, expansions)\n\n        return expansions\n\n    except Exception as e:\n        logger.debug(f\"Semantic expansion failed: {e}\")\n        return []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_expand_queries_with_prf_375": {
      "name": "expand_queries_with_prf",
      "type": "function",
      "start_line": 375,
      "end_line": 445,
      "content_hash": "a182e302a8a1122983a231e02800e68aeb744265",
      "content": "def expand_queries_with_prf(\n    queries: List[str],\n    initial_results: List[Any],\n    model: Optional['TextEmbedding'] = None,\n    max_expansions: int = None\n) -> List[str]:\n    \"\"\"\n    Expand queries using pseudo-relevance feedback from initial search results.\n    \n    Args:\n        queries: Original query strings\n        initial_results: Initial search results to use for feedback\n        model: TextEmbedding instance for semantic analysis\n        max_expansions: Maximum number of expansion terms\n        \n    Returns:\n        List of expansion terms derived from initial results\n    \"\"\"\n    if not initial_results or not queries:\n        return []\n    \n    max_expansions = max_expansions or SEMANTIC_EXPANSION_MAX_TERMS\n    \n    try:\n        # Extract candidate terms from initial results\n        candidate_terms = _extract_terms_from_results(initial_results)\n        \n        if not candidate_terms:\n            return []\n        \n        # If we have a model, use semantic similarity\n        if model and FASTEMBED_AVAILABLE:\n            # Get embeddings for queries\n            query_text = \" \".join(queries)\n            query_embeddings = list(model.embed([query_text]))\n            \n            if not query_embeddings:\n                return _expand_with_lexical_similarity(queries, candidate_terms)\n\n            query_vector = _coerce_embedding_vector(query_embeddings[0])\n            if not query_vector:\n                return _expand_with_lexical_similarity(queries, candidate_terms)\n            \n            # Get embeddings for candidates\n            candidate_embeddings = list(model.embed(candidate_terms))\n            \n            if not candidate_embeddings:\n                return _expand_with_lexical_similarity(queries, candidate_terms)\n            \n            # Calculate similarities\n            similar_terms = []\n            for i, term in enumerate(candidate_terms):\n                if i < len(candidate_embeddings):\n                    candidate_vector = _coerce_embedding_vector(candidate_embeddings[i])\n                    if not candidate_vector:\n                        continue\n                    similarity = _cosine_similarity(query_vector, candidate_vector)\n                    \n                    if similarity >= SEMANTIC_EXPANSION_SIMILARITY_THRESHOLD:\n                        similar_terms.append((term, similarity))\n            \n            # Sort by similarity and return top terms\n            similar_terms.sort(key=lambda x: x[1], reverse=True)\n            return [term for term, _ in similar_terms[:max_expansions]]\n        else:\n            # Fall back to lexical similarity\n            return _expand_with_lexical_similarity(queries, candidate_terms)\n            \n    except Exception as e:\n        logger.debug(f\"PRF expansion failed: {e}\")\n        return []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_expansion_stats_448": {
      "name": "get_expansion_stats",
      "type": "function",
      "start_line": 448,
      "end_line": 459,
      "content_hash": "bdba2ec61f083b0ae60db7fef933cb296597d3cf",
      "content": "def get_expansion_stats() -> Dict[str, Any]:\n    \"\"\"Get statistics about the expansion cache performance.\"\"\"\n    total_requests = _cache_hits + _cache_misses\n    hit_rate = (_cache_hits / total_requests * 100) if total_requests > 0 else 0\n    \n    return {\n        \"cache_hits\": _cache_hits,\n        \"cache_misses\": _cache_misses,\n        \"hit_rate_percent\": round(hit_rate, 2),\n        \"cache_size\": len(_expansion_cache),\n        \"max_cache_size\": SEMANTIC_EXPANSION_CACHE_SIZE\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_clear_expansion_cache_462": {
      "name": "clear_expansion_cache",
      "type": "function",
      "start_line": 462,
      "end_line": 467,
      "content_hash": "698d77df5ad12d860f1e61848adcca4106058648",
      "content": "def clear_expansion_cache() -> None:\n    \"\"\"Clear the expansion cache.\"\"\"\n    global _cache_hits, _cache_misses, _expansion_cache\n    _cache_hits = 0\n    _cache_misses = 0\n    _expansion_cache.clear()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}