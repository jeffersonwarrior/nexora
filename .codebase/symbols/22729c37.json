{
  "file_path": "/work/context-engine/scripts/rerank_recursive/hybrid_weights.py",
  "file_hash": "c039f4f49aeace223f01d479274e5315a22a3e8a",
  "updated_at": "2025-12-26T17:34:21.535158",
  "symbols": {
    "class_LearnedHybridWeights_9": {
      "name": "LearnedHybridWeights",
      "type": "class",
      "start_line": 9,
      "end_line": 105,
      "content_hash": "b30c351c25a654144e5fa53e148c696000889008",
      "content": "class LearnedHybridWeights:\n    \"\"\"Learns optimal dense vs. lexical balance per-collection.\"\"\"\n\n    WEIGHTS_DIR = os.environ.get(\"RERANKER_WEIGHTS_DIR\", \"/tmp/rerank_weights\")\n\n    def __init__(self, lr: float = 0.01):\n        self.lr = lr\n        self._collection = \"default\"\n        self._weights_path = self._get_weights_path(\"default\")\n        self.alpha = 0.0\n        self._momentum_alpha = 0.0\n        self._momentum = 0.9\n        self._update_count = 0\n        self._version = 0\n\n        if os.path.exists(self._weights_path):\n            try:\n                self._load_weights()\n            except Exception:\n                pass\n\n    @staticmethod\n    def _sanitize_collection(collection: str) -> str:\n        return \"\".join(c if c.isalnum() or c in \"-_\" else \"_\" for c in collection)\n\n    def _get_weights_path(self, collection: str) -> str:\n        safe_name = self._sanitize_collection(collection)\n        return os.path.join(self.WEIGHTS_DIR, f\"hybrid_{safe_name}.npz\")\n\n    def set_collection(self, collection: str):\n        self._collection = collection\n        self._weights_path = self._get_weights_path(collection)\n        if os.path.exists(self._weights_path):\n            try:\n                self._load_weights()\n            except Exception:\n                pass\n\n    def _load_weights(self):\n        import fcntl\n        lock_path = self._weights_path + \".lock\"\n        os.makedirs(os.path.dirname(lock_path) or \".\", exist_ok=True)\n        with open(lock_path, \"w\") as lock_file:\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_SH)\n            data = np.load(self._weights_path)\n            self.alpha = float(data[\"alpha\"])\n            self._version = int(data.get(\"version\", 0))\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)\n\n    def _save_weights(self):\n        import fcntl\n        os.makedirs(os.path.dirname(self._weights_path) or \".\", exist_ok=True)\n        lock_path = self._weights_path + \".lock\"\n        base_path = self._weights_path.rsplit(\".npz\", 1)[0]\n        tmp_base = base_path + \".tmp\"\n        tmp_path = tmp_base + \".npz\"\n        with open(lock_path, \"w\") as lock_file:\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)\n            np.savez(tmp_base, alpha=self.alpha, version=self._version)\n            os.replace(tmp_path, self._weights_path)\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)\n\n    @property\n    def dense_weight(self) -> float:\n        return 1.0 / (1.0 + np.exp(-self.alpha))\n\n    @property\n    def lexical_weight(self) -> float:\n        return 1.0 - self.dense_weight\n\n    def blend(self, dense_scores: np.ndarray, lexical_scores: np.ndarray) -> np.ndarray:\n        w = self.dense_weight\n        return w * dense_scores + (1 - w) * lexical_scores\n\n    def learn_from_teacher(\n        self,\n        dense_scores: np.ndarray,\n        lexical_scores: np.ndarray,\n        teacher_scores: np.ndarray,\n    ):\n        w = self.dense_weight\n        blended = self.blend(dense_scores, lexical_scores)\n        teacher_norm = (teacher_scores - teacher_scores.mean()) / (teacher_scores.std() + 1e-8)\n        blended_norm = (blended - blended.mean()) / (blended.std() + 1e-8)\n        dense_norm = (dense_scores - dense_scores.mean()) / (dense_scores.std() + 1e-8)\n        lexical_norm = (lexical_scores - lexical_scores.mean()) / (lexical_scores.std() + 1e-8)\n        error = teacher_norm - blended_norm\n        modality_diff = dense_norm - lexical_norm\n        sigmoid_grad = w * (1 - w)\n        grad = (error * modality_diff).mean() * sigmoid_grad\n        self._momentum_alpha = self._momentum * self._momentum_alpha + grad\n        self.alpha += self.lr * self._momentum_alpha\n        self.alpha = np.clip(self.alpha, -5.0, 5.0)\n        self._update_count += 1\n        if self._update_count % 50 == 0:\n            self._version += 1\n            self._save_weights()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___14": {
      "name": "__init__",
      "type": "method",
      "start_line": 14,
      "end_line": 28,
      "content_hash": "b379b46ad3fdc08e3d17d2ffe176b0e5aad796e6",
      "content": "    def __init__(self, lr: float = 0.01):\n        self.lr = lr\n        self._collection = \"default\"\n        self._weights_path = self._get_weights_path(\"default\")\n        self.alpha = 0.0\n        self._momentum_alpha = 0.0\n        self._momentum = 0.9\n        self._update_count = 0\n        self._version = 0\n\n        if os.path.exists(self._weights_path):\n            try:\n                self._load_weights()\n            except Exception:\n                pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__sanitize_collection_31": {
      "name": "_sanitize_collection",
      "type": "method",
      "start_line": 31,
      "end_line": 32,
      "content_hash": "cdbd85077085baad363b24f06dafd0ea2c67f8d3",
      "content": "    def _sanitize_collection(collection: str) -> str:\n        return \"\".join(c if c.isalnum() or c in \"-_\" else \"_\" for c in collection)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__get_weights_path_34": {
      "name": "_get_weights_path",
      "type": "method",
      "start_line": 34,
      "end_line": 36,
      "content_hash": "fcf1b6cd4ad60badbc2b2ebe33a1277fca0342e6",
      "content": "    def _get_weights_path(self, collection: str) -> str:\n        safe_name = self._sanitize_collection(collection)\n        return os.path.join(self.WEIGHTS_DIR, f\"hybrid_{safe_name}.npz\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_set_collection_38": {
      "name": "set_collection",
      "type": "method",
      "start_line": 38,
      "end_line": 45,
      "content_hash": "5d86e95df72e0d5a0b102dca9bbef687d38ca49a",
      "content": "    def set_collection(self, collection: str):\n        self._collection = collection\n        self._weights_path = self._get_weights_path(collection)\n        if os.path.exists(self._weights_path):\n            try:\n                self._load_weights()\n            except Exception:\n                pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__load_weights_47": {
      "name": "_load_weights",
      "type": "method",
      "start_line": 47,
      "end_line": 56,
      "content_hash": "2ac89254c5e63d781e54511f9297b2d3bcdb4443",
      "content": "    def _load_weights(self):\n        import fcntl\n        lock_path = self._weights_path + \".lock\"\n        os.makedirs(os.path.dirname(lock_path) or \".\", exist_ok=True)\n        with open(lock_path, \"w\") as lock_file:\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_SH)\n            data = np.load(self._weights_path)\n            self.alpha = float(data[\"alpha\"])\n            self._version = int(data.get(\"version\", 0))\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__save_weights_58": {
      "name": "_save_weights",
      "type": "method",
      "start_line": 58,
      "end_line": 69,
      "content_hash": "178cfa1bc94760ceb6acb5edc2495c28ee20f58c",
      "content": "    def _save_weights(self):\n        import fcntl\n        os.makedirs(os.path.dirname(self._weights_path) or \".\", exist_ok=True)\n        lock_path = self._weights_path + \".lock\"\n        base_path = self._weights_path.rsplit(\".npz\", 1)[0]\n        tmp_base = base_path + \".tmp\"\n        tmp_path = tmp_base + \".npz\"\n        with open(lock_path, \"w\") as lock_file:\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)\n            np.savez(tmp_base, alpha=self.alpha, version=self._version)\n            os.replace(tmp_path, self._weights_path)\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_dense_weight_72": {
      "name": "dense_weight",
      "type": "method",
      "start_line": 72,
      "end_line": 73,
      "content_hash": "fc16515c7a36a17ea8a62c371e9bfadbcdd5fecb",
      "content": "    def dense_weight(self) -> float:\n        return 1.0 / (1.0 + np.exp(-self.alpha))",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_lexical_weight_76": {
      "name": "lexical_weight",
      "type": "method",
      "start_line": 76,
      "end_line": 77,
      "content_hash": "adbd475e7806492bade55dfdfe21f53c26136425",
      "content": "    def lexical_weight(self) -> float:\n        return 1.0 - self.dense_weight",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_blend_79": {
      "name": "blend",
      "type": "method",
      "start_line": 79,
      "end_line": 81,
      "content_hash": "2f8d44c6aa1963ceb6312e9c1826d0491b4d997a",
      "content": "    def blend(self, dense_scores: np.ndarray, lexical_scores: np.ndarray) -> np.ndarray:\n        w = self.dense_weight\n        return w * dense_scores + (1 - w) * lexical_scores",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_learn_from_teacher_83": {
      "name": "learn_from_teacher",
      "type": "method",
      "start_line": 83,
      "end_line": 105,
      "content_hash": "7f56d0f929ccdb878a4b1783b74a2729f5dfefc2",
      "content": "    def learn_from_teacher(\n        self,\n        dense_scores: np.ndarray,\n        lexical_scores: np.ndarray,\n        teacher_scores: np.ndarray,\n    ):\n        w = self.dense_weight\n        blended = self.blend(dense_scores, lexical_scores)\n        teacher_norm = (teacher_scores - teacher_scores.mean()) / (teacher_scores.std() + 1e-8)\n        blended_norm = (blended - blended.mean()) / (blended.std() + 1e-8)\n        dense_norm = (dense_scores - dense_scores.mean()) / (dense_scores.std() + 1e-8)\n        lexical_norm = (lexical_scores - lexical_scores.mean()) / (lexical_scores.std() + 1e-8)\n        error = teacher_norm - blended_norm\n        modality_diff = dense_norm - lexical_norm\n        sigmoid_grad = w * (1 - w)\n        grad = (error * modality_diff).mean() * sigmoid_grad\n        self._momentum_alpha = self._momentum * self._momentum_alpha + grad\n        self.alpha += self.lr * self._momentum_alpha\n        self.alpha = np.clip(self.alpha, -5.0, 5.0)\n        self._update_count += 1\n        if self._update_count % 50 == 0:\n            self._version += 1\n            self._save_weights()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}