{
  "file_path": "/work/external-deps/Context-Engine/scripts/workspace_state.py",
  "file_hash": "e36c7a6754946e3f71d65ffb4530d0202b8fb8de",
  "updated_at": "2025-12-26T17:34:23.698539",
  "symbols": {
    "function_is_staging_enabled_35": {
      "name": "is_staging_enabled",
      "type": "function",
      "start_line": 35,
      "end_line": 40,
      "content_hash": "7226d8a8666f87134b7653d5d46b45f98baf1d4b",
      "content": "def is_staging_enabled() -> bool:\n    raw = os.environ.get(\"CTXCE_STAGING_ENABLED\", \"\")\n    v = (raw or \"\").strip().lower()\n    if not v:\n        return False\n    return v in {\"1\", \"true\", \"yes\", \"on\"}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cache_memo_recheck_seconds_43": {
      "name": "_cache_memo_recheck_seconds",
      "type": "function",
      "start_line": 43,
      "end_line": 47,
      "content_hash": "384e4fbcbf5616d56c01e9a377f03a732f3a4152",
      "content": "def _cache_memo_recheck_seconds() -> float:\n    try:\n        return float(os.environ.get(\"CACHE_MEMO_RECHECK_SECONDS\", \"60\") or 60)\n    except Exception:\n        return 60.0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__normalize_cache_key_path_50": {
      "name": "_normalize_cache_key_path",
      "type": "function",
      "start_line": 50,
      "end_line": 62,
      "content_hash": "d347081450f336d8b9780591ebb4d32ce0ffb831",
      "content": "def _normalize_cache_key_path(file_path: str) -> str:\n    \"\"\"Normalize a file path for cache keys.\n\n    Prefer os.path.abspath (no filesystem calls) over Path.resolve(), since resolve\n    can trigger expensive metadata operations on network filesystems.\n    \"\"\"\n    try:\n        return os.path.abspath(file_path)\n    except Exception:\n        try:\n            return str(Path(file_path))\n        except Exception:\n            return str(file_path)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__memoize_cache_obj_65": {
      "name": "_memoize_cache_obj",
      "type": "function",
      "start_line": 65,
      "end_line": 80,
      "content_hash": "f379d135a3cb6e45b75aa65f3a9d4afe6d7b367f",
      "content": "def _memoize_cache_obj(cache_path: Path, obj: Dict[str, Any]) -> None:\n    key = str(cache_path)\n    now = time.time()\n    sig = (-1, -1)\n    try:\n        st = cache_path.stat()\n        mtime_ns = int(\n            getattr(st, \"st_mtime_ns\", int(getattr(st, \"st_mtime\", 0) * 1_000_000_000))\n        )\n        sig = (mtime_ns, int(getattr(st, \"st_size\", 0)))\n    except OSError:\n        sig = (-1, -1)\n    with _cache_memo_lock:\n        _cache_memo[key] = obj\n        _cache_memo_last_check[key] = now\n        _cache_memo_sig[key] = sig",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cache_file_sig_83": {
      "name": "_cache_file_sig",
      "type": "function",
      "start_line": 83,
      "end_line": 94,
      "content_hash": "cac585e044d561c1644fa563588ec585d8f9c008",
      "content": "def _cache_file_sig(cache_path: Path) -> Optional[tuple[int, int]]:\n    try:\n        st = cache_path.stat()\n    except OSError:\n        return None\n    try:\n        mtime_ns = int(\n            getattr(st, \"st_mtime_ns\", int(getattr(st, \"st_mtime\", 0) * 1_000_000_000))\n        )\n    except Exception:\n        mtime_ns = int(getattr(st, \"st_mtime\", 0) * 1_000_000_000)\n    return (mtime_ns, int(getattr(st, \"st_size\", 0)))",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__server_managed_slug_from_path_97": {
      "name": "_server_managed_slug_from_path",
      "type": "function",
      "start_line": 97,
      "end_line": 127,
      "content_hash": "e6fa2f8cfcf4c2e5fdd607aa04381537ea572e89",
      "content": "def _server_managed_slug_from_path(path: Path) -> Optional[str]:\n    base = path if path.is_dir() else path.parent\n    try:\n        parts = base.resolve().parts\n    except OSError:\n        parts = base.parts\n\n    slug = next((seg for seg in reversed(parts) if _SLUGGED_REPO_RE.match(seg or \"\")), None)\n    if not slug:\n        return None\n\n    with _managed_slug_cache_lock:\n        if slug in _managed_slug_cache:\n            return slug\n        if slug in _managed_slug_cache_neg:\n            return None\n\n    work_dir = Path(os.environ.get(\"WORK_DIR\") or os.environ.get(\"WORKDIR\") or \"/work\")\n    marker = work_dir / \".codebase\" / \"repos\" / slug / \".ctxce_managed_upload\"\n    try:\n        is_managed = marker.exists()\n    except OSError:\n        is_managed = False\n\n    with _managed_slug_cache_lock:\n        if is_managed:\n            _managed_slug_cache.add(slug)\n        else:\n            _managed_slug_cache_neg.add(slug)\n\n    return slug if is_managed else None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_IndexingProgress_139": {
      "name": "IndexingProgress",
      "type": "class",
      "start_line": 139,
      "end_line": 142,
      "content_hash": "65e43da8739e87d59e23e5baa9e8f8842551f303",
      "content": "class IndexingProgress(TypedDict, total=False):\n    files_processed: int\n    total_files: Optional[int]\n    current_file: Optional[str]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_IndexingStatus_144": {
      "name": "IndexingStatus",
      "type": "class",
      "start_line": 144,
      "end_line": 148,
      "content_hash": "13bb547643647b51daaf5e0b49bf183e5332a2fd",
      "content": "class IndexingStatus(TypedDict, total=False):\n    state: IndexingState\n    started_at: Optional[str]\n    progress: Optional[IndexingProgress]\n    error: Optional[str]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_ActivityDetails_150": {
      "name": "ActivityDetails",
      "type": "class",
      "start_line": 150,
      "end_line": 158,
      "content_hash": "642a89b16c6bca1edd67641e73fdbde527b734e5",
      "content": "class ActivityDetails(TypedDict, total=False):\n    block_count: Optional[int]\n    reason: Optional[str]\n    files_processed: Optional[int]\n    total_blocks: Optional[int]\n    git_commit: Optional[str]\n    git_branch: Optional[str]\n    chunk_count: Optional[int]\n    file_size: Optional[int]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_LastActivity_160": {
      "name": "LastActivity",
      "type": "class",
      "start_line": 160,
      "end_line": 164,
      "content_hash": "5f2111389bd717e447733711bbd07b5832929949",
      "content": "class LastActivity(TypedDict, total=False):\n    timestamp: str\n    action: ActivityAction\n    file_path: Optional[str]\n    details: Optional[ActivityDetails]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_OriginInfo_166": {
      "name": "OriginInfo",
      "type": "class",
      "start_line": 166,
      "end_line": 171,
      "content_hash": "d587b548498132e3c437eb3e03e532ae86d57635",
      "content": "class OriginInfo(TypedDict, total=False):\n    repo_name: Optional[str]\n    container_path: Optional[str]\n    source_path: Optional[str]\n    collection_name: Optional[str]\n    updated_at: Optional[str]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_StagingInfo_174": {
      "name": "StagingInfo",
      "type": "class",
      "start_line": 174,
      "end_line": 184,
      "content_hash": "3a499c70253f3861955dddcf3eb855d49bfcf549",
      "content": "class StagingInfo(TypedDict, total=False):\n    collection: Optional[str]\n    status: Optional[IndexingStatus]\n    started_at: Optional[str]\n    updated_at: Optional[str]\n    env_hash: Optional[str]\n    environment: Optional[Dict[str, str]]\n    indexing_config: Optional[Dict[str, Any]]\n    indexing_config_hash: Optional[str]\n    workspace_path: Optional[str]\n    repo_name: Optional[str]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_WorkspaceState_187": {
      "name": "WorkspaceState",
      "type": "class",
      "start_line": 187,
      "end_line": 206,
      "content_hash": "d771d8e9b222fc1d9ba89bc98cdeac204c69e411",
      "content": "class WorkspaceState(TypedDict, total=False):\n    created_at: str\n    updated_at: str\n    qdrant_collection: str\n    indexing_status: Optional[IndexingStatus]\n    last_activity: Optional[LastActivity]\n    qdrant_stats: Optional[Dict[str, Any]]\n    origin: Optional[OriginInfo]\n    logical_repo_id: Optional[str]\n    indexing_config: Optional[Dict[str, Any]]\n    indexing_config_hash: Optional[str]\n    indexing_env: Optional[Dict[str, str]]\n    indexing_config_pending: Optional[Dict[str, Any]]\n    indexing_config_pending_hash: Optional[str]\n    indexing_env_pending: Optional[Dict[str, str]]\n    previous_collection: Optional[str]\n    serving_collection: Optional[str]\n    active_repo_slug: Optional[str]\n    serving_repo_slug: Optional[str]\n    staging: Optional[StagingInfo]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_is_multi_repo_mode_208": {
      "name": "is_multi_repo_mode",
      "type": "function",
      "start_line": 208,
      "end_line": 212,
      "content_hash": "e22841d70b0fdf12505a88abba89e6a626bb0b23",
      "content": "def is_multi_repo_mode() -> bool:\n    \"\"\"Check if multi-repo mode is enabled.\"\"\"\n    return os.environ.get(\"MULTI_REPO_MODE\", \"0\").strip().lower() in {\n        \"1\", \"true\", \"yes\", \"on\"\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_logical_repo_reuse_enabled_215": {
      "name": "logical_repo_reuse_enabled",
      "type": "function",
      "start_line": 215,
      "end_line": 227,
      "content_hash": "853a6baff04869930f90cc5debdc2680f393f2e6",
      "content": "def logical_repo_reuse_enabled() -> bool:\n    \"\"\"Feature flag for logical-repo / collection reuse.\n\n    Controlled by LOGICAL_REPO_REUSE env var: 1/true/yes/on => enabled.\n    When disabled, behavior falls back to legacy per-repo collection logic\n    and does not write logical_repo_id into workspace state.\n    \"\"\"\n    return os.environ.get(\"LOGICAL_REPO_REUSE\", \"\").strip().lower() in {\n        \"1\",\n        \"true\",\n        \"yes\",\n        \"on\",\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__resolve_workspace_root_234": {
      "name": "_resolve_workspace_root",
      "type": "function",
      "start_line": 234,
      "end_line": 236,
      "content_hash": "c65113dac52e288f0bb45dee9af518d231843c8c",
      "content": "def _resolve_workspace_root() -> str:\n    \"\"\"Determine the default workspace root path.\"\"\"\n    return os.environ.get(\"WORKSPACE_PATH\") or os.environ.get(\"WATCH_ROOT\") or \"/work\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__resolve_repo_context_238": {
      "name": "_resolve_repo_context",
      "type": "function",
      "start_line": 238,
      "end_line": 256,
      "content_hash": "3415b06a7dd490a0950f4e2bc57d0d00ef149248",
      "content": "def _resolve_repo_context(\n    workspace_path: Optional[str] = None,\n    repo_name: Optional[str] = None,\n) -> tuple[str, Optional[str]]:\n    \"\"\"Normalize workspace/repo context, ensuring multi-repo callers map to repo state.\"\"\"\n    resolved_workspace = workspace_path or _resolve_workspace_root()\n\n    if is_multi_repo_mode():\n        if repo_name:\n            return resolved_workspace, repo_name\n\n        if workspace_path:\n            detected = _detect_repo_name_from_path(Path(workspace_path))\n            if detected:\n                return resolved_workspace, detected\n\n        return resolved_workspace, None\n\n    return resolved_workspace, repo_name",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_state_lock_258": {
      "name": "_get_state_lock",
      "type": "function",
      "start_line": 258,
      "end_line": 269,
      "content_hash": "4fb7812d84830169eb003adcc0cd97e855f83ad7",
      "content": "def _get_state_lock(workspace_path: Optional[str] = None, repo_name: Optional[str] = None) -> threading.RLock:\n    \"\"\"Get or create a lock for the workspace or repo state and track usage.\"\"\"\n    if repo_name and is_multi_repo_mode():\n        key = f\"repo::{repo_name}\"\n    else:\n        key = str(Path(workspace_path or _resolve_workspace_root()).resolve())\n\n    with _state_lock:\n        if key not in _state_locks:\n            _state_locks[key] = threading.RLock()\n        _state_lock_last_used[key] = time.time()\n        return _state_locks[key]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_repo_state_dir_271": {
      "name": "_get_repo_state_dir",
      "type": "function",
      "start_line": 271,
      "end_line": 276,
      "content_hash": "2fb40ea8c7f251f7107de93279fa647be7358486",
      "content": "def _get_repo_state_dir(repo_name: str) -> Path:\n    \"\"\"Get the state directory for a repository.\"\"\"\n    base_dir = Path(os.environ.get(\"WORKSPACE_PATH\") or os.environ.get(\"WATCH_ROOT\") or \"/work\")\n    if is_multi_repo_mode():\n        return base_dir / STATE_DIRNAME / \"repos\" / repo_name\n    return base_dir / STATE_DIRNAME",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_state_path_278": {
      "name": "_get_state_path",
      "type": "function",
      "start_line": 278,
      "end_line": 282,
      "content_hash": "f6d52659db81ae6783e5105dfa0bba5b6401bcb5",
      "content": "def _get_state_path(workspace_path: str) -> Path:\n    \"\"\"Get the path to the state.json file for a workspace.\"\"\"\n    workspace = Path(workspace_path).resolve()\n    state_dir = workspace / STATE_DIRNAME\n    return state_dir / STATE_FILENAME",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_global_state_dir_285": {
      "name": "_get_global_state_dir",
      "type": "function",
      "start_line": 285,
      "end_line": 289,
      "content_hash": "984478effd39b63b756452c402a20602660f2253",
      "content": "def _get_global_state_dir(workspace_path: Optional[str] = None) -> Path:\n    \"\"\"Return the root .codebase directory used for workspace metadata.\"\"\"\n\n    base_dir = Path(workspace_path or _resolve_workspace_root()).resolve()\n    return base_dir / STATE_DIRNAME",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ensure_state_dir_291": {
      "name": "_ensure_state_dir",
      "type": "function",
      "start_line": 291,
      "end_line": 296,
      "content_hash": "f1d66bb0fdf618ffac4fd502645245b27b782bb9",
      "content": "def _ensure_state_dir(workspace_path: str) -> Path:\n    \"\"\"Ensure the .codebase directory exists and return the state file path.\"\"\"\n    workspace = Path(workspace_path).resolve()\n    state_dir = workspace / STATE_DIRNAME\n    state_dir.mkdir(exist_ok=True)\n    return state_dir / STATE_FILENAME",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__sanitize_name_298": {
      "name": "_sanitize_name",
      "type": "function",
      "start_line": 298,
      "end_line": 304,
      "content_hash": "211161d4e0af4c423fbbd01ed1d35554de9eb5be",
      "content": "def _sanitize_name(s: str, max_len: int = 64) -> str:\n    s = s.lower().strip()\n    s = re.sub(r\"[^a-z0-9_.-]+\", \"-\", s)\n    s = re.sub(r\"-+\", \"-\", s).strip(\"-\")\n    if not s:\n        s = \"workspace\"\n    return s[:max_len]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__detect_git_common_dir_307": {
      "name": "_detect_git_common_dir",
      "type": "function",
      "start_line": 307,
      "end_line": 323,
      "content_hash": "29d4b1ca45d5f2289923c0df5f6a73724d133a8a",
      "content": "def _detect_git_common_dir(start: Path) -> Optional[Path]:\n    try:\n        base = start if start.is_dir() else start.parent\n        r = subprocess.run(\n            [\"git\", \"-C\", str(base), \"rev-parse\", \"--git-common-dir\"],\n            capture_output=True,\n            text=True,\n        )\n        raw = (r.stdout or \"\").strip()\n        if r.returncode != 0 or not raw:\n            return None\n        p = Path(raw)\n        if not p.is_absolute():\n            p = base / p\n        return p.resolve()\n    except Exception:\n        return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_compute_logical_repo_id_326": {
      "name": "compute_logical_repo_id",
      "type": "function",
      "start_line": 326,
      "end_line": 341,
      "content_hash": "925b6b07e488e6f521984763ec1c9f3696f5e41c",
      "content": "def compute_logical_repo_id(workspace_path: str) -> str:\n    try:\n        p = Path(workspace_path).resolve()\n    except Exception:\n        p = Path(workspace_path)\n\n    common = _detect_git_common_dir(p)\n    if common is not None:\n        key = str(common)\n        prefix = \"git:\"\n    else:\n        key = str(p)\n        prefix = \"fs:\"\n\n    h = hashlib.sha1(key.encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:16]\n    return f\"{prefix}{h}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_ensure_logical_repo_id_344": {
      "name": "ensure_logical_repo_id",
      "type": "function",
      "start_line": 344,
      "end_line": 357,
      "content_hash": "d5effe6c873c8b7ffed240880bcc10186dd60f7e",
      "content": "def ensure_logical_repo_id(state: WorkspaceState, workspace_path: str) -> WorkspaceState:\n    if not isinstance(state, dict):\n        return state\n    if not logical_repo_reuse_enabled():\n        # Gate: when logical repo reuse is disabled, leave state untouched\n        return state\n    if state.get(\"logical_repo_id\"):\n        return state\n    lrid = compute_logical_repo_id(workspace_path)\n    state[\"logical_repo_id\"] = lrid\n    origin = dict(state.get(\"origin\", {}) or {})\n    origin.setdefault(\"logical_repo_id\", lrid)\n    state[\"origin\"] = origin\n    return state",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cross_process_lock_369": {
      "name": "_cross_process_lock",
      "type": "function",
      "start_line": 369,
      "end_line": 413,
      "content_hash": "6be3ce291821f41e2b3ffd6317ee868f9f862bb2",
      "content": "def _cross_process_lock(lock_path: Path):\n    \"\"\"Advisory cross-process exclusive lock using a companion .lock file.\n    Safe across container/process boundaries; pairs with atomic rename writes.\n    Ensures group-writable permissions so non-root indexers/watchers can operate.\n    \"\"\"\n\n    lock_path.parent.mkdir(parents=True, exist_ok=True)\n\n    lock_file = None\n    fd = None\n    try:\n        fd = os.open(lock_path, os.O_CREAT | os.O_RDWR, 0o664)\n        lock_file = os.fdopen(fd, \"a+\")\n    except PermissionError:\n        # If we cannot create or open the requested lock, fall back to /tmp (permissive)\n        tmp_path = Path(\"/tmp\") / (lock_path.name)\n        tmp_path.parent.mkdir(parents=True, exist_ok=True)\n        fd = os.open(tmp_path, os.O_CREAT | os.O_RDWR, 0o664)\n        lock_file = os.fdopen(fd, \"a+\")\n        lock_path = tmp_path\n\n    try:\n        try:\n            os.chmod(lock_path, 0o664)\n        except PermissionError:\n            pass\n\n        if fcntl is not None:\n            try:\n                fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)\n            except Exception:\n                pass\n        yield\n    finally:\n        try:\n            if fcntl is not None:\n                try:\n                    fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)\n                except Exception:\n                    pass\n        finally:\n            try:\n                lock_file.close()\n            except Exception:\n                pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_file_lock_path_433": {
      "name": "_get_file_lock_path",
      "type": "function",
      "start_line": 433,
      "end_line": 438,
      "content_hash": "76a0e173a1ae4116952eef52fb6ce98fcfc73b3b",
      "content": "def _get_file_lock_path(file_path: str) -> Path:\n    \"\"\"Get the lock file path for a given file.\"\"\"\n    # Use hash of file path to avoid filesystem path issues\n    import hashlib\n    path_hash = hashlib.md5(file_path.encode()).hexdigest()[:16]\n    return _FILE_LOCKS_DIR / f\"{path_hash}.lock\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_is_file_locked_441": {
      "name": "is_file_locked",
      "type": "function",
      "start_line": 441,
      "end_line": 463,
      "content_hash": "39e58f38b6c23bb11031b6edc0115862b74847f5",
      "content": "def is_file_locked(file_path: str) -> bool:\n    \"\"\"Check if a specific file is currently being indexed.\n\n    Uses file-based locking with timestamp for cross-container coordination.\n    Returns True if file is locked (lock file exists and is recent).\n    \"\"\"\n    try:\n        lock_path = _get_file_lock_path(file_path)\n        if not lock_path.exists():\n            return False\n        # Check if lock is stale\n        mtime = lock_path.stat().st_mtime\n        age = time.time() - mtime\n        if age > _FILE_LOCK_TIMEOUT_SECONDS:\n            # Stale lock - remove it\n            try:\n                lock_path.unlink()\n            except Exception:\n                pass\n            return False\n        return True\n    except Exception:\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_file_indexing_lock_467": {
      "name": "file_indexing_lock",
      "type": "function",
      "start_line": 467,
      "end_line": 532,
      "content_hash": "287e11ae583616f2e1bf8877f0501bb6f17a4b4a",
      "content": "def file_indexing_lock(file_path: str):\n    \"\"\"Acquire a lock for indexing a specific file.\n\n    Use this before processing a file to prevent indexer/watcher collision.\n    Non-blocking - raises FileExistsError if file is already locked.\n\n    Uses O_CREAT | O_EXCL for atomic lock acquisition to prevent race conditions\n    where two processes could both pass is_file_locked() and overwrite each other.\n    \"\"\"\n    lock_path = _get_file_lock_path(file_path)\n    fd = None\n\n    try:\n        lock_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # First check if there's a stale lock we should clean up\n        if lock_path.exists():\n            try:\n                mtime = lock_path.stat().st_mtime\n                age = time.time() - mtime\n                if age > _FILE_LOCK_TIMEOUT_SECONDS:\n                    # Stale lock - try to remove it\n                    lock_path.unlink()\n                else:\n                    # Fresh lock held by another process\n                    raise FileExistsError(f\"File is already being indexed: {file_path}\")\n            except FileNotFoundError:\n                # Lock was removed between exists() and stat() - continue to acquire\n                pass\n\n        # Atomic lock acquisition: O_CREAT | O_EXCL fails if file exists\n        # This prevents race condition where two processes both pass the check above\n        try:\n            fd = os.open(str(lock_path), os.O_CREAT | os.O_EXCL | os.O_WRONLY, 0o644)\n        except FileExistsError:\n            # Another process created the lock between our check and open\n            raise FileExistsError(f\"File is already being indexed: {file_path}\")\n\n        # Write lock metadata\n        lock_data = json.dumps({\n            \"file\": file_path,\n            \"locked_at\": time.time(),\n            \"pid\": os.getpid(),\n        })\n        os.write(fd, lock_data.encode())\n        os.close(fd)\n        fd = None\n\n    except FileExistsError:\n        raise\n    except Exception as e:\n        if fd is not None:\n            try:\n                os.close(fd)\n            except Exception:\n                pass\n        raise RuntimeError(f\"Could not acquire file lock: {e}\")\n\n    try:\n        yield\n    finally:\n        # Release lock\n        try:\n            lock_path.unlink()\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_is_indexing_locked_539": {
      "name": "is_indexing_locked",
      "type": "function",
      "start_line": 539,
      "end_line": 544,
      "content_hash": "dce609dccd9bdf519b8eae8f9d7cef75caf4bea3",
      "content": "def is_indexing_locked() -> bool:\n    \"\"\"DEPRECATED: Use is_file_locked() for per-file coordination.\n\n    Check if global indexing lock is held. Returns False (no global lock).\n    \"\"\"\n    return False  # Per-file locking means no global lock needed",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_indexing_lock_548": {
      "name": "indexing_lock",
      "type": "function",
      "start_line": 548,
      "end_line": 553,
      "content_hash": "3ebc91d8c9a5e5fc3f3ac75d2b9f44acff9867a9",
      "content": "def indexing_lock():\n    \"\"\"DEPRECATED: Use file_indexing_lock() for per-file coordination.\n\n    No-op for backward compatibility.\n    \"\"\"\n    yield",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__git_remote_repo_name_555": {
      "name": "_git_remote_repo_name",
      "type": "function",
      "start_line": 555,
      "end_line": 586,
      "content_hash": "112f72a364e7cf752ecf2b2006863238ea3f65c4",
      "content": "def _git_remote_repo_name(repo_path: Path) -> Optional[str]:\n    \"\"\"Return canonical repo name from git remote origin URL or toplevel.\"\"\"\n    try:\n        r = subprocess.run(\n            [\"git\", \"-C\", str(repo_path), \"config\", \"--get\", \"remote.origin.url\"],\n            capture_output=True,\n            text=True,\n            timeout=5,\n        )\n        if r.returncode == 0 and r.stdout.strip():\n            url = r.stdout.strip()\n            name = url.rstrip(\"/\").rsplit(\"/\", 1)[-1]\n            if name.endswith(\".git\"):\n                name = name[:-4]\n            if name:\n                return name\n    except Exception:\n        pass\n\n    try:\n        r = subprocess.run(\n            [\"git\", \"-C\", str(repo_path), \"rev-parse\", \"--show-toplevel\"],\n            capture_output=True,\n            text=True,\n            timeout=5,\n        )\n        top = (r.stdout or \"\").strip()\n        if r.returncode == 0 and top:\n            return Path(top).name\n    except Exception:\n        pass\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__detect_repo_name_from_path_589": {
      "name": "_detect_repo_name_from_path",
      "type": "function",
      "start_line": 589,
      "end_line": 652,
      "content_hash": "6b5098346f99f858e8aa2a0423d9671ec7ba3e5b",
      "content": "def _detect_repo_name_from_path(path: Path) -> str:\n    \"\"\"Detect repository name from path using git remote origin URL.\n\n    This ensures consistency with how the MCP server detects repos during search.\n    Priority:\n    1. Fast-path for server-managed uploads and workspace-relative paths\n    2. Git remote origin URL (canonical repo name like 'Context-Engine')\n    3. Git toplevel directory name (folder name like 'Context-Engine-hash')\n    4. Walk up to find .git and return that folder name\n    5. Return parent folder name as fallback\n    \"\"\"\n    slug = _server_managed_slug_from_path(path)\n    if slug:\n        return slug\n\n    # Fast-path for managed upload workspaces or when workspace_path == /work:\n    # derive the repo name from the first path segment relative to the workspace\n    # root instead of spawning git processes or falling back to \"work\".\n    try:\n        ws_root = Path(_resolve_workspace_root()).resolve()\n    except Exception:\n        ws_root = Path(_resolve_workspace_root())\n\n    try:\n        resolved = path.resolve()\n    except Exception:\n        resolved = path if path.is_dir() else path.parent\n\n    try:\n        rel = resolved.relative_to(ws_root)\n        if rel.parts:\n            candidate = rel.parts[0]\n            if candidate not in {\".codebase\", \".git\", \"__pycache__\"}:\n                return candidate\n    except Exception:\n        pass\n\n    try:\n        base = path if path.is_dir() else path.parent\n        git_name = _git_remote_repo_name(base)\n        if git_name:\n            return git_name\n    except Exception:\n        pass\n    try:\n        # Walk up to find .git\n        cur = path if path.is_dir() else path.parent\n        for p in [cur] + list(cur.parents):\n            try:\n                if (p / \".git\").exists():\n                    return p.name\n            except Exception:\n                continue\n    except Exception:\n        pass\n\n    try:\n        structure_name = _detect_repo_name_from_path_by_structure(path)\n        if structure_name:\n            return structure_name\n    except Exception:\n        pass\n\n    return (path if path.is_dir() else path.parent).name or \"workspace\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__generate_collection_name_655": {
      "name": "_generate_collection_name",
      "type": "function",
      "start_line": 655,
      "end_line": 660,
      "content_hash": "5cfd305ee22728be03aa81dae06450f49d3ce258",
      "content": "def _generate_collection_name(workspace_path: str) -> str:\n    ws = Path(workspace_path).resolve()\n    repo = _sanitize_name(_detect_repo_name_from_path(ws))\n    # stable suffix from absolute path\n    h = hashlib.sha1(str(ws).encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:6]\n    return _sanitize_name(f\"{repo}-{h}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__atomic_write_state_662": {
      "name": "_atomic_write_state",
      "type": "function",
      "start_line": 662,
      "end_line": 682,
      "content_hash": "f0e35654bfeb3bb081e377934fa92df88a1cadf8",
      "content": "def _atomic_write_state(state_path: Path, state: WorkspaceState) -> None:\n    \"\"\"Atomically write state to prevent corruption during concurrent access.\"\"\"\n    # Write to temp file first, then rename (atomic on most filesystems)\n    temp_path = state_path.with_suffix(f\".tmp.{uuid.uuid4().hex[:8]}\")\n    try:\n        with open(temp_path, 'w', encoding='utf-8') as f:\n            json.dump(state, f, indent=2, ensure_ascii=False)\n        temp_path.replace(state_path)\n        # Ensure state/cache files are group-writable so multiple processes\n        # (upload service, watcher, indexer) can update them.\n        try:\n            os.chmod(state_path, 0o664)\n        except PermissionError:\n            pass\n    except Exception:\n        # Clean up temp file if something went wrong\n        try:\n            temp_path.unlink(missing_ok=True)\n        except Exception:\n            pass\n        raise",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_workspace_state_684": {
      "name": "get_workspace_state",
      "type": "function",
      "start_line": 684,
      "end_line": 775,
      "content_hash": "66327aae45966036828af97ae295bd2315f14046",
      "content": "def get_workspace_state(\n    workspace_path: Optional[str] = None, repo_name: Optional[str] = None\n) -> WorkspaceState:\n    \"\"\"Get the current workspace state, creating it if it doesn't exist.\"\"\"\n\n    workspace_path, repo_name = _resolve_repo_context(workspace_path, repo_name)\n\n    if is_multi_repo_mode() and repo_name is None:\n        print(\n            f\"[workspace_state] Multi-repo: Skipping state read for workspace={workspace_path} without repo_name\"\n        )\n        return {}\n\n    lock = _get_state_lock(workspace_path, repo_name)\n    with lock:\n        state_path: Path\n        lock_scope_path: Path\n\n        if is_multi_repo_mode() and repo_name:\n            state_dir = _get_repo_state_dir(repo_name)\n            try:\n                ws_root = Path(_resolve_workspace_root())\n                ws_dir = ws_root / repo_name\n            except Exception:\n                ws_dir = None\n            try:\n                if not state_dir.exists() and (ws_dir is None or not ws_dir.exists()):\n                    return {}\n            except Exception:\n                return {}\n            state_dir.mkdir(parents=True, exist_ok=True)\n            # Ensure repo state dir is group-writable so root upload service and\n            # non-root watcher/indexer processes can both write state/cache files.\n            try:\n                os.chmod(state_dir, 0o775)\n            except Exception:\n                pass\n            state_path = state_dir / STATE_FILENAME\n            lock_scope_path = state_dir\n        else:\n            try:\n                state_path = _ensure_state_dir(workspace_path)\n                lock_scope_path = state_path.parent\n            except PermissionError:\n                lock_scope_path = _get_global_state_dir(workspace_path)\n                lock_scope_path.mkdir(parents=True, exist_ok=True)\n                state_path = lock_scope_path / STATE_FILENAME\n\n        lock_path = lock_scope_path / (STATE_FILENAME + \".lock\")\n        with _cross_process_lock(lock_path):\n            if state_path.exists():\n                try:\n                    with open(state_path, \"r\", encoding=\"utf-8-sig\") as f:\n                        state = json.load(f)\n                    if isinstance(state, dict):\n                        if logical_repo_reuse_enabled():\n                            workspace_real = str(Path(workspace_path or _resolve_workspace_root()).resolve())\n                            state = ensure_logical_repo_id(state, workspace_real)\n                            try:\n                                _atomic_write_state(state_path, state)\n                            except Exception as e:\n                                print(f\"[workspace_state] Failed to persist logical_repo_id to {state_path}: {e}\")\n                        modified = _ensure_repo_slug_defaults(state, repo_name)\n                        if modified:\n                            try:\n                                _atomic_write_state(state_path, state)\n                            except Exception:\n                                pass\n                        return state\n                except (json.JSONDecodeError, ValueError, OSError) as e:\n                    print(f\"[workspace_state] Failed to read state from {state_path}: {e}\")\n\n            now = datetime.now().isoformat()\n            collection_name = get_collection_name(repo_name)\n\n            state: WorkspaceState = {\n                \"workspace_path\": str(Path(workspace_path or _resolve_workspace_root()).resolve()),\n                \"created_at\": now,\n                \"updated_at\": now,\n                \"qdrant_collection\": collection_name,\n                \"indexing_status\": {\"state\": \"idle\"},\n            }\n            _ensure_repo_slug_defaults(state, repo_name)\n\n            if logical_repo_reuse_enabled():\n                try:\n                    state = ensure_logical_repo_id(state, state.get(\"workspace_path\", workspace_path or _resolve_workspace_root()))\n                except Exception as e:\n                    print(f\"[workspace_state] Failed to ensure logical_repo_id for {workspace_path}: {e}\")\n\n            _atomic_write_state(state_path, state)\n            return state",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_update_workspace_state_778": {
      "name": "update_workspace_state",
      "type": "function",
      "start_line": 778,
      "end_line": 830,
      "content_hash": "8bacc3f2b4e7affcbdf9fc62fb15850934e9ae43",
      "content": "def update_workspace_state(\n    workspace_path: Optional[str] = None,\n    updates: Optional[Dict[str, Any]] = None,\n    repo_name: Optional[str] = None,\n) -> WorkspaceState:\n    \"\"\"Update workspace state with the given changes.\"\"\"\n\n    workspace_path, repo_name = _resolve_repo_context(workspace_path, repo_name)\n    updates = updates or {}\n\n    if is_multi_repo_mode() and repo_name is None:\n        print(\n            f\"[workspace_state] Multi-repo: Skipping state update for workspace={workspace_path} without repo_name\"\n        )\n        return {}\n\n    if is_multi_repo_mode() and repo_name:\n        try:\n            ws_root = Path(_resolve_workspace_root())\n            # Allow updates when the repo state dir exists, even if the workspace\n            # directory is not present (e.g. dev-remote simulations where only\n            # .codebase state is persisted).\n            state_dir = _get_repo_state_dir(repo_name)\n            if not (ws_root / repo_name).exists() and not state_dir.exists():\n                return {}\n        except Exception:\n            return {}\n\n    lock = _get_state_lock(workspace_path, repo_name)\n    with lock:\n        state = get_workspace_state(workspace_path, repo_name)\n        for key, value in updates.items():\n            if key in state or key in WorkspaceState.__annotations__:\n                state[key] = value\n\n        _ensure_repo_slug_defaults(state, repo_name)\n\n        state[\"updated_at\"] = datetime.now().isoformat()\n\n        if is_multi_repo_mode() and repo_name:\n            state_dir = _get_repo_state_dir(repo_name)\n            state_dir.mkdir(parents=True, exist_ok=True)\n            state_path = state_dir / STATE_FILENAME\n        else:\n            try:\n                state_path = _ensure_state_dir(workspace_path)\n            except PermissionError:\n                state_dir = _get_global_state_dir(workspace_path)\n                state_dir.mkdir(parents=True, exist_ok=True)\n                state_path = state_dir / STATE_FILENAME\n\n        _atomic_write_state(state_path, state)\n        return state",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_update_indexing_status_832": {
      "name": "update_indexing_status",
      "type": "function",
      "start_line": 832,
      "end_line": 853,
      "content_hash": "3bebd6be5f10390c21422874d4445911f9ab1ace",
      "content": "def update_indexing_status(\n    workspace_path: Optional[str] = None,\n    status: Optional[IndexingStatus] = None,\n    repo_name: Optional[str] = None,\n) -> WorkspaceState:\n    \"\"\"Update indexing status in workspace state.\"\"\"\n    workspace_path, repo_name = _resolve_repo_context(workspace_path, repo_name)\n\n    if is_multi_repo_mode() and repo_name is None:\n        print(\n            f\"[workspace_state] Multi-repo: Skipping indexing status update for workspace={workspace_path} without repo_name\"\n        )\n        return {}\n\n    if status is None:\n        status = {\"state\": \"idle\"}\n\n    return update_workspace_state(\n        workspace_path=workspace_path,\n        updates={\"indexing_status\": status},\n        repo_name=repo_name,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_set_staging_state_856": {
      "name": "set_staging_state",
      "type": "function",
      "start_line": 856,
      "end_line": 870,
      "content_hash": "bc59bc75b231b52b0d945f5c42f9979fce2e7ec7",
      "content": "def set_staging_state(\n    *,\n    workspace_path: Optional[str] = None,\n    repo_name: Optional[str] = None,\n    staging: Optional[StagingInfo] = None,\n) -> WorkspaceState:\n    \"\"\"Persist staging metadata for a workspace/repo.\"\"\"\n    updates: Dict[str, Any] = {\"staging\": staging}\n    if staging:\n        staging.setdefault(\"updated_at\", datetime.now().isoformat())\n    return update_workspace_state(\n        workspace_path=workspace_path,\n        repo_name=repo_name,\n        updates=updates,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_promote_pending_indexing_config_873": {
      "name": "promote_pending_indexing_config",
      "type": "function",
      "start_line": 873,
      "end_line": 898,
      "content_hash": "8a453772fee628af385d9dc0b662f29010ee4215",
      "content": "def promote_pending_indexing_config(\n    *,\n    workspace_path: Optional[str] = None,\n    repo_name: Optional[str] = None,\n) -> WorkspaceState:\n    \"\"\"Promote pending indexing config/env snapshots to active.\"\"\"\n    state = get_workspace_state(workspace_path, repo_name) or {}\n    pending_cfg = state.get(\"indexing_config_pending\")\n    pending_hash = state.get(\"indexing_config_pending_hash\")\n    pending_env = state.get(\"indexing_env_pending\")\n\n    if not pending_cfg and not pending_env:\n        return state\n\n    cfg = pending_cfg or state.get(\"indexing_config\") or get_indexing_config_snapshot()\n    cfg_hash = pending_hash or compute_indexing_config_hash(cfg)\n    env_snapshot = pending_env or state.get(\"indexing_env\") or dict(os.environ)\n\n    return persist_indexing_config(\n        workspace_path=workspace_path,\n        repo_name=repo_name,\n        config=cfg,\n        config_hash=cfg_hash,\n        environment=env_snapshot,\n        pending=False,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_collection_state_snapshot_901": {
      "name": "get_collection_state_snapshot",
      "type": "function",
      "start_line": 901,
      "end_line": 942,
      "content_hash": "ed17d79874aeb11b42fa4d0a44639ad396d02ef6",
      "content": "def get_collection_state_snapshot(\n    *,\n    workspace_path: Optional[str],\n    repo_name: Optional[str],\n) -> Dict[str, Any]:\n    \"\"\"Return current active + staging collection metadata for bridge/search consumers.\"\"\"\n    state = get_workspace_state(workspace_path, repo_name) or {}\n    if not isinstance(state, dict):\n        state = {}\n\n    resolved_workspace = state.get(\"workspace_path\") or workspace_path\n    active_collection = state.get(\"qdrant_collection\")\n    serving_collection = state.get(\"serving_collection\") or active_collection\n    active_repo_slug = state.get(\"active_repo_slug\") or repo_name\n    serving_repo_slug = state.get(\"serving_repo_slug\") or active_repo_slug\n\n    snapshot: Dict[str, Any] = {\n        \"workspace_path\": resolved_workspace,\n        \"repo_name\": repo_name,\n        \"active_collection\": active_collection,\n        \"serving_collection\": serving_collection,\n        \"previous_collection\": state.get(\"previous_collection\"),\n        \"active_repo_slug\": active_repo_slug,\n        \"serving_repo_slug\": serving_repo_slug,\n        \"indexing_status\": state.get(\"indexing_status\"),\n        \"staging\": None,\n    }\n\n    staging_enabled = bool(is_staging_enabled() if callable(is_staging_enabled) else False)\n    staging_info = state.get(\"staging\")\n    if staging_enabled and isinstance(staging_info, dict) and staging_info.get(\"collection\"):\n        snapshot[\"staging\"] = {\n            \"collection\": staging_info.get(\"collection\"),\n            \"status\": staging_info.get(\"status\"),\n            \"started_at\": staging_info.get(\"started_at\"),\n            \"updated_at\": staging_info.get(\"updated_at\"),\n            \"env_hash\": staging_info.get(\"env_hash\"),\n            \"workspace_path\": staging_info.get(\"workspace_path\"),\n            \"repo_name\": staging_info.get(\"repo_name\") or repo_name,\n        }\n\n    return snapshot",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_staging_targets_945": {
      "name": "get_staging_targets",
      "type": "function",
      "start_line": 945,
      "end_line": 980,
      "content_hash": "b1dd59fd5b8e1bd17f0af74810979a6dd90d65f3",
      "content": "def get_staging_targets(\n    *,\n    workspace_path: Optional[str],\n    repo_name: Optional[str],\n) -> Dict[str, Any]:\n    \"\"\"Return canonical/old slug hints plus staging metadata if staging is active.\"\"\"\n\n    snapshot = get_collection_state_snapshot(workspace_path=workspace_path, repo_name=repo_name)\n    if not snapshot:\n        return {}\n\n    active_slug = snapshot.get(\"active_repo_slug\") or repo_name\n    serving_slug = snapshot.get(\"serving_repo_slug\") or active_slug\n    staging_info = snapshot.get(\"staging\")\n\n    canonical_slug: Optional[str] = None\n    if isinstance(active_slug, str) and active_slug.strip():\n        canonical_slug = active_slug[:-4] if active_slug.endswith(\"_old\") else active_slug\n    elif isinstance(serving_slug, str) and serving_slug.strip():\n        canonical_slug = serving_slug[:-4] if serving_slug.endswith(\"_old\") else serving_slug\n    elif repo_name:\n        canonical_slug = str(repo_name)\n\n    result: Dict[str, Any] = {\n        \"workspace_path\": snapshot.get(\"workspace_path\") or workspace_path,\n        \"repo_name\": repo_name,\n        \"active_slug\": active_slug,\n        \"serving_slug\": serving_slug,\n        \"staging\": staging_info if isinstance(staging_info, dict) else None,\n    }\n\n    if canonical_slug:\n        result[\"canonical_slug\"] = canonical_slug\n        result[\"old_slug\"] = f\"{canonical_slug}_old\"\n\n    return result",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_update_staging_status_983": {
      "name": "update_staging_status",
      "type": "function",
      "start_line": 983,
      "end_line": 999,
      "content_hash": "909421134f388d6578c4cea320a10527084fa6bf",
      "content": "def update_staging_status(\n    *,\n    workspace_path: Optional[str],\n    repo_name: Optional[str],\n    status: IndexingStatus,\n) -> WorkspaceState:\n    state = get_workspace_state(workspace_path, repo_name)\n    staging = dict(state.get(\"staging\", {}) or {})\n    if not staging:\n        return state\n    staging[\"status\"] = status\n    staging[\"updated_at\"] = datetime.now().isoformat()\n    return set_staging_state(\n        workspace_path=workspace_path,\n        repo_name=repo_name,\n        staging=staging,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_clear_staging_collection_1002": {
      "name": "clear_staging_collection",
      "type": "function",
      "start_line": 1002,
      "end_line": 1007,
      "content_hash": "9f66a4826cfcf00f415e18092412e451888ac71f",
      "content": "def clear_staging_collection(\n    *,\n    workspace_path: Optional[str],\n    repo_name: Optional[str],\n) -> WorkspaceState:\n    return set_staging_state(workspace_path=workspace_path, repo_name=repo_name, staging=None)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_activate_staging_collection_1010": {
      "name": "activate_staging_collection",
      "type": "function",
      "start_line": 1010,
      "end_line": 1046,
      "content_hash": "ca025be7c6e38cba49d5aefc537ce01d30e3b120",
      "content": "def activate_staging_collection(\n    *,\n    workspace_path: Optional[str],\n    repo_name: Optional[str],\n) -> WorkspaceState:\n    state = get_workspace_state(workspace_path, repo_name)\n    staging = dict(state.get(\"staging\", {}) or {})\n    collection = staging.get(\"collection\")\n    if not collection:\n        return state\n\n    updates: Dict[str, Any] = {\n        \"previous_collection\": state.get(\"qdrant_collection\"),\n        \"qdrant_collection\": collection,\n        \"staging\": None,\n    }\n\n    status = staging.get(\"status\")\n    if isinstance(status, dict):\n        updates[\"indexing_status\"] = status\n\n    if staging.get(\"indexing_config\"):\n        updates[\"indexing_config\"] = staging.get(\"indexing_config\")\n    if staging.get(\"indexing_config_hash\"):\n        updates[\"indexing_config_hash\"] = staging.get(\"indexing_config_hash\")\n    if staging.get(\"environment\"):\n        updates[\"indexing_env\"] = staging.get(\"environment\")\n    # Clear pending snapshots once activation succeeds\n    updates[\"indexing_config_pending\"] = None\n    updates[\"indexing_config_pending_hash\"] = None\n    updates[\"indexing_env_pending\"] = None\n\n    return update_workspace_state(\n        workspace_path=workspace_path,\n        repo_name=repo_name,\n        updates=updates,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_update_repo_origin_1049": {
      "name": "update_repo_origin",
      "type": "function",
      "start_line": 1049,
      "end_line": 1087,
      "content_hash": "3eba55c53ec69a8af049da4a06b0e07efe35b038",
      "content": "def update_repo_origin(\n    workspace_path: Optional[str] = None,\n    repo_name: Optional[str] = None,\n    *,\n    container_path: Optional[str] = None,\n    source_path: Optional[str] = None,\n    collection_name: Optional[str] = None,\n) -> WorkspaceState:\n    \"\"\"Update origin metadata for a repository/workspace.\"\"\"\n\n    resolved_workspace, resolved_repo = _resolve_repo_context(workspace_path, repo_name)\n\n    if is_multi_repo_mode() and resolved_repo is None:\n        return {}\n\n    state = get_workspace_state(resolved_workspace, resolved_repo)\n    if not state:\n        state = {}\n\n    origin: OriginInfo = dict(state.get(\"origin\", {}))  # type: ignore[arg-type]\n    if resolved_repo:\n        origin[\"repo_name\"] = resolved_repo\n    if container_path or workspace_path:\n        origin[\"container_path\"] = container_path or workspace_path\n    if source_path:\n        origin[\"source_path\"] = source_path\n    if collection_name:\n        origin[\"collection_name\"] = collection_name\n    origin[\"updated_at\"] = datetime.now().isoformat()\n\n    updates: Dict[str, Any] = {\"origin\": origin}\n    if collection_name:\n        updates.setdefault(\"qdrant_collection\", collection_name)\n\n    return update_workspace_state(\n        workspace_path=resolved_workspace,\n        updates=updates,\n        repo_name=resolved_repo,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_log_activity_1090": {
      "name": "log_activity",
      "type": "function",
      "start_line": 1090,
      "end_line": 1141,
      "content_hash": "3324009241677de95f697e7e5a43b5c486e7d080",
      "content": "def log_activity(\n    repo_name: Optional[str] = None,\n    action: Optional[ActivityAction] = None,\n    file_path: Optional[str] = None,\n    details: Optional[ActivityDetails] = None,\n    workspace_path: Optional[str] = None,\n) -> None:\n    \"\"\"Log activity to workspace state.\"\"\"\n\n    if not action:\n        return\n\n    activity = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"action\": action,\n        \"file_path\": file_path,\n        \"details\": details or {},\n    }\n\n    resolved_workspace = workspace_path or _resolve_workspace_root()\n\n    if is_multi_repo_mode() and repo_name:\n        try:\n            ws_root = Path(_resolve_workspace_root())\n            if not (ws_root / repo_name).exists():\n                return\n        except Exception:\n            return\n        state_dir = _get_repo_state_dir(repo_name)\n        state_dir.mkdir(parents=True, exist_ok=True)\n        state_path = state_dir / STATE_FILENAME\n        lock_path = state_path.with_suffix(\".lock\")\n\n        with _cross_process_lock(lock_path):\n            try:\n                if state_path.exists():\n                    with open(state_path, \"r\", encoding=\"utf-8-sig\") as f:\n                        state = json.load(f)\n                else:\n                    state = {\"created_at\": datetime.now().isoformat()}\n            except Exception:\n                state = {\"created_at\": datetime.now().isoformat()}\n\n            state[\"last_activity\"] = activity\n            state[\"updated_at\"] = datetime.now().isoformat()\n            _atomic_write_state(state_path, state)\n    else:\n        update_workspace_state(\n            workspace_path=resolved_workspace,\n            updates={\"last_activity\": activity},\n            repo_name=repo_name,\n        )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__generate_collection_name_from_repo_1144": {
      "name": "_generate_collection_name_from_repo",
      "type": "function",
      "start_line": 1144,
      "end_line": 1152,
      "content_hash": "177fe30cfa0bfa8420ce71590f122025ba58472a",
      "content": "def _generate_collection_name_from_repo(repo_name: str) -> str:\n    \"\"\"Generate collection name with 8-char hash for local workspaces.\n\n    Used by local indexer/watcher. Remote uploads use 16+8 char pattern\n    for collision avoidance when folder names may be identical.\n    \"\"\"\n    hash_obj = hashlib.sha256(repo_name.encode())\n    short_hash = hash_obj.hexdigest()[:8]\n    return f\"{repo_name}-{short_hash}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__normalize_repo_name_for_collection_1154": {
      "name": "_normalize_repo_name_for_collection",
      "type": "function",
      "start_line": 1154,
      "end_line": 1181,
      "content_hash": "29b26f2b7925a3ae93fb888c2c755dc4ff1fce0a",
      "content": "def _normalize_repo_name_for_collection(repo_name: str) -> str:\n    \"\"\"Normalize repo identifier to a stable base name for collection naming.\n\n    In multi-repo remote mode, repo_name may be a slug like \"name-<16hex>\" used\n    for folder collision avoidance. For Qdrant collections we always want the\n    base repo directory name, so strip a trailing 16-hex segment when present.\n    \"\"\"\n    try:\n        # Special-case staging clone slugs (\"..._old\"): we still want to strip the\n        # remote-upload 16-hex suffix from the *base* repo name.\n        is_old = False\n        raw = repo_name\n        try:\n            if raw.endswith(\"_old\"):\n                is_old = True\n                raw = raw[:-4]\n        except Exception:\n            is_old = False\n            raw = repo_name\n\n        m = re.match(r\"^(.*)-([0-9a-f]{16})$\", raw)\n        if m:\n            base = (m.group(1) or \"\").strip()\n            if base:\n                return f\"{base}_old\" if is_old else base\n    except Exception:\n        pass\n    return repo_name",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__collection_name_for_repo_slug_1184": {
      "name": "_collection_name_for_repo_slug",
      "type": "function",
      "start_line": 1184,
      "end_line": 1201,
      "content_hash": "2157fd90704b99d19599470e43539e9f5e37e540",
      "content": "def _collection_name_for_repo_slug(normalized_repo: str, *, is_old_slug: bool) -> Optional[str]:\n    if not normalized_repo:\n        return None\n\n    # If repo_name is a staging clone slug (\"..._old\"), compute the canonical collection name\n    # from the base repo and then append \"_old\".\n    if is_old_slug:\n        try:\n            base_repo = normalized_repo[:-4] if normalized_repo.endswith(\"_old\") else normalized_repo\n            base_coll = _generate_collection_name_from_repo(base_repo)\n            return f\"{base_coll}_old\"\n        except Exception:\n            return None\n\n    if is_multi_repo_mode():\n        return _generate_collection_name_from_repo(normalized_repo)\n\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_collection_name_1204": {
      "name": "get_collection_name",
      "type": "function",
      "start_line": 1204,
      "end_line": 1243,
      "content_hash": "9ae194255ee79485aef1fb4aa0eb8817687d09c8",
      "content": "def get_collection_name(repo_name: Optional[str] = None) -> str:\n    \"\"\"Get collection name for repository or workspace.\n\n    Priority:\n    1. Explicit COLLECTION_NAME env var - master override when set to a real value\n       (if repo_name is an *_old clone, append _old to the override unless already present)\n    2. Derive from repo slug (including *_old suffix handling)\n    3. Fallback: \"global-collection\"\n\n    This ensures COLLECTION_NAME works as a master override in both local dev\n    and container environments, while still allowing deterministic derivation\n    from repo slugs (including staging clone slugs).\n    \"\"\"\n\n    # COLLECTION_NAME always wins when explicitly set to a real value.\n    env_coll = os.environ.get(\"COLLECTION_NAME\", \"\").strip()\n    if env_coll and env_coll not in PLACEHOLDER_COLLECTION_NAMES:\n        try:\n            if isinstance(repo_name, str) and repo_name.endswith(\"_old\") and not env_coll.endswith(\"_old\"):\n                return f\"{env_coll}_old\"\n        except Exception:\n            pass\n        return env_coll\n\n    normalized = _normalize_repo_name_for_collection(repo_name) if repo_name else None\n    is_old_slug = False\n    try:\n        if isinstance(repo_name, str) and repo_name.endswith(\"_old\"):\n            is_old_slug = True\n    except Exception:\n        is_old_slug = False\n\n    derived = None\n    if normalized:\n        derived = _collection_name_for_repo_slug(normalized, is_old_slug=is_old_slug)\n    if derived:\n        return derived\n\n    # Default fallback\n    return \"global-collection\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__detect_repo_name_from_path_by_structure_1245": {
      "name": "_detect_repo_name_from_path_by_structure",
      "type": "function",
      "start_line": 1245,
      "end_line": 1285,
      "content_hash": "79c6e434b2479cbf6e045e461533a65508efb559",
      "content": "def _detect_repo_name_from_path_by_structure(path: Path) -> str:\n    \"\"\"Detect repository name from path structure (fallback when git is unavailable).\"\"\"\n    try:\n        resolved_path = path.resolve()\n    except Exception:\n        return None\n\n    candidate_roots: List[Path] = []\n    for root_str in (\n        os.environ.get(\"WATCH_ROOT\"),\n        os.environ.get(\"WORKSPACE_PATH\"),\n        \"/work\",\n        os.environ.get(\"HOST_ROOT\"),\n    ):\n        if not root_str:\n            continue\n        try:\n            root_path = Path(root_str).resolve()\n        except Exception:\n            continue\n        if root_path not in candidate_roots:\n            candidate_roots.append(root_path)\n\n    for base in candidate_roots:\n        try:\n            rel_path = resolved_path.relative_to(base)\n        except ValueError:\n            continue\n\n        if not rel_path.parts:\n            continue\n\n        repo_name = rel_path.parts[0]\n        if repo_name in (\".codebase\", \".git\", \"__pycache__\"):\n            continue\n\n        repo_path = base / repo_name\n        if repo_path.exists() or resolved_path == repo_path or str(resolved_path).startswith(str(repo_path) + os.sep):\n            return repo_name\n\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__normalize_repo_slug_1287": {
      "name": "_normalize_repo_slug",
      "type": "function",
      "start_line": 1287,
      "end_line": 1292,
      "content_hash": "52792cadddea8001b60c7928621735d497b83c42",
      "content": "def _normalize_repo_slug(candidate: Optional[str]) -> Optional[str]:\n    if not candidate:\n        return None\n    if _SLUGGED_REPO_RE.match(candidate):\n        return candidate\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_repo_name_from_path_1295": {
      "name": "_extract_repo_name_from_path",
      "type": "function",
      "start_line": 1295,
      "end_line": 1335,
      "content_hash": "8978c62f2cc1606d8af63c190914b83ce8510646",
      "content": "def _extract_repo_name_from_path(workspace_path: str) -> str:\n    \"\"\"Extract repository slug or canonical name from workspace path.\n\n    Accepts canonical slugs (repo-hash), `_old` slugs, and falls back to git remote name.\n    \"\"\"\n    if not workspace_path:\n        return \"\"\n\n    try:\n        path = Path(workspace_path).resolve()\n    except Exception:\n        path = Path(workspace_path)\n\n    slug = _server_managed_slug_from_path(path)\n    if slug:\n        return slug\n\n    try:\n        repo_path = path if path.is_dir() else path.parent\n        if (repo_path / \".git\").exists():\n            name = _git_remote_repo_name(repo_path)\n            if name:\n                return name\n    except Exception:\n        pass\n\n    try:\n        candidate = _normalize_repo_slug(path.name)\n        if candidate:\n            return candidate\n    except Exception:\n        pass\n\n    try:\n        candidate = _normalize_repo_slug(path.parent.name)\n        if candidate:\n            return candidate\n    except Exception:\n        pass\n\n    return path.name",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ensure_repo_slug_defaults_1338": {
      "name": "_ensure_repo_slug_defaults",
      "type": "function",
      "start_line": 1338,
      "end_line": 1352,
      "content_hash": "e383fb996d9ce1c126a903820cff47de54e57f46",
      "content": "def _ensure_repo_slug_defaults(state: WorkspaceState, repo_name: Optional[str]) -> bool:\n    modified = False\n    active_slug = state.get(\"active_repo_slug\")\n    if not active_slug and repo_name:\n        state[\"active_repo_slug\"] = repo_name\n        active_slug = repo_name\n        modified = True\n    serving_slug = state.get(\"serving_repo_slug\")\n    if not serving_slug:\n        state[\"serving_repo_slug\"] = active_slug or repo_name\n        modified = True\n    if not state.get(\"serving_collection\") and state.get(\"qdrant_collection\"):\n        state[\"serving_collection\"] = state.get(\"qdrant_collection\")\n        modified = True\n    return modified",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_cache_path_1354": {
      "name": "_get_cache_path",
      "type": "function",
      "start_line": 1354,
      "end_line": 1360,
      "content_hash": "a7139d16966033f6dbe9291f5cdccc688d553490",
      "content": "def _get_cache_path(workspace_path: str) -> Path:\n    \"\"\"Get the path to the cache.json file.\"\"\"\n    try:\n        workspace = Path(os.path.abspath(workspace_path))\n    except Exception:\n        workspace = Path(workspace_path)\n    return workspace / STATE_DIRNAME / CACHE_FILENAME",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__read_cache_file_uncached_1363": {
      "name": "_read_cache_file_uncached",
      "type": "function",
      "start_line": 1363,
      "end_line": 1375,
      "content_hash": "29d939323106a860fe2332a95187a367549c5f4f",
      "content": "def _read_cache_file_uncached(cache_path: Path) -> Dict[str, Any]:\n    if not cache_path.exists():\n        now = datetime.now().isoformat()\n        return {\"file_hashes\": {}, \"created_at\": now, \"updated_at\": now}\n    try:\n        with open(cache_path, \"r\", encoding=\"utf-8-sig\") as f:\n            obj = json.load(f)\n            if isinstance(obj, dict) and isinstance(obj.get(\"file_hashes\"), dict):\n                return obj\n    except (OSError, json.JSONDecodeError, ValueError):\n        pass\n    now = datetime.now().isoformat()\n    return {\"file_hashes\": {}, \"created_at\": now, \"updated_at\": now}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__read_cache_file_cached_1378": {
      "name": "_read_cache_file_cached",
      "type": "function",
      "start_line": 1378,
      "end_line": 1397,
      "content_hash": "ec91e630c12d7eacc963366d0ccaa5fa085f9071",
      "content": "def _read_cache_file_cached(cache_path: Path) -> Dict[str, Any]:\n    key = str(cache_path)\n    now = time.time()\n\n    with _cache_memo_lock:\n        last_check = _cache_memo_last_check.get(key, 0.0)\n        if key in _cache_memo and (now - last_check) < _cache_memo_recheck_seconds():\n            return _cache_memo[key]\n\n    sig = _cache_file_sig(cache_path)\n    with _cache_memo_lock:\n        _cache_memo_last_check[key] = now\n        if sig is not None and _cache_memo_sig.get(key) == sig and key in _cache_memo:\n            return _cache_memo[key]\n\n    obj = _read_cache_file_uncached(cache_path)\n    with _cache_memo_lock:\n        _cache_memo[key] = obj\n        _cache_memo_sig[key] = sig or (-1, -1)\n        return obj",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__read_cache_cached_1400": {
      "name": "_read_cache_cached",
      "type": "function",
      "start_line": 1400,
      "end_line": 1401,
      "content_hash": "44729b439cf1209ee5fc69825c024acae78dcab8",
      "content": "def _read_cache_cached(workspace_path: str) -> Dict[str, Any]:\n    return _read_cache_file_cached(_get_cache_path(workspace_path))",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__read_cache_1404": {
      "name": "_read_cache",
      "type": "function",
      "start_line": 1404,
      "end_line": 1408,
      "content_hash": "a18d10a470eb05f7008a00cb06acb4195d78bb70",
      "content": "def _read_cache(workspace_path: str) -> Dict[str, Any]:\n    \"\"\"Read cache file, return empty dict if it doesn't exist or is invalid.\"\"\"\n\n    cache_path = _get_cache_path(workspace_path)\n    return _read_cache_file_uncached(cache_path)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__write_cache_1411": {
      "name": "_write_cache",
      "type": "function",
      "start_line": 1411,
      "end_line": 1429,
      "content_hash": "15e05ec7d9ad6494d16e954dcba4b08516cd6cd0",
      "content": "def _write_cache(workspace_path: str, cache: Dict[str, Any]) -> None:\n    \"\"\"Atomic write of cache file with cross-process locking.\"\"\"\n\n    lock = _get_state_lock(workspace_path)\n    with lock:\n        cache_path = _get_cache_path(workspace_path)\n        cache_path.parent.mkdir(parents=True, exist_ok=True)\n        lock_path = cache_path.with_suffix(cache_path.suffix + \".lock\")\n        with _cross_process_lock(lock_path):\n            tmp = cache_path.with_suffix(f\".tmp.{uuid.uuid4().hex[:8]}\")\n            try:\n                with open(tmp, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(cache, f, ensure_ascii=False, indent=2)\n                tmp.replace(cache_path)\n            finally:\n                try:\n                    tmp.unlink(missing_ok=True)\n                except Exception:\n                    pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_cached_file_hash_1432": {
      "name": "get_cached_file_hash",
      "type": "function",
      "start_line": 1432,
      "end_line": 1453,
      "content_hash": "7bcb7e7438881328e0bfe3e4eec20c459644ca62",
      "content": "def get_cached_file_hash(file_path: str, repo_name: Optional[str] = None) -> str:\n    \"\"\"Get cached file hash for tracking changes.\"\"\"\n    if is_multi_repo_mode() and repo_name:\n        state_dir = _get_repo_state_dir(repo_name)\n        cache_path = state_dir / CACHE_FILENAME\n\n        cache = _read_cache_file_cached(cache_path)\n        file_hashes = cache.get(\"file_hashes\", {})\n        fp = _normalize_cache_key_path(file_path)\n        val = file_hashes.get(fp, \"\")\n        if isinstance(val, dict):\n            return str(val.get(\"hash\") or \"\")\n        return str(val or \"\")\n    else:\n        cache = _read_cache_cached(_resolve_workspace_root())\n        fp = _normalize_cache_key_path(file_path)\n        val = cache.get(\"file_hashes\", {}).get(fp, \"\")\n        if isinstance(val, dict):\n            return str(val.get(\"hash\") or \"\")\n        return str(val or \"\")\n\n    return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_set_cached_file_hash_1456": {
      "name": "set_cached_file_hash",
      "type": "function",
      "start_line": 1456,
      "end_line": 1541,
      "content_hash": "4790a807ed42aa97f08d828d5d5313a67adad730",
      "content": "def set_cached_file_hash(file_path: str, file_hash: str, repo_name: Optional[str] = None) -> None:\n    \"\"\"Set cached file hash for tracking changes.\"\"\"\n    fp = _normalize_cache_key_path(file_path)\n\n    st_size: Optional[int] = None\n    st_mtime: Optional[int] = None\n    try:\n        st = Path(file_path).stat()\n        st_size = int(getattr(st, \"st_size\", 0))\n        st_mtime = int(getattr(st, \"st_mtime\", 0))\n    except Exception:\n        st_size = None\n        st_mtime = None\n\n    if is_multi_repo_mode() and repo_name:\n        try:\n            ws_root = Path(_resolve_workspace_root())\n            if not (ws_root / repo_name).exists():\n                return\n        except Exception:\n            return\n        state_dir = _get_repo_state_dir(repo_name)\n        cache_path = state_dir / CACHE_FILENAME\n        state_dir.mkdir(parents=True, exist_ok=True)\n\n        if cache_path.exists():\n            cache = _read_cache_file_cached(cache_path)\n        else:\n            cache = {\"file_hashes\": {}, \"created_at\": datetime.now().isoformat()}\n\n        existing = cache.get(\"file_hashes\", {}).get(fp)\n        if isinstance(existing, dict) and st_size is not None and st_mtime is not None:\n            if (\n                str(existing.get(\"hash\") or \"\") == str(file_hash or \"\")\n                and int(existing.get(\"size\") or 0) == int(st_size)\n                and int(existing.get(\"mtime\") or 0) == int(st_mtime)\n            ):\n                return\n\n        entry: Any = file_hash\n        try:\n            if st_size is not None and st_mtime is not None:\n                entry = {\"hash\": file_hash, \"size\": st_size, \"mtime\": st_mtime}\n            else:\n                st = Path(file_path).stat()\n                entry = {\n                    \"hash\": file_hash,\n                    \"size\": int(getattr(st, \"st_size\", 0)),\n                    \"mtime\": int(getattr(st, \"st_mtime\", 0)),\n                }\n        except OSError:\n            pass\n\n        cache.setdefault(\"file_hashes\", {})[fp] = entry\n        cache[\"updated_at\"] = datetime.now().isoformat()\n\n        _atomic_write_state(cache_path, cache)  # reuse atomic writer for files\n        _memoize_cache_obj(cache_path, cache)\n        return\n\n    cache = _read_cache_cached(_resolve_workspace_root())\n    existing = cache.get(\"file_hashes\", {}).get(fp)\n    if isinstance(existing, dict) and st_size is not None and st_mtime is not None:\n        if (\n            str(existing.get(\"hash\") or \"\") == str(file_hash or \"\")\n            and int(existing.get(\"size\") or 0) == int(st_size)\n            and int(existing.get(\"mtime\") or 0) == int(st_mtime)\n        ):\n            return\n    entry: Any = file_hash\n    try:\n        if st_size is not None and st_mtime is not None:\n            entry = {\"hash\": file_hash, \"size\": st_size, \"mtime\": st_mtime}\n        else:\n            st = Path(file_path).stat()\n            entry = {\n                \"hash\": file_hash,\n                \"size\": int(getattr(st, \"st_size\", 0)),\n                \"mtime\": int(getattr(st, \"st_mtime\", 0)),\n            }\n    except OSError:\n        pass\n    cache.setdefault(\"file_hashes\", {})[fp] = entry\n    cache[\"updated_at\"] = datetime.now().isoformat()\n    _write_cache(_resolve_workspace_root(), cache)\n    _memoize_cache_obj(_get_cache_path(_resolve_workspace_root()), cache)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_cached_file_meta_1544": {
      "name": "get_cached_file_meta",
      "type": "function",
      "start_line": 1544,
      "end_line": 1565,
      "content_hash": "3f860b9ee0618af13868a2436992f01385b50726",
      "content": "def get_cached_file_meta(file_path: str, repo_name: Optional[str] = None) -> Dict[str, Any]:\n    fp = _normalize_cache_key_path(file_path)\n    if is_multi_repo_mode() and repo_name:\n        state_dir = _get_repo_state_dir(repo_name)\n        cache_path = state_dir / CACHE_FILENAME\n\n        cache = _read_cache_file_cached(cache_path)\n        file_hashes = cache.get(\"file_hashes\", {})\n        val = file_hashes.get(fp)\n    else:\n        cache = _read_cache_cached(_resolve_workspace_root())\n        val = cache.get(\"file_hashes\", {}).get(fp)\n\n    if isinstance(val, dict):\n        return {\n            \"hash\": str(val.get(\"hash\") or \"\"),\n            \"size\": val.get(\"size\"),\n            \"mtime\": val.get(\"mtime\"),\n        }\n    if isinstance(val, str):\n        return {\"hash\": val}\n    return {}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_remove_cached_file_1568": {
      "name": "remove_cached_file",
      "type": "function",
      "start_line": 1568,
      "end_line": 1593,
      "content_hash": "54884a7b021a129237c2a56b4f01391df3bf3841",
      "content": "def remove_cached_file(file_path: str, repo_name: Optional[str] = None) -> None:\n    \"\"\"Remove file entry from cache.\"\"\"\n    if is_multi_repo_mode() and repo_name:\n        state_dir = _get_repo_state_dir(repo_name)\n        cache_path = state_dir / CACHE_FILENAME\n\n        if cache_path.exists():\n            cache = _read_cache_file_cached(cache_path)\n            file_hashes = cache.get(\"file_hashes\", {})\n\n            fp = _normalize_cache_key_path(file_path)\n            if fp in file_hashes:\n                file_hashes.pop(fp, None)\n                cache[\"updated_at\"] = datetime.now().isoformat()\n\n                _atomic_write_state(cache_path, cache)\n                _memoize_cache_obj(cache_path, cache)\n        return\n\n    cache = _read_cache_cached(_resolve_workspace_root())\n    fp = _normalize_cache_key_path(file_path)\n    if fp in cache.get(\"file_hashes\", {}):\n        cache[\"file_hashes\"].pop(fp, None)\n        cache[\"updated_at\"] = datetime.now().isoformat()\n        _write_cache(_resolve_workspace_root(), cache)\n        _memoize_cache_obj(_get_cache_path(_resolve_workspace_root()), cache)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_cleanup_old_cache_locks_1596": {
      "name": "cleanup_old_cache_locks",
      "type": "function",
      "start_line": 1596,
      "end_line": 1633,
      "content_hash": "78ddc981c61d3e4663521b1383704d557c34fae0",
      "content": "def cleanup_old_cache_locks(max_idle_seconds: int = 900) -> int:\n    \"\"\"Best-effort cleanup of idle cache locks.\n\n    Removes locks that have been idle (not requested via _get_state_lock) for longer than max_idle_seconds\n    and whose lock can be acquired without blocking (i.e., not held).\n    Returns the number of locks removed.\n    \"\"\"\n    now = time.time()\n    removed = 0\n    with _state_lock:\n        stale_keys = []\n        for ws, lock in list(_state_locks.items()):\n            last = _state_lock_last_used.get(ws, 0.0)\n            # Prefer also pruning locks whose workspace no longer exists\n            ws_exists = True\n            try:\n                ws_exists = Path(ws).exists()\n            except Exception:\n                ws_exists = False\n            if (now - last) > max_idle_seconds or not ws_exists:\n                acquired = False\n                try:\n                    acquired = lock.acquire(blocking=False)\n                except Exception:\n                    acquired = False\n                if acquired:\n                    try:\n                        stale_keys.append(ws)\n                    finally:\n                        try:\n                            lock.release()\n                        except Exception:\n                            pass\n        for ws in stale_keys:\n            _state_locks.pop(ws, None)\n            _state_lock_last_used.pop(ws, None)\n            removed += 1\n    return removed",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_collection_mappings_1636": {
      "name": "get_collection_mappings",
      "type": "function",
      "start_line": 1636,
      "end_line": 1697,
      "content_hash": "d0950b96c0096575670bf9efb4da80d1948e5657",
      "content": "def get_collection_mappings(search_root: Optional[str] = None) -> List[Dict[str, Any]]:\n    \"\"\"Enumerate collection mappings with origin metadata.\"\"\"\n\n    root_path = Path(search_root or _resolve_workspace_root()).resolve()\n    mappings: List[Dict[str, Any]] = []\n\n    try:\n        if is_multi_repo_mode():\n            repos_root = root_path / STATE_DIRNAME / \"repos\"\n            if repos_root.exists():\n                for repo_dir in sorted(p for p in repos_root.iterdir() if p.is_dir()):\n                    repo_name = repo_dir.name\n                    state_path = repo_dir / STATE_FILENAME\n                    if not state_path.exists():\n                        continue\n                    try:\n                        with open(state_path, \"r\", encoding=\"utf-8-sig\") as f:\n                            state = json.load(f) or {}\n                    except Exception as e:\n                        print(f\"[workspace_state] Failed to read repo state from {state_path}: {e}\")\n                        continue\n\n                    origin = state.get(\"origin\", {}) or {}\n                    mappings.append(\n                        {\n                            \"repo_name\": repo_name,\n                            \"collection_name\": state.get(\"qdrant_collection\")\n                            or get_collection_name(repo_name),\n                            \"container_path\": origin.get(\"container_path\")\n                            or str((Path(_resolve_workspace_root()) / repo_name).resolve()),\n                            \"source_path\": origin.get(\"source_path\"),\n                            \"state_file\": str(state_path),\n                            \"updated_at\": state.get(\"updated_at\"),\n                        }\n                    )\n        else:\n            state_path = root_path / STATE_DIRNAME / STATE_FILENAME\n            if state_path.exists():\n                try:\n                    with open(state_path, \"r\", encoding=\"utf-8-sig\") as f:\n                        state = json.load(f) or {}\n                except Exception:\n                    state = {}\n\n                origin = state.get(\"origin\", {}) or {}\n                repo_name = origin.get(\"repo_name\") or Path(root_path).name\n                mappings.append(\n                    {\n                        \"repo_name\": repo_name,\n                        \"collection_name\": state.get(\"qdrant_collection\")\n                        or get_collection_name(repo_name),\n                        \"container_path\": origin.get(\"container_path\")\n                        or str(root_path),\n                        \"source_path\": origin.get(\"source_path\"),\n                        \"state_file\": str(state_path),\n                        \"updated_at\": state.get(\"updated_at\"),\n                    }\n                )\n    except Exception:\n        return mappings\n\n    return mappings",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__env_truthy_1700": {
      "name": "_env_truthy",
      "type": "function",
      "start_line": 1700,
      "end_line": 1707,
      "content_hash": "5e054045e85b7a16b2c0674ee514864f028024f7",
      "content": "def _env_truthy(name: str, default: bool = False) -> bool:\n    try:\n        v = os.environ.get(name)\n        if v is None:\n            return bool(default)\n        return str(v).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n    except Exception:\n        return bool(default)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__env_int_1710": {
      "name": "_env_int",
      "type": "function",
      "start_line": 1710,
      "end_line": 1720,
      "content_hash": "8729a7eef626c7c4385954892109b3e577f29256",
      "content": "def _env_int(name: str) -> Optional[int]:\n    try:\n        v = os.environ.get(name)\n        if v is None:\n            return None\n        v = str(v).strip()\n        if not v:\n            return None\n        return int(v)\n    except Exception:\n        return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_indexing_config_snapshot_1723": {
      "name": "get_indexing_config_snapshot",
      "type": "function",
      "start_line": 1723,
      "end_line": 1740,
      "content_hash": "d75aeaa43f797a0e7e504d87f66b38545fa64dc1",
      "content": "def get_indexing_config_snapshot() -> Dict[str, Any]:\n    return {\n        \"embedding_model\": os.environ.get(\"EMBEDDING_MODEL\"),\n        \"embedding_provider\": os.environ.get(\"EMBEDDING_PROVIDER\"),\n        \"refrag_mode\": _env_truthy(\"REFRAG_MODE\", False),\n        \"qwen3_embedding_enabled\": _env_truthy(\"QWEN3_EMBEDDING_ENABLED\", False),\n        \"index_semantic_chunks\": _env_truthy(\"INDEX_SEMANTIC_CHUNKS\", True),\n        \"index_micro_chunks\": _env_truthy(\"INDEX_MICRO_CHUNKS\", False),\n        \"micro_chunk_tokens\": _env_int(\"MICRO_CHUNK_TOKENS\"),\n        \"micro_chunk_stride\": _env_int(\"MICRO_CHUNK_STRIDE\"),\n        \"max_micro_chunks_per_file\": _env_int(\"MAX_MICRO_CHUNKS_PER_FILE\"),\n        \"index_chunk_lines\": _env_int(\"INDEX_CHUNK_LINES\"),\n        \"index_chunk_overlap\": _env_int(\"INDEX_CHUNK_OVERLAP\"),\n        \"use_tree_sitter\": _env_truthy(\"USE_TREE_SITTER\", False),\n        \"index_use_enhanced_ast\": _env_truthy(\"INDEX_USE_ENHANCED_AST\", False),\n        \"mini_vec_dim\": _env_int(\"MINI_VEC_DIM\"),\n        \"lex_sparse_mode\": _env_truthy(\"LEX_SPARSE_MODE\", False),\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_compute_indexing_config_hash_1743": {
      "name": "compute_indexing_config_hash",
      "type": "function",
      "start_line": 1743,
      "end_line": 1748,
      "content_hash": "d0a47e66ed6fb17eca6fe066772f3601d28f1961",
      "content": "def compute_indexing_config_hash(cfg: Dict[str, Any]) -> str:\n    try:\n        payload = json.dumps(cfg, sort_keys=True, ensure_ascii=False, separators=(\",\", \":\"))\n    except Exception:\n        payload = str(cfg)\n    return hashlib.sha1(payload.encode(\"utf-8\", errors=\"ignore\")).hexdigest()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_persist_indexing_config_1751": {
      "name": "persist_indexing_config",
      "type": "function",
      "start_line": 1751,
      "end_line": 1793,
      "content_hash": "cf2c393b7e0f8252a4f45a825e7d0fbf3fdab7c5",
      "content": "def persist_indexing_config(\n    *,\n    workspace_path: Optional[str] = None,\n    repo_name: Optional[str] = None,\n    environment: Optional[Dict[str, str]] = None,\n    config: Optional[Dict[str, Any]] = None,\n    config_hash: Optional[str] = None,\n    pending: bool = False,\n) -> WorkspaceState:\n    state = get_workspace_state(workspace_path, repo_name) or {}\n    if not isinstance(state, dict):\n        state = {}\n\n    cfg = config or get_indexing_config_snapshot()\n    cfg_hash = config_hash or compute_indexing_config_hash(cfg)\n    env_snapshot = environment or dict(os.environ)\n\n    updates: Dict[str, Any] = {}\n    if pending:\n        # Avoid clobbering an existing pending snapshot (e.g. staging pending env)\n        # unless the caller explicitly provides an override.\n        if config is not None or state.get(\"indexing_config_pending\") is None:\n            updates[\"indexing_config_pending\"] = cfg\n            updates[\"indexing_config_pending_hash\"] = cfg_hash\n        if environment is not None or state.get(\"indexing_env_pending\") is None:\n            updates[\"indexing_env_pending\"] = env_snapshot\n    else:\n        updates[\"indexing_config\"] = cfg\n        updates[\"indexing_config_hash\"] = cfg_hash\n        # Only overwrite indexing_env when explicitly provided or missing.\n        # This prevents background services (watcher/indexer) from clobbering an\n        # already persisted env snapshot (including staging-promoted env).\n        if environment is not None or not state.get(\"indexing_env\"):\n            updates[\"indexing_env\"] = env_snapshot\n        updates[\"indexing_config_pending\"] = None\n        updates[\"indexing_config_pending_hash\"] = None\n        updates[\"indexing_env_pending\"] = None\n\n    return update_workspace_state(\n        workspace_path=workspace_path,\n        repo_name=repo_name,\n        updates=updates,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_find_collection_for_logical_repo_1796": {
      "name": "find_collection_for_logical_repo",
      "type": "function",
      "start_line": 1796,
      "end_line": 1852,
      "content_hash": "c6c618ae12bbe373eef87af026416bb2af3fdf3c",
      "content": "def find_collection_for_logical_repo(logical_repo_id: str, search_root: Optional[str] = None) -> Optional[str]:\n    if not logical_repo_reuse_enabled():\n        return None\n\n    root_path = Path(search_root or _resolve_workspace_root()).resolve()\n\n    try:\n        if is_multi_repo_mode():\n            repos_root = root_path / STATE_DIRNAME / \"repos\"\n            if repos_root.exists():\n                for repo_dir in repos_root.iterdir():\n                    if not repo_dir.is_dir():\n                        continue\n                    state_path = repo_dir / STATE_FILENAME\n                    if not state_path.exists():\n                        continue\n                    try:\n                        with open(state_path, \"r\", encoding=\"utf-8-sig\") as f:\n                            state = json.load(f) or {}\n                    except Exception:\n                        continue\n\n                    ws = state.get(\"workspace_path\") or str(root_path)\n                    state = ensure_logical_repo_id(state, ws)\n                    if state.get(\"logical_repo_id\") == logical_repo_id:\n                        coll = state.get(\"qdrant_collection\")\n                        if coll:\n                            try:\n                                _atomic_write_state(state_path, state)\n                            except Exception as e:\n                                print(f\"[workspace_state] Failed to persist logical_repo_id mapping to {state_path}: {e}\")\n                            return coll\n\n        state_path = root_path / STATE_DIRNAME / STATE_FILENAME\n        if state_path.exists():\n            try:\n                with open(state_path, \"r\", encoding=\"utf-8-sig\") as f:\n                    state = json.load(f) or {}\n            except Exception as e:\n                print(f\"[workspace_state] Failed to read workspace state from {state_path}: {e}\")\n                state = {}\n\n            ws = state.get(\"workspace_path\") or str(root_path)\n            state = ensure_logical_repo_id(state, ws)\n            if state.get(\"logical_repo_id\") == logical_repo_id:\n                coll = state.get(\"qdrant_collection\")\n                if coll:\n                    try:\n                        _atomic_write_state(state_path, state)\n                    except Exception as e:\n                        print(f\"[workspace_state] Failed to persist logical_repo_id mapping to {state_path}: {e}\")\n                    return coll\n    except Exception as e:\n        print(f\"[workspace_state] Error while searching collections for logical_repo_id={logical_repo_id}: {e}\")\n        return None\n\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_or_create_collection_for_logical_repo_1855": {
      "name": "get_or_create_collection_for_logical_repo",
      "type": "function",
      "start_line": 1855,
      "end_line": 1932,
      "content_hash": "1b2da1f7a6c62417766dd7eab647dbb637865eb4",
      "content": "def get_or_create_collection_for_logical_repo(\n    workspace_path: str,\n    preferred_repo_name: Optional[str] = None,\n) -> str:\n    # Gate entire logical-repo based resolution behind feature flag\n    if not logical_repo_reuse_enabled():\n        base_repo = preferred_repo_name\n        try:\n            coll = get_collection_name(base_repo)\n        except Exception:\n            coll = get_collection_name(None)\n        try:\n            update_workspace_state(\n                workspace_path=workspace_path,\n                updates={\"qdrant_collection\": coll},\n                repo_name=preferred_repo_name,\n            )\n        except Exception as e:\n            print(f\"[workspace_state] Failed to persist legacy qdrant_collection for {workspace_path}: {e}\")\n        return coll\n    try:\n        ws = Path(workspace_path).resolve()\n    except Exception:\n        ws = Path(workspace_path)\n\n    common = _detect_git_common_dir(ws)\n    if common is not None:\n        canonical_root = common.parent\n    else:\n        canonical_root = ws\n\n    ws_path = str(canonical_root)\n\n    try:\n        state = get_workspace_state(workspace_path=ws_path, repo_name=preferred_repo_name)\n    except Exception:\n        state = {}\n\n    if not isinstance(state, dict):\n        state = {}\n\n    try:\n        state = ensure_logical_repo_id(state, ws_path)\n    except Exception:\n        pass\n\n    lrid = state.get(\"logical_repo_id\")\n    if isinstance(lrid, str) and lrid:\n        coll = find_collection_for_logical_repo(lrid, search_root=ws_path)\n        if isinstance(coll, str) and coll:\n            if state.get(\"qdrant_collection\") != coll:\n                try:\n                    update_workspace_state(\n                        workspace_path=ws_path,\n                        updates={\"qdrant_collection\": coll, \"logical_repo_id\": lrid},\n                        repo_name=preferred_repo_name,\n                    )\n                except Exception:\n                    pass\n            return coll\n\n    coll = state.get(\"qdrant_collection\")\n    if not isinstance(coll, str) or not coll:\n        base_repo = preferred_repo_name\n        try:\n            coll = get_collection_name(base_repo)\n        except Exception:\n            coll = get_collection_name(None)\n        try:\n            update_workspace_state(\n                workspace_path=ws_path,\n                updates={\"qdrant_collection\": coll},\n                repo_name=preferred_repo_name,\n            )\n        except Exception:\n            pass\n\n    return coll",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_symbol_cache_path_1937": {
      "name": "_get_symbol_cache_path",
      "type": "function",
      "start_line": 1937,
      "end_line": 1951,
      "content_hash": "92d06ee403d78873d95dec69117b2bdf8743d990",
      "content": "def _get_symbol_cache_path(file_path: str) -> Path:\n    \"\"\"Get symbol cache file path for a given file.\"\"\"\n    try:\n        fp = _normalize_cache_key_path(file_path)\n        # Create symbol cache using file hash to handle renames\n        file_hash = hashlib.md5(fp.encode('utf-8')).hexdigest()[:8]\n        if is_multi_repo_mode():\n            repo_name = _detect_repo_name_from_path(Path(file_path))\n            if repo_name:\n                state_dir = _get_repo_state_dir(repo_name)\n                return state_dir / \"symbols\" / f\"{file_hash}.json\"\n        return _get_cache_path(_resolve_workspace_root()).parent / \"symbols\" / f\"{file_hash}.json\"\n    except Exception:\n        # Fallback: use file name\n        return _get_cache_path(_resolve_workspace_root()).parent / \"symbols\" / f\"{Path(file_path).name}.json\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_cached_symbols_1954": {
      "name": "get_cached_symbols",
      "type": "function",
      "start_line": 1954,
      "end_line": 1966,
      "content_hash": "283d41070e40f646a274ce157a52f84443a5b86b",
      "content": "def get_cached_symbols(file_path: str) -> dict:\n    \"\"\"Load cached symbol metadata for a file.\"\"\"\n    cache_path = _get_symbol_cache_path(file_path)\n\n    if not cache_path.exists():\n        return {}\n\n    try:\n        with open(cache_path, 'r', encoding='utf-8-sig') as f:\n            cache_data = json.load(f)\n            return cache_data.get(\"symbols\", {})\n    except Exception:\n        return {}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_set_cached_symbols_1969": {
      "name": "set_cached_symbols",
      "type": "function",
      "start_line": 1969,
      "end_line": 1993,
      "content_hash": "42de30a261487cbac3807f154189235c72d5016c",
      "content": "def set_cached_symbols(file_path: str, symbols: dict, file_hash: str) -> None:\n    \"\"\"Save symbol metadata for a file. Extends existing to include pseudo data.\"\"\"\n    cache_path = _get_symbol_cache_path(file_path)\n    cache_path.parent.mkdir(parents=True, exist_ok=True)\n\n    try:\n        cache_data = {\n            \"file_path\": str(file_path),\n            \"file_hash\": file_hash,\n            \"updated_at\": datetime.now().isoformat(),\n            \"symbols\": symbols\n        }\n\n        with open(cache_path, 'w', encoding='utf-8') as f:\n            json.dump(cache_data, f, indent=2)\n\n        # Ensure symbol cache files are group-writable so both indexer and\n        # watcher processes (potentially different users sharing a group)\n        # can update them on shared volumes.\n        try:\n            os.chmod(cache_path, 0o664)\n        except PermissionError:\n            pass\n    except Exception as e:\n        print(f\"[SYMBOL_CACHE_WARNING] Failed to save symbol cache for {file_path}: {e}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_cached_pseudo_1996": {
      "name": "get_cached_pseudo",
      "type": "function",
      "start_line": 1996,
      "end_line": 2022,
      "content_hash": "b072de9bb6699071223146bb7b57c8653521c480",
      "content": "def get_cached_pseudo(file_path: str, symbol_id: str) -> tuple[str, list[str]]:\n    \"\"\"Load cached pseudo description and tags for a specific symbol.\n\n    Returns:\n        (pseudo, tags) tuple, or (\"\", []) if not found\n    \"\"\"\n    cached_symbols = get_cached_symbols(file_path)\n\n    if symbol_id in cached_symbols:\n        symbol_info = cached_symbols[symbol_id]\n        pseudo = symbol_info.get(\"pseudo\", \"\")\n        tags = symbol_info.get(\"tags\", [])\n\n        # Ensure correct types\n        if isinstance(pseudo, str):\n            pseudo = pseudo\n        else:\n            pseudo = \"\"\n\n        if isinstance(tags, list):\n            tags = [str(tag) for tag in tags]\n        else:\n            tags = []\n\n        return pseudo, tags\n\n    return \"\", []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_set_cached_pseudo_2025": {
      "name": "set_cached_pseudo",
      "type": "function",
      "start_line": 2025,
      "end_line": 2041,
      "content_hash": "b6dbfcba75074b3e35234d132aa36f3d88100034",
      "content": "def set_cached_pseudo(file_path: str, symbol_id: str, pseudo: str, tags: list[str], file_hash: str) -> None:\n    \"\"\"Update pseudo data for a specific symbol in the cache.\n\n    This function updates only the pseudo data without recreating the entire symbol cache,\n    making it efficient for incremental updates during indexing.\n    \"\"\"\n    cached_symbols = get_cached_symbols(file_path)\n\n    # Update the symbol with pseudo data\n    if symbol_id in cached_symbols:\n        cached_symbols[symbol_id][\"pseudo\"] = pseudo\n        cached_symbols[symbol_id][\"tags\"] = tags\n\n        # Save the updated cache only when we actually have symbol entries, to\n        # avoid creating empty symbol cache files before the base symbol set\n        # has been seeded by the indexer/smart reindex path.\n        set_cached_symbols(file_path, cached_symbols, file_hash)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_update_symbols_with_pseudo_2044": {
      "name": "update_symbols_with_pseudo",
      "type": "function",
      "start_line": 2044,
      "end_line": 2066,
      "content_hash": "385087ba72268752ce7b854358f3bd2e8d8ee4fe",
      "content": "def update_symbols_with_pseudo(file_path: str, symbols_with_pseudo: dict, file_hash: str) -> None:\n    \"\"\"Update symbols cache with pseudo data for multiple symbols at once.\n\n    Args:\n        file_path: Path to the file\n        symbols_with_pseudo: Dict mapping symbol_id to (symbol_info, pseudo, tags) tuples\n        file_hash: Current file hash\n    \"\"\"\n    cached_symbols = get_cached_symbols(file_path)\n\n    # Update symbols with their new pseudo data\n    for symbol_id, (symbol_info, pseudo, tags) in symbols_with_pseudo.items():\n        if symbol_id in cached_symbols:\n            # Update existing symbol with pseudo data\n            cached_symbols[symbol_id][\"pseudo\"] = pseudo\n            cached_symbols[symbol_id][\"tags\"] = tags\n\n            # Update content hash from symbol_info if available\n            if isinstance(symbol_info, dict):\n                cached_symbols[symbol_id].update(symbol_info)\n\n    # Save the updated cache\n    set_cached_symbols(file_path, cached_symbols, file_hash)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_remove_cached_symbols_2069": {
      "name": "remove_cached_symbols",
      "type": "function",
      "start_line": 2069,
      "end_line": 2076,
      "content_hash": "1c86ade54cc8a37f47e5ee9e17f2c0a4c919d3aa",
      "content": "def remove_cached_symbols(file_path: str) -> None:\n    \"\"\"Remove symbol cache for a file (when file is deleted).\"\"\"\n    cache_path = _get_symbol_cache_path(file_path)\n    try:\n        if cache_path.exists():\n            cache_path.unlink()\n    except Exception:\n        pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_clear_symbol_cache_2079": {
      "name": "clear_symbol_cache",
      "type": "function",
      "start_line": 2079,
      "end_line": 2132,
      "content_hash": "0e3d00875a349bc04159fc485dea9868a3b92881",
      "content": "def clear_symbol_cache(\n    workspace_path: Optional[str] = None,\n    repo_name: Optional[str] = None,\n) -> int:\n    \"\"\"\n    Clear symbol cache files for a workspace/repo.\n\n    Returns the number of symbol cache directories removed.\n    \"\"\"\n    dirs_removed = 0\n    workspace_root = workspace_path or _resolve_workspace_root()\n\n    target_dirs: List[Path] = []\n    if is_multi_repo_mode() and repo_name:\n        target_dirs.append(_get_repo_state_dir(repo_name) / \"symbols\")\n    else:\n        try:\n            cache_parent = _get_cache_path(workspace_root).parent\n        except Exception:\n            cache_parent = Path(workspace_root) / \".codebase\"\n        target_dirs.append(cache_parent / \"symbols\")\n\n    for symbols_dir in target_dirs:\n        if not symbols_dir.exists():\n            continue\n        for cache_file in symbols_dir.glob(\"*.json\"):\n            file_path = \"\"\n            try:\n                with cache_file.open(\"r\", encoding=\"utf-8-sig\") as f:\n                    data = json.load(f)\n                file_path = str(data.get(\"file_path\") or \"\")\n            except Exception:\n                file_path = \"\"\n            if file_path:\n                remove_cached_symbols(file_path)\n            else:\n                try:\n                    cache_file.unlink()\n                except Exception:\n                    pass\n\n        # Best-effort cleanup of empty symbols directory\n        try:\n            next(symbols_dir.iterdir())\n        except StopIteration:\n            try:\n                symbols_dir.rmdir()\n                dirs_removed += 1\n            except Exception:\n                pass\n        except Exception:\n            pass\n\n    return dirs_removed",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_compare_symbol_changes_2135": {
      "name": "compare_symbol_changes",
      "type": "function",
      "start_line": 2135,
      "end_line": 2157,
      "content_hash": "ad0c8b177f205c2e63c00c5c1358d74546ad3a0f",
      "content": "def compare_symbol_changes(old_symbols: dict, new_symbols: dict) -> tuple[list, list]:\n    \"\"\"\n    Compare old and new symbols to identify changes.\n\n    Returns:\n        (unchanged_symbols, changed_symbols)\n    \"\"\"\n    unchanged = []\n    changed = []\n\n    for symbol_id, symbol_info in new_symbols.items():\n        if symbol_id in old_symbols:\n            old_info = old_symbols[symbol_id]\n            # Compare content hash\n            if old_info.get(\"content_hash\") == symbol_info.get(\"content_hash\"):\n                unchanged.append(symbol_id)\n            else:\n                changed.append(symbol_id)\n        else:\n            # New symbol\n            changed.append(symbol_id)\n\n    return unchanged, changed",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_list_workspaces_2160": {
      "name": "list_workspaces",
      "type": "function",
      "start_line": 2160,
      "end_line": 2295,
      "content_hash": "894322d1347ad019509bce350ab51ef7d77da8c9",
      "content": "def list_workspaces(\n    search_root: Optional[str] = None,\n    use_qdrant_fallback: bool = True,\n) -> List[Dict[str, Any]]:\n    \"\"\"Scan for workspaces via local filesystem or Qdrant collections.\n\n    Supports both local/mounted and remote client-server scenarios:\n    - Local: Scans filesystem for .codebase/state.json files\n    - Remote: Falls back to querying Qdrant collections for workspace metadata\n\n    Args:\n        search_root: Directory to scan for local mode; defaults to parent of /work.\n        use_qdrant_fallback: If True and no local workspaces found, query Qdrant.\n\n    Returns:\n        List of workspace info dicts with keys:\n        - workspace_path: str\n        - collection_name: str\n        - last_updated: str or int (ISO timestamp or unix)\n        - indexing_state: str\n        - source: \"local\" or \"qdrant\" (indicates discovery method)\n    \"\"\"\n    if search_root is None:\n        # Default to parent of workspace root\n        try:\n            search_root = str(Path(_resolve_workspace_root()).parent)\n        except Exception:\n            search_root = \"/work\"\n\n    root_path = Path(search_root).resolve()\n    workspaces: List[Dict[str, Any]] = []\n    seen_paths: set = set()\n\n    # --- Local filesystem scan ---\n    try:\n        # Find all state.json files\n        for state_file in root_path.rglob(f\"{STATE_DIRNAME}/{STATE_FILENAME}\"):\n            try:\n                # Skip if in repos subdirectory (multi-repo per-repo states)\n                if \"repos\" in state_file.parts:\n                    continue\n\n                workspace_path = str(state_file.parent.parent.resolve())\n\n                # Skip duplicates\n                if workspace_path in seen_paths:\n                    continue\n                seen_paths.add(workspace_path)\n\n                # Read state file\n                with open(state_file, \"r\", encoding=\"utf-8-sig\") as f:\n                    state = json.load(f)\n\n                if not isinstance(state, dict):\n                    continue\n\n                # Extract info\n                collection_name = state.get(\"qdrant_collection\", \"\")\n                updated_at = state.get(\"updated_at\", \"\")\n\n                indexing_status = state.get(\"indexing_status\", {})\n                if isinstance(indexing_status, dict):\n                    indexing_state = indexing_status.get(\"state\", \"unknown\")\n                else:\n                    indexing_state = \"unknown\"\n\n                workspaces.append({\n                    \"workspace_path\": workspace_path,\n                    \"collection_name\": collection_name,\n                    \"last_updated\": updated_at,\n                    \"indexing_state\": indexing_state,\n                    \"source\": \"local\",\n                })\n            except Exception:\n                continue\n\n        # Also check multi-repo states\n        if is_multi_repo_mode():\n            repos_root = root_path / STATE_DIRNAME / \"repos\"\n            if repos_root.exists():\n                for repo_dir in repos_root.iterdir():\n                    if not repo_dir.is_dir():\n                        continue\n                    state_file = repo_dir / STATE_FILENAME\n                    if not state_file.exists():\n                        continue\n                    try:\n                        with open(state_file, \"r\", encoding=\"utf-8-sig\") as f:\n                            state = json.load(f)\n\n                        if not isinstance(state, dict):\n                            continue\n\n                        repo_name = repo_dir.name\n                        workspace_path = state.get(\"workspace_path\", str(root_path / repo_name))\n\n                        if workspace_path in seen_paths:\n                            continue\n                        seen_paths.add(workspace_path)\n\n                        collection_name = state.get(\"qdrant_collection\", \"\")\n                        updated_at = state.get(\"updated_at\", \"\")\n\n                        indexing_status = state.get(\"indexing_status\", {})\n                        if isinstance(indexing_status, dict):\n                            indexing_state = indexing_status.get(\"state\", \"unknown\")\n                        else:\n                            indexing_state = \"unknown\"\n\n                        workspaces.append({\n                            \"workspace_path\": workspace_path,\n                            \"collection_name\": collection_name,\n                            \"last_updated\": updated_at,\n                            \"indexing_state\": indexing_state,\n                            \"repo_name\": repo_name,\n                            \"source\": \"local\",\n                        })\n                    except Exception:\n                        continue\n    except Exception:\n        pass\n\n    # --- Qdrant fallback for remote scenarios ---\n    if not workspaces and use_qdrant_fallback:\n        try:\n            workspaces = _list_workspaces_from_qdrant(seen_paths)\n        except Exception:\n            pass\n\n    # Sort by last_updated descending\n    try:\n        workspaces.sort(key=lambda w: w.get(\"last_updated\", \"\"), reverse=True)\n    except Exception:\n        pass\n\n    return workspaces",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__list_workspaces_from_qdrant_2298": {
      "name": "_list_workspaces_from_qdrant",
      "type": "function",
      "start_line": 2298,
      "end_line": 2393,
      "content_hash": "ce19cce43de5ea82f76f94a047709800da1a7883",
      "content": "def _list_workspaces_from_qdrant(seen_paths: set) -> List[Dict[str, Any]]:\n    \"\"\"Query Qdrant collections to discover workspaces (for remote scenarios).\n\n    Samples points from each collection to extract workspace metadata.\n    \"\"\"\n    workspaces: List[Dict[str, Any]] = []\n\n    try:\n        from qdrant_client import QdrantClient\n    except ImportError:\n        return workspaces\n\n    qdrant_url = os.environ.get(\"QDRANT_URL\", \"http://localhost:6333\")\n    qdrant_key = os.environ.get(\"QDRANT_API_KEY\")\n\n    try:\n        client = QdrantClient(\n            url=qdrant_url,\n            api_key=qdrant_key,\n            timeout=float(os.environ.get(\"QDRANT_TIMEOUT\", \"10\") or 10),\n        )\n\n        # List all collections\n        collections = client.get_collections().collections\n\n        for coll in collections:\n            coll_name = coll.name\n            if not coll_name:\n                continue\n\n            # Sample a few points to extract workspace metadata\n            try:\n                points, _ = client.scroll(\n                    collection_name=coll_name,\n                    limit=5,\n                    with_payload=True,\n                    with_vectors=False,\n                )\n\n                if not points:\n                    continue\n\n                # Extract workspace info from sampled points\n                workspace_path = None\n                repo_name = None\n                last_ingested = None\n\n                for pt in points:\n                    payload = getattr(pt, \"payload\", {}) or {}\n                    md = payload.get(\"metadata\", {}) or {}\n\n                    # Try to get workspace path from metadata\n                    if not workspace_path:\n                        workspace_path = (\n                            md.get(\"workspace_path\")\n                            or md.get(\"source_root\")\n                            or payload.get(\"workspace_path\")\n                        )\n\n                    # Try to get repo name\n                    if not repo_name:\n                        repo_name = md.get(\"repo\") or md.get(\"repo_name\")\n\n                    # Get ingestion timestamp\n                    ts = md.get(\"ingested_at\") or payload.get(\"ingested_at\")\n                    if ts and (last_ingested is None or ts > last_ingested):\n                        last_ingested = ts\n\n                # Build workspace entry\n                ws_path = workspace_path or f\"/work/{repo_name}\" if repo_name else f\"[{coll_name}]\"\n\n                if ws_path in seen_paths:\n                    continue\n                seen_paths.add(ws_path)\n\n                workspaces.append({\n                    \"workspace_path\": ws_path,\n                    \"collection_name\": coll_name,\n                    \"last_updated\": last_ingested or \"\",\n                    \"indexing_state\": \"indexed\",  # If points exist, it's indexed\n                    \"repo_name\": repo_name or \"\",\n                    \"source\": \"qdrant\",\n                })\n            except Exception:\n                # Collection exists but couldn't sample - still report it\n                workspaces.append({\n                    \"workspace_path\": f\"[{coll_name}]\",\n                    \"collection_name\": coll_name,\n                    \"last_updated\": \"\",\n                    \"indexing_state\": \"unknown\",\n                    \"source\": \"qdrant\",\n                })\n    except Exception:\n        pass\n\n    return workspaces",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}