{
  "file_path": "/work/context-engine/scripts/hybrid_search.py",
  "file_hash": "0ce80558159dd5e5cc585f70b44a699d3e494c84",
  "updated_at": "2025-12-26T17:34:20.260732",
  "symbols": {
    "function__compile_regex_310": {
      "name": "_compile_regex",
      "type": "function",
      "start_line": 310,
      "end_line": 312,
      "content_hash": "6ffe2fe924e20c49af54a305512f09e9661ce943",
      "content": "def _compile_regex(pattern: str, flags: int = 0):\n    \"\"\"Cached regex compilation for repeated patterns.\"\"\"\n    return re.compile(pattern, flags)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__split_ident_lex_317": {
      "name": "_split_ident_lex",
      "type": "function",
      "start_line": 317,
      "end_line": 326,
      "content_hash": "1d8a335c7773458403ba8e22ddf2898e95acd850",
      "content": "def _split_ident_lex(s: str) -> List[str]:\n    \"\"\"Split identifier into tokens (snake_case and camelCase aware).\"\"\"\n    parts = re.split(r\"[^A-Za-z0-9]+\", s)\n    out: List[str] = []\n    for p in parts:\n        if not p:\n            continue\n        segs = re.findall(r\"[A-Z]?[a-z]+|[A-Z]+(?![a-z])|\\d+\", p)\n        out.extend([x for x in segs if x])\n    return [x.lower() for x in out if x and x.lower() not in _STOP]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_run_hybrid_search_336": {
      "name": "run_hybrid_search",
      "type": "function",
      "start_line": 336,
      "end_line": 1956,
      "content_hash": "46fc1597e38954a12dd12f999ebbb7aa0ad1758f",
      "content": "def run_hybrid_search(\n    queries: List[str],\n    limit: int = 10,\n    per_path: int = 1,\n    language: str | None = None,\n    under: str | None = None,\n    kind: str | None = None,\n    symbol: str | None = None,\n    ext: str | None = None,\n    not_filter: str | None = None,\n    case: str | None = None,\n    path_regex: str | None = None,\n    path_glob: str | list[str] | None = None,\n    not_glob: str | list[str] | None = None,\n    expand: bool = True,\n    model: Any = None,\n    collection: str | None = None,\n    mode: str | None = None,\n    repo: str | list[str] | None = None,  # Filter by repo name(s); \"*\" to disable auto-filter\n    per_query: int | None = None,  # Base candidate retrieval per query (default: adaptive)\n) -> List[Dict[str, Any]]:\n    client = QdrantClient(url=os.environ.get(\"QDRANT_URL\", QDRANT_URL), api_key=API_KEY)\n    model_name = os.environ.get(\"EMBEDDING_MODEL\", MODEL_NAME)\n    if model:\n        _model = model\n    elif _EMBEDDER_FACTORY:\n        _model = _get_embedding_model(model_name)\n    else:\n        _model = TextEmbedding(model_name=model_name)\n    vec_name = _sanitize_vector_name(model_name)\n\n    # Parse Query DSL and merge with explicit args\n    raw_queries = list(queries)\n    clean_queries, dsl = parse_query_dsl(raw_queries)\n    eff_language = language or dsl.get(\"language\")\n    eff_under = under or dsl.get(\"under\")\n    eff_kind = kind or dsl.get(\"kind\")\n    eff_symbol = symbol or dsl.get(\"symbol\")\n    eff_ext = ext or dsl.get(\"ext\")\n    eff_not = not_filter or dsl.get(\"not\")\n    eff_case = case or dsl.get(\"case\") or os.environ.get(\"HYBRID_CASE\", \"insensitive\")\n    # Repo filter: explicit param > DSL > auto-detect from env\n    eff_repo = repo or dsl.get(\"repo\")\n    # Normalize repo to list for multi-repo support\n    if eff_repo and isinstance(eff_repo, str):\n        if eff_repo.strip() == \"*\":\n            eff_repo = None  # \"*\" means search all repos\n        else:\n            eff_repo = [r.strip() for r in eff_repo.split(\",\") if r.strip()]\n    elif eff_repo and isinstance(eff_repo, (list, tuple)):\n        eff_repo = [str(r).strip() for r in eff_repo if str(r).strip() and str(r).strip() != \"*\"]\n        if not eff_repo:\n            eff_repo = None\n    # Auto-detect repo from env if not specified and auto-filter is enabled\n    if eff_repo is None and str(os.environ.get(\"REPO_AUTO_FILTER\", \"1\")).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}:\n        auto_repo = os.environ.get(\"CURRENT_REPO\") or os.environ.get(\"REPO_NAME\")\n        if auto_repo and auto_repo.strip():\n            eff_repo = [auto_repo.strip()]\n    eff_path_regex = path_regex\n\n    def _to_list(x):\n        if x is None:\n            return []\n        if isinstance(x, (list, tuple)):\n            out = []\n            for e in x:\n                s = str(e).strip()\n                if s:\n                    out.append(s)\n            return out\n        s = str(x).strip()\n        return [s] if s else []\n\n    eff_path_globs = _to_list(path_glob)\n    eff_not_globs = _to_list(not_glob)\n\n    # Normalize glob patterns: allow repo-relative (e.g., \"src/*.py\") to match\n    # stored absolute paths (e.g., \"/work/src/...\"). We keep both original and\n    # absolute-prefixed variants for matching.\n    def _normalize_globs(globs: list[str]) -> list[str]:\n        out: list[str] = []\n        try:\n            for g in (globs or []):\n                s = str(g).strip().replace(\"\\\\\", \"/\")\n                if not s:\n                    continue\n                out.append(s)\n                if not s.startswith(\"/\"):\n                    # /work/<slug>/... is common; include both direct /work and slug-aware variants\n                    stripped = s.lstrip(\"/\")\n                    out.append(\"/work/\" + stripped)\n                    out.append(\"/work/*/\" + stripped)\n        except Exception:\n            pass\n        # Dedup while preserving order\n        seen = set()\n        dedup: list[str] = []\n        for g in out:\n            if g not in seen:\n                dedup.append(g)\n                seen.add(g)\n        return dedup\n\n    eff_path_globs_norm = _normalize_globs(eff_path_globs)\n    eff_not_globs_norm = _normalize_globs(eff_not_globs)\n\n    # Normalize under\n    def _norm_under(u: str | None) -> str | None:\n        if not u:\n            return None\n        u = str(u).strip().replace(\"\\\\\", \"/\")\n        u = \"/\".join([p for p in u.split(\"/\") if p])\n        if not u:\n            return None\n        if not u.startswith(\"/\"):\n            v = \"/work/\" + u\n        else:\n            v = \"/work/\" + u.lstrip(\"/\") if not u.startswith(\"/work/\") else u\n        return v\n\n    eff_under = _norm_under(eff_under)\n\n    # Expansion knobs that affect query construction/results (must be part of cache key)\n    try:\n        llm_max = int(os.environ.get(\"LLM_EXPAND_MAX\", \"0\") or 0)\n    except (ValueError, TypeError):\n        llm_max = 0\n    try:\n        _semantic_enabled = _env_truthy(os.environ.get(\"SEMANTIC_EXPANSION_ENABLED\"), True)\n    except Exception:\n        _semantic_enabled = True\n    try:\n        _semantic_max_terms = int(os.environ.get(\"SEMANTIC_EXPANSION_MAX_TERMS\", \"3\") or 3)\n    except (ValueError, TypeError):\n        _semantic_max_terms = 3\n    _code_signal_syms = os.environ.get(\"CODE_SIGNAL_SYMBOLS\", \"\").strip()\n\n    # Results cache: return cached results for identical (queries, filters, knobs)\n    _USE_CACHE = (MAX_RESULTS_CACHE > 0) and _env_truthy(os.environ.get(\"HYBRID_RESULTS_CACHE_ENABLED\"), True)\n    cache_key = None\n    if _USE_CACHE:\n        try:\n            cache_key = (\n                \"v2\",\n                tuple(clean_queries),\n                int(limit or 0),\n                int(per_path or 0),\n                str(eff_language or \"\"),\n                str(eff_under or \"\"),\n                str(eff_kind or \"\"),\n                str(eff_symbol or \"\"),\n                str(eff_ext or \"\"),\n                str(eff_not or \"\"),\n                str(eff_case or \"\"),\n                tuple(eff_path_globs_norm or ()),\n                tuple(eff_not_globs_norm or ()),\n                str(eff_repo or \"\"),\n                str(eff_path_regex or \"\"),\n                bool(expand),\n                int(llm_max),\n                bool(_semantic_enabled),\n                int(_semantic_max_terms),\n                str(_code_signal_syms),\n                str(vec_name),\n                str(_collection()),\n                _env_truthy(os.environ.get(\"HYBRID_ADAPTIVE_WEIGHTS\"), True),\n                _env_truthy(os.environ.get(\"HYBRID_MMR\"), True),\n                str(mode or \"\"),\n            )\n        except Exception:\n            cache_key = None\n        if cache_key is not None:\n            if UNIFIED_CACHE_AVAILABLE:\n                val = _RESULTS_CACHE.get(cache_key)\n                if val is not None:\n                    if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                        logger.debug(\"cache hit for hybrid results (unified)\")\n                    return val\n                # Fallback to local in-process dict to ensure deterministic hits (esp. in unit tests)\n                try:\n                    with _RESULTS_LOCK:\n                        if cache_key in _RESULTS_CACHE_OD:\n                            if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                                logger.debug(\"cache hit for hybrid results (fallback OD)\")\n                            return _RESULTS_CACHE_OD[cache_key]\n                except Exception:\n                    pass\n            else:\n                with _RESULTS_LOCK:\n                    if cache_key in _RESULTS_CACHE:\n                        val = _RESULTS_CACHE.pop(cache_key)\n                        _RESULTS_CACHE[cache_key] = val\n                        if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                            logger.debug(\"cache hit for hybrid results (legacy)\")\n                        return val\n\n    # Build optional filter\n    flt = None\n    must = []\n    if eff_language:\n        must.append(\n            models.FieldCondition(\n                key=\"metadata.language\", match=models.MatchValue(value=eff_language)\n            )\n        )\n    # Repo filter: supports single repo or list of repos (for related codebases)\n    if eff_repo:\n        if isinstance(eff_repo, list) and len(eff_repo) == 1:\n            must.append(\n                models.FieldCondition(\n                    key=\"metadata.repo\", match=models.MatchValue(value=eff_repo[0])\n                )\n            )\n        elif isinstance(eff_repo, list) and len(eff_repo) > 1:\n            # Multiple repos: use MatchAny for OR logic\n            must.append(\n                models.FieldCondition(\n                    key=\"metadata.repo\", match=models.MatchAny(any=eff_repo)\n                )\n            )\n        elif isinstance(eff_repo, str):\n            must.append(\n                models.FieldCondition(\n                    key=\"metadata.repo\", match=models.MatchValue(value=eff_repo)\n                )\n            )\n    if eff_under:\n        must.append(\n            models.FieldCondition(\n                key=\"metadata.path_prefix\", match=models.MatchValue(value=eff_under)\n            )\n        )\n    if eff_kind:\n        must.append(\n            models.FieldCondition(\n                key=\"metadata.kind\", match=models.MatchValue(value=eff_kind)\n            )\n        )\n    if eff_symbol:\n        must.append(\n            models.FieldCondition(\n                key=\"metadata.symbol\", match=models.MatchValue(value=eff_symbol)\n            )\n        )\n\n    # After attempting cache get, run deduplication. If duplicate, serve cached result if present.\n    if DEDUPLICATION_AVAILABLE:\n        request_data = {\n            'queries': queries,\n            'limit': limit,\n            'per_path': per_path,\n            'language': language,\n            'under': under,\n            'kind': kind,\n            'symbol': symbol,\n            'ext': ext,\n            'not': not_filter,\n            'case': case,\n            'path_regex': path_regex,\n            'path_glob': path_glob,\n            'not_glob': not_glob,\n            'expand': expand,\n            'collection': _collection(),\n            'vector_name': vec_name,\n            'mode': mode,\n        }\n        is_duplicate, similar_fp = is_duplicate_request(request_data)\n        if is_duplicate:\n            # Prefer serving from cache on duplicate\n            if cache_key is not None:\n                if UNIFIED_CACHE_AVAILABLE:\n                    val = _RESULTS_CACHE.get(cache_key)\n                    if val is not None:\n                        if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                            logger.debug(\"duplicate served from cache (unified)\")\n                        return val\n                    try:\n                        with _RESULTS_LOCK:\n                            if cache_key in _RESULTS_CACHE_OD:\n                                if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                                    logger.debug(\"duplicate served from cache (fallback OD)\")\n                                return _RESULTS_CACHE_OD[cache_key]\n                    except Exception:\n                        pass\n                else:\n                    with _RESULTS_LOCK:\n                        if cache_key in _RESULTS_CACHE:\n                            val = _RESULTS_CACHE.pop(cache_key)\n                            _RESULTS_CACHE[cache_key] = val\n                            if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                                logger.debug(\"duplicate served from cache (legacy)\")\n                            return val\n            if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                logger.debug(\"Duplicate without cache; bypassing dedup and continuing search\")\n\n\n    # Add ext filter (file extension) - server-side when possible\n    if eff_ext:\n        ext_clean = eff_ext.lower().lstrip(\".\")\n        must.append(\n            models.FieldCondition(\n                key=\"metadata.ext\", match=models.MatchValue(value=ext_clean)\n            )\n        )\n\n    # Add not filter (simple text exclusion on path) - server-side when possible\n    must_not = []\n    if eff_not:\n        # Try MatchText for substring exclusion; fallback to post-filter if unsupported\n        try:\n            must_not.append(\n                models.FieldCondition(\n                    key=\"metadata.path\", match=models.MatchText(text=eff_not)\n                )\n            )\n        except Exception:\n            # Will be handled by post-filter\n            pass\n\n    flt = models.Filter(must=must, must_not=must_not) if (must or must_not) else None\n    flt = _sanitize_filter_obj(flt)\n\n\n    # Build query list (LLM-assisted first, then synonym expansion)\n    qlist = list(clean_queries)\n\n    # Filter-only mode: derive implicit queries from DSL tokens\n    # e.g., \"lang:python\" -> add \"python\" as a query term for ranking\n    if not qlist or (len(qlist) == 1 and not qlist[0].strip()):\n        qlist = []  # Start fresh\n        if eff_language:\n            qlist.append(eff_language)\n        if eff_symbol:\n            qlist.append(eff_symbol)\n        if eff_kind:\n            qlist.append(eff_kind)\n        if eff_ext:\n            # Add extension without dot as query term\n            ext_term = eff_ext.lstrip(\".\")\n            if ext_term:\n                qlist.append(ext_term)\n        if eff_under:\n            # Add path segments as query terms\n            parts = [p for p in str(eff_under).replace(\"\\\\\", \"/\").split(\"/\") if p]\n            for p in parts[-2:]:  # Last 2 path segments\n                if p and p not in qlist:\n                    qlist.append(p)\n    if llm_max > 0:\n        _llm_more = _llm_expand_queries(qlist, eff_language, max_new=llm_max)\n        for s in _llm_more:\n            if s and s not in qlist:\n                qlist.append(s)\n    if expand:\n        # Use enhanced expansion with semantic similarity if available\n        if SEMANTIC_EXPANSION_AVAILABLE:\n            qlist = expand_queries_enhanced(\n                qlist, eff_language,\n                max_extra=max(2, _semantic_max_terms),\n                client=client,\n                model=_model,\n                collection=_collection()\n            )\n        else:\n            qlist = expand_queries(qlist, eff_language)\n\n    # Query sharpening: derive basename tokens from path_glob to steer retrieval/gating\n    try:\n        if eff_path_globs or eff_path_globs_norm:\n            def _bn(p: str) -> str:\n                s = str(p or \"\").replace(\"\\\\\", \"/\").strip()\n                # drop any trailing slashes and take last segment\n                parts = [t for t in s.split(\"/\") if t]\n                return parts[-1] if parts else \"\"\n            globs_src = list(eff_path_globs or []) + list(eff_path_globs_norm or [])\n            basenames = []\n            for g in globs_src:\n                b = _bn(g)\n                if b and b not in basenames:\n                    basenames.append(b)\n            for b in basenames:\n                if b and b not in qlist:\n                    qlist.append(b)\n                # also add stem (filename without extension) as a lexical hint\n                stem = b.rsplit(\".\", 1)[0] if \".\" in b else b\n                if stem and stem not in qlist:\n                    qlist.append(stem)\n            # Add short path segments (e.g., \"scripts/hybrid_search\") to steer lexical hashing\n            for g in globs_src:\n                s = str(g or \"\").replace(\"\\\\\", \"/\").strip()\n                parts = [t for t in s.split(\"/\") if t]\n                if len(parts) >= 2:\n                    last2 = \"/\".join(parts[-2:])\n                    if last2 and last2 not in qlist:\n                        qlist.append(last2)\n                if len(parts) >= 3:\n                    last3 = \"/\".join(parts[-3:])\n                    if last3 and last3 not in qlist:\n                        qlist.append(last3)\n    except Exception:\n        pass\n\n    # --- Code signal symbols: add extracted symbols from query analysis ---\n    # These are passed via CODE_SIGNAL_SYMBOLS env var from repo_search\n    try:\n        _code_signal_syms = os.environ.get(\"CODE_SIGNAL_SYMBOLS\", \"\").strip()\n        if _code_signal_syms:\n            for sym in _code_signal_syms.split(\",\"):\n                sym = sym.strip()\n                if sym and len(sym) > 1 and sym not in qlist:\n                    qlist.append(sym)\n    except Exception:\n        pass\n\n    # === Large codebase scaling (automatic) ===\n    _coll_stats = _get_collection_stats(client, _collection(collection))\n    _coll_size = _coll_stats.get(\"points_count\", 0)\n    _has_filters = bool(eff_language or eff_repo or eff_under or eff_kind or eff_symbol or eff_ext)\n\n    # Scale RRF k for better score discrimination at scale\n    _scaled_rrf_k = _scale_rrf_k(RRF_K, _coll_size)\n\n    # Adaptive per_query: retrieve more candidates from larger collections\n    # Use explicit per_query if provided, otherwise compute adaptively\n    _base_per_query = per_query if per_query is not None else max(24, limit)\n    _scaled_per_query = _adaptive_per_query(_base_per_query, _coll_size, _has_filters)\n\n    if os.environ.get(\"DEBUG_HYBRID_SEARCH\") and _coll_size >= LARGE_COLLECTION_THRESHOLD:\n        logger.debug(f\"Large collection scaling: size={_coll_size}, rrf_k={_scaled_rrf_k}, per_query={_scaled_per_query}\")\n\n    # Local RRF function using scaled k\n    def _scaled_rrf(rank: int) -> float:\n        return 1.0 / (_scaled_rrf_k + rank)\n\n    # Lexical vector query (with scaled retrieval)\n    # Use sparse vectors when LEX_SPARSE_MODE is enabled for lossless matching\n    score_map: Dict[str, Dict[str, Any]] = {}\n    _used_sparse_lex = False  # Track if we actually used sparse (for scoring)\n    try:\n        if LEX_SPARSE_MODE:\n            sparse_vec = lex_sparse_vector(qlist)\n            lex_results = sparse_lex_query(client, sparse_vec, flt, _scaled_per_query, collection)\n            # Fallback to dense lex if sparse returned empty (collection may not have sparse index)\n            if not lex_results:\n                lex_vec = lex_hash_vector(qlist)\n                lex_results = lex_query(client, lex_vec, flt, _scaled_per_query, collection)\n                if lex_results:\n                    logger.debug(\"LEX_SPARSE_MODE enabled but sparse query returned empty; fell back to dense lex\")\n            else:\n                _used_sparse_lex = True  # Actually used sparse vectors\n        else:\n            lex_vec = lex_hash_vector(qlist)\n            lex_results = lex_query(client, lex_vec, flt, _scaled_per_query, collection)\n    except Exception as e:\n        # On sparse query failure, try falling back to dense lex\n        if LEX_SPARSE_MODE:\n            try:\n                lex_vec = lex_hash_vector(qlist)\n                lex_results = lex_query(client, lex_vec, flt, _scaled_per_query, collection)\n                logger.warning(\"LEX_SPARSE_MODE sparse query failed (%s); fell back to dense lex\", e)\n            except Exception:\n                lex_results = []\n        else:\n            lex_results = []\n\n    # Per-query adaptive weights (default ON, gentle clamps)\n    _USE_ADAPT = _env_truthy(os.environ.get(\"HYBRID_ADAPTIVE_WEIGHTS\"), True)\n    if _USE_ADAPT:\n        try:\n            _AD_DENSE_W, _AD_LEX_VEC_W, _AD_LEX_TEXT_W = _adaptive_weights(_compute_query_stats(qlist))\n        except Exception:\n            _AD_DENSE_W, _AD_LEX_VEC_W, _AD_LEX_TEXT_W = DENSE_WEIGHT, LEX_VECTOR_WEIGHT, LEXICAL_WEIGHT\n    else:\n        _AD_DENSE_W, _AD_LEX_VEC_W, _AD_LEX_TEXT_W = DENSE_WEIGHT, LEX_VECTOR_WEIGHT, LEXICAL_WEIGHT\n\n    for rank, p in enumerate(lex_results, 1):\n        pid = str(p.id)\n        score_map.setdefault(\n            pid,\n            {\n                \"pt\": p,\n                \"s\": 0.0,\n                \"d\": 0.0,\n                \"lx\": 0.0,\n                \"sym_sub\": 0.0,\n                \"sym_eq\": 0.0,\n                \"core\": 0.0,\n                \"vendor\": 0.0,\n                \"langb\": 0.0,\n                \"rec\": 0.0,\n                \"test\": 0.0,\n            },\n        )\n        # Sparse vectors: use actual similarity score (preserves match quality signal)\n        # Dense vectors: use RRF rank (backwards compatible)\n        if _used_sparse_lex:\n            _lex_w = _AD_LEX_VEC_W if _USE_ADAPT else LEX_VECTOR_WEIGHT\n            lxs = sparse_lex_score(float(getattr(p, 'score', 0) or 0), weight=_lex_w)\n        else:\n            lxs = (_AD_LEX_VEC_W * _scaled_rrf(rank)) if _USE_ADAPT else (LEX_VECTOR_WEIGHT * _scaled_rrf(rank))\n        score_map[pid][\"lx\"] += lxs\n        score_map[pid][\"s\"] += lxs\n\n    # Dense queries - filter out empty strings for filter-only mode (e.g., \"lang:python\")\n    qlist_for_embed = [q for q in qlist if q and q.strip()]\n    embedded = _embed_queries_cached(_model, qlist_for_embed) if qlist_for_embed else []\n    # Ensure collection schema is compatible with current search settings (named vectors)\n    try:\n        if embedded:\n            dim = len(embedded[0])\n            _ensure_collection(client, _collection(collection), dim, vec_name)\n    except Exception:\n        pass\n    # Optional gate-first using mini vectors to restrict dense search to candidates\n    # Adaptive gating: disable for short/ambiguous queries to avoid over-filtering\n    flt_gated = flt\n    try:\n        gate_first = str(os.environ.get(\"REFRAG_GATE_FIRST\", \"0\")).strip().lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }\n        refrag_on = str(os.environ.get(\"REFRAG_MODE\", \"\")).strip().lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }\n        cand_n = int(os.environ.get(\"REFRAG_CANDIDATES\", \"200\") or 200)\n    except (ValueError, TypeError):\n        gate_first, refrag_on, cand_n = False, False, 200\n\n    # Adaptive mini-gate: disable for queries that are too short or lack strong identifiers\n    should_bypass_gate = False\n    if gate_first and refrag_on:\n        # Check query characteristics\n        try:\n            # Count total tokens across queries\n            total_tokens = sum(len(_split_ident(q)) for q in qlist)\n            # Check for strong identifiers (ALL_CAPS, camelCase, or has underscore)\n            has_strong_id = any(\n                any(t.isupper() or \"_\" in t or any(c.isupper() for c in t[1:]) and any(c.islower() for c in t)\n                    for t in _split_ident(q))\n                for q in qlist\n            )\n            # If query is very short (<3 tokens) and has no strong identifiers, bypass gate\n            if total_tokens < 3 and not has_strong_id:\n                should_bypass_gate = True\n                if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                    logger.debug(f\"Adaptive gate bypass (short query, tokens={total_tokens}, strong_id={has_strong_id})\")\n            # If we have strict filters (language, under, symbol, ext), relax candidate count\n            if eff_language or eff_under or eff_symbol or eff_ext:\n                cand_n = max(cand_n, limit * 5)\n                if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                    logger.debug(f\"Adaptive gate relaxed candidate count to {cand_n} due to filters\")\n        except Exception:\n            pass\n\n    _gate_first_ran = False\n    if gate_first and refrag_on and not should_bypass_gate:\n        try:\n            # ReFRAG gate-first: Use MINI vectors to prefilter candidates\n            mini_queries = [_project_mini(list(v), MINI_VEC_DIM) for v in embedded]\n\n            # Get top candidates using MINI vectors (fast prefilter)\n            candidate_ids = set()\n            for mv in mini_queries:\n                mini_results = dense_query(client, MINI_VECTOR_NAME, mv, flt, cand_n, collection)\n                for result in mini_results:\n                    if hasattr(result, 'id'):\n                        candidate_ids.add(result.id)\n\n            if candidate_ids:\n                # Server-side gating without requiring payload fields: prefer HasIdCondition\n                from qdrant_client import models as _models\n                try:\n                    gating_cond = _models.HasIdCondition(has_id=list(candidate_ids))\n                    gating_kind = \"has_id\"\n                except Exception:\n                    # Fallback to pid_str if HasIdCondition unavailable\n                    id_vals = [str(cid) for cid in candidate_ids]\n                    gating_cond = _models.FieldCondition(\n                        key=\"pid_str\",\n                        match=_models.MatchAny(any=id_vals),\n                    )\n                    gating_kind = \"pid_str\"\n                if flt is None:\n                    flt_gated = _models.Filter(must=[gating_cond])\n                else:\n                    must = list(flt.must or [])\n                    must.append(gating_cond)\n                    flt_gated = _models.Filter(must=must, should=flt.should, must_not=flt.must_not)\n                if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                    logger.debug(f\"ReFRAG gate-first (server-side-{gating_kind}): {len(candidate_ids)} candidates\")\n                    logger.debug(f\"flt_gated.must has {len(flt_gated.must or [])} conditions\")\n                    logger.debug(f\"flt_gated.must_not has {len(flt_gated.must_not or [])} conditions\")\n            else:\n                # No candidates -> no gating\n                flt_gated = flt\n            # Mark gate-first as successful only after all logic completes\n            _gate_first_ran = True\n        except Exception as e:\n            if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                logger.debug(f\"ReFRAG gate-first failed: {e}, proceeding without gating\")\n            # Fallback to normal search (no gating)\n            flt_gated = flt\n    else:\n        flt_gated = flt\n\n    # Sanitize filter: if empty, drop it to avoid Qdrant 400s on invalid filters\n    try:\n        if flt_gated is not None:\n            _m = [c for c in (getattr(flt_gated, \"must\", None) or []) if c is not None]\n            _s = [c for c in (getattr(flt_gated, \"should\", None) or []) if c is not None]\n            _mn = [c for c in (getattr(flt_gated, \"must_not\", None) or []) if c is not None]\n            if not _m and not _s and not _mn:\n                flt_gated = None\n    except Exception:\n        pass\n\n    flt_gated = _sanitize_filter_obj(flt_gated)\n\n    # Parallel dense query execution for multiple queries (threshold >= 4 to avoid thread overhead for small N)\n    try:\n        _parallel_threshold = int(os.environ.get(\"PARALLEL_DENSE_THRESHOLD\", \"4\") or 4)\n    except (ValueError, TypeError):\n        logger.warning(\n            \"Invalid PARALLEL_DENSE_THRESHOLD value %r, using default 4\",\n            os.environ.get(\"PARALLEL_DENSE_THRESHOLD\"),\n        )\n        _parallel_threshold = 4\n    if len(embedded) >= _parallel_threshold and os.environ.get(\"PARALLEL_DENSE_QUERIES\", \"1\") == \"1\":\n        executor = _get_query_executor()\n        futures = [\n            executor.submit(\n                dense_query,\n                client,\n                vec_name,\n                v,\n                flt_gated,\n                _scaled_per_query,\n                collection,\n                queries[i] if i < len(queries) else None,\n            )\n            for i, v in enumerate(embedded)\n        ]\n        result_sets: List[List[Any]] = [f.result() for f in futures]\n    else:\n        result_sets: List[List[Any]] = [\n            dense_query(\n                client,\n                vec_name,\n                v,\n                flt_gated,\n                _scaled_per_query,\n                collection,\n                query_text=queries[i] if i < len(queries) else None,\n            )\n            for i, v in enumerate(embedded)\n        ]\n    if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n        total_dense_results = sum(len(rs) for rs in result_sets)\n        logger.debug(f\"Dense query returned {total_dense_results} total results across {len(result_sets)} queries\")\n\n    # Optional ReFRAG-style mini-vector gating: add compact-vector RRF if enabled\n    try:\n        if not _gate_first_ran and os.environ.get(\"REFRAG_MODE\", \"\").strip().lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }:\n            try:\n                mini_queries = [_project_mini(list(v), MINI_VEC_DIM) for v in embedded]\n                mini_sets: List[List[Any]] = [\n                    dense_query(client, MINI_VECTOR_NAME, mv, flt, _scaled_per_query, collection)\n                    for mv in mini_queries\n                ]\n                for res in mini_sets:\n                    for rank, p in enumerate(res, 1):\n                        pid = str(p.id)\n                        score_map.setdefault(\n                            pid,\n                            {\n                                \"pt\": p,\n                                \"s\": 0.0,\n                                \"d\": 0.0,\n                                \"lx\": 0.0,\n                                \"sym_sub\": 0.0,\n                                \"sym_eq\": 0.0,\n                                \"core\": 0.0,\n                                \"vendor\": 0.0,\n                                \"langb\": 0.0,\n                                \"rec\": 0.0,\n                                \"test\": 0.0,\n                            },\n                        )\n                        dens = float(HYBRID_MINI_WEIGHT) * _scaled_rrf(rank)\n                        score_map[pid][\"d\"] += dens\n                        score_map[pid][\"s\"] += dens\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    # Enhanced PRF with semantic similarity\n    if SEMANTIC_EXPANSION_AVAILABLE and score_map:\n        # Local PRF dense weight (fallback if not set later)\n        try:\n            prf_dw = float(os.environ.get(\"PRF_DENSE_WEIGHT\", \"0.4\") or 0.4)\n        except Exception:\n            prf_dw = 0.4\n\n        try:\n            # Get top results for PRF context (sorted by score, not arbitrary dict order)\n            sorted_items = sorted(score_map.items(), key=lambda x: x[1][\"s\"], reverse=True)\n            top_results = [rec[\"pt\"] for _, rec in sorted_items[:8]]\n\n            if top_results:\n                semantic_prf_terms = expand_queries_with_prf(\n                    clean_queries, top_results, _model, max_terms=4\n                )\n\n                # Create PRF queries using semantic terms\n                semantic_prf_qs = []\n                for term in semantic_prf_terms:\n                    base = clean_queries[0] if clean_queries else (qlist[0] if qlist else \"\")\n                    cand = (base + \" \" + term).strip()\n                    if cand and cand not in qlist and cand not in semantic_prf_qs:\n                        semantic_prf_qs.append(cand)\n                        if len(semantic_prf_qs) >= 3:  # Limit semantic PRF queries\n                            break\n\n                if semantic_prf_qs:\n                    # Dense semantic PRF pass\n                    embedded_sem_prf = _embed_queries_cached(_model, semantic_prf_qs)\n                    result_sets_sem_prf: List[List[Any]] = [\n                        dense_query(client, vec_name, v, flt, max(8, limit // 3 or 4), collection)\n                        for v in embedded_sem_prf\n                    ]\n                    for res_sem_prf in result_sets_sem_prf:\n                        for rank, p in enumerate(res_sem_prf, 1):\n                            pid = str(p.id)\n                            score_map.setdefault(\n                                pid,\n                                {\n                                    \"pt\": p,\n                                    \"s\": 0.0,\n                                    \"d\": 0.0,\n                                    \"lx\": 0.0,\n                                    \"sym_sub\": 0.0,\n                                    \"sym_eq\": 0.0,\n                                    \"core\": 0.0,\n                                    \"vendor\": 0.0,\n                                    \"langb\": 0.0,\n                                    \"rec\": 0.0,\n                                    \"test\": 0.0,\n                                },\n                            )\n                            # Lower weight for semantic PRF to avoid over-diversification\n                            dens = 0.3 * prf_dw * _scaled_rrf(rank)\n                            score_map[pid][\"d\"] += dens\n                            score_map[pid][\"s\"] += dens\n\n                    if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                        logger.debug(f\"Semantic PRF added {len(semantic_prf_qs)} queries with terms: {semantic_prf_terms}\")\n\n        except Exception as e:\n            if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                logger.debug(f\"Semantic PRF failed: {e}\")\n\n    lex_results2: List[Any] = []\n\n    # Pseudo-Relevance Feedback (default-on): mine top terms from current results and run a light second pass\n    try:\n        prf_enabled = _env_truthy(os.environ.get(\"PRF_ENABLED\"), True)\n    except (ValueError, TypeError):\n        prf_enabled = True\n\n    # Lightweight BM25-style lexical boost (default ON)\n    try:\n        _USE_BM25 = _env_truthy(os.environ.get(\"HYBRID_BM25\"), True)\n    except Exception:\n        _USE_BM25 = True\n    try:\n        _BM25_W = float(os.environ.get(\"HYBRID_BM25_WEIGHT\", \"0.2\") or 0.2)\n    except Exception:\n        _BM25_W = 0.2\n    _bm25_tok_w = _bm25_token_weights_from_results(qlist, (lex_results or []) + (lex_results2 or [])) if _USE_BM25 else {}\n\n    if prf_enabled and score_map:\n        try:\n            top_docs = int(os.environ.get(\"PRF_TOP_DOCS\", \"8\") or 8)\n        except (ValueError, TypeError):\n            top_docs = 8\n        try:\n            max_terms = int(os.environ.get(\"PRF_MAX_TERMS\", \"6\") or 6)\n        except (ValueError, TypeError):\n            max_terms = 6\n        try:\n            extra_q = int(os.environ.get(\"PRF_EXTRA_QUERIES\", \"4\") or 4)\n        except (ValueError, TypeError):\n            extra_q = 4\n        try:\n            prf_dw = float(os.environ.get(\"PRF_DENSE_WEIGHT\", \"0.4\") or 0.4)\n        except (ValueError, TypeError):\n            prf_dw = 0.4\n        try:\n            prf_lw = float(os.environ.get(\"PRF_LEX_WEIGHT\", \"0.6\") or 0.6)\n        except (ValueError, TypeError):\n            prf_lw = 0.6\n        terms = _prf_terms_from_results(\n            score_map, top_docs=top_docs, max_terms=max_terms\n        )\n        base = clean_queries[0] if clean_queries else (qlist[0] if qlist else \"\")\n        prf_qs: List[str] = []\n        for t in terms:\n            cand = (base + \" \" + t).strip()\n            if cand and cand not in qlist and cand not in prf_qs:\n                prf_qs.append(cand)\n                if len(prf_qs) >= extra_q:\n                    break\n        if prf_qs:\n            # Lexical PRF pass (use sparse when enabled, with fallback)\n            _prf_limit = max(12, limit // 2 or 6)\n            try:\n                if LEX_SPARSE_MODE:\n                    sparse_vec2 = lex_sparse_vector(prf_qs)\n                    lex_results2 = sparse_lex_query(client, sparse_vec2, flt, _prf_limit, collection)\n                    if not lex_results2:\n                        lex_vec2 = lex_hash_vector(prf_qs)\n                        lex_results2 = lex_query(client, lex_vec2, flt, _prf_limit, collection)\n                else:\n                    lex_vec2 = lex_hash_vector(prf_qs)\n                    lex_results2 = lex_query(client, lex_vec2, flt, _prf_limit, collection)\n            except Exception:\n                if LEX_SPARSE_MODE:\n                    try:\n                        lex_vec2 = lex_hash_vector(prf_qs)\n                        lex_results2 = lex_query(client, lex_vec2, flt, _prf_limit, collection)\n                    except Exception:\n                        lex_results2 = []\n                else:\n                    lex_results2 = []\n            for rank, p in enumerate(lex_results2, 1):\n                pid = str(p.id)\n                score_map.setdefault(\n                    pid,\n                    {\n                        \"pt\": p,\n                        \"s\": 0.0,\n                        \"d\": 0.0,\n                        \"lx\": 0.0,\n                        \"sym_sub\": 0.0,\n                        \"sym_eq\": 0.0,\n                        \"core\": 0.0,\n                        \"vendor\": 0.0,\n                        \"langb\": 0.0,\n                        \"rec\": 0.0,\n                        \"test\": 0.0,\n                    },\n                )\n                lxs = prf_lw * _scaled_rrf(rank)\n                score_map[pid][\"lx\"] += lxs\n                score_map[pid][\"s\"] += lxs\n            # Dense PRF pass\n            try:\n                embedded2 = _embed_queries_cached(_model, prf_qs)\n                _prf_per_query = max(12, _scaled_per_query // 2)\n                result_sets2: List[List[Any]] = [\n                    dense_query(client, vec_name, v, flt, _prf_per_query, collection)\n                    for v in embedded2\n                ]\n                for res2 in result_sets2:\n                    for rank, p in enumerate(res2, 1):\n                        pid = str(p.id)\n                        score_map.setdefault(\n                            pid,\n                            {\n                                \"pt\": p,\n                                \"s\": 0.0,\n                                \"d\": 0.0,\n                                \"lx\": 0.0,\n                                \"sym_sub\": 0.0,\n                                \"sym_eq\": 0.0,\n                                \"core\": 0.0,\n                                \"vendor\": 0.0,\n                                \"langb\": 0.0,\n                                \"rec\": 0.0,\n                                \"test\": 0.0,\n                            },\n                        )\n                        dens = prf_dw * _scaled_rrf(rank)\n                        score_map[pid][\"d\"] += dens\n                        score_map[pid][\"s\"] += dens\n            except Exception:\n                pass\n\n    # Add dense scores (with scaled RRF)\n    for res in result_sets:\n        for rank, p in enumerate(res, 1):\n            pid = str(p.id)\n            score_map.setdefault(\n                pid,\n                {\n                    \"pt\": p,\n                    \"s\": 0.0,\n                    \"d\": 0.0,\n                    \"lx\": 0.0,\n                    \"sym_sub\": 0.0,\n                    \"sym_eq\": 0.0,\n                    \"core\": 0.0,\n                    \"vendor\": 0.0,\n                    \"langb\": 0.0,\n                    \"rec\": 0.0,\n                    \"test\": 0.0,\n                },\n            )\n            dens = (_AD_DENSE_W * _scaled_rrf(rank)) if _USE_ADAPT else (DENSE_WEIGHT * _scaled_rrf(rank))\n            score_map[pid][\"d\"] += dens\n            score_map[pid][\"s\"] += dens\n\n    # Lexical + boosts\n    timestamps: List[int] = []\n    # Mode-aware tweaks for implementation/docs weighting. Modes:\n    # - None / \"code_first\": full IMPLEMENTATION_BOOST and DOCUMENTATION_PENALTY\n    # - \"balanced\": keep impl boost, halve doc penalty\n    # - \"docs_first\": reduce impl boost slightly and disable doc penalty\n    eff_mode = (mode or \"\").strip().lower()\n    impl_boost = IMPLEMENTATION_BOOST\n    doc_penalty = DOCUMENTATION_PENALTY\n    test_penalty = TEST_FILE_PENALTY\n    # Query intent detection: boost implementation files more when query signals code search\n    if _detect_implementation_intent(qlist):\n        impl_boost += INTENT_IMPL_BOOST\n        # Also increase test/doc penalties when user clearly wants implementation\n        test_penalty += INTENT_IMPL_BOOST\n        doc_penalty += INTENT_IMPL_BOOST * 0.5\n    if eff_mode in {\"balanced\"}:\n        doc_penalty = DOCUMENTATION_PENALTY * 0.5\n    elif eff_mode in {\"docs_first\", \"docs-first\", \"docs\"}:\n        impl_boost = IMPLEMENTATION_BOOST * 0.5\n        doc_penalty = 0.0\n    for pid, rec in list(score_map.items()):\n        payload = rec[\"pt\"].payload or {}\n        base_md = payload.get(\"metadata\") or {}\n        # Merge top-level pseudo/tags into the view passed to lexical_score so\n        # HYBRID_PSEUDO_BOOST can see index-time GLM/llamacpp labels.\n        md = dict(base_md)\n        if \"pseudo\" in payload:\n            md[\"pseudo\"] = payload[\"pseudo\"]\n        if \"tags\" in payload:\n            md[\"tags\"] = payload[\"tags\"]\n\n        lx = (_AD_LEX_TEXT_W * lexical_score(qlist, md, token_weights=_bm25_tok_w, bm25_weight=_BM25_W)) if _USE_ADAPT else (LEXICAL_WEIGHT * lexical_score(qlist, md, token_weights=_bm25_tok_w, bm25_weight=_BM25_W))\n        rec[\"lx\"] += lx\n        rec[\"s\"] += lx\n        ts = md.get(\"last_modified_at\") or md.get(\"ingested_at\")\n        if isinstance(ts, int):\n            timestamps.append(ts)\n        sym = str(md.get(\"symbol\") or \"\").lower()\n        sym_path = str(md.get(\"symbol_path\") or \"\").lower()\n        sym_text = f\"{sym} {sym_path}\"\n        # Pre-split symbol into parts for token-level matching (camelCase/snake_case)\n        sym_parts = set(p.lower() for p in _split_ident(md.get(\"symbol\") or \"\") if len(p) >= 2)\n        for q in qlist:\n            ql = q.lower()\n            if not ql:\n                continue\n            if ql in sym_text:\n                rec[\"sym_sub\"] += SYMBOL_BOOST\n                rec[\"s\"] += SYMBOL_BOOST\n            # Exact match: full symbol OR any split part matches query\n            if ql == sym or ql == sym_path or ql in sym_parts:\n                rec[\"sym_eq\"] += SYMBOL_EQUALITY_BOOST\n                rec[\"s\"] += SYMBOL_EQUALITY_BOOST\n        path = str(md.get(\"path\") or \"\")\n        # Filename match boost: query matches file basename or stem parts\n        if path:\n            basename = path.rsplit(\"/\", 1)[-1].lower()\n            stem = basename.rsplit(\".\", 1)[0] if \".\" in basename else basename\n            stem_parts = set(p.lower() for p in _split_ident(stem) if len(p) >= 2)\n            for q in qlist:\n                ql = q.lower()\n                if ql and len(ql) >= 3 and (ql == stem or ql in stem_parts or ql in basename):\n                    rec[\"sym_eq\"] += SYMBOL_EQUALITY_BOOST * 0.5\n                    rec[\"s\"] += SYMBOL_EQUALITY_BOOST * 0.5\n                    break\n        if CORE_FILE_BOOST > 0.0 and path and is_core_file(path):\n            rec[\"core\"] += CORE_FILE_BOOST\n            rec[\"s\"] += CORE_FILE_BOOST\n        if VENDOR_PENALTY > 0.0 and path and is_vendor_path(path):\n            rec[\"vendor\"] -= VENDOR_PENALTY\n            rec[\"s\"] -= VENDOR_PENALTY\n        if test_penalty > 0.0 and path and is_test_file(path):\n            rec[\"test\"] -= test_penalty\n            rec[\"s\"] -= test_penalty\n\n        # Additional file-type weighting\n        path_lower = path.lower()\n        ext = (\".\" + path_lower.rsplit(\".\", 1)[-1]) if \".\" in path_lower else \"\"\n        # Penalize config/metadata files\n        if CONFIG_FILE_PENALTY > 0.0 and path:\n            if ext in {\".json\", \".yml\", \".yaml\", \".toml\", \".ini\"} or \"/.codebase/\" in path_lower or \"/.kiro/\" in path_lower:\n                rec[\"cfg\"] = float(rec.get(\"cfg\", 0.0)) - CONFIG_FILE_PENALTY\n                rec[\"s\"] -= CONFIG_FILE_PENALTY\n        # Boost likely implementation files (mode-aware)\n        if impl_boost > 0.0 and path:\n            if ext in {\".py\", \".ts\", \".tsx\", \".js\", \".jsx\", \".go\", \".java\", \".rs\", \".rb\", \".php\", \".cs\", \".cpp\", \".c\", \".hpp\", \".h\"}:\n                rec[\"impl\"] = float(rec.get(\"impl\", 0.0)) + impl_boost\n                rec[\"s\"] += impl_boost\n        # Penalize docs (README/docs/markdown) relative to implementation files (mode-aware)\n        if doc_penalty > 0.0 and path:\n            if (\n                \"readme\" in path_lower\n                or \"/docs/\" in path_lower\n                or \"/documentation/\" in path_lower\n                or path_lower.endswith(\".md\")\n            ):\n                rec[\"doc\"] = float(rec.get(\"doc\", 0.0)) - doc_penalty\n                rec[\"s\"] -= doc_penalty\n\n        if LANG_MATCH_BOOST > 0.0 and path and eff_language:\n            lang = str(eff_language).lower()\n            md_lang = str((md.get(\"language\") or \"\").lower())\n            if (lang and md_lang and md_lang == lang) or lang_matches_path(lang, path):\n                rec[\"langb\"] += LANG_MATCH_BOOST\n                rec[\"s\"] += LANG_MATCH_BOOST\n\n        # Memory blending: apply penalty to memory-like entries to prevent swamping code results\n        # Only apply if query doesn't explicitly ask for memories/notes\n        kind = str(md.get(\"kind\") or \"\").lower()\n        if kind == \"memory\":\n            qlow = \" \".join(qlist).lower()\n            is_memory_query = any(w in qlow for w in [\"remember\", \"note\", \"recall\", \"memo\", \"stored\"])\n            if not is_memory_query:\n                # Apply penalty and cap at 1 memory result unless explicitly requested\n                memory_penalty = float(os.environ.get(\"HYBRID_MEMORY_PENALTY\", \"0.15\") or 0.15)\n                rec[\"mem_penalty\"] = float(rec.get(\"mem_penalty\", 0.0)) - memory_penalty\n                rec[\"s\"] -= memory_penalty\n                # Simple lexical overlap check: drop memories with no query token overlap\n                try:\n                    text_lower = str(md.get(\"text\") or md.get(\"information\") or \"\").lower()\n                    query_tokens = set()\n                    for q in qlist:\n                        query_tokens.update(_split_ident(q))\n                    has_overlap = any(t in text_lower for t in query_tokens if len(t) >= 3)\n                    if not has_overlap:\n                        # Strong penalty for irrelevant memories\n                        rec[\"mem_penalty\"] -= 0.5\n                        rec[\"s\"] -= 0.5\n                except Exception:\n                    pass\n\n    if timestamps and RECENCY_WEIGHT > 0.0:\n        tmin, tmax = min(timestamps), max(timestamps)\n        span = max(1, tmax - tmin)\n        for rec in score_map.values():\n            md = (rec[\"pt\"].payload or {}).get(\"metadata\") or {}\n            ts = md.get(\"last_modified_at\") or md.get(\"ingested_at\")\n            if isinstance(ts, int):\n                norm = (ts - tmin) / span\n                rec_comp = RECENCY_WEIGHT * norm\n                rec[\"rec\"] += rec_comp\n                rec[\"s\"] += rec_comp\n\n    # === Large codebase score normalization ===\n    # Spread compressed score distributions for better discrimination\n    _normalize_scores(score_map, _coll_size)\n\n    def _tie_key(m: Dict[str, Any]):\n        md = (m[\"pt\"].payload or {}).get(\"metadata\") or {}\n        sp = str(md.get(\"symbol_path\") or md.get(\"symbol\") or \"\")\n        path = str(md.get(\"path\") or \"\")\n        start_line = int(md.get(\"start_line\") or 0)\n        return (-float(m[\"s\"]), len(sp), path, start_line)\n\n    if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n        logger.debug(f\"score_map has {len(score_map)} items before ranking (coll_size={_coll_size})\")\n    ranked = sorted(score_map.values(), key=_tie_key)\n    if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n        logger.debug(f\"ranked has {len(ranked)} items after sorting\")\n\n    # Lightweight keyword bump: prefer spans whose local snippet contains query tokens\n    try:\n        kb = float(os.environ.get(\"HYBRID_KEYWORD_BUMP\", \"0.3\") or 0.3)\n        kcap = float(os.environ.get(\"HYBRID_KEYWORD_CAP\", \"0.6\") or 0.6)\n    except Exception:\n        kb, kcap = 0.3, 0.6\n    # Build lowercase keyword set from queries (simple split, keep >=3 chars + special tokens)\n    kw: set[str] = set()\n    for q in qlist:\n        ql = (q or \"\").lower()\n        for tok in re.findall(r\"[a-zA-Z0-9_\\-]+\", ql):\n            t = tok.strip()\n            if len(t) >= 3:\n                kw.add(t)\n\n    import io as _io\n\n    def _snippet_contains(md: dict) -> int:\n        # returns number of keyword hits found in a small local snippet\n        try:\n            path = str(md.get(\"path\") or \"\")\n            sline = int(md.get(\"start_line\") or 0)\n            eline = int(md.get(\"end_line\") or 0)\n            txt = (md.get(\"text\") or md.get(\"code\") or \"\")\n            if not txt and path and sline:\n                p = path\n                try:\n                    if not os.path.isabs(p):\n                        p = os.path.join(\"/work\", p)\n                    realp = os.path.realpath(p)\n                    if realp == \"/work\" or realp.startswith(\"/work/\"):\n                        with open(realp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                            lines = f.readlines()\n                        si = max(1, sline - 3)\n                        ei = min(len(lines), max(sline, eline) + 3)\n                        txt = \"\".join(lines[si-1:ei])\n                except Exception:\n                    txt = txt or \"\"\n            lt = (txt or \"\").lower()\n            if not lt:\n                return 0\n            hits = 0\n            for t in kw:\n                if t and t in lt:\n                    hits += 1\n            return hits\n        except Exception:\n            return 0\n\n    def _snippet_comment_ratio(md: dict) -> float:\n        # Estimate fraction of non-blank lines that are comments (language-agnostic heuristics)\n        try:\n            path = str(md.get(\"path\") or \"\")\n            sline = int(md.get(\"start_line\") or 0)\n            eline = int(md.get(\"end_line\") or 0)\n            txt = (md.get(\"text\") or md.get(\"code\") or \"\")\n            if not txt and path and sline:\n                p = path\n                try:\n                    if not os.path.isabs(p):\n                        p = os.path.join(\"/work\", p)\n                    realp = os.path.realpath(p)\n                    if realp == \"/work\" or realp.startswith(\"/work/\"):\n                        with open(realp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                            lines = f.readlines()\n                        si = max(1, sline - 3)\n                        ei = min(len(lines), max(sline, eline) + 3)\n                        txt = \"\".join(lines[si-1:ei])\n                except Exception:\n                    txt = txt or \"\"\n            if not txt:\n                return 0.0\n            total = 0\n            comment = 0\n            in_block = False\n            for raw in txt.splitlines():\n                line = raw.strip()\n                if not line:\n                    continue\n                total += 1\n                # HTML/XML comments\n                if line.startswith(\"<!--\"):\n                    in_block = True\n                    comment += 1\n                    continue\n                if in_block:\n                    comment += 1\n                    if \"-->\" in line:\n                        in_block = False\n                    continue\n                # C/JS block comments\n                if line.startswith(\"/*\"):\n                    in_block = True\n                    comment += 1\n                    continue\n                if in_block:\n                    comment += 1\n                    if \"*/\" in line:\n                        in_block = False\n                    continue\n                # Single-line comments for many languages\n                if line.startswith(\"//\") or line.startswith(\"#\"):\n                    comment += 1\n                    continue\n                # Python docstring-like lines (treat as comment-ish for ranking)\n                if line.startswith(\"\\\"\\\"\\\"\") or line.startswith(\"'''\"):\n                    comment += 1\n                    continue\n            if total == 0:\n                return 0.0\n            return comment / float(total)\n        except Exception:\n            return 0.0\n\n    # Apply bump to top-N ranked (limited for speed)\n    topN = min(len(ranked), 200)\n    for i in range(topN):\n        m = ranked[i]\n        md = (m[\"pt\"].payload or {}).get(\"metadata\") or {}\n        hits = _snippet_contains(md)\n        if hits > 0 and kb > 0.0:\n            bump = min(kcap, kb * float(hits))\n            m[\"s\"] += bump\n        # Apply comment-heavy penalty to de-emphasize comments/doc blocks\n        try:\n            if COMMENT_PENALTY > 0.0:\n                ratio = _snippet_comment_ratio(md)\n                thr = float(COMMENT_RATIO_THRESHOLD)\n                if ratio >= thr:\n                    scale = (ratio - thr) / max(1e-6, 1.0 - thr)\n                    pen = min(float(COMMENT_PENALTY), float(COMMENT_PENALTY) * max(0.0, scale))\n                    if pen > 0:\n                        m[\"cmt\"] = float(m.get(\"cmt\", 0.0)) - pen\n                        m[\"s\"] -= pen\n        except Exception:\n            pass\n\n    # Re-sort after bump\n    ranked = sorted(ranked, key=_tie_key)\n\n    # Cluster by path adjacency\n    clusters: Dict[str, List[Dict[str, Any]]] = {}\n    for m in ranked:\n        md = (m[\"pt\"].payload or {}).get(\"metadata\") or {}\n        path = str(md.get(\"path\") or \"\")\n        start_line = int(md.get(\"start_line\") or 0)\n        end_line = int(md.get(\"end_line\") or 0)\n        lst = clusters.setdefault(path, [])\n        merged_flag = False\n        for c in lst:\n            if (\n                start_line <= c[\"end\"] + CLUSTER_LINES\n                and end_line >= c[\"start\"] - CLUSTER_LINES\n            ):\n                if float(m[\"s\"]) > float(c[\"m\"][\"s\"]):\n                    c[\"m\"] = m\n                c[\"start\"] = min(c[\"start\"], start_line)\n                c[\"end\"] = max(c[\"end\"], end_line)\n                merged_flag = True\n                break\n        if not merged_flag:\n            lst.append({\"start\": start_line, \"end\": end_line, \"m\": m})\n\n    ranked = sorted([c[\"m\"] for lst in clusters.values() for c in lst], key=_tie_key)\n    if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n        logger.debug(f\"ranked has {len(ranked)} items after clustering\")\n\n\n    # Optional MMR diversification (default ON; preserves top-1)\n    if _env_truthy(os.environ.get(\"HYBRID_MMR\"), True):\n        try:\n            _mmr_k = min(len(ranked), max(20, int(os.environ.get(\"MMR_K\", str((limit or 10) * 3)) or 30)))\n        except Exception:\n            _mmr_k = min(len(ranked), max(20, (limit or 10) * 3))\n        try:\n            _mmr_lambda = float(os.environ.get(\"MMR_LAMBDA\", \"0.7\") or 0.7)\n        except Exception:\n            _mmr_lambda = 0.7\n        if (limit or 0) >= 10 or (not per_path) or (per_path <= 0):\n            ranked = _mmr_diversify(ranked, k=_mmr_k, lambda_=_mmr_lambda)\n\n    # Client-side filters and per-path diversification\n    import re as _re, fnmatch as _fnm\n\n    case_sensitive = str(eff_case or \"\").lower() == \"sensitive\"\n\n    def _match_glob(pat: str, path: str) -> bool:\n        if not pat:\n            return True\n        if case_sensitive:\n            return _fnm.fnmatchcase(path, pat)\n        return _fnm.fnmatchcase(path.lower(), pat.lower())\n\n    if eff_not or eff_path_regex or eff_ext or eff_path_globs or eff_not_globs:\n\n        def _pass_filters(m: Dict[str, Any]) -> bool:\n            md = (m[\"pt\"].payload or {}).get(\"metadata\") or {}\n            path = str(md.get(\"path\") or \"\")\n            rel = path[6:] if path.startswith(\"/work/\") else path\n            pp = str(md.get(\"path_prefix\") or \"\")\n            p_for_sub = path if case_sensitive else path.lower()\n            pp_for_sub = pp if case_sensitive else pp.lower()\n            if eff_not:\n                nn = eff_not if case_sensitive else eff_not.lower()\n                if nn in p_for_sub or nn in pp_for_sub:\n                    return False\n            if eff_not_globs_norm and any(_match_glob(g, path) or _match_glob(g, rel) for g in eff_not_globs_norm):\n                return False\n            if eff_ext:\n                ex = eff_ext.lower().lstrip(\".\")\n                if not path.lower().endswith(\".\" + ex):\n                    return False\n            if eff_path_regex:\n                flags = 0 if case_sensitive else _re.IGNORECASE\n                try:\n                    if not _re.search(eff_path_regex, path, flags=flags):\n                        return False\n                except Exception:\n                    pass\n            if eff_path_globs_norm and not any(_match_glob(g, path) or _match_glob(g, rel) for g in eff_path_globs_norm):\n                return False\n            return True\n\n        ranked = [m for m in ranked if _pass_filters(m)]\n\n    # ReFRAG-lite span compaction and budgeting is NOT applied here in run_hybrid_search\n    # It's only applied in context_answer where token budgeting is needed for LLM context\n    # Removing this to avoid over-filtering search results\n\n    if per_path and per_path > 0:\n        counts: Dict[str, int] = {}\n        merged: List[Dict[str, Any]] = []\n        if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n            logger.debug(f\"Applying per_path={per_path} limiting to {len(ranked)} ranked results\")\n        for m in ranked:\n            md = (m[\"pt\"].payload or {}).get(\"metadata\") or {}\n            path = str(md.get(\"path\", \"\"))\n            c = counts.get(path, 0)\n            if c < per_path:\n                merged.append(m)\n                counts[path] = c + 1\n            if len(merged) >= limit:\n                break\n        if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n            logger.debug(f\"After per_path limiting: {len(merged)} results from {len(counts)} unique paths\")\n    else:\n        merged = ranked[:limit]\n\n    # Emit structured items\n    # Build directory \u2192 paths map for related hints (same dir siblings)\n    dir_to_paths: Dict[str, set] = {}\n    try:\n        for _m in merged:\n            _md = (_m[\"pt\"].payload or {}).get(\"metadata\") or {}\n            _pp = str(_md.get(\"path_prefix\") or \"\")\n            _p = str(_md.get(\"path\") or \"\")\n            if _pp and _p:\n                dir_to_paths.setdefault(_pp, set()).add(_p)\n    except Exception:\n        dir_to_paths = {}\n    # Precompute known paths for quick membership checks\n    all_paths: set = set()\n    try:\n        for _s in dir_to_paths.values():\n            all_paths |= set(_s)\n    except Exception:\n        all_paths = set()\n\n    # Build path -> host_path map so we can emit related_paths in host space\n    # when PATH_EMIT_MODE prefers host paths. This keeps human-facing paths\n    # consistent while still preserving container paths for backend use.\n    host_map: Dict[str, str] = {}\n    try:\n        for _m in merged:\n            _md = (_m[\"pt\"].payload or {}).get(\"metadata\") or {}\n            _p = str(_md.get(\"path\") or \"\").strip()\n            _h = str(_md.get(\"host_path\") or \"\").strip()\n            if _p and _h:\n                host_map[_p] = _h\n    except Exception:\n        host_map = {}\n\n    items: List[Dict[str, Any]] = []\n    if not merged:\n        if _USE_CACHE and cache_key is not None:\n            if UNIFIED_CACHE_AVAILABLE:\n                # Use unified caching system\n                _RESULTS_CACHE.set(cache_key, items)\n                if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                    logger.debug(\"cache store for hybrid results\")\n            else:\n                # Fallback to original caching system\n                with _RESULTS_LOCK:\n                    _RESULTS_CACHE[cache_key] = items\n                    if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                        logger.debug(\"cache store for hybrid results\")\n                    while len(_RESULTS_CACHE) > MAX_RESULTS_CACHE:\n                        _RESULTS_CACHE.popitem(last=False)\n        return items\n\n    for m in merged:\n        md = (m[\"pt\"].payload or {}).get(\"metadata\") or {}\n        # Prefer merged bounds if present\n        start_line = m.get(\"_merged_start\") or md.get(\"start_line\")\n        end_line = m.get(\"_merged_end\") or md.get(\"end_line\")\n        # Store both fusion_score (from hybrid) and rerank_score (if available) separately\n        fusion_score = float(m.get(\"s\", 0.0))\n        rerank_score = m.get(\"rerank_score\")  # None if not reranked\n\n        comp = {\n            \"dense_rrf\": round(float(m.get(\"d\", 0.0)), 4),\n            \"lexical\": round(float(m.get(\"lx\", 0.0)), 4),\n            \"symbol_substr\": round(float(m.get(\"sym_sub\", 0.0)), 4),\n            \"symbol_exact\": round(float(m.get(\"sym_eq\", 0.0)), 4),\n            \"core_boost\": round(float(m.get(\"core\", 0.0)), 4),\n            \"vendor_penalty\": round(float(m.get(\"vendor\", 0.0)), 4),\n            \"lang_boost\": round(float(m.get(\"langb\", 0.0)), 4),\n            \"recency\": round(float(m.get(\"rec\", 0.0)), 4),\n            \"test_penalty\": round(float(m.get(\"test\", 0.0)), 4),\n            # new components\n            \"config_penalty\": round(float(m.get(\"cfg\", 0.0)), 4),\n            \"impl_boost\": round(float(m.get(\"impl\", 0.0)), 4),\n            \"doc_penalty\": round(float(m.get(\"doc\", 0.0)), 4),\n        }\n\n        # Add reranker info to components if present\n        if rerank_score is not None:\n            comp[\"rerank\"] = round(float(rerank_score), 4)\n        # Build \"why\" explanation only if enabled (reduces tokens by default)\n        why = None\n        if INCLUDE_WHY:\n            why = []\n            if comp[\"dense_rrf\"]:\n                why.append(f\"dense_rrf:{comp['dense_rrf']}\")\n            for k in (\"lexical\", \"symbol_substr\", \"symbol_exact\", \"core_boost\", \"lang_boost\", \"impl_boost\"):\n                if comp[k]:\n                    why.append(f\"{k}:{comp[k]}\")\n            if comp[\"vendor_penalty\"]:\n                why.append(f\"vendor_penalty:{comp['vendor_penalty']}\")\n            for k in (\"test_penalty\", \"config_penalty\", \"doc_penalty\"):\n                if comp.get(k):\n                    why.append(f\"{k}:{comp[k]}\")\n            if comp[\"recency\"]:\n                why.append(f\"recency:{comp['recency']}\")\n        # Related hints\n        _imports = md.get(\"imports\") or []\n        _calls = md.get(\"calls\") or []\n        _symp = md.get(\"symbol_path\") or md.get(\"symbol\") or \"\"\n        _pp = str(md.get(\"path_prefix\") or \"\")\n        _path = str(md.get(\"path\") or \"\")\n        _related_set = set()\n        # Same-dir siblings\n        try:\n            if _pp in dir_to_paths:\n                for p in dir_to_paths[_pp]:\n                    if p != _path:\n                        _related_set.add(p)\n        except Exception:\n            pass\n        # Import-based hints: resolve relative/quoted path-like imports\n        try:\n            import re as _re, posixpath as _ppath\n\n            def _pathlike_segments(s: str) -> list[str]:\n                s = str(s or \"\")\n                segs = []\n                # quoted segments first\n                for mmm in _re.findall(r\"[\\\"']([^\\\"']+)[\\\"']\", s):\n                    if \"/\" in mmm or mmm.startswith(\".\"):\n                        segs.append(mmm)\n                # fall back to whitespace tokens containing '/' or starting with '.'\n                for tok in str(s).replace(\",\", \" \").split():\n                    if (\"/\" in tok) or tok.startswith(\".\"):\n                        segs.append(tok)\n                return segs\n\n            def _resolve(seg: str) -> list[str]:\n                try:\n                    seg = seg.strip()\n                    # base dir from path_prefix\n                    base = _pp or \"\"\n                    candidates = []\n                    # choose join rule\n                    if seg.startswith(\"./\") or seg.startswith(\"../\") or \"/\" in seg:\n                        j = _ppath.normpath(_ppath.join(base, seg)) if not seg.startswith(\"/\") else _ppath.normpath(seg)\n                        candidates.append(j)\n                        # add extensions if last segment lacks a dot\n                        last = j.split(\"/\")[-1]\n                        if \".\" not in last:\n                            for ext in [\".py\", \".js\", \".ts\", \".tsx\", \".jsx\", \".mjs\", \".cjs\"]:\n                                candidates.append(j + ext)\n                    out = set()\n                    for c in candidates:\n                        if c in all_paths:\n                            out.add(c)\n                        if c.startswith(\"/\") and c.lstrip(\"/\") in all_paths:\n                            out.add(c.lstrip(\"/\"))\n                        if c.startswith(\"/work/\") and c[len(\"/work/\"):] in all_paths:\n                            out.add(c[len(\"/work/\"):])\n                    return list(out)\n                except Exception:\n                    return []\n\n            for imp in (_imports or []):\n                for seg in _pathlike_segments(imp):\n                    for cand in _resolve(seg):\n                        if cand != _path:\n                            _related_set.add(cand)\n        except Exception:\n            pass\n\n        _related = sorted(_related_set)[:10]\n        # Align related_paths with PATH_EMIT_MODE when possible: in host/auto\n        # modes, prefer host paths when we have a mapping; in container mode,\n        # keep container/path-space values as-is.\n        _related_out = _related\n        try:\n            _mode_related = str(os.environ.get(\"PATH_EMIT_MODE\", \"auto\")).strip().lower()\n        except Exception:\n            _mode_related = \"auto\"\n        if _mode_related in {\"host\", \"auto\"}:\n            try:\n                _mapped: List[str] = []\n                for rp in _related:\n                    _mapped.append(host_map.get(rp, rp))\n                _related_out = _mapped\n            except Exception:\n                _related_out = _related\n        # Best-effort snippet text directly from payload for downstream LLM stitching\n        _payload = (m[\"pt\"].payload or {}) if m.get(\"pt\") is not None else {}\n        _metadata = _payload.get(\"metadata\", {}) or {}\n        _text = (\n            _payload.get(\"code\") or\n            _metadata.get(\"code\") or\n            _payload.get(\"text\") or\n            _metadata.get(\"text\") or\n            \"\"\n        )\n        # Carry through pseudo/tags so downstream consumers (e.g., repo_search reranker)\n        # can incorporate index-time GLM/llm labels into their own scoring or display.\n        _pseudo = _payload.get(\"pseudo\")\n        if _pseudo is None:\n            _pseudo = _metadata.get(\"pseudo\")\n        _tags = _payload.get(\"tags\")\n        if _tags is None:\n            _tags = _metadata.get(\"tags\")\n        # Skip memory-like points without a real file path\n        if not _path or not _path.strip():\n            if os.environ.get(\"DEBUG_HYBRID_FILTER\"):\n                logger.debug(f\"Filtered out item with empty path: {_metadata}\")\n            continue\n\n        # Emit path: prefer original host path when available; also include container path\n        _emit_path = _path\n        _host = \"\"\n        _cont = \"\"\n        try:\n            _host = str(_metadata.get(\"host_path\") or \"\").strip()\n            _cont = str(_metadata.get(\"container_path\") or \"\").strip()\n            _repo = str(_metadata.get(\"repo\") or \"\").strip()\n            _pp = str(_metadata.get(\"path_prefix\") or \"\").strip()\n            _mode = str(os.environ.get(\"PATH_EMIT_MODE\", \"auto\")).strip().lower()\n\n            if _mode == \"host\" and _host:\n                _emit_path = _host\n            elif _mode == \"container\" and _cont:\n                _emit_path = _cont\n            else:\n                # Auto mode: prefer host when available, else container; then fallback normalization\n                if _host:\n                    _emit_path = _host\n                elif _cont:\n                    _emit_path = _cont\n                else:\n                    # Auto/compat fallback: normalize to container form if repo+prefix known; else map cwd to /work\n                    if _repo and _pp and isinstance(_emit_path, str):\n                        _pp_norm = _pp.rstrip(\"/\") + \"/\"\n                        if _emit_path.startswith(_pp_norm):\n                            _rel = _emit_path[len(_pp_norm):]\n                            if _rel:\n                                _emit_path = f\"/work/{_repo}/\" + _rel.lstrip(\"/\")\n                    if isinstance(_emit_path, str):\n                        _cwd = os.getcwd().rstrip(\"/\") + \"/\"\n                        if _emit_path.startswith(_cwd):\n                            _rel = _emit_path[len(_cwd):]\n                            if _rel:\n                                _emit_path = \"/work/\" + _rel\n        except Exception:\n            pass\n\n        item = {\n            \"score\": round(float(m[\"s\"]), 4),\n            \"raw_score\": float(m[\"s\"]),  # expose raw fused score for downstream budgeter\n            \"fusion_score\": round(fusion_score, 4),  # Always store fusion score\n            \"rerank_score\": round(float(rerank_score), 4) if rerank_score is not None else None,  # Store rerank separately\n            \"path\": _emit_path,\n            \"host_path\": _host,\n            \"container_path\": _cont,\n            \"symbol\": _symp,\n            \"start_line\": start_line,\n            \"end_line\": end_line,\n            \"components\": comp,\n            \"relations\": {\"imports\": _imports, \"calls\": _calls, \"symbol_path\": _symp},\n            \"related_paths\": _related_out,\n            \"span_budgeted\": bool(m.get(\"_merged_start\") is not None),\n            \"budget_tokens_used\": m.get(\"_budget_tokens\"),\n            \"text\": _text,\n            \"pseudo\": _pseudo,\n            \"tags\": _tags,\n        }\n        if why is not None:\n            item[\"why\"] = why\n        items.append(item)\n    if _USE_CACHE and cache_key is not None:\n        if UNIFIED_CACHE_AVAILABLE:\n            _RESULTS_CACHE.set(cache_key, items)\n            # Mirror into local fallback dict for deterministic hits in tests\n            try:\n                with _RESULTS_LOCK:\n                    _RESULTS_CACHE_OD[cache_key] = items\n                    while len(_RESULTS_CACHE_OD) > MAX_RESULTS_CACHE:\n                        # pop oldest inserted (like LRU/FIFO)\n                        try:\n                            _RESULTS_CACHE_OD.popitem(last=False)\n                        except Exception:\n                            break\n            except Exception:\n                pass\n            if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                logger.debug(\"cache store for hybrid results\")\n        else:\n            with _RESULTS_LOCK:\n                _RESULTS_CACHE[cache_key] = items\n                if os.environ.get(\"DEBUG_HYBRID_SEARCH\"):\n                    logger.debug(\"cache store for hybrid results\")\n                while len(_RESULTS_CACHE) > MAX_RESULTS_CACHE:\n                    _RESULTS_CACHE.popitem(last=False)\n    return items",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__to_list_396": {
      "name": "_to_list",
      "type": "function",
      "start_line": 396,
      "end_line": 407,
      "content_hash": "3eb0997c1008b1c1c11dd6e33e9950b0fc4c2258",
      "content": "    def _to_list(x):\n        if x is None:\n            return []\n        if isinstance(x, (list, tuple)):\n            out = []\n            for e in x:\n                s = str(e).strip()\n                if s:\n                    out.append(s)\n            return out\n        s = str(x).strip()\n        return [s] if s else []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__normalize_globs_415": {
      "name": "_normalize_globs",
      "type": "function",
      "start_line": 415,
      "end_line": 437,
      "content_hash": "56d8ade8f71412150924ad55004046ab30c6bb84",
      "content": "    def _normalize_globs(globs: list[str]) -> list[str]:\n        out: list[str] = []\n        try:\n            for g in (globs or []):\n                s = str(g).strip().replace(\"\\\\\", \"/\")\n                if not s:\n                    continue\n                out.append(s)\n                if not s.startswith(\"/\"):\n                    # /work/<slug>/... is common; include both direct /work and slug-aware variants\n                    stripped = s.lstrip(\"/\")\n                    out.append(\"/work/\" + stripped)\n                    out.append(\"/work/*/\" + stripped)\n        except Exception:\n            pass\n        # Dedup while preserving order\n        seen = set()\n        dedup: list[str] = []\n        for g in out:\n            if g not in seen:\n                dedup.append(g)\n                seen.add(g)\n        return dedup",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__norm_under_443": {
      "name": "_norm_under",
      "type": "function",
      "start_line": 443,
      "end_line": 454,
      "content_hash": "8b1fc4097d74c895b52012fda625bf1495db4437",
      "content": "    def _norm_under(u: str | None) -> str | None:\n        if not u:\n            return None\n        u = str(u).strip().replace(\"\\\\\", \"/\")\n        u = \"/\".join([p for p in u.split(\"/\") if p])\n        if not u:\n            return None\n        if not u.startswith(\"/\"):\n            v = \"/work/\" + u\n        else:\n            v = \"/work/\" + u.lstrip(\"/\") if not u.startswith(\"/work/\") else u\n        return v",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__bn_704": {
      "name": "_bn",
      "type": "function",
      "start_line": 704,
      "end_line": 708,
      "content_hash": "e2fa556515465275d5eee573b7430857b9a19a6b",
      "content": "            def _bn(p: str) -> str:\n                s = str(p or \"\").replace(\"\\\\\", \"/\").strip()\n                # drop any trailing slashes and take last segment\n                parts = [t for t in s.split(\"/\") if t]\n                return parts[-1] if parts else \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__scaled_rrf_766": {
      "name": "_scaled_rrf",
      "type": "function",
      "start_line": 766,
      "end_line": 767,
      "content_hash": "1f04750592339e659e6511d0f19873e2c0f4d68f",
      "content": "    def _scaled_rrf(rank: int) -> float:\n        return 1.0 / (_scaled_rrf_k + rank)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__tie_key_1406": {
      "name": "_tie_key",
      "type": "function",
      "start_line": 1406,
      "end_line": 1411,
      "content_hash": "8dad678ae35c9415303f7083cb3869dbc528b48e",
      "content": "    def _tie_key(m: Dict[str, Any]):\n        md = (m[\"pt\"].payload or {}).get(\"metadata\") or {}\n        sp = str(md.get(\"symbol_path\") or md.get(\"symbol\") or \"\")\n        path = str(md.get(\"path\") or \"\")\n        start_line = int(md.get(\"start_line\") or 0)\n        return (-float(m[\"s\"]), len(sp), path, start_line)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__snippet_contains_1436": {
      "name": "_snippet_contains",
      "type": "function",
      "start_line": 1436,
      "end_line": 1466,
      "content_hash": "fc04c3fe4feab36d1715773ab0498302f69d948a",
      "content": "    def _snippet_contains(md: dict) -> int:\n        # returns number of keyword hits found in a small local snippet\n        try:\n            path = str(md.get(\"path\") or \"\")\n            sline = int(md.get(\"start_line\") or 0)\n            eline = int(md.get(\"end_line\") or 0)\n            txt = (md.get(\"text\") or md.get(\"code\") or \"\")\n            if not txt and path and sline:\n                p = path\n                try:\n                    if not os.path.isabs(p):\n                        p = os.path.join(\"/work\", p)\n                    realp = os.path.realpath(p)\n                    if realp == \"/work\" or realp.startswith(\"/work/\"):\n                        with open(realp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                            lines = f.readlines()\n                        si = max(1, sline - 3)\n                        ei = min(len(lines), max(sline, eline) + 3)\n                        txt = \"\".join(lines[si-1:ei])\n                except Exception:\n                    txt = txt or \"\"\n            lt = (txt or \"\").lower()\n            if not lt:\n                return 0\n            hits = 0\n            for t in kw:\n                if t and t in lt:\n                    hits += 1\n            return hits\n        except Exception:\n            return 0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__snippet_comment_ratio_1468": {
      "name": "_snippet_comment_ratio",
      "type": "function",
      "start_line": 1468,
      "end_line": 1531,
      "content_hash": "a70127c7c7270c5c220495f862e8d184ecf3c597",
      "content": "    def _snippet_comment_ratio(md: dict) -> float:\n        # Estimate fraction of non-blank lines that are comments (language-agnostic heuristics)\n        try:\n            path = str(md.get(\"path\") or \"\")\n            sline = int(md.get(\"start_line\") or 0)\n            eline = int(md.get(\"end_line\") or 0)\n            txt = (md.get(\"text\") or md.get(\"code\") or \"\")\n            if not txt and path and sline:\n                p = path\n                try:\n                    if not os.path.isabs(p):\n                        p = os.path.join(\"/work\", p)\n                    realp = os.path.realpath(p)\n                    if realp == \"/work\" or realp.startswith(\"/work/\"):\n                        with open(realp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                            lines = f.readlines()\n                        si = max(1, sline - 3)\n                        ei = min(len(lines), max(sline, eline) + 3)\n                        txt = \"\".join(lines[si-1:ei])\n                except Exception:\n                    txt = txt or \"\"\n            if not txt:\n                return 0.0\n            total = 0\n            comment = 0\n            in_block = False\n            for raw in txt.splitlines():\n                line = raw.strip()\n                if not line:\n                    continue\n                total += 1\n                # HTML/XML comments\n                if line.startswith(\"<!--\"):\n                    in_block = True\n                    comment += 1\n                    continue\n                if in_block:\n                    comment += 1\n                    if \"-->\" in line:\n                        in_block = False\n                    continue\n                # C/JS block comments\n                if line.startswith(\"/*\"):\n                    in_block = True\n                    comment += 1\n                    continue\n                if in_block:\n                    comment += 1\n                    if \"*/\" in line:\n                        in_block = False\n                    continue\n                # Single-line comments for many languages\n                if line.startswith(\"//\") or line.startswith(\"#\"):\n                    comment += 1\n                    continue\n                # Python docstring-like lines (treat as comment-ish for ranking)\n                if line.startswith(\"\\\"\\\"\\\"\") or line.startswith(\"'''\"):\n                    comment += 1\n                    continue\n            if total == 0:\n                return 0.0\n            return comment / float(total)\n        except Exception:\n            return 0.0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__match_glob_1605": {
      "name": "_match_glob",
      "type": "function",
      "start_line": 1605,
      "end_line": 1610,
      "content_hash": "857a73bdb2b8d3254462369ce548008dc05bfda5",
      "content": "    def _match_glob(pat: str, path: str) -> bool:\n        if not pat:\n            return True\n        if case_sensitive:\n            return _fnm.fnmatchcase(path, pat)\n        return _fnm.fnmatchcase(path.lower(), pat.lower())",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__pass_filters_1614": {
      "name": "_pass_filters",
      "type": "function",
      "start_line": 1614,
      "end_line": 1640,
      "content_hash": "5a8adaf66e8339e0ad91db64faef86646cd1ef04",
      "content": "        def _pass_filters(m: Dict[str, Any]) -> bool:\n            md = (m[\"pt\"].payload or {}).get(\"metadata\") or {}\n            path = str(md.get(\"path\") or \"\")\n            rel = path[6:] if path.startswith(\"/work/\") else path\n            pp = str(md.get(\"path_prefix\") or \"\")\n            p_for_sub = path if case_sensitive else path.lower()\n            pp_for_sub = pp if case_sensitive else pp.lower()\n            if eff_not:\n                nn = eff_not if case_sensitive else eff_not.lower()\n                if nn in p_for_sub or nn in pp_for_sub:\n                    return False\n            if eff_not_globs_norm and any(_match_glob(g, path) or _match_glob(g, rel) for g in eff_not_globs_norm):\n                return False\n            if eff_ext:\n                ex = eff_ext.lower().lstrip(\".\")\n                if not path.lower().endswith(\".\" + ex):\n                    return False\n            if eff_path_regex:\n                flags = 0 if case_sensitive else _re.IGNORECASE\n                try:\n                    if not _re.search(eff_path_regex, path, flags=flags):\n                        return False\n                except Exception:\n                    pass\n            if eff_path_globs_norm and not any(_match_glob(g, path) or _match_glob(g, rel) for g in eff_path_globs_norm):\n                return False\n            return True",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__pathlike_segments_1782": {
      "name": "_pathlike_segments",
      "type": "function",
      "start_line": 1782,
      "end_line": 1793,
      "content_hash": "41ebc576b02b010ca0c7de76d89810523370ab61",
      "content": "            def _pathlike_segments(s: str) -> list[str]:\n                s = str(s or \"\")\n                segs = []\n                # quoted segments first\n                for mmm in _re.findall(r\"[\\\"']([^\\\"']+)[\\\"']\", s):\n                    if \"/\" in mmm or mmm.startswith(\".\"):\n                        segs.append(mmm)\n                # fall back to whitespace tokens containing '/' or starting with '.'\n                for tok in str(s).replace(\",\", \" \").split():\n                    if (\"/\" in tok) or tok.startswith(\".\"):\n                        segs.append(tok)\n                return segs",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__resolve_1795": {
      "name": "_resolve",
      "type": "function",
      "start_line": 1795,
      "end_line": 1820,
      "content_hash": "416125fd0899c204520a5d1cf2c9abc120258333",
      "content": "            def _resolve(seg: str) -> list[str]:\n                try:\n                    seg = seg.strip()\n                    # base dir from path_prefix\n                    base = _pp or \"\"\n                    candidates = []\n                    # choose join rule\n                    if seg.startswith(\"./\") or seg.startswith(\"../\") or \"/\" in seg:\n                        j = _ppath.normpath(_ppath.join(base, seg)) if not seg.startswith(\"/\") else _ppath.normpath(seg)\n                        candidates.append(j)\n                        # add extensions if last segment lacks a dot\n                        last = j.split(\"/\")[-1]\n                        if \".\" not in last:\n                            for ext in [\".py\", \".js\", \".ts\", \".tsx\", \".jsx\", \".mjs\", \".cjs\"]:\n                                candidates.append(j + ext)\n                    out = set()\n                    for c in candidates:\n                        if c in all_paths:\n                            out.add(c)\n                        if c.startswith(\"/\") and c.lstrip(\"/\") in all_paths:\n                            out.add(c.lstrip(\"/\"))\n                        if c.startswith(\"/work/\") and c[len(\"/work/\"):] in all_paths:\n                            out.add(c[len(\"/work/\"):])\n                    return list(out)\n                except Exception:\n                    return []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_main_1959": {
      "name": "main",
      "type": "function",
      "start_line": 1959,
      "end_line": 2023,
      "content_hash": "e237266c636851e79a03b48ffd0d302b5247a296",
      "content": "def main():\n    \"\"\"CLI entrypoint - delegates to run_hybrid_search and formats output.\"\"\"\n    ap = argparse.ArgumentParser(description=\"Hybrid search: dense + lexical RRF\")\n    ap.add_argument(\"--query\", \"-q\", action=\"append\", required=True, help=\"Query strings\")\n    ap.add_argument(\"--language\", type=str, default=None)\n    ap.add_argument(\"--under\", type=str, default=None)\n    ap.add_argument(\"--kind\", type=str, default=None)\n    ap.add_argument(\"--symbol\", type=str, default=None)\n    ap.add_argument(\"--expand\", dest=\"expand\", action=\"store_true\",\n                    default=_env_truthy(os.environ.get(\"HYBRID_EXPAND\"), False))\n    ap.add_argument(\"--no-expand\", dest=\"expand\", action=\"store_false\")\n    ap.add_argument(\"--per-path\", type=int, default=int(os.environ.get(\"HYBRID_PER_PATH\", \"1\") or 1))\n    _per_query_env = os.environ.get(\"HYBRID_PER_QUERY\")\n    _per_query_default = int(_per_query_env) if _per_query_env and _per_query_env.strip().isdigit() else None\n    ap.add_argument(\"--per-query\", type=int, default=_per_query_default,\n                    help=\"Candidate retrieval per query (default: adaptive based on limit/collection size). \"\n                         \"Also settable via HYBRID_PER_QUERY env var.\")\n    ap.add_argument(\"--limit\", type=int, default=10)\n    ap.add_argument(\"--json\", dest=\"json_out\", action=\"store_true\")\n    ap.add_argument(\"--quiet\", dest=\"quiet\", action=\"store_true\")\n    ap.add_argument(\"--ext\", type=str, default=None)\n    ap.add_argument(\"--not\", dest=\"not_filter\", type=str, default=None)\n    ap.add_argument(\"--collection\", type=str, default=None)\n    ap.add_argument(\"--case\", type=str, choices=[\"sensitive\", \"insensitive\"],\n                    default=os.environ.get(\"HYBRID_CASE\", \"insensitive\"))\n    ap.add_argument(\"--path-regex\", dest=\"path_regex\", type=str, default=None)\n    ap.add_argument(\"--path-glob\", dest=\"path_glob\", type=str, default=None)\n    ap.add_argument(\"--not-glob\", dest=\"not_glob\", type=str, default=None)\n    ap.add_argument(\"--repo\", type=str, default=None, help=\"Filter by repo name(s)\")\n    args = ap.parse_args()\n\n    # Delegate to run_hybrid_search\n    results = run_hybrid_search(\n        queries=args.query,\n        limit=args.limit,\n        per_path=args.per_path,\n        language=args.language,\n        under=args.under,\n        kind=args.kind,\n        symbol=args.symbol,\n        ext=args.ext,\n        not_filter=args.not_filter,\n        case=args.case,\n        path_regex=args.path_regex,\n        path_glob=args.path_glob,\n        not_glob=args.not_glob,\n        expand=args.expand,\n        collection=args.collection,\n        repo=args.repo,\n        per_query=args.per_query,\n    )\n\n    # Handle empty results\n    if not results:\n        if args.quiet:\n            sys.exit(1)\n        print(\"No results found.\", file=sys.stderr)\n        return\n\n    # Output results\n    for item in results:\n        if args.json_out:\n            print(json.dumps(item))\n        else:\n            print(f\"{item.get('score', 0):.3f}\\t{item.get('path')}\\t{item.get('symbol', '')}\\t{item.get('start_line')}-{item.get('end_line')}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}