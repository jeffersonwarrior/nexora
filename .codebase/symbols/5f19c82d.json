{
  "file_path": "/work/external-deps/Context-Engine/scripts/mcp_impl/search_history.py",
  "file_hash": "d539d19e1ccb4409e2cdce3b5f7d4e7bbf79f66d",
  "updated_at": "2025-12-26T17:34:22.365967",
  "symbols": {
    "function__search_commits_for_impl_32": {
      "name": "_search_commits_for_impl",
      "type": "function",
      "start_line": 32,
      "end_line": 304,
      "content_hash": "4205709def1631657553240bac056c549c62e7b0",
      "content": "async def _search_commits_for_impl(\n    query: Any = None,\n    path: Any = None,\n    collection: Any = None,\n    limit: Any = None,\n    max_points: Any = None,\n    default_collection_fn=None,\n    get_embedding_model_fn=None,\n) -> Dict[str, Any]:\n    \"\"\"Search git commit history indexed in Qdrant.\n\n    What it does:\n    - Queries commit documents ingested by scripts/ingest_history.py\n    - Filters by optional file path (metadata.files contains path)\n\n    Parameters:\n    - query: str or list[str]; matched lexically against commit message/text\n    - path: str (optional). Relative path under /work; filters commits that touched this file\n    - collection: str (optional). Defaults to env/WS collection\n    - limit: int (optional, default 10). Max commits to return\n    - max_points: int (optional). Safety cap on scanned points (default 1000)\n\n    Returns:\n    - {\"ok\": true, \"results\": [{\"commit_id\", \"author_name\", \"authored_date\", \"message\", \"files\"}, ...], \"scanned\": int}\n    - On error: {\"ok\": false, \"error\": \"...\"}\n    \"\"\"\n    # Get default collection function\n    if default_collection_fn is None:\n        from scripts.mcp_impl.workspace import _default_collection\n        default_collection_fn = _default_collection\n\n    # Normalize inputs\n    q_terms: list[str] = []\n    if isinstance(query, (list, tuple)):\n        for x in query:\n            for tok in str(x).strip().split():\n                if tok.strip():\n                    q_terms.append(tok.strip().lower())\n    elif query is not None:\n        qs = str(query).strip()\n        if qs:\n            for tok in qs.split():\n                if tok.strip():\n                    q_terms.append(tok.strip().lower())\n    p = str(path or \"\").strip()\n    coll = str(collection or \"\").strip() or default_collection_fn()\n    try:\n        lim = int(limit) if limit not in (None, \"\") else 10\n    except (ValueError, TypeError):\n        lim = 10\n    try:\n        mcap = int(max_points) if max_points not in (None, \"\") else 1000\n    except (ValueError, TypeError):\n        mcap = 1000\n    \n    use_scoring = bool(q_terms)\n    max_ids_for_scan = mcap if use_scoring else lim\n\n    try:\n        from qdrant_client import QdrantClient  # type: ignore\n        from qdrant_client import models as qmodels  # type: ignore\n\n        client = QdrantClient(\n            url=QDRANT_URL,\n            api_key=os.environ.get(\"QDRANT_API_KEY\"),\n            timeout=float(os.environ.get(\"QDRANT_TIMEOUT\", \"20\") or 20),\n        )\n\n        # Restrict to commit documents ingested by ingest_history.py\n        filt = qmodels.Filter(\n            must=[\n                qmodels.FieldCondition(\n                    key=\"metadata.language\", match=qmodels.MatchValue(value=\"git\")\n                ),\n                qmodels.FieldCondition(\n                    key=\"metadata.kind\", match=qmodels.MatchValue(value=\"git_message\")\n                ),\n            ]\n        )\n\n        # Optional vector-augmented scoring\n        vector_scores: Dict[str, float] = {}\n        use_vectors = use_scoring and str(\n            os.environ.get(\"COMMIT_VECTOR_SEARCH\", \"0\") or \"1\"\n        ).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n        \n        if use_vectors:\n            try:\n                try:\n                    from scripts.utils import sanitize_vector_name as _sanitize_vector_name\n                except Exception:\n                    _sanitize_vector_name = None\n\n                model_name = os.environ.get(\"MODEL_NAME\", \"BAAI/bge-base-en-v1.5\")\n                vec_name: Optional[str]\n                if _sanitize_vector_name is not None:\n                    try:\n                        vec_name = _sanitize_vector_name(model_name)\n                    except Exception:\n                        vec_name = None\n                else:\n                    vec_name = None\n\n                if vec_name:\n                    if get_embedding_model_fn is None:\n                        from scripts.mcp_impl.admin_tools import _get_embedding_model\n                        get_embedding_model_fn = _get_embedding_model\n                    \n                    embed_model = get_embedding_model_fn(model_name)\n                    qtext = \" \".join(q_terms) if q_terms else \"\"\n                    if qtext.strip():\n                        qvec = next(embed_model.embed([qtext])).tolist()\n\n                        def _vec_search():\n                            return client.search(\n                                collection_name=coll,\n                                query_vector={vec_name: qvec},\n                                query_filter=filt,\n                                limit=min(mcap, 128),\n                                with_payload=True,\n                                with_vectors=False,\n                            )\n\n                        v_hits = await asyncio.to_thread(_vec_search)\n                        for sp in v_hits or []:\n                            payload_v = getattr(sp, \"payload\", {}) or {}\n                            md_v = payload_v.get(\"metadata\") or {}\n                            cid_v = md_v.get(\"commit_id\") or md_v.get(\"symbol\")\n                            scid_v = str(cid_v) if cid_v is not None else \"\"\n                            if not scid_v:\n                                continue\n                            try:\n                                vs = float(getattr(sp, \"score\", 0.0) or 0.0)\n                            except Exception:\n                                vs = 0.0\n                            if vs <= 0.0:\n                                continue\n                            if scid_v not in vector_scores or vs > vector_scores[scid_v]:\n                                vector_scores[scid_v] = vs\n            except Exception:\n                vector_scores = {}\n\n        page = None\n        scanned = 0\n        out: list[dict[str, Any]] = []\n        seen_ids: set[str] = set()\n        while scanned < mcap and len(seen_ids) < max_ids_for_scan:\n            sc, page = await asyncio.to_thread(\n                lambda: client.scroll(\n                    collection_name=coll,\n                    with_payload=True,\n                    with_vectors=False,\n                    limit=200,\n                    offset=page,\n                    scroll_filter=filt,\n                )\n            )\n            if not sc:\n                break\n            for pt in sc:\n                scanned += 1\n                if scanned > mcap:\n                    break\n                payload = getattr(pt, \"payload\", {}) or {}\n                md = payload.get(\"metadata\") or {}\n                msg = str(md.get(\"message\") or \"\")\n                info = str(payload.get(\"information\") or \"\")\n                files = md.get(\"files\") or []\n                try:\n                    files_list = [str(f) for f in files]\n                except Exception:\n                    files_list = []\n                # Optional lineage-style metadata\n                lg = md.get(\"lineage_goal\")\n                if isinstance(lg, str):\n                    lineage_goal = lg.strip()\n                else:\n                    lineage_goal = \"\"\n                ls_raw = md.get(\"lineage_symbols\") or []\n                if isinstance(ls_raw, list):\n                    lineage_symbols = [\n                        str(x).strip() for x in ls_raw if str(x).strip()\n                    ][:6]\n                else:\n                    lineage_symbols = []\n                lt_raw = md.get(\"lineage_tags\") or []\n                if isinstance(lt_raw, list):\n                    lineage_tags = [\n                        str(x).strip() for x in lt_raw if str(x).strip()\n                    ][:6]\n                else:\n                    lineage_tags = []\n\n                # Field-aware lexical scoring\n                score = 0.0\n                if q_terms:\n                    msg_l = msg.lower()\n                    info_l = info.lower()\n                    goal_l = lineage_goal.lower() if lineage_goal else \"\"\n                    sym_l = \" \".join(lineage_symbols).lower() if lineage_symbols else \"\"\n                    tags_l = \" \".join(lineage_tags).lower() if lineage_tags else \"\"\n                    hits = 0\n                    for t in q_terms:\n                        term_hit = False\n                        if goal_l and t in goal_l:\n                            score += 3.0\n                            term_hit = True\n                        if tags_l and t in tags_l:\n                            score += 2.0\n                            term_hit = True\n                        if sym_l and t in sym_l:\n                            score += 1.5\n                            term_hit = True\n                        if msg_l and t in msg_l:\n                            score += 1.0\n                            term_hit = True\n                        if info_l and t in info_l:\n                            score += 0.5\n                            term_hit = True\n                        if term_hit:\n                            hits += 1\n                    if hits == 0:\n                        continue\n                if p:\n                    if not any(p in f for f in files_list):\n                        continue\n                cid = md.get(\"commit_id\") or md.get(\"symbol\")\n                scid = str(cid) if cid is not None else \"\"\n                if not scid or scid in seen_ids:\n                    continue\n                # Blend in vector similarity score\n                try:\n                    if use_scoring and vector_scores and scid in vector_scores:\n                        vec_score = float(vector_scores.get(scid, 0.0) or 0.0)\n                        if vec_score > 0.0:\n                            weight = float(\n                                os.environ.get(\"COMMIT_VECTOR_WEIGHT\", \"2.0\") or 2.0\n                            )\n                            score += weight * vec_score\n                except Exception:\n                    pass\n                seen_ids.add(scid)\n                out.append(\n                    {\n                        \"commit_id\": cid,\n                        \"author_name\": md.get(\"author_name\"),\n                        \"authored_date\": md.get(\"authored_date\"),\n                        \"message\": msg.splitlines()[0] if msg else \"\",\n                        \"files\": files_list,\n                        \"lineage_goal\": lineage_goal,\n                        \"lineage_symbols\": lineage_symbols,\n                        \"lineage_tags\": lineage_tags,\n                        \"_score\": score,\n                    }\n                )\n                if len(seen_ids) >= max_ids_for_scan:\n                    break\n        results = out\n        if use_scoring and results:\n            try:\n                results = sorted(\n                    results,\n                    key=lambda c: float(c.get(\"_score\", 0.0)),\n                    reverse=True,\n                )\n            except Exception:\n                pass\n            results = results[:lim]\n            for c in results:\n                c.pop(\"_score\", None)\n        return {\"ok\": True, \"results\": results, \"scanned\": scanned, \"collection\": coll}\n    except Exception as e:\n        return {\"ok\": False, \"error\": str(e), \"collection\": coll}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__vec_search_145": {
      "name": "_vec_search",
      "type": "function",
      "start_line": 145,
      "end_line": 153,
      "content_hash": "72a112cb0c3049f62dc44ec0d13495cd97913ca4",
      "content": "                        def _vec_search():\n                            return client.search(\n                                collection_name=coll,\n                                query_vector={vec_name: qvec},\n                                query_filter=filt,\n                                limit=min(mcap, 128),\n                                with_payload=True,\n                                with_vectors=False,\n                            )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__change_history_for_path_impl_307": {
      "name": "_change_history_for_path_impl",
      "type": "function",
      "start_line": 307,
      "end_line": 438,
      "content_hash": "c69ae71b13d3394d1d69ac942fa1764e9f34ed80",
      "content": "async def _change_history_for_path_impl(\n    path: Any,\n    collection: Any = None,\n    max_points: Any = None,\n    include_commits: Any = None,\n    default_collection_fn=None,\n    search_commits_fn=None,\n) -> Dict[str, Any]:\n    \"\"\"Summarize recent change metadata for a file path from the index.\n\n    Parameters:\n    - path: str. Relative path under /work.\n    - collection: str (optional). Defaults to env/WS default.\n    - max_points: int (optional). Safety cap on scanned points.\n    - include_commits: bool (optional). If true, attach a small list of recent commits\n      touching this path based on the commit index.\n\n    Returns:\n    - {\"ok\": true, \"summary\": {...}} or {\"ok\": false, \"error\": \"...\"}.\n    \"\"\"\n    # Get default collection function\n    if default_collection_fn is None:\n        from scripts.mcp_impl.workspace import _default_collection\n        default_collection_fn = _default_collection\n\n    p = str(path or \"\").strip()\n    if not p:\n        return {\"error\": \"path required\"}\n    coll = str(collection or \"\").strip() or default_collection_fn()\n    try:\n        mcap = int(max_points) if max_points not in (None, \"\") else 200\n    except (ValueError, TypeError):\n        mcap = 200\n    # Treat include_commits as a loose boolean flag\n    inc_commits = False\n    if include_commits not in (None, \"\"):\n        try:\n            inc_commits = str(include_commits).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n        except Exception:\n            inc_commits = False\n\n    try:\n        from qdrant_client import QdrantClient  # type: ignore\n        from qdrant_client import models as qmodels  # type: ignore\n\n        client = QdrantClient(\n            url=QDRANT_URL,\n            api_key=os.environ.get(\"QDRANT_API_KEY\"),\n            timeout=float(os.environ.get(\"QDRANT_TIMEOUT\", \"20\") or 20),\n        )\n        # Strict exact match on metadata.path (Compose maps to /work)\n        filt = qmodels.Filter(\n            must=[\n                qmodels.FieldCondition(\n                    key=\"metadata.path\", match=qmodels.MatchValue(value=p)\n                )\n            ]\n        )\n        page = None\n        total = 0\n        hashes = set()\n        last_mods = []\n        ingested = []\n        churns = []\n        while total < mcap:\n            sc, page = await asyncio.to_thread(\n                lambda: client.scroll(\n                    collection_name=coll,\n                    with_payload=True,\n                    with_vectors=False,\n                    limit=200,\n                    offset=page,\n                    scroll_filter=filt,\n                )\n            )\n            if not sc:\n                break\n            for pt in sc:\n                md = (getattr(pt, \"payload\", {}) or {}).get(\"metadata\") or {}\n                fh = md.get(\"file_hash\")\n                if fh:\n                    hashes.add(str(fh))\n                lm = md.get(\"last_modified_at\")\n                ia = md.get(\"ingested_at\")\n                ch = md.get(\"churn_count\")\n                if lm is not None:\n                    last_mods.append(int(lm))\n                if ia is not None:\n                    ingested.append(int(ia))\n                if ch is not None:\n                    churns.append(int(ch))\n                total += 1\n                if total >= mcap:\n                    break\n        summary: Dict[str, Any] = {\n            \"path\": p,\n            \"points_scanned\": total,\n            \"distinct_hashes\": len(hashes),\n            \"last_modified_min\": min(last_mods) if last_mods else None,\n            \"last_modified_max\": max(last_mods) if last_mods else None,\n            \"ingested_min\": min(ingested) if ingested else None,\n            \"ingested_max\": max(ingested) if ingested else None,\n            \"churn_count_max\": max(churns) if churns else None,\n        }\n        if inc_commits:\n            try:\n                if search_commits_fn is None:\n                    search_commits_fn = _search_commits_for_impl\n                commits = await search_commits_fn(\n                    query=None,\n                    path=p,\n                    collection=coll,\n                    limit=10,\n                    max_points=1000,\n                )\n                if isinstance(commits, dict) and commits.get(\"ok\"):\n                    raw = commits.get(\"results\") or []\n                    seen: set[str] = set()\n                    uniq: list[dict[str, Any]] = []\n                    for c in raw:\n                        cid = c.get(\"commit_id\") if isinstance(c, dict) else None\n                        scid = str(cid) if cid is not None else \"\"\n                        if not scid or scid in seen:\n                            continue\n                        seen.add(scid)\n                        uniq.append(c)\n                    summary[\"commits\"] = uniq\n            except Exception:\n                pass\n        return {\"ok\": True, \"summary\": summary}\n    except Exception as e:\n        return {\"ok\": False, \"error\": str(e), \"path\": p}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}