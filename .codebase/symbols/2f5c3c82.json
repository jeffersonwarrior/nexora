{
  "file_path": "/work/context-engine/scripts/progressive_train.py",
  "file_hash": "e21d26f6c2328f0d9ac4e1662c84ca7e998de0ac",
  "updated_at": "2025-12-26T17:34:23.541336",
  "symbols": {
    "function_measure_quality_18": {
      "name": "measure_quality",
      "type": "function",
      "start_line": 18,
      "end_line": 35,
      "content_hash": "f507b52f8c04e2307e378af8eac0ed6e2d4e8d51",
      "content": "def measure_quality(collection='eval'):\n    \"\"\"Measure MRR against ONNX ground truth.\"\"\"\n    eval_qs = DEFAULT_EVAL_QUERIES[:6]\n    mrrs = []\n    for eq in eval_qs:\n        cands = get_candidates(eq, limit=20)\n        if not cands:\n            continue\n        onnx_ranked = rerank_onnx(eq, [c.copy() for c in cands])\n        onnx_top5 = set(c['path'] for c in onnx_ranked[:5])\n        learn_ranked = rerank_learning(eq, [c.copy() for c in cands], collection=collection)\n        for rank, c in enumerate(learn_ranked, 1):\n            if c['path'] in onnx_top5:\n                mrrs.append(1.0 / rank)\n                break\n        else:\n            mrrs.append(0.0)\n    return np.mean(mrrs) if mrrs else 0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_main_37": {
      "name": "main",
      "type": "function",
      "start_line": 37,
      "end_line": 87,
      "content_hash": "9c430387d0af85860f0fac45a2e6add30c4daffd",
      "content": "def main():\n    all_queries = (DEFAULT_EVAL_QUERIES + EXTRA_QUERIES) * 15\n    checkpoints = [50, 100, 150, 200, 250, 300]\n    results = []\n    query_count = 0\n    \n    print('Queries | Samples | Loss   | MRR   | Distill%', flush=True)\n    print('-' * 50, flush=True)\n    \n    for query in all_queries:\n        candidates = get_candidates(query, limit=25)\n        if candidates:\n            rerank_with_learning(query, candidates, learn_from_onnx=True, collection='eval')\n            query_count += 1\n        \n        if query_count in checkpoints:\n            # Clear stale locks and process events\n            import glob\n            for lock in glob.glob('/tmp/rerank_weights/eval_*.lock'):\n                try:\n                    os.remove(lock)\n                except Exception:\n                    pass\n\n            learner = CollectionLearner(collection='eval')\n            learner.process_events()\n            m = learner.scorer.get_metrics()\n            \n            # Measure quality\n            mrr = measure_quality()\n            \n            print(f'{query_count:7} | {m.get(\"total_samples\",0):7} | {m.get(\"avg_loss\",0):6.3f} | {mrr:.3f} | {mrr*100:.1f}%', flush=True)\n            results.append({\n                'queries': query_count,\n                'samples': m.get('total_samples', 0),\n                'loss': round(float(m.get('avg_loss', 0)), 3),\n                'mrr': round(mrr, 3),\n                'distill_pct': round(mrr * 100, 1),\n            })\n            checkpoints.remove(query_count)\n        \n        if query_count >= 300:\n            break\n    \n    print('\\n' + '=' * 50)\n    print('PROGRESSIVE TRAINING RESULTS')\n    print('=' * 50)\n    for r in results:\n        print(f\"  {r['queries']:3} queries \u2192 MRR {r['mrr']:.3f} ({r['distill_pct']}% of ONNX)\")\n    \n    return results",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}