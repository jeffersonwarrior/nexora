{
  "file_path": "/work/context-engine/scripts/rerank_recursive/expander.py",
  "file_hash": "46261a31281655b773b1e06273333728538b536b",
  "updated_at": "2025-12-26T17:34:23.836795",
  "symbols": {
    "class_QueryExpander_13": {
      "name": "QueryExpander",
      "type": "class",
      "start_line": 13,
      "end_line": 147,
      "content_hash": "bfda56693f5b5339274ce05a75823afdc97d883c",
      "content": "class QueryExpander:\n    \"\"\"\n    Learns query expansions (synonyms/related terms) from usage patterns.\n\n    Observes which terms co-occur with successful retrievals and builds\n    a lightweight term\u2192expansion mapping per-collection.\n    \"\"\"\n\n    WEIGHTS_DIR = os.environ.get(\"RERANKER_WEIGHTS_DIR\", \"/tmp/rerank_weights\")\n    MAX_EXPANSIONS_PER_TERM = 5\n    MIN_CONFIDENCE = 0.3\n    DECAY_RATE = 0.995\n\n    def __init__(self, lr: float = 0.1):\n        self.lr = lr\n        self._collection = \"default\"\n        self._weights_path = self._get_weights_path(\"default\")\n        self.expansions: Dict[str, Dict[str, float]] = {}\n        self._update_count = 0\n        self._version = 0\n\n        if os.path.exists(self._weights_path):\n            try:\n                self._load_weights()\n            except Exception:\n                pass\n\n    @staticmethod\n    def _sanitize_collection(collection: str) -> str:\n        return \"\".join(c if c.isalnum() or c in \"-_\" else \"_\" for c in collection)\n\n    def _get_weights_path(self, collection: str) -> str:\n        safe_name = self._sanitize_collection(collection)\n        return os.path.join(self.WEIGHTS_DIR, f\"expander_{safe_name}.json\")\n\n    def set_collection(self, collection: str):\n        self._collection = collection\n        self._weights_path = self._get_weights_path(collection)\n        if os.path.exists(self._weights_path):\n            try:\n                self._load_weights()\n            except Exception:\n                pass\n\n    def _load_weights(self):\n        import json\n        import fcntl\n        lock_path = self._weights_path + \".lock\"\n        os.makedirs(os.path.dirname(lock_path) or \".\", exist_ok=True)\n        with open(lock_path, \"w\") as lock_file:\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_SH)\n            with open(self._weights_path, \"r\") as f:\n                data = json.load(f)\n            self.expansions = data.get(\"expansions\", {})\n            self._version = data.get(\"version\", 0)\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)\n\n    def _save_weights(self):\n        import json\n        import fcntl\n        os.makedirs(os.path.dirname(self._weights_path) or \".\", exist_ok=True)\n        lock_path = self._weights_path + \".lock\"\n        tmp_path = self._weights_path + \".tmp\"\n        with open(lock_path, \"w\") as lock_file:\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)\n            with open(tmp_path, \"w\") as f:\n                json.dump({\"expansions\": self.expansions, \"version\": self._version}, f)\n            os.replace(tmp_path, self._weights_path)\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)\n\n    def _tokenize(self, text: str) -> List[str]:\n        tokens = re.findall(r'[a-zA-Z_][a-zA-Z0-9_]*', text.lower())\n        return [t for t in tokens if len(t) > 2 and t not in _COMMON_TOKENS]\n\n    def expand(self, query: str, max_expansions: int = 3) -> List[str]:\n        query_tokens = set(self._tokenize(query))\n        candidates: List[Tuple[str, float]] = []\n        for token in query_tokens:\n            if token in self.expansions:\n                for exp_term, conf in self.expansions[token].items():\n                    if exp_term not in query_tokens and conf >= self.MIN_CONFIDENCE:\n                        candidates.append((exp_term, conf))\n        candidates.sort(key=lambda x: -x[1])\n        return [term for term, _ in candidates[:max_expansions]]\n\n    def learn_from_teacher(\n        self,\n        query: str,\n        doc_texts: List[str],\n        teacher_scores: np.ndarray,\n    ):\n        query_tokens = set(self._tokenize(query))\n        if not query_tokens:\n            return\n\n        weights = np.exp(teacher_scores - teacher_scores.max())\n        weights = weights / (weights.sum() + 1e-8)\n\n        doc_term_weights: Dict[str, float] = {}\n        for doc_text, weight in zip(doc_texts, weights):\n            for token in self._tokenize(doc_text):\n                if token not in query_tokens:\n                    doc_term_weights[token] = doc_term_weights.get(token, 0.0) + weight\n\n        for query_term in query_tokens:\n            if query_term not in self.expansions:\n                self.expansions[query_term] = {}\n\n            term_expansions = self.expansions[query_term]\n\n            for exp in list(term_expansions.keys()):\n                term_expansions[exp] = float(term_expansions[exp] * self.DECAY_RATE)\n                if term_expansions[exp] < 0.01:\n                    del term_expansions[exp]\n\n            for doc_term, weight in doc_term_weights.items():\n                if weight > 0.1:\n                    old_conf = term_expansions.get(doc_term, 0.0)\n                    new_conf = old_conf + self.lr * (weight - old_conf)\n                    term_expansions[doc_term] = float(min(new_conf, 1.0))\n\n            if len(term_expansions) > self.MAX_EXPANSIONS_PER_TERM * 2:\n                sorted_exp = sorted(term_expansions.items(), key=lambda x: -x[1])\n                self.expansions[query_term] = dict(sorted_exp[:self.MAX_EXPANSIONS_PER_TERM])\n\n        self._update_count += 1\n        if self._update_count % 20 == 0:\n            self._version += 1\n            self._save_weights()\n\n    def get_stats(self) -> Dict[str, Any]:\n        total_terms = len(self.expansions)\n        total_expansions = sum(len(v) for v in self.expansions.values())\n        avg_expansions = total_expansions / max(total_terms, 1)\n        return {\"terms\": total_terms, \"expansions\": total_expansions, \"avg_per_term\": avg_expansions, \"version\": self._version}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___26": {
      "name": "__init__",
      "type": "method",
      "start_line": 26,
      "end_line": 38,
      "content_hash": "510ef58c608594a6831ddacf5aab990a9d81d18b",
      "content": "    def __init__(self, lr: float = 0.1):\n        self.lr = lr\n        self._collection = \"default\"\n        self._weights_path = self._get_weights_path(\"default\")\n        self.expansions: Dict[str, Dict[str, float]] = {}\n        self._update_count = 0\n        self._version = 0\n\n        if os.path.exists(self._weights_path):\n            try:\n                self._load_weights()\n            except Exception:\n                pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__sanitize_collection_41": {
      "name": "_sanitize_collection",
      "type": "method",
      "start_line": 41,
      "end_line": 42,
      "content_hash": "cdbd85077085baad363b24f06dafd0ea2c67f8d3",
      "content": "    def _sanitize_collection(collection: str) -> str:\n        return \"\".join(c if c.isalnum() or c in \"-_\" else \"_\" for c in collection)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__get_weights_path_44": {
      "name": "_get_weights_path",
      "type": "method",
      "start_line": 44,
      "end_line": 46,
      "content_hash": "f6ee3062effd8eef764fbd5035a90c7ae077dc5f",
      "content": "    def _get_weights_path(self, collection: str) -> str:\n        safe_name = self._sanitize_collection(collection)\n        return os.path.join(self.WEIGHTS_DIR, f\"expander_{safe_name}.json\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_set_collection_48": {
      "name": "set_collection",
      "type": "method",
      "start_line": 48,
      "end_line": 55,
      "content_hash": "5d86e95df72e0d5a0b102dca9bbef687d38ca49a",
      "content": "    def set_collection(self, collection: str):\n        self._collection = collection\n        self._weights_path = self._get_weights_path(collection)\n        if os.path.exists(self._weights_path):\n            try:\n                self._load_weights()\n            except Exception:\n                pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__load_weights_57": {
      "name": "_load_weights",
      "type": "method",
      "start_line": 57,
      "end_line": 68,
      "content_hash": "6c98454b8f12f8932b93cb0f4279af46520a3dbd",
      "content": "    def _load_weights(self):\n        import json\n        import fcntl\n        lock_path = self._weights_path + \".lock\"\n        os.makedirs(os.path.dirname(lock_path) or \".\", exist_ok=True)\n        with open(lock_path, \"w\") as lock_file:\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_SH)\n            with open(self._weights_path, \"r\") as f:\n                data = json.load(f)\n            self.expansions = data.get(\"expansions\", {})\n            self._version = data.get(\"version\", 0)\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__save_weights_70": {
      "name": "_save_weights",
      "type": "method",
      "start_line": 70,
      "end_line": 81,
      "content_hash": "9506a5fa3ec7f71cbf8fbd9506e26162a110c6a8",
      "content": "    def _save_weights(self):\n        import json\n        import fcntl\n        os.makedirs(os.path.dirname(self._weights_path) or \".\", exist_ok=True)\n        lock_path = self._weights_path + \".lock\"\n        tmp_path = self._weights_path + \".tmp\"\n        with open(lock_path, \"w\") as lock_file:\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)\n            with open(tmp_path, \"w\") as f:\n                json.dump({\"expansions\": self.expansions, \"version\": self._version}, f)\n            os.replace(tmp_path, self._weights_path)\n            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__tokenize_83": {
      "name": "_tokenize",
      "type": "method",
      "start_line": 83,
      "end_line": 85,
      "content_hash": "2d1e4dbb042ad52ad7608170c611bb9199c39b72",
      "content": "    def _tokenize(self, text: str) -> List[str]:\n        tokens = re.findall(r'[a-zA-Z_][a-zA-Z0-9_]*', text.lower())\n        return [t for t in tokens if len(t) > 2 and t not in _COMMON_TOKENS]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_expand_87": {
      "name": "expand",
      "type": "method",
      "start_line": 87,
      "end_line": 96,
      "content_hash": "8a9f3d175f8aa9bf4027b7b62d1a98d8e731d3d6",
      "content": "    def expand(self, query: str, max_expansions: int = 3) -> List[str]:\n        query_tokens = set(self._tokenize(query))\n        candidates: List[Tuple[str, float]] = []\n        for token in query_tokens:\n            if token in self.expansions:\n                for exp_term, conf in self.expansions[token].items():\n                    if exp_term not in query_tokens and conf >= self.MIN_CONFIDENCE:\n                        candidates.append((exp_term, conf))\n        candidates.sort(key=lambda x: -x[1])\n        return [term for term, _ in candidates[:max_expansions]]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_learn_from_teacher_98": {
      "name": "learn_from_teacher",
      "type": "method",
      "start_line": 98,
      "end_line": 141,
      "content_hash": "baadcabcb4e868672fe0075f22359cf0fc7e9faa",
      "content": "    def learn_from_teacher(\n        self,\n        query: str,\n        doc_texts: List[str],\n        teacher_scores: np.ndarray,\n    ):\n        query_tokens = set(self._tokenize(query))\n        if not query_tokens:\n            return\n\n        weights = np.exp(teacher_scores - teacher_scores.max())\n        weights = weights / (weights.sum() + 1e-8)\n\n        doc_term_weights: Dict[str, float] = {}\n        for doc_text, weight in zip(doc_texts, weights):\n            for token in self._tokenize(doc_text):\n                if token not in query_tokens:\n                    doc_term_weights[token] = doc_term_weights.get(token, 0.0) + weight\n\n        for query_term in query_tokens:\n            if query_term not in self.expansions:\n                self.expansions[query_term] = {}\n\n            term_expansions = self.expansions[query_term]\n\n            for exp in list(term_expansions.keys()):\n                term_expansions[exp] = float(term_expansions[exp] * self.DECAY_RATE)\n                if term_expansions[exp] < 0.01:\n                    del term_expansions[exp]\n\n            for doc_term, weight in doc_term_weights.items():\n                if weight > 0.1:\n                    old_conf = term_expansions.get(doc_term, 0.0)\n                    new_conf = old_conf + self.lr * (weight - old_conf)\n                    term_expansions[doc_term] = float(min(new_conf, 1.0))\n\n            if len(term_expansions) > self.MAX_EXPANSIONS_PER_TERM * 2:\n                sorted_exp = sorted(term_expansions.items(), key=lambda x: -x[1])\n                self.expansions[query_term] = dict(sorted_exp[:self.MAX_EXPANSIONS_PER_TERM])\n\n        self._update_count += 1\n        if self._update_count % 20 == 0:\n            self._version += 1\n            self._save_weights()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_get_stats_143": {
      "name": "get_stats",
      "type": "method",
      "start_line": 143,
      "end_line": 147,
      "content_hash": "fbb3e4e4ce45cc352e440a0f771c5683ee6f450c",
      "content": "    def get_stats(self) -> Dict[str, Any]:\n        total_terms = len(self.expansions)\n        total_expansions = sum(len(v) for v in self.expansions.values())\n        avg_expansions = total_expansions / max(total_terms, 1)\n        return {\"terms\": total_terms, \"expansions\": total_expansions, \"avg_per_term\": avg_expansions, \"version\": self._version}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}