{
  "file_path": "/work/external-deps/helix-db/helix-cli/src/commands/build.rs",
  "file_hash": "b7fd6ff4e3247963d7ecfe4d4a08b280b56429a1",
  "updated_at": "2025-12-26T17:34:24.079152",
  "symbols": {
    "struct_MetricsData_16": {
      "name": "MetricsData",
      "type": "struct",
      "start_line": 16,
      "end_line": 163,
      "content_hash": "3bc715756e482b03c6d58071d4d07022b2908cde",
      "content": "pub struct MetricsData {\n    pub queries_string: String,\n    pub num_of_queries: u32,\n}\nuse helix_db::{\n    helix_engine::traversal_core::config::Config,\n    helixc::{\n        analyzer::analyze,\n        generator::Source as GeneratedSource,\n        parser::{\n            HelixParser,\n            types::{Content, HxFile, Source},\n        },\n    },\n};\nuse std::{fmt::Write, fs};\n\n// Development flag - set to true when working on V2 locally\nconst DEV_MODE: bool = cfg!(debug_assertions);\nconst HELIX_REPO_URL: &str = \"https://github.com/helixdb/helix-db.git\";\n\n// Get the cargo workspace root at compile time\nconst CARGO_MANIFEST_DIR: &str = env!(\"CARGO_MANIFEST_DIR\");\n\npub async fn run(\n    instance_name: Option<String>,\n    metrics_sender: &MetricsSender,\n) -> Result<MetricsData> {\n    let start_time = Instant::now();\n\n    // Load project context\n    let project = ProjectContext::find_and_load(None)?;\n\n    // Get instance name - prompt if not provided\n    let instance_name = match instance_name {\n        Some(name) => name,\n        None if prompts::is_interactive() => {\n            let instances = project.config.list_instances_with_types();\n            prompts::intro(\n                \"helix build\",\n                Some(\n                    \"This will build your selected instance based on the configuration in helix.toml.\",\n                ),\n            )?;\n            prompts::select_instance(&instances)?\n        }\n        None => {\n            let instances = project.config.list_instances();\n            return Err(eyre::eyre!(\n                \"No instance specified. Available instances: {}\",\n                instances\n                    .into_iter()\n                    .cloned()\n                    .collect::<Vec<_>>()\n                    .join(\", \")\n            ));\n        }\n    };\n\n    // Get instance config\n    let instance_config = project.config.get_instance(&instance_name)?;\n\n    print_status(\"BUILD\", &format!(\"Building instance '{instance_name}'\"));\n\n    // Ensure Helix repo is cached\n    ensure_helix_repo_cached().await?;\n\n    // Prepare instance workspace\n    prepare_instance_workspace(&project, &instance_name).await?;\n\n    // Compile project queries into the workspace\n    let compile_result = compile_project(&project, &instance_name).await;\n\n    // Collect metrics data\n    let compile_time = start_time.elapsed().as_secs() as u32;\n    let success = compile_result.is_ok();\n    let error_messages = compile_result.as_ref().err().map(|e| e.to_string());\n\n    // Get metrics data from compilation result or use defaults\n    let metrics_data = match &compile_result {\n        Ok(data) => data.clone(),\n        Err(_) => MetricsData {\n            queries_string: String::new(),\n            num_of_queries: 0,\n        },\n    };\n\n    // Send compile metrics\n    metrics_sender.send_compile_event(\n        instance_name.clone(),\n        metrics_data.queries_string.clone(),\n        metrics_data.num_of_queries,\n        compile_time,\n        success,\n        error_messages,\n    );\n\n    // Propagate compilation error if any\n    compile_result?;\n\n    // Generate Docker files\n    generate_docker_files(&project, &instance_name, instance_config.clone()).await?;\n\n    // For local instances, build Docker image\n    if instance_config.should_build_docker_image() {\n        let runtime = project.config.project.container_runtime;\n        DockerManager::check_runtime_available(runtime)?;\n        let docker = DockerManager::new(&project);\n\n        let mut spinner = Spinner::new(\"DOCKER\", \"Building Docker image...\");\n        spinner.start();\n\n        match docker.build_image(&instance_name, instance_config.docker_build_target()) {\n            Ok(()) => {\n                spinner.stop();\n            }\n            Err(e) => {\n                spinner.stop();\n                // Check if this is a Rust compilation error\n                if let Some(DockerBuildError::RustCompilation { output, .. }) =\n                    e.downcast_ref::<DockerBuildError>()\n                {\n                    handle_docker_rust_compilation_failure(output, &project)?;\n                }\n                return Err(e);\n            }\n        }\n    }\n\n    print_success(&format!(\"Instance '{instance_name}' built successfully\"));\n\n    Ok(metrics_data.clone())\n}\n\npub(crate) async fn ensure_helix_repo_cached() -> Result<()> {\n    let repo_cache = get_helix_repo_cache()?;\n\n    if needs_cache_recreation(&repo_cache)? {\n        recreate_helix_cache(&repo_cache).await?;\n    } else if repo_cache.exists() {\n        update_helix_cache(&repo_cache).await?;\n    } else {\n        create_helix_cache(&repo_cache).await?;\n    }\n\n    Ok(())\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_needs_cache_recreation_164": {
      "name": "needs_cache_recreation",
      "type": "function",
      "start_line": 164,
      "end_line": 220,
      "content_hash": "50164a33d1d26a19b53de3f3676ce98e49112a65",
      "content": "fn needs_cache_recreation(repo_cache: &std::path::Path) -> Result<bool> {\n    if !repo_cache.exists() {\n        return Ok(false);\n    }\n\n    let is_git_repo = repo_cache.join(\".git\").exists();\n\n    match (DEV_MODE, is_git_repo) {\n        (true, true) => {\n            print_status(\n                \"CACHE\",\n                \"Cache is git repo but DEV_MODE requires copy - recreating...\",\n            );\n            Ok(true)\n        }\n        (false, false) => {\n            print_status(\n                \"CACHE\",\n                \"Cache is copy but production mode requires git repo - recreating...\",\n            );\n            Ok(true)\n        }\n        _ => Ok(false),\n    }\n}\n\nasync fn recreate_helix_cache(repo_cache: &std::path::Path) -> Result<()> {\n    std::fs::remove_dir_all(repo_cache)?;\n    create_helix_cache(repo_cache).await\n}\n\nasync fn create_helix_cache(repo_cache: &std::path::Path) -> Result<()> {\n    print_status(\"CACHE\", \"Caching Helix repository (first time setup)...\");\n\n    if DEV_MODE {\n        create_dev_cache(repo_cache)?;\n    } else {\n        create_git_cache(repo_cache)?;\n    }\n\n    print_success(\"Helix repository cached successfully\");\n    Ok(())\n}\n\nasync fn update_helix_cache(repo_cache: &std::path::Path) -> Result<()> {\n    print_status(\"UPDATE\", \"Updating Helix repository cache...\");\n\n    if DEV_MODE {\n        update_dev_cache(repo_cache)?;\n    } else {\n        update_git_cache(repo_cache)?;\n    }\n\n    print_success(\"Helix repository updated\");\n    Ok(())\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_create_dev_cache_221": {
      "name": "create_dev_cache",
      "type": "function",
      "start_line": 221,
      "end_line": 229,
      "content_hash": "860145464b7d1d9c78864da4c4da618558a75470",
      "content": "fn create_dev_cache(repo_cache: &std::path::Path) -> Result<()> {\n    let workspace_root = std::path::Path::new(CARGO_MANIFEST_DIR)\n        .parent() // helix-cli -> helix-db\n        .ok_or_else(|| eyre::eyre!(\"Cannot determine workspace root\"))?;\n\n    print_status(\"DEV\", \"Development mode: copying local workspace...\");\n    copy_dir_recursive_excluding(workspace_root, repo_cache)\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_create_git_cache_230": {
      "name": "create_git_cache",
      "type": "function",
      "start_line": 230,
      "end_line": 245,
      "content_hash": "d3f99836f78857067429ba424582a43fe023c3ff",
      "content": "fn create_git_cache(repo_cache: &std::path::Path) -> Result<()> {\n    let output = std::process::Command::new(\"git\")\n        .args([\"clone\", HELIX_REPO_URL, &repo_cache.to_string_lossy()])\n        .output()?;\n\n    if !output.status.success() {\n        let stderr = String::from_utf8_lossy(&output.stderr);\n        let error = crate::errors::CliError::new(\"failed to clone Helix repository\")\n            .with_context(stderr.to_string())\n            .with_hint(\"ensure git is installed and you have internet connectivity\");\n        return Err(eyre::eyre!(\"{}\", error.render()));\n    }\n\n    Ok(())\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_update_dev_cache_246": {
      "name": "update_dev_cache",
      "type": "function",
      "start_line": 246,
      "end_line": 257,
      "content_hash": "345a82099dd84a65a91757fb6b84758d45d65fe0",
      "content": "fn update_dev_cache(repo_cache: &std::path::Path) -> Result<()> {\n    let workspace_root = std::path::Path::new(CARGO_MANIFEST_DIR)\n        .parent()\n        .ok_or_else(|| eyre::eyre!(\"Cannot determine workspace root\"))?;\n\n    // Remove old cache and copy fresh\n    if repo_cache.exists() {\n        std::fs::remove_dir_all(repo_cache)?;\n    }\n    copy_dir_recursive_excluding(workspace_root, repo_cache)\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_update_git_cache_258": {
      "name": "update_git_cache",
      "type": "function",
      "start_line": 258,
      "end_line": 372,
      "content_hash": "a0162ec3217636533cd5da9e4cce0723b225ae59",
      "content": "fn update_git_cache(repo_cache: &std::path::Path) -> Result<()> {\n    let output = std::process::Command::new(\"git\")\n        .args([\"pull\"])\n        .current_dir(repo_cache)\n        .output()?;\n\n    if !output.status.success() {\n        let stderr = String::from_utf8_lossy(&output.stderr);\n        return Err(eyre::eyre!(\n            \"Failed to update Helix repository:\\n{}\",\n            stderr\n        ));\n    }\n\n    Ok(())\n}\n\npub(crate) async fn prepare_instance_workspace(\n    project: &ProjectContext,\n    instance_name: &str,\n) -> Result<()> {\n    print_status(\n        \"PREPARE\",\n        &format!(\"Preparing workspace for '{instance_name}'\"),\n    );\n\n    // Ensure instance directories exist\n    project.ensure_instance_dirs(instance_name)?;\n\n    // Copy cached repo to instance workspace for Docker build context\n    let repo_cache = get_helix_repo_cache()?;\n    let instance_workspace = project.instance_workspace(instance_name);\n    let repo_copy_path = instance_workspace.join(\"helix-repo-copy\");\n\n    // Remove existing copy if it exists\n    if repo_copy_path.exists() {\n        std::fs::remove_dir_all(&repo_copy_path)?;\n    }\n\n    // Copy cached repo to instance workspace\n    copy_dir_recursive_excluding(&repo_cache, &repo_copy_path)?;\n\n    print_status(\n        \"COPY\",\n        &format!(\"Copied cached repo to {}\", repo_copy_path.display()),\n    );\n\n    Ok(())\n}\n\npub(crate) async fn compile_project(\n    project: &ProjectContext,\n    instance_name: &str,\n) -> Result<MetricsData> {\n    print_status(\"COMPILE\", \"Compiling Helix queries...\");\n\n    // Create helix-container directory in instance workspace for generated files\n    let instance_workspace = project.instance_workspace(instance_name);\n    let helix_container_dir = instance_workspace.join(\"helix-container\");\n    let src_dir = helix_container_dir.join(\"src\");\n\n    // Create the directories\n    fs::create_dir_all(&src_dir)?;\n\n    // Generate config.hx.json from helix.toml\n    let instance = project.config.get_instance(instance_name)?;\n    let legacy_config_json = instance.to_legacy_json();\n    let legacy_config_str = serde_json::to_string_pretty(&legacy_config_json)?;\n    fs::write(src_dir.join(\"config.hx.json\"), legacy_config_str)?;\n\n    // Read and compile the .hx files using the same logic as the original CLI\n    print_status(\"CODEGEN\", \"Generating Rust code from Helix queries...\");\n\n    // Collect all .hx files for compilation\n    let hx_files = collect_hx_files(&project.root, &project.config.project.queries)?;\n\n    // Generate content and compile using helix-db compilation logic\n    let (analyzed_source, metrics_data) = compile_helix_files(&hx_files, &src_dir)?;\n\n    // Write the generated Rust code to queries.rs\n    let mut generated_rust_code = String::new();\n    write!(&mut generated_rust_code, \"{analyzed_source}\")?;\n    fs::write(src_dir.join(\"queries.rs\"), generated_rust_code)?;\n\n    print_success(\"Helix queries compiled to Rust files\");\n    Ok(metrics_data)\n}\n\nasync fn generate_docker_files(\n    project: &ProjectContext,\n    instance_name: &str,\n    instance_config: InstanceInfo<'_>,\n) -> Result<()> {\n    if !instance_config.should_build_docker_image() {\n        // Cloud instances don't need Docker files\n        return Ok(());\n    }\n\n    let docker = DockerManager::new(project);\n\n    print_status(docker.runtime.label(), \"Generating configuration...\");\n    // Generate Dockerfile\n    let dockerfile_content = docker.generate_dockerfile(instance_name, instance_config.clone())?;\n    let dockerfile_path = project.dockerfile_path(instance_name);\n    fs::write(&dockerfile_path, dockerfile_content)?;\n\n    // Generate docker-compose.yml\n    let compose_content = docker.generate_docker_compose(instance_name, instance_config.clone())?;\n    let compose_path = project.docker_compose_path(instance_name);\n    fs::write(&compose_path, compose_content)?;\n\n    print_success(\"Docker configuration generated\");\n    Ok(())\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_compile_helix_files_373": {
      "name": "compile_helix_files",
      "type": "function",
      "start_line": 373,
      "end_line": 428,
      "content_hash": "b4b6afb997d856fdbf80da13a4c07f78c317b258",
      "content": "fn compile_helix_files(\n    files: &[std::fs::DirEntry],\n    instance_src_dir: &std::path::Path,\n) -> Result<(GeneratedSource, MetricsData)> {\n    print_status(\"PARSE\", \"Parsing Helix files...\");\n\n    // Generate content from the files\n    let content = generate_content(files)?;\n\n    // Parse the content\n    print_status(\"ANALYZE\", \"Analyzing Helix files...\");\n    let source = parse_content(&content)?;\n\n    // Extract metrics data during parsing\n    let query_names: Vec<String> = source.queries.iter().map(|q| q.name.clone()).collect();\n    let metrics_data = MetricsData {\n        queries_string: query_names.join(\"\\n\"),\n        num_of_queries: query_names.len() as u32,\n    };\n\n    // Run static analysis\n    let mut analyzed_source = analyze_source(source, &content.files)?;\n\n    // Read and set the config from the instance workspace\n    analyzed_source.config = read_config(instance_src_dir)?;\n\n    Ok((analyzed_source, metrics_data))\n}\n\n/// Generates a Content object from a vector of DirEntry objects\n/// Returns a Content object with the files and source\npub(crate) fn generate_content(files: &[std::fs::DirEntry]) -> Result<Content> {\n    let files: Vec<HxFile> = files\n        .iter()\n        .map(|file| {\n            let name = file.path().to_string_lossy().into_owned();\n            let content = fs::read_to_string(file.path())\n                .map_err(|e| eyre::eyre!(\"Failed to read file {name}: {e}\"))?;\n            Ok(HxFile { name, content })\n        })\n        .collect::<Result<Vec<_>>>()?;\n\n    let content = files\n        .iter()\n        .map(|file| file.content.clone())\n        .collect::<Vec<String>>()\n        .join(\"\\n\");\n\n    Ok(Content {\n        content,\n        files,\n        source: Source::default(),\n    })\n}\n\n/// Uses the helix parser to parse the content into a Source object",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_parse_content_429": {
      "name": "parse_content",
      "type": "function",
      "start_line": 429,
      "end_line": 435,
      "content_hash": "81b2c157297976023eb3e3d03973773148c57621",
      "content": "fn parse_content(content: &Content) -> Result<Source> {\n    let source = HelixParser::parse_source(content).map_err(|e| eyre::eyre!(\"Parse error: {e}\"))?;\n    Ok(source)\n}\n\n/// Runs the static analyzer on the parsed source to catch errors and generate diagnostics if any.\n/// Otherwise returns the generated source object which is an IR used to transpile the queries to rust.",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_analyze_source_436": {
      "name": "analyze_source",
      "type": "function",
      "start_line": 436,
      "end_line": 459,
      "content_hash": "eef0b9601b27880683cd4f2b6626622f1e5e46e3",
      "content": "fn analyze_source(source: Source, files: &[HxFile]) -> Result<GeneratedSource> {\n    if source.schema.is_empty() {\n        let error = crate::errors::CliError::new(\"no schema definitions found in project\")\n            .with_hint(\"add at least one schema definition like 'N::User { name: String }' to your .hx files\");\n        return Err(eyre::eyre!(\"{}\", error.render()));\n    }\n\n    let (diagnostics, generated_source) =\n        analyze(&source).map_err(|e| eyre::eyre!(\"Analysis error: {}\", e))?;\n    if !diagnostics.is_empty() {\n        let mut error_msg = String::new();\n        for diag in diagnostics {\n            let filepath = diag.filepath.clone().unwrap_or(\"queries.hx\".to_string());\n            let snippet_src = diagnostic_source(&filepath, files, &source.source);\n            error_msg.push_str(&diag.render(snippet_src.as_ref(), &filepath));\n            error_msg.push('\\n');\n        }\n        return Err(eyre::eyre!(\"Compilation failed:\\n{error_msg}\"));\n    }\n\n    Ok(generated_source)\n}\n\n/// Read the config.hx.json file from the instance workspace",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_read_config_460": {
      "name": "read_config",
      "type": "function",
      "start_line": 460,
      "end_line": 474,
      "content_hash": "bab8e2fa676fca81eed67b5eac0e31d6c307603d",
      "content": "fn read_config(instance_src_dir: &std::path::Path) -> Result<Config> {\n    let config_path = instance_src_dir.join(\"config.hx.json\");\n\n    if !config_path.exists() {\n        return Err(eyre::eyre!(\n            \"config.hx.json not found in instance workspace\"\n        ));\n    }\n\n    let config =\n        Config::from_file(config_path).map_err(|e| eyre::eyre!(\"Failed to load config: {e}\"))?;\n    Ok(config)\n}\n\n/// Handle Rust compilation failure during Docker build - print errors and offer GitHub issue creation.",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_handle_docker_rust_compilation_failure_475": {
      "name": "handle_docker_rust_compilation_failure",
      "type": "function",
      "start_line": 475,
      "end_line": 513,
      "content_hash": "cef5b88e0074667a0036217a3108091979b4bd13",
      "content": "fn handle_docker_rust_compilation_failure(\n    docker_output: &str,\n    project: &ProjectContext,\n) -> Result<()> {\n    print_error(\"Rust compilation failed during Docker build\");\n    println!();\n    println!(\"This may indicate a bug in the Helix code generator.\");\n    println!();\n\n    // Offer to create GitHub issue\n    print_warning(\"You can report this issue to help improve Helix.\");\n    println!();\n\n    let should_create =\n        print_confirm(\"Would you like to create a GitHub issue with diagnostic information?\")?;\n\n    if !should_create {\n        return Ok(());\n    }\n\n    // Filter to get just cargo errors from the Docker output\n    let cargo_errors = filter_errors_only(docker_output);\n\n    // Collect .hx content\n    let hx_content = collect_hx_contents(&project.root, &project.config.project.queries)\n        .unwrap_or_else(|_| String::from(\"[Could not read .hx files]\"));\n\n    // Build and open GitHub issue (no generated Rust available from Docker build)\n    let issue = GitHubIssueBuilder::new(cargo_errors).with_hx_content(hx_content);\n\n    print_status(\"BROWSER\", \"Opening GitHub issue page...\");\n    println!(\"Please review the content before submitting.\");\n\n    issue.open_in_browser()?;\n\n    print_success(\"GitHub issue page opened in your browser\");\n\n    Ok(())\n}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}