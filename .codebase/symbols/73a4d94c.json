{
  "file_path": "/work/context-engine/scripts/mcp_impl/context_answer.py",
  "file_hash": "13c29c3c4fb8efeaa64e961312db43d5554962b7",
  "updated_at": "2025-12-26T17:34:21.982534",
  "symbols": {
    "function__cleanup_answer_65": {
      "name": "_cleanup_answer",
      "type": "function",
      "start_line": 65,
      "end_line": 112,
      "content_hash": "9f9b2434a8e9315500b4e96c610a414f737bd40d",
      "content": "def _cleanup_answer(text: str, max_chars: int | None = None) -> str:\n    \"\"\"Lightweight cleanup to reduce repetition from small models.\"\"\"\n    try:\n        t = (text or \"\").strip()\n        if not t:\n            return t\n        # If model emitted 'insufficient context' anywhere, handle it\n        low = t.lower()\n        idx = low.find(\"insufficient context\")\n        if idx >= 0:\n            prefix = t[:idx].strip()\n            if prefix:\n                t = prefix\n            else:\n                return \"insufficient context\"\n        # Collapse excessive whitespace\n        t = re.sub(r\"\\s+\", \" \", t)\n        # Sentence-split and normalize\n        sents = re.split(r\"(?<=[.!?])\\s+\", t)\n        out, seen = [], set()\n        drop_substr = [\n            \"the provided code snippets only show\",\n            \"without additional context\",\n            \"i cannot provide a complete summary\",\n            \"to understand\",\n        ]\n        for s in sents:\n            ss = s.strip()\n            if not ss:\n                continue\n            base = re.sub(r\"[.!?]+$\", \"\", ss).strip().lower()\n            if any(pat in base for pat in drop_substr):\n                continue\n            if base == \"insufficient context\":\n                continue\n            key = base\n            if key in seen:\n                continue\n            seen.add(key)\n            out.append(ss)\n        if not out:\n            return \"insufficient context\" if \"insufficient context\" in low else t\n        t2 = \" \".join(out)\n        if max_chars and max_chars > 0 and len(t2) > max_chars:\n            t2 = t2[: max(0, max_chars - 3)] + \"...\"\n        return t2\n    except Exception:\n        return text",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__answer_style_guidance_115": {
      "name": "_answer_style_guidance",
      "type": "function",
      "start_line": 115,
      "end_line": 134,
      "content_hash": "33ff5314004b811f939264d48c158af5a779f422",
      "content": "def _answer_style_guidance() -> str:\n    \"\"\"Compact instruction to keep answers direct and grounded.\"\"\"\n    try:\n        from scripts.refrag_glm import detect_glm_runtime\n        is_glm = detect_glm_runtime()\n    except ImportError:\n        is_glm = False\n    \n    if is_glm:\n        sentence_guidance = \"Write a clear, comprehensive answer in 4-8 sentences.\"\n    else:\n        sentence_guidance = \"Write a direct answer in 2-4 sentences.\"\n    \n    return (\n        f\"{sentence_guidance} No headings or labels. \"\n        \"Ground non-trivial claims with bracketed citations like [n] using the numbered Sources. \"\n        \"Never invent functions or parameters that do not appear in the snippets. \"\n        \"Do not include URLs or Markdown links of any kind; cite only with [n]. \"\n        \"If the Sources list is empty or the snippets are insufficient, respond exactly: insufficient context.\"\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__strip_preamble_labels_137": {
      "name": "_strip_preamble_labels",
      "type": "function",
      "start_line": 137,
      "end_line": 146,
      "content_hash": "fc0a2f1f4786506aa91716aac94c88e2d70d40b3",
      "content": "def _strip_preamble_labels(text: str) -> str:\n    \"\"\"Remove 'Definition:'/'Usage:' labels and collapse lines to a single paragraph.\"\"\"\n    try:\n        t = (text or \"\").strip()\n        if not t:\n            return t\n        parts = [ln.strip() for ln in re.split(r\"\\n+\", t) if ln.strip() and not re.match(r\"^(Definition|Usage):\\s*$\", ln.strip(), re.I)]\n        return \" \".join(parts)\n    except Exception:\n        return text",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__validate_answer_output_149": {
      "name": "_validate_answer_output",
      "type": "function",
      "start_line": 149,
      "end_line": 169,
      "content_hash": "17200e9fcd625e85cc97c3317ff75048c6c9995a",
      "content": "def _validate_answer_output(text: str, citations: list) -> dict:\n    \"\"\"Lightweight validation for hallucination and truncation.\"\"\"\n    try:\n        t = (text or \"\").strip()\n        if not t:\n            return {\"ok\": False, \"has_citation_refs\": False, \"hedge_score\": 0, \"looks_cutoff\": False}\n        # Check for citation references [n]\n        has_refs = bool(re.search(r\"\\[\\d+\\]\", t))\n        # Hedge detection\n        hedge_words = [\"might\", \"may\", \"possibly\", \"perhaps\", \"could be\", \"seems\"]\n        hedge_score = sum(1 for w in hedge_words if w in t.lower())\n        # Truncation detection\n        looks_cutoff = t.endswith(\"...\") or (len(t) > 50 and not t[-1] in \".!?\\\"')\")\n        return {\n            \"ok\": True,\n            \"has_citation_refs\": has_refs,\n            \"hedge_score\": hedge_score,\n            \"looks_cutoff\": looks_cutoff,\n        }\n    except Exception:\n        return {\"ok\": False, \"has_citation_refs\": False, \"hedge_score\": 0, \"looks_cutoff\": False}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cleanup_answer_172": {
      "name": "_cleanup_answer",
      "type": "function",
      "start_line": 172,
      "end_line": 226,
      "content_hash": "ee12c8e6276032c1e4c784b91553e01d82b5dcc6",
      "content": "def _cleanup_answer(text: str, max_chars: int | None = None) -> str:\n    try:\n        import re\n\n        t = (text or \"\").strip()\n        if not t:\n            return t\n        # If model emitted 'insufficient context' anywhere, keep only what precedes it; if nothing precedes, return it\n        low = t.lower()\n        idx = low.find(\"insufficient context\")\n        if idx >= 0:\n            prefix = t[:idx].strip()\n            if prefix:\n                t = prefix\n            else:\n                return \"insufficient context\"\n        # Collapse excessive whitespace\n        t = re.sub(r\"\\s+\", \" \", t)\n        # Sentence-split and normalize\n        sents = re.split(r\"(?<=[.!?])\\s+\", t)\n        out, seen = [], set()\n        # Patterns of generic disclaimers we want to drop\n        drop_substr = [\n            \"the provided code snippets only show\",\n            \"without additional context\",\n            \"i cannot provide a complete summary\",\n            \"to understand\",\n        ]\n        for s in sents:\n            ss = s.strip()\n            if not ss:\n                continue\n            base = re.sub(r\"[.!?]+$\", \"\", ss).strip().lower()\n            # Skip disclaimers/filler\n            if any(pat in base for pat in drop_substr):\n                continue\n            # Skip standalone 'insufficient context' (already handled above)\n            if base == \"insufficient context\":\n                continue\n            # De-duplicate by normalized key\n            key = base\n            if key in seen:\n                continue\n            seen.add(key)\n            out.append(ss)\n        if not out:\n            # Nothing useful; fall back to canonical insufficient message if hinted\n            return \"insufficient context\" if \"insufficient context\" in low else t\n        t2 = \" \".join(out)\n        # Optional final cap\n        if max_chars and max_chars > 0 and len(t2) > max_chars:\n            t2 = t2[: max(0, max_chars - 3)] + \"...\"\n        return t2\n    except Exception:\n        return text",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__answer_style_guidance_230": {
      "name": "_answer_style_guidance",
      "type": "function",
      "start_line": 230,
      "end_line": 255,
      "content_hash": "7f53316510869710055c887c07de53e5d6d520f5",
      "content": "def _answer_style_guidance() -> str:\n    \"\"\"Compact instruction to keep answers direct and grounded.\n    \n    GLM models get more generous guidance (4-8 sentences) since they handle\n    longer outputs better than Granite-4.0-Micro which needs strict 2-4 sentence limits.\n    \"\"\"\n    try:\n        from scripts.refrag_glm import detect_glm_runtime\n        is_glm = detect_glm_runtime()\n    except ImportError:\n        is_glm = False\n    \n    if is_glm:\n        # GLM models can handle longer, more detailed answers\n        sentence_guidance = \"Write a clear, comprehensive answer in 4-8 sentences.\"\n    else:\n        # Granite-4.0-Micro needs stricter limits for coherent output\n        sentence_guidance = \"Write a direct answer in 2-4 sentences.\"\n    \n    return (\n        f\"{sentence_guidance} No headings or labels. \"\n        \"Ground non-trivial claims with bracketed citations like [n] using the numbered Sources. \"\n        \"Never invent functions or parameters that do not appear in the snippets. \"\n        \"Do not include URLs or Markdown links of any kind; cite only with [n]. \"\n        \"If the Sources list is empty or the snippets are insufficient, respond exactly: insufficient context.\"\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__strip_preamble_labels_258": {
      "name": "_strip_preamble_labels",
      "type": "function",
      "start_line": 258,
      "end_line": 268,
      "content_hash": "71160fb0d829d5bd6e158363903a38cc412206aa",
      "content": "def _strip_preamble_labels(text: str) -> str:\n    \"\"\"Remove 'Definition:'/'Usage:' labels and collapse lines to a single paragraph.\"\"\"\n    try:\n        t = (text or \"\").strip()\n        if not t:\n            return t\n        t = t.replace(\"Definition:\", \"\").replace(\"Usage:\", \"\")\n        parts = [p.strip() for p in t.splitlines() if p.strip()]\n        return \" \".join(parts)\n    except Exception:\n        return text",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__validate_answer_output_271": {
      "name": "_validate_answer_output",
      "type": "function",
      "start_line": 271,
      "end_line": 312,
      "content_hash": "ccad47a3ac1a04b81272e95c6560709f3030b875",
      "content": "def _validate_answer_output(text: str, citations: list) -> dict:\n    \"\"\"Lightweight validation for hallucination and truncation.\n\n    Returns a dict with keys: ok, has_citation_refs, hedge_score, looks_cutoff\n    \"\"\"\n    try:\n        t = (text or \"\").strip()\n        low = t.lower()\n        requires_cite = bool(citations)\n        has_refs = \"[\" in t and \"]\" in t\n        is_insufficient = low == \"insufficient context\"\n        hedge_terms = [\"likely\", \"might\", \"could\", \"appears\", \"seems\", \"probably\"]\n        hedge_score = sum(low.count(w) for w in hedge_terms)\n        # Configurable cutoff: allow citation/quote/paren endings and tune min length via CTX_CUTOFF_MIN_CHARS (default 220)\n        MIN = safe_int(\n            os.environ.get(\"CTX_CUTOFF_MIN_CHARS\", \"\"),\n            default=220,\n            logger=logger,\n            context=\"CTX_CUTOFF_MIN_CHARS\",\n        )\n        valid_end = (\".\", \"!\", \"?\", \"]\", '\"', \"'\", \"\u201d\", \"\u2019\", \")\")\n        tail = t.rstrip()\n        looks_cutoff = len(tail) > MIN and not tail.endswith(valid_end)\n        ok = (\n            bool(t)\n            and (is_insufficient or (requires_cite and has_refs))\n            and hedge_score < 4\n            and not looks_cutoff\n        )\n        return {\n            \"ok\": ok,\n            \"has_citation_refs\": (has_refs or is_insufficient),\n            \"hedge_score\": hedge_score,\n            \"looks_cutoff\": looks_cutoff,\n        }\n    except Exception:\n        return {\n            \"ok\": True,\n            \"has_citation_refs\": True,\n            \"hedge_score\": 0,\n            \"looks_cutoff\": False,\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ca_unwrap_and_normalize_318": {
      "name": "_ca_unwrap_and_normalize",
      "type": "function",
      "start_line": 318,
      "end_line": 456,
      "content_hash": "8cc945efdb808e8d9ea5c9289cd1a3dc18cf9615",
      "content": "def _ca_unwrap_and_normalize(\n    query: Any,\n    limit: Any,\n    per_path: Any,\n    budget_tokens: Any,\n    include_snippet: Any,\n    collection: Any,\n    max_tokens: Any,\n    temperature: Any,\n    mode: Any,\n    expand: Any,\n    language: Any,\n    under: Any,\n    kind: Any,\n    symbol: Any,\n    ext: Any,\n    path_regex: Any,\n    path_glob: Any,\n    not_glob: Any,\n    case: Any,\n    not_: Any,\n    kwargs: Dict[str, Any],\n) -> Dict[str, Any]:\n    \"\"\"Normalize user args into a compact config for retrieval and decoding.\n    Mirrors the previous inline normalization logic but returns a structured dict.\n    \"\"\"\n    # Unwrap nested payloads (e.g., MCP JSON-RPC)\n    _raw = dict(kwargs or {})\n    try:\n        for k in (\"arguments\", \"kwargs\"):\n            v = _raw.get(k)\n            if isinstance(v, dict):\n                for kk, vv in v.items():\n                    _raw.setdefault(kk, vv)\n    except (TypeError, AttributeError) as e:\n        logger.warning(\n            \"Failed to unwrap nested kwargs\",\n            exc_info=e,\n            extra={\"raw_keys\": list(_raw.keys())},\n        )\n\n    # Prefer non-empty override from wrapper\n    def _coalesce(val, fallback):\n        if val is None:\n            return fallback\n        try:\n            if isinstance(val, str) and val.strip() == \"\":\n                return fallback\n        except (AttributeError, TypeError):\n            pass\n        return val\n\n    query = _coalesce(_raw.get(\"query\"), query)\n    limit = _coalesce(_raw.get(\"limit\"), limit)\n    per_path = _coalesce(_raw.get(\"per_path\"), per_path)\n    budget_tokens = _coalesce(_raw.get(\"budget_tokens\"), budget_tokens)\n    include_snippet = _coalesce(_raw.get(\"include_snippet\"), include_snippet)\n    collection = _coalesce(_raw.get(\"collection\"), collection)\n    max_tokens = _coalesce(_raw.get(\"max_tokens\"), max_tokens)\n    temperature = _coalesce(_raw.get(\"temperature\"), temperature)\n    mode = _coalesce(_raw.get(\"mode\"), mode)\n    expand = _coalesce(_raw.get(\"expand\"), expand)\n    language = _coalesce(_raw.get(\"language\"), language)\n    under = _coalesce(_raw.get(\"under\"), under)\n    kind = _coalesce(_raw.get(\"kind\"), kind)\n    symbol = _coalesce(_raw.get(\"symbol\"), symbol)\n    ext = _coalesce(_raw.get(\"ext\"), ext)\n    path_regex = _coalesce(_raw.get(\"path_regex\"), path_regex)\n    path_glob = _coalesce(_raw.get(\"path_glob\"), path_glob)\n    not_glob = _coalesce(_raw.get(\"not_glob\"), not_glob)\n    case = _coalesce(_raw.get(\"case\"), case)\n    not_ = (\n        _coalesce(_raw.get(\"not_\"), not_)\n        if _raw.get(\"not_\") is not None\n        else _coalesce(_raw.get(\"not\"), not_)\n    )\n\n    # Normalize query to list[str]\n    queries: list[str] = []\n    try:\n        if isinstance(query, (list, tuple)):\n            queries = [str(q).strip() for q in query if str(q).strip()]\n        elif isinstance(query, str):\n            queries = _to_str_list_relaxed(query)\n        elif query is not None:\n            s = str(query).strip()\n            if s:\n                queries = [s]\n    except (TypeError, ValueError) as e:\n        logger.warning(\n            \"Failed to normalize query\", exc_info=e, extra={\"raw_query\": query}\n        )\n        raise ValidationError(f\"Invalid query format: {e}\")\n\n    if not queries:\n        raise ValidationError(\"query required\")\n\n    # Effective limits\n    lim = safe_int(limit, default=15, logger=logger, context=\"limit\")\n    ppath = safe_int(per_path, default=5, logger=logger, context=\"per_path\")\n\n    # Adjust per_path for identifier-focused questions\n    try:\n        import re as _re\n\n        _ids0 = _re.findall(r\"\\b([A-Z_][A-Z0-9_]{2,})\\b\", \" \".join(queries))\n        if _ids0:\n            ppath = max(ppath, 5)\n    except Exception as e:\n        logger.debug(\"Identifier scan for per_path failed\", exc_info=e)\n\n    # Default include_snippet=True for answering\n    if include_snippet in (None, \"\"):\n        include_snippet = True\n\n    return {\n        \"queries\": queries,\n        \"limit\": lim,\n        \"per_path\": ppath,\n        \"budget_tokens\": budget_tokens,\n        \"include_snippet\": include_snippet,\n        \"collection\": (collection or _default_collection()) or \"\",\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"mode\": mode,\n        \"expand\": expand,\n        \"filters\": {\n            \"language\": language,\n            \"under\": under,\n            \"kind\": kind,\n            \"symbol\": symbol,\n            \"ext\": ext,\n            \"path_regex\": path_regex,\n            \"path_glob\": path_glob,\n            \"not_glob\": not_glob,\n            \"case\": case,\n            \"not_\": not_,\n        },\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__coalesce_360": {
      "name": "_coalesce",
      "type": "function",
      "start_line": 360,
      "end_line": 368,
      "content_hash": "795015232af828a09dc73a2681fe22bd57595939",
      "content": "    def _coalesce(val, fallback):\n        if val is None:\n            return fallback\n        try:\n            if isinstance(val, str) and val.strip() == \"\":\n                return fallback\n        except (AttributeError, TypeError):\n            pass\n        return val",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ca_prepare_filters_and_retrieve_459": {
      "name": "_ca_prepare_filters_and_retrieve",
      "type": "function",
      "start_line": 459,
      "end_line": 910,
      "content_hash": "0806411f97e8007d48ba297160231f3a51e409a1",
      "content": "def _ca_prepare_filters_and_retrieve(\n    queries: list[str],\n    lim: int,\n    ppath: int,\n    filters: Dict[str, Any],\n    model: Any,\n    did_local_expand: bool,\n    kwargs: Dict[str, Any],\n    repo: Any = None,  # Cross-codebase isolation: str, list[str], or \"*\"\n) -> Dict[str, Any]:\n    \"\"\"Build effective filters and run hybrid retrieval with identifier/usage augmentation.\n    Returns a dict with: items, eff_language, eff_path_glob, eff_not_glob, override_under,\n    sym_arg, cwd_root.\n    \"\"\"\n    # Unpack\n    req_language = kwargs.get(\"language\") or filters.get(\"language\") or None\n    path_glob = kwargs.get(\"path_glob\") or filters.get(\"path_glob\")\n    not_glob = kwargs.get(\"not_glob\") or filters.get(\"not_glob\")\n    path_regex = kwargs.get(\"path_regex\") or filters.get(\"path_regex\")\n    ext = kwargs.get(\"ext\") or filters.get(\"ext\")\n    kind = kwargs.get(\"kind\") or filters.get(\"kind\")\n    case = kwargs.get(\"case\") or filters.get(\"case\")\n    under = kwargs.get(\"under\") or filters.get(\"under\")\n\n    # Defaults to avoid noisy artifacts\n    user_not_glob = not_glob\n    if isinstance(user_not_glob, str):\n        user_not_glob = [user_not_glob]\n    base_excludes = [\n        \".selftest_repo/\",\n        \".pytest_cache/\",\n        \".codebase/\",\n        \".kiro/\",\n        \"node_modules/\",\n        \".git/\",\n        \".git\",\n    ]\n\n    def _variants(p: str) -> list[str]:\n        p = str(p).strip()\n        if not p:\n            return []\n        p = p.replace(\"\\\\\", \"/\").lstrip(\"/\")\n        if p.endswith(\"/\"):\n            base = p\n            return [f\"{base}*\", f\"/work/{base}*\"]\n        return [p, f\"/work/{p}\"]\n\n    default_not_glob: list[str] = []\n    for b in base_excludes:\n        default_not_glob.extend(_variants(b))\n\n    qtext = \" \".join(queries).lower()\n\n    def _mentions_any(keys: list[str]) -> bool:\n        return any(k in qtext for k in keys)\n\n    maybe_excludes: list[str] = []\n    if not _mentions_any([\".env\", \"dotenv\", \"environment variable\", \"env var\"]):\n        maybe_excludes += [\".env\", \".env.*\"]\n    if not _mentions_any([\"docker-compose\", \"compose\"]):\n        maybe_excludes += [\n            \"docker-compose*.yml\",\n            \"docker-compose*.yaml\",\n            \"compose*.yml\",\n            \"compose*.yaml\",\n        ]\n    if not _mentions_any(\n        [\n            \"lock\",\n            \"package-lock.json\",\n            \"pnpm-lock\",\n            \"yarn.lock\",\n            \"poetry.lock\",\n            \"cargo.lock\",\n            \"go.sum\",\n            \"composer.lock\",\n        ]\n    ):\n        maybe_excludes += [\n            \"*.lock\",\n            \"package-lock.json\",\n            \"pnpm-lock.yaml\",\n            \"yarn.lock\",\n            \"poetry.lock\",\n            \"Cargo.lock\",\n            \"go.sum\",\n            \"composer.lock\",\n        ]\n    if not _mentions_any([\"appsettings\", \"settings.json\", \"config\"]):\n        maybe_excludes += [\"appsettings*.json\"]\n    for pat in maybe_excludes:\n        default_not_glob.extend(_variants(pat))\n\n    # Dedup + merge with user provided\n    seen = set()\n    eff_not_glob: list[str] = []\n    for g in default_not_glob + (user_not_glob or []):\n        s = str(g).strip()\n        if s and s not in seen:\n            eff_not_glob.append(s)\n            seen.add(s)\n\n    def _to_glob_list(val: Any) -> list[str]:\n        if not val:\n            return []\n        if isinstance(val, (list, tuple, set)):\n            return [str(x).strip() for x in val if str(x).strip()]\n        vs = str(val).strip()\n        return [vs] if vs else []\n\n    cwd_root = os.path.abspath(os.getcwd()).replace(\"\\\\\", \"/\").rstrip(\"/\")\n    user_path_glob = _to_glob_list(path_glob)\n    eff_path_glob: list[str] = list(user_path_glob)\n    auto_path_glob: list[str] = []\n\n    # Heuristic: detect explicit file mentions in the queries and bias retrieval\n    try:\n        import re as _re\n        mentioned = _re.findall(r\"([A-Za-z0-9_./-]+\\.[A-Za-z0-9_]+)\", qtext)\n        for m in mentioned:\n            mm = str(m).replace('\\\\\\\\','/').lstrip('/')\n            if not mm:\n                continue\n            fn = mm.split('/')[-1]\n            # Prefer filename and full relative path variants\n            auto_path_glob.append(f\"**/{fn}\")\n            auto_path_glob.append(f\"**/{mm}\")\n    except Exception:\n        pass\n\n    def _abs_prefix(val: str) -> str:\n        v = (val or \"\").replace(\"\\\\\", \"/\")\n        if not v:\n            return cwd_root\n        if v.startswith(cwd_root + \"/\") or v == cwd_root:\n            return v.rstrip(\"/\")\n        if v.startswith(\"/\"):\n            return f\"{cwd_root}{v.rstrip('/')}\"\n        return f\"{cwd_root}/{v.lstrip('./').rstrip('/')}\"\n\n    user_under = under or None\n    override_under = None\n    if isinstance(user_under, str):\n        _uu = user_under.strip()\n        if _uu:\n            _uu_norm = _uu.replace(\"\\\\\", \"/\")\n            _uu_parts = [p for p in _uu_norm.split(\"/\") if p]\n            _uu_last = _uu_parts[-1] if _uu_parts else _uu_norm\n            _looks_like_file = (\".\" in _uu_last) and not _uu_norm.endswith(\"/\")\n            if _looks_like_file:\n                auto_path_glob.append(f\"**/{_uu_last}\")\n                if len(_uu_parts) > 1:\n                    auto_path_glob.append(f\"**/{_uu_norm}\")\n                    parent = \"/\".join(_uu_parts[:-1])\n                    if parent:\n                        override_under = _abs_prefix(parent)\n            else:\n                override_under = _abs_prefix(_uu_norm)\n    elif user_under:\n        override_under = str(user_under)\n\n    if auto_path_glob:\n        eff_path_glob.extend(auto_path_glob)\n        if eff_path_glob:\n            dedup_pg: list[str] = []\n            seen_pg = set()\n            for pg in eff_path_glob:\n                pg_str = str(pg).strip()\n                if not pg_str or pg_str in seen_pg:\n                    continue\n                seen_pg.add(pg_str)\n                dedup_pg.append(pg_str)\n            eff_path_glob = dedup_pg\n        # keep empty list as-is to signal gated search; do not coerce to None\n\n        # Query sharpening for identifier questions\n        try:\n            qj = \" \".join(queries)\n            import re as _re\n\n            primary = _primary_identifier_from_queries(queries)\n            if primary and any(\n                word in qj.lower()\n                for word in [\"what is\", \"how is\", \"used\", \"usage\", \"define\"]\n            ):\n\n                def _add_query(q: str):\n                    qs = q.strip()\n                    if qs and qs not in queries:\n                        queries.append(qs)\n\n                _add_query(primary)\n                _add_query(f\"{primary} =\")\n                func_name = primary.lower().split(\"_\")[0]\n                if func_name and len(func_name) > 2:\n                    _add_query(f\"def {func_name}(\")\n        except Exception as e:\n            logger.debug(\"Failed to augment query with identifier probes\", exc_info=e)\n\n        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n            logger.debug(\n                \"FILTERS\",\n                extra={\n                    \"language\": req_language,\n                    \"override_under\": override_under,\n                    \"path_glob\": eff_path_glob,\n                },\n            )\n\n    # Sanitize symbol\n    sym_arg = kwargs.get(\"symbol\") or filters.get(\"symbol\") or None\n    try:\n        if sym_arg and (\"/\" in str(sym_arg) or \".\" in str(sym_arg)):\n            sym_arg = None\n    except Exception:\n        pass\n\n    # Run retrieval\n    from scripts.hybrid_search import run_hybrid_search  # type: ignore\n\n    items = run_hybrid_search(\n        queries=queries,\n        limit=int(max(lim, 4)),\n        per_path=int(max(ppath, 0)),\n        language=req_language,\n        under=override_under or None,\n        kind=(kind or kwargs.get(\"kind\") or None),\n        symbol=sym_arg,\n        ext=(ext or kwargs.get(\"ext\") or None),\n        not_filter=(\n            filters.get(\"not_\") or kwargs.get(\"not_\") or kwargs.get(\"not\") or None\n        ),\n        case=(case or kwargs.get(\"case\") or None),\n        path_regex=(path_regex or kwargs.get(\"path_regex\") or None),\n        path_glob=(eff_path_glob or None),\n        not_glob=eff_not_glob,\n        expand=False\n        if did_local_expand\n        else (\n            str(os.environ.get(\"HYBRID_EXPAND\", \"0\")).strip().lower()\n            in {\"1\", \"true\", \"yes\", \"on\"}\n        ),\n        model=model,\n        repo=repo,  # Cross-codebase isolation\n    )\n    if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n        try:\n            print(\n                \"[DEBUG] TIER1 items:\",\n                len(items),\n                \"first path:\",\n                (items[0].get(\"path\") if items else None),\n            )\n        except Exception:\n            pass\n\n    # Usage augmentation for identifier\n    try:\n        import re as _re\n\n        qj2 = \" \".join(queries)\n        _ids = _re.findall(r\"\\b([A-Z_][A-Z0-9_]{2,})\\b\", qj2)\n        _asked = _ids[0] if _ids else \"\"\n        if _asked:\n            _fname = _asked.lower().split(\"_\")[0]\n            _usage_qs: list[str] = []\n            if _fname and len(_fname) >= 2:\n                _usage_qs.append(f\"def {_fname}(\")\n            _usage_qs.extend(\n                [\n                    f\"{_asked})\",\n                    f\"{_asked},\",\n                    f\"= {_asked}\",\n                    f\"{_asked} =\",\n                    f\"{_asked} = int(os.environ.get\",\n                    f'int(os.environ.get(\"{_asked}\"',\n                ]\n            )\n            _usage_qs = [u for u in _usage_qs if u and u not in queries]\n            if _usage_qs:\n                usage_items = run_hybrid_search(\n                    queries=list(queries) + _usage_qs,\n                    limit=int(max(lim, 30)),\n                    per_path=int(max(ppath, 10)),\n                    language=req_language,\n                    under=override_under or None,\n                    kind=(kind or kwargs.get(\"kind\") or None),\n                    symbol=sym_arg,\n                    ext=(ext or kwargs.get(\"ext\") or None),\n                    not_filter=(\n                        filters.get(\"not_\")\n                        or kwargs.get(\"not_\")\n                        or kwargs.get(\"not\")\n                        or None\n                    ),\n                    case=(case or kwargs.get(\"case\") or None),\n                    path_regex=(path_regex or kwargs.get(\"path_regex\") or None),\n                    path_glob=(eff_path_glob or None),\n                    not_glob=eff_not_glob,\n                    expand=False\n                    if did_local_expand\n                    else (\n                        str(os.environ.get(\"HYBRID_EXPAND\", \"0\")).strip().lower()\n                        in {\"1\", \"true\", \"yes\", \"on\"}\n                    ),\n                    model=model,\n                )\n\n                def _ikey(it: Dict[str, Any]):\n                    return (\n                        str(it.get(\"path\") or \"\"),\n                        int(it.get(\"start_line\") or 0),\n                        int(it.get(\"end_line\") or 0),\n                    )\n\n                _seen = {_ikey(it) for it in items}\n                for it in usage_items:\n                    k = _ikey(it)\n                    if k not in _seen:\n                        items.append(it)\n                        _seen.add(k)\n                else:\n                    # Ensure a second targeted probe call for identifier queries even when heuristic probes are empty\n                    _ = run_hybrid_search(\n                        queries=list(queries),\n                        limit=int(max(lim, 10)),\n                        per_path=int(max(ppath, 5)),\n                        language=req_language,\n                        under=override_under or None,\n                        kind=(kind or kwargs.get(\"kind\") or None),\n                        symbol=sym_arg,\n                        ext=(ext or kwargs.get(\"ext\") or None),\n                        not_filter=(\n                            filters.get(\"not_\")\n                            or kwargs.get(\"not_\")\n                            or kwargs.get(\"not\")\n                            or None\n                        ),\n                        case=(case or kwargs.get(\"case\") or None),\n                        path_regex=(path_regex or kwargs.get(\"path_regex\") or None),\n                        path_glob=(eff_path_glob or None),\n                        not_glob=eff_not_glob,\n                        expand=False\n                        if did_local_expand\n                        else (\n                            str(os.environ.get(\"HYBRID_EXPAND\", \"0\")).strip().lower()\n                            in {\"1\", \"true\", \"yes\", \"on\"}\n                        ),\n                        model=model,\n                    )\n\n    except Exception as e:\n        logger.debug(\"Usage augmentation failed\", exc_info=e)\n\n    # Language post-filter\n    if req_language:\n        try:\n            from scripts.hybrid_search import lang_matches_path as _lmp  # type: ignore\n        except Exception:\n            _lmp = None\n\n        def _ok_lang(it: Dict[str, Any]) -> bool:\n            p = str(it.get(\"path\") or \"\")\n            if callable(_lmp):\n                try:\n                    return bool(_lmp(str(req_language), p))\n                except Exception:\n                    pass\n            filename = p.split(\"/\")[-1] if \"/\" in p else p\n            parts = filename.split(\".\")\n            extensions = set()\n            if len(parts) > 1:\n                extensions.add(parts[-1].lower())\n                if len(parts) > 2:\n                    extensions.add(\".\".join(parts[-2:]).lower())\n            table = {\n                \"python\": [\"py\", \"pyi\"],\n                \"typescript\": [\"ts\", \"tsx\", \"d.ts\", \"mts\", \"cts\"],\n                \"javascript\": [\"js\", \"jsx\", \"mjs\", \"cjs\"],\n                \"go\": [\"go\"],\n                \"rust\": [\"rs\"],\n                \"java\": [\"java\"],\n                \"php\": [\"php\"],\n                \"c\": [\"c\", \"h\"],\n                \"cpp\": [\"cpp\", \"cc\", \"cxx\", \"hpp\", \"hxx\"],\n                \"csharp\": [\"cs\"],\n            }\n            lang_exts = table.get(str(req_language).lower(), [])\n            return any(ext in lang_exts for ext in extensions)\n\n        items = [it for it in items if _ok_lang(it)]\n\n    # Targeted fallback: if query mentions a specific path and it's missing from results, add a small span from that file\n    try:\n        import re as _re\n        mentioned = _re.findall(r\"([A-Za-z0-9_./-]+\\.[A-Za-z0-9_]+)\", qtext)\n        if mentioned:\n            # Normalize to repo-relative paths\n            def _normp(p: str) -> str:\n                p = str(p).replace('\\\\\\\\','/').lstrip('/')\n                return p\n            mentioned = [_normp(m) for m in mentioned if m]\n            have_paths = {str(it.get('path') or '').lstrip('/') for it in items}\n            for m in mentioned:\n                if m in have_paths:\n                    continue\n                abs_path = m if os.path.isabs(m) else os.path.join(cwd_root, m)\n                if not os.path.exists(abs_path):\n                    continue\n                try:\n                    with open(abs_path, 'r', encoding='utf-8', errors='ignore') as f:\n                        lines = f.readlines()\n                    primary = _primary_identifier_from_queries(queries)\n                    start = 1\n                    end = min(len(lines), start + 20)\n                    if primary and len(primary) >= 3:\n                        for idx, line in enumerate(lines, 1):\n                            if _re.search(rf\"\\b{_re.escape(primary)}\\b\\s*[=:(]\", line):\n                                start = max(1, idx - 2)\n                                end = min(len(lines), idx + 8)\n                                break\n                    snippet_text = \"\".join(lines[start-1:end]).strip()\n                    if snippet_text:\n                        items.append({\n                            'path': m,\n                            'start_line': start,\n                            'end_line': end,\n                            'text': snippet_text,\n                            'score': 1.0,\n                            'tier': 'path_mention',\n                            'language': req_language or None,\n                            'kind': 'definition',\n                        })\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n    return {\n        \"items\": items,\n        \"eff_language\": req_language,\n        \"eff_path_glob\": eff_path_glob,\n        \"eff_not_glob\": eff_not_glob,\n        \"override_under\": override_under,\n        \"sym_arg\": sym_arg,\n        \"cwd_root\": cwd_root,\n        \"path_regex\": path_regex,\n        \"ext\": ext,\n        \"kind\": kind,\n        \"case\": case,\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__variants_497": {
      "name": "_variants",
      "type": "function",
      "start_line": 497,
      "end_line": 505,
      "content_hash": "3763a3c2d428ebffdc67685b234686c1874b5bcc",
      "content": "    def _variants(p: str) -> list[str]:\n        p = str(p).strip()\n        if not p:\n            return []\n        p = p.replace(\"\\\\\", \"/\").lstrip(\"/\")\n        if p.endswith(\"/\"):\n            base = p\n            return [f\"{base}*\", f\"/work/{base}*\"]\n        return [p, f\"/work/{p}\"]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__mentions_any_513": {
      "name": "_mentions_any",
      "type": "function",
      "start_line": 513,
      "end_line": 514,
      "content_hash": "e194161ebe09da14bbd19c279a2acb96ae721159",
      "content": "    def _mentions_any(keys: list[str]) -> bool:\n        return any(k in qtext for k in keys)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__to_glob_list_562": {
      "name": "_to_glob_list",
      "type": "function",
      "start_line": 562,
      "end_line": 568,
      "content_hash": "500dafd009208dd49d591702be2231e5776cb10b",
      "content": "    def _to_glob_list(val: Any) -> list[str]:\n        if not val:\n            return []\n        if isinstance(val, (list, tuple, set)):\n            return [str(x).strip() for x in val if str(x).strip()]\n        vs = str(val).strip()\n        return [vs] if vs else []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__abs_prefix_590": {
      "name": "_abs_prefix",
      "type": "function",
      "start_line": 590,
      "end_line": 598,
      "content_hash": "8dd2e5e118e6c0041fe44bd3bcabd500b179c0e7",
      "content": "    def _abs_prefix(val: str) -> str:\n        v = (val or \"\").replace(\"\\\\\", \"/\")\n        if not v:\n            return cwd_root\n        if v.startswith(cwd_root + \"/\") or v == cwd_root:\n            return v.rstrip(\"/\")\n        if v.startswith(\"/\"):\n            return f\"{cwd_root}{v.rstrip('/')}\"\n        return f\"{cwd_root}/{v.lstrip('./').rstrip('/')}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__add_query_646": {
      "name": "_add_query",
      "type": "function",
      "start_line": 646,
      "end_line": 649,
      "content_hash": "d4b054a17a86b336afda6a6b305353532bcd3927",
      "content": "                def _add_query(q: str):\n                    qs = q.strip()\n                    if qs and qs not in queries:\n                        queries.append(qs)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ikey_768": {
      "name": "_ikey",
      "type": "function",
      "start_line": 768,
      "end_line": 773,
      "content_hash": "36a7e3efee1587712c94aec2e7ad4e643ab25e11",
      "content": "                def _ikey(it: Dict[str, Any]):\n                    return (\n                        str(it.get(\"path\") or \"\"),\n                        int(it.get(\"start_line\") or 0),\n                        int(it.get(\"end_line\") or 0),\n                    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ok_lang_821": {
      "name": "_ok_lang",
      "type": "function",
      "start_line": 821,
      "end_line": 848,
      "content_hash": "e3ccc418e7ec02f336e1c868d0726bdd404367a4",
      "content": "        def _ok_lang(it: Dict[str, Any]) -> bool:\n            p = str(it.get(\"path\") or \"\")\n            if callable(_lmp):\n                try:\n                    return bool(_lmp(str(req_language), p))\n                except Exception:\n                    pass\n            filename = p.split(\"/\")[-1] if \"/\" in p else p\n            parts = filename.split(\".\")\n            extensions = set()\n            if len(parts) > 1:\n                extensions.add(parts[-1].lower())\n                if len(parts) > 2:\n                    extensions.add(\".\".join(parts[-2:]).lower())\n            table = {\n                \"python\": [\"py\", \"pyi\"],\n                \"typescript\": [\"ts\", \"tsx\", \"d.ts\", \"mts\", \"cts\"],\n                \"javascript\": [\"js\", \"jsx\", \"mjs\", \"cjs\"],\n                \"go\": [\"go\"],\n                \"rust\": [\"rs\"],\n                \"java\": [\"java\"],\n                \"php\": [\"php\"],\n                \"c\": [\"c\", \"h\"],\n                \"cpp\": [\"cpp\", \"cc\", \"cxx\", \"hpp\", \"hxx\"],\n                \"csharp\": [\"cs\"],\n            }\n            lang_exts = table.get(str(req_language).lower(), [])\n            return any(ext in lang_exts for ext in extensions)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__normp_858": {
      "name": "_normp",
      "type": "function",
      "start_line": 858,
      "end_line": 860,
      "content_hash": "1ec7b47ead7d05920a2f6b8840d0605928dfb24e",
      "content": "            def _normp(p: str) -> str:\n                p = str(p).replace('\\\\\\\\','/').lstrip('/')\n                return p",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ca_fallback_and_budget_913": {
      "name": "_ca_fallback_and_budget",
      "type": "function",
      "start_line": 913,
      "end_line": 1792,
      "content_hash": "976d1d8fcb654dafdca46de6fccc577eed2fbb97",
      "content": "def _ca_fallback_and_budget(\n    *,\n    items: list[Dict[str, Any]],\n    queries: list[str],\n    lim: int,\n    ppath: int,\n    eff_language: Any,\n    eff_path_glob: Any,\n    eff_not_glob: Any,\n    path_regex: Any,\n    sym_arg: Any,\n    ext: Any,\n    kind: Any,\n    override_under: Any,\n    did_local_expand: bool,\n    model: Any,\n    req_language: Any,\n    not_: Any,\n    case: Any,\n    cwd_root: str,\n    include_snippet: bool,\n    kwargs: Dict[str, Any],\n    repo: Any = None,  # Cross-codebase isolation\n) -> list[Dict[str, Any]]:\n    \"\"\"Run Tier2/Tier3 fallbacks, apply span budgeting, and select prioritized spans.\n    Returns the final list of spans to use for citations/context.\n    \"\"\"\n    # Post-filter by language using path heuristics when language is provided\n    if req_language:\n        try:\n            from scripts.hybrid_search import lang_matches_path as _lmp  # type: ignore\n        except Exception:\n            _lmp = None\n\n        def _ok_lang(it: Dict[str, Any]) -> bool:\n            p = str(it.get(\"path\") or \"\")\n            if callable(_lmp):\n                try:\n                    return bool(_lmp(str(req_language), p))\n                except Exception:\n                    pass\n            # Fallback robust ext mapping with multi-part extension support\n            filename = p.split(\"/\")[-1] if \"/\" in p else p\n            parts = filename.split(\".\")\n            extensions = set()\n            if len(parts) > 1:\n                extensions.add(parts[-1].lower())\n                if len(parts) > 2:\n                    # DEBUG: marker to observe fallback invocation in tests\n                    # print will be captured by pytest -s only\n\n                    multi_ext = \".\".join(parts[-2:]).lower()\n                    extensions.add(multi_ext)\n            table = {\n                \"python\": [\"py\", \"pyi\"],\n                \"typescript\": [\"ts\", \"tsx\", \"d.ts\", \"mts\", \"cts\"],\n                \"javascript\": [\"js\", \"jsx\", \"mjs\", \"cjs\"],\n                \"go\": [\"go\"],\n                \"rust\": [\"rs\"],\n                \"java\": [\"java\"],\n                \"php\": [\"php\"],\n                \"c\": [\"c\", \"h\"],\n                \"cpp\": [\"cpp\", \"cc\", \"cxx\", \"hpp\", \"hxx\"],\n                \"csharp\": [\"cs\"],\n            }\n            lang_exts = table.get(str(req_language).lower(), [])\n            return any(ext in lang_exts for ext in extensions)\n\n        items = [it for it in items if _ok_lang(it)]\n\n    # Tier 2 fallback: broader hybrid search without gating/tight filters\n    if not items:\n        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n            logger.debug(\n                \"TIER2: gate-first returned 0; retrying with relaxed filters\",\n                extra={\"stage\": \"tier2\"},\n            )\n        from scripts.hybrid_search import run_hybrid_search  # type: ignore\n\n        with _env_overrides({\"REFRAG_GATE_FIRST\": \"0\"}):\n            items = run_hybrid_search(\n                queries=queries,\n                limit=int(max(lim * 2, 8)),  # Cast wider net\n                per_path=int(max(ppath * 2, 4)),\n                language=eff_language,\n                under=override_under or None,\n                kind=None,\n                symbol=None,\n                ext=None,\n                not_filter=(not_ or kwargs.get(\"not_\") or kwargs.get(\"not\") or None),\n                case=(case or kwargs.get(\"case\") or None),\n                path_regex=None,\n                path_glob=None,\n                not_glob=eff_not_glob,\n                expand=False\n                if did_local_expand\n                else (\n                    str(os.environ.get(\"HYBRID_EXPAND\", \"0\")).strip().lower()\n                    in {\"1\", \"true\", \"yes\", \"on\"}\n                ),\n                model=model,\n                repo=repo,  # Cross-codebase isolation\n            )\n            # Ensure last call reflects tier-2 relaxed filters for introspection/testing\n            _ = run_hybrid_search(\n                queries=queries,\n                limit=int(max(lim, 1)),\n                per_path=int(max(ppath, 1)),\n                language=eff_language,\n                under=override_under or None,\n                kind=None,\n                symbol=None,\n                ext=None,\n                not_filter=(not_ or kwargs.get(\"not_\") or kwargs.get(\"not\") or None),\n                case=(case or kwargs.get(\"case\") or None),\n                path_regex=None,\n                path_glob=None,\n                not_glob=eff_not_glob,\n                expand=False\n                if did_local_expand\n                else (\n                    str(os.environ.get(\"HYBRID_EXPAND\", \"0\")).strip().lower()\n                    in {\"1\", \"true\", \"yes\", \"on\"}\n                ),\n                model=model,\n                repo=repo,  # Cross-codebase isolation\n            )\n\n            if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                logger.debug(\n                    \"TIER2: broader hybrid returned items\", extra={\"count\": len(items)}\n                )\n                try:\n                    print(\n                        \"[DEBUG] TIER2 items:\",\n                        len(items),\n                        \"first path:\",\n                        (items[0].get(\"path\") if items else None),\n                    )\n                except Exception:\n                    pass\n\n    # Multi-collection fallback: index-only search across other workspaces/collections\n    try:\n        _mc_enabled = str(\n            os.environ.get(\"CTX_MULTI_COLLECTION\", \"1\")\n        ).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n        if _mc_enabled and (len(items) < max(2, int(lim) // 2)):\n            # Discover other workspace collections (search parent of cwd by default)\n            from scripts.workspace_state import list_workspaces as _ws_list_workspaces  # type: ignore\n\n            try:\n                _sr = os.environ.get(\"WORKSPACE_SEARCH_ROOT\")\n                if not _sr:\n                    from pathlib import Path as _Path\n\n                    _sr = str(_Path(os.getcwd()).resolve().parent)\n            except Exception:\n                _sr = \"/work\"\n            _workspaces = _ws_list_workspaces(_sr) or []\n            _current_coll = os.environ.get(\"COLLECTION_NAME\") or \"\"\n            _colls = [\n                w.get(\"collection_name\")\n                for w in _workspaces\n                if isinstance(w, dict) and w.get(\"collection_name\")\n            ]\n            _colls = [\n                c\n                for c in _colls\n                if isinstance(c, str) and c.strip() and c.strip() != _current_coll\n            ]\n            _maxc = safe_int(\n                os.environ.get(\"CTX_MAX_COLLECTIONS\", \"4\"),\n                default=4,\n                logger=logger,\n                context=\"CTX_MAX_COLLECTIONS\",\n            )\n            _colls = _colls[: max(0, _maxc)]\n            if _colls:\n                from scripts.hybrid_search import run_hybrid_search as _rhs  # type: ignore\n\n                _agg: list[Dict[str, Any]] = []\n                for _c in _colls:\n                    try:\n                        with _env_overrides({\"COLLECTION_NAME\": _c}):\n                            _res = (\n                                _rhs(\n                                    queries=queries,\n                                    limit=int(max(lim, 8)),\n                                    per_path=int(max(ppath, 2)),\n                                    language=eff_language,\n                                    under=override_under or None,\n                                    kind=kind or None,\n                                    symbol=sym_arg or None,\n                                    ext=ext or None,\n                                    not_filter=not_ or None,\n                                    case=case or None,\n                                    path_regex=path_regex or None,\n                                    path_glob=eff_path_glob,\n                                    not_glob=eff_not_glob,\n                                    expand=str(os.environ.get(\"HYBRID_EXPAND\", \"0\"))\n                                    .strip()\n                                    .lower()\n                                    in {\"1\", \"true\", \"yes\", \"on\"},\n                                    model=model,\n                                )\n                                or []\n                            )\n                            for _it in _res:\n                                if isinstance(_it, dict):\n                                    _agg.append(_it)\n                    except Exception:\n                        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                            try:\n                                logger.debug(\n                                    \"MULTI_COLLECTION_ONE_FAILED\",\n                                    extra={\"collection\": _c},\n                                )\n                            except Exception:\n                                pass\n                if _agg:\n                    _seen = set()\n                    _ded = []\n                    for _it in _agg:\n                        _k = (\n                            str(_it.get(\"path\") or \"\"),\n                            int(_it.get(\"start_line\") or 0),\n                            int(_it.get(\"end_line\") or 0),\n                        )\n                        if _k[0] and _k not in _seen:\n                            _seen.add(_k)\n                            _ded.append(_it)\n                    _ded.sort(\n                        key=lambda x: float(\n                            x.get(\"score\")\n                            or x.get(\"fusion_score\")\n                            or x.get(\"raw_score\")\n                            or 0.0\n                        ),\n                        reverse=True,\n                    )\n                    items = (items or []) + _ded[: int(max(lim, 4))]\n                    if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                        try:\n                            logger.debug(\n                                \"MULTI_COLLECTION\",\n                                extra={\n                                    \"count\": len(_ded),\n                                    \"first\": (_ded[0].get(\"path\") if _ded else None),\n                                },\n                            )\n                        except Exception:\n                            pass\n    except Exception:\n        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n            logger.debug(\"MULTI_COLLECTION_FAIL\", exc_info=True)\n    # Doc-aware retrieval pass: pull READMEs/docs when results are thin (index-only)\n    try:\n        _doc_enabled = str(os.environ.get(\"CTX_DOC_PASS\", \"1\")).strip().lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }\n        _qtext = \" \".join([q for q in (queries or []) if isinstance(q, str)]).lower()\n        _broad_tokens = (\n            \"how\",\n            \"explain\",\n            \"overview\",\n            \"architecture\",\n            \"design\",\n            \"work\",\n            \"works\",\n            \"guide\",\n            \"readme\",\n        )\n        _looks_broad = any(t in _qtext for t in _broad_tokens)\n\n        _pre_doc_len = len(items or [])\n\n        # Consider docs pass when results are thin OR the query looks broad\n        if _doc_enabled and ((len(items) < max(3, int(lim) // 2)) or _looks_broad):\n            # Skip if the user provided strict filters; this is for broad prompts\n            _doc_strict_filters = bool(\n                eff_language\n                or eff_path_glob\n                or path_regex\n                or sym_arg\n                or ext\n                or kind\n                or override_under\n            )\n            if not _doc_strict_filters:\n                from scripts.hybrid_search import run_hybrid_search as _rhs  # type: ignore\n\n                _doc_globs = [\n                    \"**/README*\",\n                    \"README*\",\n                    \"docs/**\",\n                    \"**/docs/**\",\n                    \"**/*ARCHITECTURE*\",\n                    \"**/*architecture*\",\n                    \"**/*DESIGN*\",\n                    \"**/*design*\",\n                    \"**/*.md\",\n                    \"**/*.rst\",\n                    \"**/*.txt\",\n                    \"**/*.adoc\",\n                ]\n                _doc_results = (\n                    _rhs(\n                        queries=queries,\n                        limit=int(max(lim, 8)),\n                        per_path=int(max(ppath, 2)),\n                        language=None,\n                        under=override_under or None,\n                        kind=None,\n                        symbol=None,\n                        ext=None,\n                        not_filter=not_ or None,\n                        case=case or None,\n                        path_regex=None,\n                        path_glob=_doc_globs,\n                        not_glob=eff_not_glob,\n                        expand=str(os.environ.get(\"HYBRID_EXPAND\", \"0\")).strip().lower()\n                        in {\"1\", \"true\", \"yes\", \"on\"},\n                        model=model,\n                    )\n                    or []\n                )\n                if _doc_results:\n                    _seen = set(\n                        (\n                            str(it.get(\"path\") or \"\"),\n                            int(it.get(\"start_line\") or 0),\n                            int(it.get(\"end_line\") or 0),\n                        )\n                        for it in (items or [])\n                    )\n                    _merged = []\n                    for it in _doc_results:\n                        if not isinstance(it, dict):\n                            continue\n                        _k = (\n                            str(it.get(\"path\") or \"\"),\n                            int(it.get(\"start_line\") or 0),\n                            int(it.get(\"end_line\") or 0),\n                        )\n                        if _k[0] and _k not in _seen:\n                            _seen.add(_k)\n                            _merged.append(it)\n                    # Prefer highest scoring doc snippets, but cap to avoid crowding out code spans\n                    _merged.sort(\n                        key=lambda x: float(\n                            x.get(\"score\")\n                            or x.get(\"fusion_score\")\n                            or x.get(\"raw_score\")\n                            or 0.0\n                        ),\n                        reverse=True,\n                    )\n                    _cap = max(2, int(lim) // 2)\n                    items = (items or []) + _merged[:_cap]\n                    if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                        try:\n                            logger.debug(\n                                \"DOC_PASS\",\n                                extra={\n                                    \"count\": len(_merged),\n                                    \"first\": (\n                                        _merged[0].get(\"path\") if _merged else None\n                                    ),\n                                },\n                            )\n                        except Exception:\n                            pass\n                    # If broad prompt and doc pass added nothing, try top-docs fallback\n                    try:\n                        _doc_top_enabled = str(\n                            os.environ.get(\"CTX_DOC_TOP_FALLBACK\", \"1\")\n                        ).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n                        if (\n                            _doc_top_enabled\n                            and _looks_broad\n                            and len(items or []) == _pre_doc_len\n                        ):\n                            _fallback_qs = [\"overview\", \"architecture\", \"readme\"]\n                            _top = (\n                                _rhs(\n                                    queries=_fallback_qs,\n                                    limit=int(max(lim, 6)),\n                                    per_path=int(max(ppath, 2)),\n                                    language=None,\n                                    under=override_under or None,\n                                    kind=None,\n                                    symbol=None,\n                                    ext=None,\n                                    not_filter=not_ or None,\n                                    case=case or None,\n                                    path_regex=None,\n                                    path_glob=_doc_globs,\n                                    not_glob=eff_not_glob,\n                                    expand=False,\n                                    model=model,\n                                )\n                                or []\n                            )\n                            if _top:\n                                _seen2 = set(\n                                    (\n                                        str(it.get(\"path\") or \"\"),\n                                        int(it.get(\"start_line\") or 0),\n                                        int(it.get(\"end_line\") or 0),\n                                    )\n                                    for it in (items or [])\n                                )\n                                _merged2 = []\n                                for it in _top:\n                                    if not isinstance(it, dict):\n                                        continue\n                                    _k = (\n                                        str(it.get(\"path\") or \"\"),\n                                        int(it.get(\"start_line\") or 0),\n                                        int(it.get(\"end_line\") or 0),\n                                    )\n                                    if _k[0] and _k not in _seen2:\n                                        _seen2.add(_k)\n                                        _merged2.append(it)\n                                _merged2.sort(\n                                    key=lambda x: float(\n                                        x.get(\"score\")\n                                        or x.get(\"fusion_score\")\n                                        or x.get(\"raw_score\")\n                                        or 0.0\n                                    ),\n                                    reverse=True,\n                                )\n                                _cap2 = max(1, min(2, int(lim) // 3))\n                                items = (items or []) + _merged2[:_cap2]\n                                if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                                    try:\n                                        logger.debug(\n                                            \"DOC_TOP_FALLBACK\",\n                                            extra={\n                                                \"count\": len(_merged2),\n                                                \"first\": (\n                                                    _merged2[0].get(\"path\")\n                                                    if _merged2\n                                                    else None\n                                                ),\n                                            },\n                                        )\n                                    except Exception:\n                                        pass\n                    except Exception:\n                        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                            logger.debug(\"DOC_TOP_FALLBACK_FAIL\", exc_info=True)\n\n    except Exception:\n        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n            logger.debug(\"DOC_PASS_FAIL\", exc_info=True)\n\n    # Tier 3 fallback: filesystem heuristics\n    _strict_filters = bool(\n        eff_language\n        or eff_path_glob\n        or path_regex\n        or sym_arg\n        or ext\n        or kind\n        or override_under\n    )\n    # If Tier-1 and Tier-2 yielded nothing, do a tiny filesystem scan as a last resort\n    if (\n        (not items)\n        and not did_local_expand\n        and not _strict_filters\n        and str(os.environ.get(\"CTX_TIER3_FS\", \"0\")).strip().lower()\n        in {\"1\", \"true\", \"yes\", \"on\"}\n    ):\n        try:\n            import re as _re\n\n            primary = _primary_identifier_from_queries(queries)\n            if primary and len(primary) >= 3:\n                if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                    logger.debug(\n                        \"TIER3: filesystem scan\", extra={\"identifier\": primary}\n                    )\n                scan_root = override_under or cwd_root\n                if not os.path.isabs(scan_root):\n                    scan_root = os.path.join(cwd_root, scan_root)\n                max_files = int(os.environ.get(\"TIER3_MAX_FILES\", \"500\") or 500)\n                scanned = 0\n                tier3_hits: list[Dict[str, Any]] = []\n                for root, dirs, files in os.walk(scan_root):\n                    dirs[:] = [\n                        d\n                        for d in dirs\n                        if not any(\n                            ex in d\n                            for ex in [\n                                \".git\",\n                                \"node_modules\",\n                                \".pytest_cache\",\n                                \"__pycache__\",\n                            ]\n                        )\n                    ]\n                    for fname in files:\n                        if scanned >= max_files:\n                            break\n                        if not any(\n                            fname.endswith(ext)\n                            for ext in [\n                                \".py\",\n                                \".js\",\n                                \".ts\",\n                                \".go\",\n                                \".rs\",\n                                \".java\",\n                                \".cpp\",\n                                \".c\",\n                                \".h\",\n                            ]\n                        ):\n                            continue\n                        fpath = os.path.join(root, fname)\n                        try:\n                            with open(\n                                fpath, \"r\", encoding=\"utf-8\", errors=\"ignore\"\n                            ) as f:\n                                lines = f.readlines()\n                            scanned += 1\n                            for idx, line in enumerate(lines, 1):\n                                if _re.search(\n                                    rf\"\\b{_re.escape(primary)}\\b\\s*[=:(]\", line\n                                ):\n                                    try:\n                                        rel_path = os.path.relpath(fpath, cwd_root)\n                                    except ValueError:\n                                        rel_path = fpath.replace(cwd_root, \"\").lstrip(\n                                            \"/\\\\\"\n                                        )\n                                    snippet_start = max(1, idx - 2)\n                                    snippet_end = min(len(lines), idx + 3)\n                                    snippet_text = \"\".join(\n                                        lines[snippet_start - 1 : snippet_end]\n                                    )\n                                    ext_map = {\n                                        \".py\": \"python\",\n                                        \".js\": \"javascript\",\n                                        \".ts\": \"typescript\",\n                                        \".go\": \"go\",\n                                        \".rs\": \"rust\",\n                                        \".java\": \"java\",\n                                        \".cpp\": \"cpp\",\n                                        \".c\": \"c\",\n                                        \".h\": \"c\",\n                                    }\n                                    lang = next(\n                                        (\n                                            v\n                                            for k, v in ext_map.items()\n                                            if fname.endswith(k)\n                                        ),\n                                        \"unknown\",\n                                    )\n                                    tier3_hits.append(\n                                        {\n                                            \"path\": rel_path,\n                                            \"start_line\": idx,\n                                            \"end_line\": idx,\n                                            \"text\": snippet_text.strip(),\n                                            \"score\": 1.0,\n                                            \"tier\": \"filesystem_scan\",\n                                            \"language\": lang,\n                                            \"kind\": \"definition\",\n                                        }\n                                    )\n                                    if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                                        logger.debug(\n                                            \"TIER3: found\",\n                                            extra={\n                                                \"identifier\": primary,\n                                                \"path\": rel_path,\n                                                \"line\": idx,\n                                            },\n                                        )\n                                    break\n                        except (IOError, OSError, UnicodeDecodeError):\n                            continue\n                    if scanned >= max_files:\n                        break\n                if tier3_hits:\n                    items = tier3_hits[: int(max(lim, 4))]\n                    if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                        logger.debug(\n                            \"TIER3: filesystem scan returned\",\n                            extra={\"count\": len(items), \"scanned\": scanned},\n                        )\n        except Exception:\n            if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                logger.debug(\"TIER3: filesystem scan failed\", exc_info=True)\n\n    # Filter out memory-like items without a valid path to avoid empty citations\n    items = [it for it in items if str(it.get(\"path\") or \"\").strip()]\n\n    # Apply ReFRAG span budgeting to compress context\n    from scripts.hybrid_search import _merge_and_budget_spans  # type: ignore\n\n    try:\n        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n            logger.debug(\"BUDGET_BEFORE\", extra={\"items\": len(items)})\n        _pairs = {}\n        try:\n            # Relax budgets for context_answer unless explicitly disabled via CTX_RELAX_BUDGETS=0\n            if str(os.environ.get(\"CTX_RELAX_BUDGETS\", \"1\")).strip().lower() in {\n                \"1\",\n                \"true\",\n                \"yes\",\n                \"on\",\n            }:\n                # GLM models have much larger context windows - use higher budgets\n                try:\n                    from scripts.refrag_glm import detect_glm_runtime\n                    is_glm = detect_glm_runtime()\n                except ImportError:\n                    is_glm = False\n                \n                if is_glm:\n                    # GLM: 200K context allows much more code context\n                    _default_budget = \"8192\"  # 8x more than Granite\n                    _default_spans = \"24\"     # 3x more spans\n                else:\n                    # Granite/llamacpp: tighter limits\n                    _default_budget = \"1024\"\n                    _default_spans = \"8\"\n                \n                _pairs = {\n                    \"MICRO_BUDGET_TOKENS\": os.environ.get(\n                        \"MICRO_BUDGET_TOKENS\", _default_budget\n                    ),\n                    \"MICRO_OUT_MAX_SPANS\": os.environ.get(\"MICRO_OUT_MAX_SPANS\", _default_spans),\n                }\n        except Exception:\n            _pairs = {\"MICRO_BUDGET_TOKENS\": \"5000\", \"MICRO_OUT_MAX_SPANS\": \"8\"}\n        with _env_overrides(_pairs):\n            budgeted = _merge_and_budget_spans(items)\n        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n            logger.debug(\"BUDGET_AFTER\", extra={\"items\": len(budgeted)})\n        if not budgeted and items:\n            if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                logger.debug(\"BUDGET_EMPTY_FALLBACK\")\n            budgeted = items\n    except (ImportError, AttributeError, KeyError):\n        logger.warning(\"Span budgeting failed, using raw items\", exc_info=True)\n        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n            logger.debug(\"BUDGET_FAILED\", exc_info=True)\n        budgeted = items\n\n    # Enforce an output max spans knob - do this BEFORE env restore\n    try:\n        out_max = int(os.environ.get(\"MICRO_OUT_MAX_SPANS\", \"12\") or 12)\n    except (ValueError, TypeError):\n        out_max = 12\n    span_cap = max(0, min(out_max, max(0, int(lim))))\n    source_spans = list(budgeted) if budgeted else list(items)\n\n    # Prefer spans that actually contain the main identifier when one is present\n    def _read_span_snippet(span: Dict[str, Any]) -> str:\n        cached = span.get(\"_ident_snippet\")\n        if cached is not None:\n            return str(cached)\n        if not include_snippet:\n            return \"\"\n        try:\n            path = str(span.get(\"path\") or \"\")\n            container_path = str(span.get(\"container_path\") or \"\")\n            host_path = str(span.get(\"host_path\") or \"\")\n            sline = int(span.get(\"start_line\") or 0)\n            eline = int(span.get(\"end_line\") or 0)\n            if not (path or container_path or host_path) or sline <= 0:\n                span[\"_ident_snippet\"] = \"\"\n                return \"\"\n\n            # Build list of candidate paths to try:\n            # 1. Container path (/work/...) - works in Docker/k8s\n            # 2. Host path from metadata - works locally\n            # 3. Relative path from cwd - fallback for local dev\n            candidates: list[str] = []\n            raw_path = container_path or path\n            fp = raw_path\n            if not os.path.isabs(fp):\n                fp = os.path.join(\"/work\", fp)\n            realp = os.path.realpath(fp)\n            if realp.startswith(\"/work/\"):\n                candidates.append(realp)\n            # Try host_path if available (populated by indexer for local runs)\n            if host_path and os.path.isabs(host_path):\n                candidates.append(host_path)\n            # Also try workspace-relative for local dev when /work doesn't exist\n            if not os.path.exists(\"/work\") and path and not os.path.isabs(path):\n                local_rel = os.path.join(os.getcwd(), path)\n                if os.path.exists(local_rel):\n                    candidates.append(local_rel)\n\n            # Try each candidate until we find one that exists\n            for cand in candidates:\n                if os.path.isfile(cand):\n                    with open(cand, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                        lines = f.readlines()\n                    si = max(1, sline - 1)\n                    ei = min(len(lines), max(sline, eline) + 1)\n                    snippet = \"\".join(lines[si - 1 : ei])\n                    span[\"_ident_snippet\"] = snippet\n                    return snippet\n\n            span[\"_ident_snippet\"] = \"\"\n            return \"\"\n        except Exception:\n            span[\"_ident_snippet\"] = \"\"\n            return \"\"\n\n    def _span_haystack(span: Dict[str, Any]) -> str:\n        parts = [\n            str(span.get(\"text\") or \"\"),\n            str(span.get(\"symbol\") or \"\"),\n            str(\n                (span.get(\"relations\") or {}).get(\"symbol_path\")\n                if isinstance(span.get(\"relations\"), dict)\n                else \"\"\n            ),\n            str(span.get(\"path\") or \"\"),\n            str(span.get(\"_ident_snippet\") or \"\"),\n        ]\n        return \" \".join(parts).lower()\n\n    def _span_key(span: Dict[str, Any]) -> tuple[str, int, int]:\n        return (\n            str(span.get(\"path\") or \"\"),\n            int(span.get(\"start_line\") or 0),\n            int(span.get(\"end_line\") or 0),\n        )\n\n    primary_ident = _primary_identifier_from_queries(queries)\n    if primary_ident and source_spans:\n        ident_lower = primary_ident.lower()\n        spans_with_ident: list[Dict[str, Any]] = []\n        spans_without_ident: list[Dict[str, Any]] = []\n        for span in source_spans:\n            hay = _span_haystack(span)\n            contains = ident_lower in hay\n            if not contains:\n                extra = _read_span_snippet(span)\n                if extra:\n                    hay = (hay + \" \" + extra.lower()).strip()\n                    contains = ident_lower in hay\n            if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                logger.debug(\n                    \"IDENT_HAY\",\n                    extra={\n                        \"path\": span.get(\"path\"),\n                        \"contains_ident\": \"yes\" if contains else \"no\",\n                        \"preview\": hay[:80],\n                    },\n                )\n            if contains:\n                spans_with_ident.append(span)\n            else:\n                spans_without_ident.append(span)\n        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n            logger.debug(\n                \"IDENT_FILTER\",\n                extra={\n                    \"ident\": primary_ident,\n                    \"with\": len(spans_with_ident),\n                    \"without\": len(spans_without_ident),\n                },\n            )\n        if spans_with_ident:\n            source_spans = spans_with_ident + spans_without_ident\n        elif budgeted and items:\n            ident_candidates: list[Dict[str, Any]] = []\n            seen = set()\n            for span in items:\n                key = _span_key(span)\n                if key in seen:\n                    continue\n                hay = _span_haystack(span)\n                if ident_lower not in hay:\n                    extra = _read_span_snippet(span)\n                    if extra:\n                        hay = (hay + \" \" + extra.lower()).strip()\n                if ident_lower in hay:\n                    ident_candidates.append(span)\n                    seen.add(key)\n            if ident_candidates:\n                for span in source_spans:\n                    key = _span_key(span)\n                    if key not in seen:\n                        ident_candidates.append(span)\n                        seen.add(key)\n                if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                    logger.debug(\n                        \"IDENT_AUGMENT\",\n                        extra={\n                            \"candidates\": len(ident_candidates),\n                            \"ident\": primary_ident,\n                        },\n                    )\n                source_spans = ident_candidates\n            else:\n                if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                    logger.debug(\"IDENT_AUGMENT_NONE\", extra={\"ident\": primary_ident})\n\n    if span_cap:\n        spans = source_spans[:span_cap]\n    else:\n        spans = []\n\n    # Lift a definition span (IDENT = ...) to the front when possible\n    try:\n        if spans and primary_ident:\n            import re as _re\n\n            def _is_def_span(span: Dict[str, Any]) -> bool:\n                sn = _read_span_snippet(span) or \"\"\n                for _ln in sn.splitlines():\n                    if _re.match(rf\"\\s*{_re.escape(primary_ident)}\\s*=\\s*\", _ln):\n                        return True\n                return False\n\n            cand = next((sp for sp in source_spans if _is_def_span(sp)), None)\n            if not cand:\n                cand = next((sp for sp in items if _is_def_span(sp)), None)\n            if cand:\n                keyset = {\n                    (\n                        str(s.get(\"path\") or \"\"),\n                        int(s.get(\"start_line\") or 0),\n                        int(s.get(\"end_line\") or 0),\n                    )\n                    for s in spans\n                }\n                ckey = (\n                    str(cand.get(\"path\") or \"\"),\n                    int(cand.get(\"start_line\") or 0),\n                    int(cand.get(\"end_line\") or 0),\n                )\n                if ckey not in keyset:\n                    spans = [cand] + (\n                        spans[:-1] if span_cap and len(spans) >= span_cap else spans\n                    )\n                    if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                        logger.debug(\n                            \"IDENT_DEF_LIFT\",\n                            extra={\n                                \"path\": cand.get(\"path\"),\n                                \"start\": cand.get(\"start_line\"),\n                                \"end\": cand.get(\"end_line\"),\n                            },\n                        )\n    except Exception:\n        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n            logger.debug(\"IDENT_DEF_LIFT_FAILED\", exc_info=True)\n\n    if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n        logger.debug(\n            \"SPAN_SELECTION\",\n            extra={\n                \"items\": len(items),\n                \"budgeted\": len(budgeted),\n                \"out_max\": out_max,\n                \"lim\": lim,\n                \"spans\": len(spans),\n            },\n        )\n\n    return spans",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ok_lang_947": {
      "name": "_ok_lang",
      "type": "function",
      "start_line": 947,
      "end_line": 979,
      "content_hash": "aee25a28fe0316947e514b2787463c062f8459b0",
      "content": "        def _ok_lang(it: Dict[str, Any]) -> bool:\n            p = str(it.get(\"path\") or \"\")\n            if callable(_lmp):\n                try:\n                    return bool(_lmp(str(req_language), p))\n                except Exception:\n                    pass\n            # Fallback robust ext mapping with multi-part extension support\n            filename = p.split(\"/\")[-1] if \"/\" in p else p\n            parts = filename.split(\".\")\n            extensions = set()\n            if len(parts) > 1:\n                extensions.add(parts[-1].lower())\n                if len(parts) > 2:\n                    # DEBUG: marker to observe fallback invocation in tests\n                    # print will be captured by pytest -s only\n\n                    multi_ext = \".\".join(parts[-2:]).lower()\n                    extensions.add(multi_ext)\n            table = {\n                \"python\": [\"py\", \"pyi\"],\n                \"typescript\": [\"ts\", \"tsx\", \"d.ts\", \"mts\", \"cts\"],\n                \"javascript\": [\"js\", \"jsx\", \"mjs\", \"cjs\"],\n                \"go\": [\"go\"],\n                \"rust\": [\"rs\"],\n                \"java\": [\"java\"],\n                \"php\": [\"php\"],\n                \"c\": [\"c\", \"h\"],\n                \"cpp\": [\"cpp\", \"cc\", \"cxx\", \"hpp\", \"hxx\"],\n                \"csharp\": [\"cs\"],\n            }\n            lang_exts = table.get(str(req_language).lower(), [])\n            return any(ext in lang_exts for ext in extensions)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__read_span_snippet_1583": {
      "name": "_read_span_snippet",
      "type": "function",
      "start_line": 1583,
      "end_line": 1635,
      "content_hash": "9f5bb902b045ead9a500ef81b7a8005b63c41337",
      "content": "    def _read_span_snippet(span: Dict[str, Any]) -> str:\n        cached = span.get(\"_ident_snippet\")\n        if cached is not None:\n            return str(cached)\n        if not include_snippet:\n            return \"\"\n        try:\n            path = str(span.get(\"path\") or \"\")\n            container_path = str(span.get(\"container_path\") or \"\")\n            host_path = str(span.get(\"host_path\") or \"\")\n            sline = int(span.get(\"start_line\") or 0)\n            eline = int(span.get(\"end_line\") or 0)\n            if not (path or container_path or host_path) or sline <= 0:\n                span[\"_ident_snippet\"] = \"\"\n                return \"\"\n\n            # Build list of candidate paths to try:\n            # 1. Container path (/work/...) - works in Docker/k8s\n            # 2. Host path from metadata - works locally\n            # 3. Relative path from cwd - fallback for local dev\n            candidates: list[str] = []\n            raw_path = container_path or path\n            fp = raw_path\n            if not os.path.isabs(fp):\n                fp = os.path.join(\"/work\", fp)\n            realp = os.path.realpath(fp)\n            if realp.startswith(\"/work/\"):\n                candidates.append(realp)\n            # Try host_path if available (populated by indexer for local runs)\n            if host_path and os.path.isabs(host_path):\n                candidates.append(host_path)\n            # Also try workspace-relative for local dev when /work doesn't exist\n            if not os.path.exists(\"/work\") and path and not os.path.isabs(path):\n                local_rel = os.path.join(os.getcwd(), path)\n                if os.path.exists(local_rel):\n                    candidates.append(local_rel)\n\n            # Try each candidate until we find one that exists\n            for cand in candidates:\n                if os.path.isfile(cand):\n                    with open(cand, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                        lines = f.readlines()\n                    si = max(1, sline - 1)\n                    ei = min(len(lines), max(sline, eline) + 1)\n                    snippet = \"\".join(lines[si - 1 : ei])\n                    span[\"_ident_snippet\"] = snippet\n                    return snippet\n\n            span[\"_ident_snippet\"] = \"\"\n            return \"\"\n        except Exception:\n            span[\"_ident_snippet\"] = \"\"\n            return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__span_haystack_1637": {
      "name": "_span_haystack",
      "type": "function",
      "start_line": 1637,
      "end_line": 1649,
      "content_hash": "7d85ea6fc9adb133957d67265044f3c622c4a525",
      "content": "    def _span_haystack(span: Dict[str, Any]) -> str:\n        parts = [\n            str(span.get(\"text\") or \"\"),\n            str(span.get(\"symbol\") or \"\"),\n            str(\n                (span.get(\"relations\") or {}).get(\"symbol_path\")\n                if isinstance(span.get(\"relations\"), dict)\n                else \"\"\n            ),\n            str(span.get(\"path\") or \"\"),\n            str(span.get(\"_ident_snippet\") or \"\"),\n        ]\n        return \" \".join(parts).lower()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__span_key_1651": {
      "name": "_span_key",
      "type": "function",
      "start_line": 1651,
      "end_line": 1656,
      "content_hash": "934397215b2d09f69ab6ab265dc54b03b4f9c38e",
      "content": "    def _span_key(span: Dict[str, Any]) -> tuple[str, int, int]:\n        return (\n            str(span.get(\"path\") or \"\"),\n            int(span.get(\"start_line\") or 0),\n            int(span.get(\"end_line\") or 0),\n        )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__is_def_span_1739": {
      "name": "_is_def_span",
      "type": "function",
      "start_line": 1739,
      "end_line": 1744,
      "content_hash": "099a077d15e7d35eba94d611702a00f9cc6a02dd",
      "content": "            def _is_def_span(span: Dict[str, Any]) -> bool:\n                sn = _read_span_snippet(span) or \"\"\n                for _ln in sn.splitlines():\n                    if _re.match(rf\"\\s*{_re.escape(primary_ident)}\\s*=\\s*\", _ln):\n                        return True\n                return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ca_build_citations_and_context_1795": {
      "name": "_ca_build_citations_and_context",
      "type": "function",
      "start_line": 1795,
      "end_line": 1957,
      "content_hash": "0c8136ac50f8578f0a17449eb0f205083e253a6b",
      "content": "def _ca_build_citations_and_context(\n    *,\n    spans: list[Dict[str, Any]],\n    include_snippet: bool,\n    queries: list[str],\n) -> tuple[\n    list[Dict[str, Any]],\n    list[str],\n    dict[int, str],\n    str | None,\n    str,\n    int | None,\n    int | None,\n]:\n    \"\"\"Build citations, read snippets, assemble context blocks, and extract def/usage hints.\n    Returns (citations, context_blocks, snippets_by_id, asked_ident, def_line_exact, def_id, usage_id).\n    \"\"\"\n    citations: list[Dict[str, Any]] = []\n    snippets_by_id: dict[int, str] = {}\n    context_blocks: list[str] = []\n\n    asked_ident = _primary_identifier_from_queries(queries)\n    _def_line_exact: str = \"\"\n    _def_id: int | None = None\n    _usage_id: int | None = None\n\n    for idx, it in enumerate(spans, 1):\n        path = str(it.get(\"path\") or \"\")\n        sline = int(it.get(\"start_line\") or 0)\n        eline = int(it.get(\"end_line\") or 0)\n        _hostp = it.get(\"host_path\")\n        _contp = it.get(\"container_path\")\n        # Provide both container-absolute and repo-relative forms for compatibility\n        def _norm(p: str) -> str:\n            try:\n                if p.startswith(\"/work/\"):\n                    return p[len(\"/work/\"):]\n                return p.lstrip(\"/\") if p.startswith(\"/work\") else p\n            except Exception:\n                return p\n        _cit = {\n            \"id\": idx,\n            \"path\": path,  # keep original for backward compatibility (tests expect /work/...)\n            \"rel_path\": _norm(path),\n            \"start_line\": sline,\n            \"end_line\": eline,\n        }\n        if _hostp:\n            _cit[\"host_path\"] = _norm(str(_hostp))\n        if _contp:\n            _cit[\"container_path\"] = str(_contp)\n        citations.append(_cit)\n\n        snippet = str(it.get(\"text\") or \"\").strip()\n        if not snippet and it.get(\"_ident_snippet\"):\n            snippet = str(it.get(\"_ident_snippet\")).strip()\n        if not snippet and path and sline and include_snippet:\n            try:\n                import os as _os\n\n                # Build list of candidate paths to try:\n                # 1. Container path (/work/...) - works in Docker/k8s\n                # 2. Host path from metadata - works locally\n                # 3. Relative path from cwd - fallback for local dev\n                candidates: list[str] = []\n                fp = path\n                if not _os.path.isabs(fp):\n                    fp = _os.path.join(\"/work\", fp)\n                realp = _os.path.realpath(fp)\n                if realp.startswith(\"/work/\"):\n                    candidates.append(realp)\n                # Try host_path if available (populated by indexer for local runs)\n                if _hostp and _os.path.isabs(str(_hostp)):\n                    candidates.append(str(_hostp))\n                # Also try workspace-relative for local dev when /work doesn't exist\n                if not _os.path.exists(\"/work\") and not _os.path.isabs(path):\n                    # Try from cwd (assuming cwd is workspace root)\n                    local_rel = _os.path.join(_os.getcwd(), path)\n                    if _os.path.exists(local_rel):\n                        candidates.append(local_rel)\n\n                # Try each candidate until we find one that exists\n                for cand in candidates:\n                    if _os.path.isfile(cand):\n                        with open(cand, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                            lines = f.readlines()\n                        try:\n                            margin = int(_os.environ.get(\"CTX_READ_MARGIN\", \"1\") or 1)\n                        except (ValueError, TypeError):\n                            margin = 1\n                        si = max(1, sline - margin)\n                        ei = min(len(lines), max(sline, eline) + margin)\n                        snippet = \"\".join(lines[si - 1 : ei])\n                        it[\"_ident_snippet\"] = snippet\n                        break\n            except Exception:\n                snippet = \"\"\n        if not snippet:\n            snippet = str(it.get(\"text\") or \"\").strip()\n        if not snippet and it.get(\"_ident_snippet\"):\n            snippet = str(it.get(\"_ident_snippet\")).strip()\n\n        snippets_by_id[idx] = snippet\n        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n            logger.debug(\n                \"SNIPPET\",\n                extra={\n                    \"idx\": idx,\n                    \"source\": (\"payload\" if it.get(\"text\") else \"fs\"),\n                    \"path\": path,\n                    \"sline\": sline,\n                    \"eline\": eline,\n                    \"length\": len(snippet) if snippet else 0,\n                    \"has_rrf_k\": (\"RRF_K\" in snippet) if snippet else False,\n                    \"empty\": not bool(snippet),\n                },\n            )\n        header = f\"[{idx}] {path}:{sline}-{eline}\"\n        try:\n            MAX_SNIPPET_CHARS = int(os.environ.get(\"CTX_SNIPPET_CHARS\", \"1200\") or 1200)\n        except (ValueError, TypeError):\n            MAX_SNIPPET_CHARS = 1200\n        if snippet and len(snippet) > MAX_SNIPPET_CHARS:\n            snippet = snippet[:MAX_SNIPPET_CHARS] + \"\\n...\"\n        block = header + \"\\n\" + (snippet.strip() if snippet else \"(no code)\")\n        context_blocks.append(block)\n\n        # Extract definition/usage occurrences for robust formatting\n        try:\n            if asked_ident and snippet:\n                import re as _re\n\n                for _ln in str(snippet).splitlines():\n                    if not _def_line_exact and _re.match(\n                        rf\"\\s*{_re.escape(asked_ident)}\\s*=\", _ln\n                    ):\n                        _def_line_exact = _ln.strip()\n                        _def_id = idx\n                    elif (asked_ident in _ln) and (_def_id != idx):\n                        if _usage_id is None:\n                            _usage_id = idx\n        except Exception:\n            pass\n\n    if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n        logger.debug(\n            \"CONTEXT_BLOCKS\",\n            extra={\n                \"spans\": len(spans),\n                \"context_blocks\": len(context_blocks),\n                \"previews\": [block[:300] for block in context_blocks[:3]],\n            },\n        )\n\n    return (\n        citations,\n        context_blocks,\n        snippets_by_id,\n        asked_ident,\n        _def_line_exact,\n        _def_id,\n        _usage_id,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__norm_1828": {
      "name": "_norm",
      "type": "function",
      "start_line": 1828,
      "end_line": 1834,
      "content_hash": "51ca403ba28393cb57e3f3522f8d84fa0a827e02",
      "content": "        def _norm(p: str) -> str:\n            try:\n                if p.startswith(\"/work/\"):\n                    return p[len(\"/work/\"):]\n                return p.lstrip(\"/\") if p.startswith(\"/work\") else p\n            except Exception:\n                return p",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ca_ident_supplement_1960": {
      "name": "_ca_ident_supplement",
      "type": "function",
      "start_line": 1960,
      "end_line": 2028,
      "content_hash": "c192440ebda4ca9c490356c1fee5aa35f4c02113",
      "content": "def _ca_ident_supplement(\n    paths: list[str], ident: str, *, include_snippet: bool, max_hits: int = 4\n) -> list[Dict[str, Any]]:\n    \"\"\"Lightweight FS supplement: when an identifier is asked but the retrieved spans\n    missed its definition/usage, scan a small set of candidate files for that identifier\n    and return minimal spans around the hits. Keeps scope tiny and safe.\n    \"\"\"\n    import os as _os\n    import re as _re\n\n    out: list[Dict[str, Any]] = []\n    seen: set[tuple[str, int, int]] = set()\n    ident = str(ident or \"\").strip()\n    if not ident:\n        return out\n    try:\n        margin = int(_os.environ.get(\"CTX_READ_MARGIN\", \"1\") or 1)\n    except Exception:\n        margin = 1\n    pat_def = _re.compile(rf\"\\b{_re.escape(ident)}\\b\\s*=\")\n    pat_any = _re.compile(rf\"\\b{_re.escape(ident)}\\b\")\n\n    for p in paths or []:\n        if len(out) >= max_hits:\n            break\n        try:\n            fp = str(p)\n            if not fp:\n                continue\n            if not _os.path.isabs(fp):\n                fp = _os.path.join(\"/work\", fp)\n            realp = _os.path.realpath(fp)\n            if not realp.startswith(\"/work/\") or not _os.path.exists(realp):\n                continue\n            with open(realp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                lines = f.readlines()\n            # Prefer explicit definitions first\n            hits: list[tuple[str, int]] = []\n            for idx, line in enumerate(lines, start=1):\n                if pat_def.search(line):\n                    hits.append((\"def\", idx))\n            if not hits:\n                for idx, line in enumerate(lines, start=1):\n                    if pat_any.search(line):\n                        hits.append((\"use\", idx))\n            for kind, idx in hits:\n                key = (p, idx, idx)\n                if key in seen:\n                    continue\n                snippet = \"\"\n                if include_snippet:\n                    si = max(1, idx - margin)\n                    ei = min(len(lines), idx + margin)\n                    snippet = \"\".join(lines[si - 1 : ei])\n                out.append(\n                    {\n                        \"path\": p,\n                        \"start_line\": idx,\n                        \"end_line\": idx,\n                        \"_ident_snippet\": snippet,\n                    }\n                )\n                seen.add(key)\n                if len(out) >= max_hits:\n                    break\n        except Exception:\n            # Best-effort supplement; ignore errors\n            continue\n    return out",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ca_decoder_params_2031": {
      "name": "_ca_decoder_params",
      "type": "function",
      "start_line": 2031,
      "end_line": 2082,
      "content_hash": "467fd5db29b91495c5b9fda838801c8dba1be844",
      "content": "def _ca_decoder_params(max_tokens: Any) -> tuple[int, float, int, float, list[str]]:\n    def _to_int(v, d):\n        try:\n            return int(v)\n        except (ValueError, TypeError):\n            return d\n\n    def _to_float(v, d):\n        try:\n            return float(v)\n        except (ValueError, TypeError):\n            return d\n\n    stop_env = os.environ.get(\"DECODER_STOP\", \"\")\n    default_stops = [\n        \"<|end_of_text|>\",\n        \"<|start_of_role|>\",\n        \"<|end_of_response|>\",\n        \"\\n\\n\\n\",\n    ]\n    stops = default_stops + [s for s in (stop_env.split(\",\") if stop_env else []) if s]\n    \n    # Granite/llamacpp: use env var or 2000 default\n    # GLM: dynamically use model's max_output_tokens from config\n    try:\n        from scripts.refrag_glm import detect_glm_runtime, get_glm_model_name, get_model_config\n        is_glm = detect_glm_runtime()\n    except ImportError:\n        is_glm = False\n    \n    if is_glm:\n        # Pull dynamic limit from GLM model config (imports already succeeded above)\n        glm_model = get_glm_model_name()\n        model_config = get_model_config(glm_model)\n        # Respect env override if set, otherwise use model's max output\n        env_override = os.environ.get(\"DECODER_MAX_TOKENS\", \"\").strip()\n        if env_override:\n            default_max_tokens = _to_int(env_override, model_config.get(\"max_output_tokens\", 4000))\n        else:\n            # Use model's actual max output capability\n            default_max_tokens = model_config.get(\"max_output_tokens\", 4000)\n    else:\n        # Granite/llamacpp\n        default_max_tokens = 2000\n    \n    mtok = _to_int(\n        max_tokens, _to_int(os.environ.get(\"DECODER_MAX_TOKENS\", str(default_max_tokens)), default_max_tokens)\n    )\n    temp = 0.0\n    top_k = _to_int(os.environ.get(\"DECODER_TOP_K\", \"20\"), 20)\n    top_p = _to_float(os.environ.get(\"DECODER_TOP_P\", \"0.85\"), 0.85)\n    return mtok, temp, top_k, top_p, stops",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__to_int_2032": {
      "name": "_to_int",
      "type": "function",
      "start_line": 2032,
      "end_line": 2036,
      "content_hash": "d99fd8e06199ed932eb1f01f9d1b4b8926334f11",
      "content": "    def _to_int(v, d):\n        try:\n            return int(v)\n        except (ValueError, TypeError):\n            return d",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__to_float_2038": {
      "name": "_to_float",
      "type": "function",
      "start_line": 2038,
      "end_line": 2042,
      "content_hash": "bad09f3fa70f4a4434bbe7f5701425f6aa4301d9",
      "content": "    def _to_float(v, d):\n        try:\n            return float(v)\n        except (ValueError, TypeError):\n            return d",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ca_build_prompt_2085": {
      "name": "_ca_build_prompt",
      "type": "function",
      "start_line": 2085,
      "end_line": 2113,
      "content_hash": "8846c549e4157c9f0931b70ae68cd5c8e3fe056c",
      "content": "def _ca_build_prompt(\n    context_blocks: list[str], citations: list[Dict[str, Any]], queries: list[str]\n) -> str:\n    qtxt = \"\\n\".join(queries)\n    docs_text = \"\\n\\n\".join(context_blocks) if context_blocks else \"(no code found)\"\n    sources_footer = (\n        \"\\n\".join([f\"[{c.get('id')}] {c.get('path')}\" for c in citations])\n        if citations\n        else \"\"\n    )\n    system_msg = (\n        \"You are a helpful assistant with access to the following code snippets. \"\n        \"You may use one or more snippets to assist with the user query.\\n\\n\"\n        \"Code snippets:\\n\"\n        f\"{docs_text}\\n\\n\"\n        \"Write the response to the user's input by strictly aligning with the facts in the provided code snippets. \"\n        \"If the information needed to answer the question is not available in the snippets, \"\n        \"inform the user that the question cannot be answered based on the available data.\"\n    )\n    if sources_footer:\n        system_msg += f\"\\nSources:\\n{sources_footer}\"\n    system_msg += \"\\n\" + _answer_style_guidance()\n    user_msg = f\"{qtxt}\"\n    prompt = (\n        f\"<|start_of_role|>system<|end_of_role|>{system_msg}<|end_of_text|>\\n\"\n        f\"<|start_of_role|>user<|end_of_role|>{user_msg}<|end_of_text|>\\n\"\n        \"<|start_of_role|>assistant<|end_of_role|>\"\n    )\n    return prompt",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ca_decode_2116": {
      "name": "_ca_decode",
      "type": "function",
      "start_line": 2116,
      "end_line": 2204,
      "content_hash": "6c17bb3640b69f6613faae97ede121558fccae40",
      "content": "def _ca_decode(\n    prompt: str,\n    *,\n    mtok: int,\n    temp: float,\n    top_k: int,\n    top_p: float,\n    stops: list[str],\n    timeout: float | None = None,\n) -> str:\n    # Select decoder runtime: explicit REFRAG_RUNTIME takes priority,\n    # otherwise auto-detect based on which API keys are configured\n    runtime_kind = str(os.environ.get(\"REFRAG_RUNTIME\", \"\")).strip().lower()\n    if not runtime_kind:\n        # Auto-detect based on available API keys\n        if os.environ.get(\"MINIMAX_API_KEY\", \"\").strip():\n            runtime_kind = \"minimax\"\n        elif os.environ.get(\"GLM_API_KEY\", \"\").strip():\n            runtime_kind = \"glm\"\n        else:\n            runtime_kind = \"llamacpp\"\n    if runtime_kind == \"glm\":\n        from scripts.refrag_glm import GLMRefragClient  # type: ignore\n\n        client = GLMRefragClient()\n    elif runtime_kind == \"minimax\":\n        from scripts.refrag_minimax import MiniMaxRefragClient  # type: ignore\n\n        client = MiniMaxRefragClient()\n    else:\n        from scripts.refrag_llamacpp import LlamaCppRefragClient  # type: ignore\n\n        client = LlamaCppRefragClient()\n    base_tokens = int(max(16, mtok))\n    last_err: Optional[Exception] = None\n    import time as _time\n    for attempt in range(3):\n        # Gradually reduce token budget on retries\n        cur_tokens = (\n            base_tokens if attempt == 0 else max(16, base_tokens // (2 if attempt == 1 else 3))\n        )\n        try:\n            gen_kwargs = {\n                \"max_tokens\": cur_tokens,\n                \"temperature\": temp,\n                \"top_p\": top_p,\n                \"stop\": stops,\n            }\n            if runtime_kind in (\"glm\", \"minimax\"):\n                timeout_value: Optional[float] = None\n                if timeout is not None:\n                    try:\n                        timeout_value = float(timeout)\n                    except Exception:\n                        timeout_value = None\n                if timeout_value is None:\n                    # Check runtime-specific timeout env var, then fall back to generic\n                    env_key = \"MINIMAX_TIMEOUT_SEC\" if runtime_kind == \"minimax\" else \"GLM_TIMEOUT_SEC\"\n                    raw_timeout = os.environ.get(env_key, \"\").strip()\n                    if raw_timeout:\n                        try:\n                            timeout_value = float(raw_timeout)\n                        except Exception:\n                            timeout_value = None\n                if timeout_value is not None:\n                    gen_kwargs[\"timeout\"] = timeout_value\n            else:\n                gen_kwargs.update(\n                    {\n                        \"top_k\": top_k,\n                        \"repeat_penalty\": float(\n                            os.environ.get(\"DECODER_REPEAT_PENALTY\", \"1.15\") or 1.15\n                        ),\n                        \"repeat_last_n\": int(\n                            os.environ.get(\"DECODER_REPEAT_LAST_N\", \"128\") or 128\n                        ),\n                    }\n                )\n            return client.generate_with_soft_embeddings(prompt=prompt, **gen_kwargs)\n        except Exception as e:\n            last_err = e\n            # Allow quick retries with reduced budget and tiny backoff to rescue transient 5xx\n            if attempt < 2:\n                _time.sleep(0.2 * (attempt + 1))\n                continue\n            raise\n    if last_err:\n        raise last_err\n    raise RuntimeError(\"decoder call failed without explicit error\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__ca_postprocess_answer_2207": {
      "name": "_ca_postprocess_answer",
      "type": "function",
      "start_line": 2207,
      "end_line": 2370,
      "content_hash": "41d17d03ee1fa3b373aad4495e3e941770aa3c68",
      "content": "def _ca_postprocess_answer(\n    answer: str,\n    citations: list[Dict[str, Any]],\n    *,\n    asked_ident: str | None = None,\n    def_line_exact: str | None = None,\n    def_id: int | None = None,\n    usage_id: int | None = None,\n    snippets_by_id: dict[int, str] | None = None,\n) -> str:\n    import re as _re\n\n    snippets_by_id = snippets_by_id or {}\n    txt = (answer or \"\").strip()\n    # Strip leaked stop tokens\n    for stop_tok in [\"<|end_of_text|>\", \"<|start_of_role|>\", \"<|end_of_response|>\"]:\n        txt = txt.replace(stop_tok, \"\")\n    # Remove accidental URLs/Markdown links; enforce bracket citations only\n    import re as _re\n    txt = _re.sub(r\"https?://\\S+\", \"\", txt)\n    # Convert Markdown links [text](url) or even incomplete [text]( to [text]\n    txt = _re.sub(r\"\\[([^\\]]+)\\]\\s*\\([^\\)]*\\)?\", r\"[\\1]\", txt)\n    # Cleanup repetition\n    txt = _cleanup_answer(\n        txt,\n        max_chars=(\n            safe_int(\n                os.environ.get(\"CTX_SUMMARY_CHARS\", \"\"),\n                default=0,\n                logger=logger,\n                context=\"CTX_SUMMARY_CHARS\",\n            )\n            or None\n        ),\n    )\n\n    # Strict two-line (optional via env); otherwise remove labels and keep concise\n    try:\n        def_part = \"\"\n        usage_part = \"\"\n        if \"Usage:\" in txt:\n            parts = txt.split(\"Usage:\", 1)\n            def_part = parts[0]\n            usage_part = parts[1]\n            if \"Definition:\" in def_part:\n                def_part = def_part.split(\"Definition:\", 1)[1]\n        elif \"Definition:\" in txt:\n            def_part = txt.split(\"Definition:\", 1)[1]\n        else:\n            def_part = txt\n\n        def _fmt_citation(cid: int | None) -> str:\n            return f\" [{cid}]\" if cid is not None else \"\"\n\n        def_line = None\n        if asked_ident and def_line_exact:\n            cid = (\n                def_id\n                if (def_id is not None)\n                else (citations[0][\"id\"] if citations else None)\n            )\n            def_line = f'Definition: \"{def_line_exact}\"{_fmt_citation(cid)}'\n        else:\n            cand = def_part.strip().strip(\"\\n \")\n            if asked_ident and asked_ident not in cand:\n                cand = \"\"\n            m = _re.search(r'\"([^\"]+)\"', cand)\n\n            q = m.group(1) if m else cand\n            if asked_ident and asked_ident in q:\n                cid = citations[0][\"id\"] if citations else None\n                def_line = f'Definition: \"{q.strip()}\"{_fmt_citation(cid)}'\n        if not def_line:\n            def_line = \"Definition: Not found in provided snippets.\"\n\n        usage_text = \"\"\n        usage_cid: int | None = None\n        try:\n            if asked_ident and (usage_id is not None):\n                _sn = snippets_by_id.get(usage_id) or \"\"\n                if _sn:\n                    for _ln in _sn.splitlines():\n                        if _re.match(rf\"\\s*{_re.escape(asked_ident)}\\s*=\", _ln):\n                            continue\n                        if asked_ident in _ln:\n                            usage_text = _ln.strip()\n                            usage_cid = usage_id\n                            break\n        except Exception:\n            usage_text = \"\"\n            usage_cid = None\n        if not usage_text:\n            usage_text = usage_part.strip().replace(\"\\n\", \" \") if usage_part else \"\"\n            usage_text = _re.sub(r\"\\s+\", \" \", usage_text).strip()\n        if not usage_text:\n            if usage_id is not None:\n                usage_text = \"Appears in the shown code.\"\n                usage_cid = usage_id\n            else:\n                usage_text = \"Not found in provided snippets.\"\n                usage_cid = (\n                    def_id\n                    if (def_id is not None)\n                    else (citations[0][\"id\"] if citations else None)\n                )\n\n        if \"[\" not in usage_text and \"]\" not in usage_text:\n            uid = (\n                usage_cid\n                if (usage_cid is not None)\n                else (\n                    usage_id\n                    if (usage_id is not None)\n                    else (\n                        def_id\n                        if (def_id is not None)\n                        else (citations[0][\"id\"] if citations else None)\n                    )\n                )\n            )\n            usage_line = f\"Usage: {usage_text}{_fmt_citation(uid)}\"\n        else:\n            usage_line = f\"Usage: {usage_text}\"\n\n        if str(os.environ.get(\"CTX_ENFORCE_TWO_LINES\", \"0\")).strip().lower() in {\n            \"1\",\n            \"true\",\n            \"yes\",\n            \"on\",\n        }:\n            txt = f\"{def_line}\\n{usage_line}\".strip()\n        else:\n            txt = _strip_preamble_labels(txt)\n    except Exception:\n        txt = txt.strip()\n\n    if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n        logger.debug(\"LLM_ANSWER\", extra={\"len\": len(txt), \"preview\": txt[:200]})\n\n    if citations and (\"[\" not in txt or \"]\" not in txt):\n        try:\n            first_id = citations[0].get(\"id\")\n            if first_id is not None:\n                txt = txt.rstrip() + f\" [{first_id}]\"\n        except Exception:\n            pass\n\n    _val = _validate_answer_output(txt, citations)\n    if not _val.get(\"ok\", True) and citations:\n        try:\n            fallback = _synthesize_from_citations(\n                asked_ident=asked_ident,\n                def_line_exact=def_line_exact,\n                def_id=def_id,\n                usage_id=usage_id,\n                snippets_by_id=snippets_by_id,\n                citations=citations,\n            )\n            if fallback and fallback.strip():\n                return fallback\n        except Exception:\n            pass\n        return \"insufficient context\"\n    return txt",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__fmt_citation_2258": {
      "name": "_fmt_citation",
      "type": "function",
      "start_line": 2258,
      "end_line": 2259,
      "content_hash": "9d0e6e095fbf97f1a6bf2fa001a89575d2fae507",
      "content": "        def _fmt_citation(cid: int | None) -> str:\n            return f\" [{cid}]\" if cid is not None else \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__synthesize_from_citations_2373": {
      "name": "_synthesize_from_citations",
      "type": "function",
      "start_line": 2373,
      "end_line": 2464,
      "content_hash": "024834d2c270ce08b7f93652860c951a08f25c63",
      "content": "def _synthesize_from_citations(\n    *,\n    asked_ident: str | None,\n    def_line_exact: str | None,\n    def_id: int | None,\n    usage_id: int | None,\n    snippets_by_id: dict[int, str] | None,\n    citations: list[Dict[str, Any]],\n) -> str:\n    \"\"\"Build a concise, extractive fallback answer from available snippets/citations.\n    Returns 1\u20132 short lines with inline bracket citations when possible.\n    \"\"\"\n    import re as _re\n\n    snippets_by_id = snippets_by_id or {}\n\n    def _fmt(cid: int | None) -> str:\n        return f\" [{cid}]\" if cid is not None else \"\"\n\n    lines: list[str] = []\n\n    # Prefer a definition-style line when an identifier is asked\n    if asked_ident:\n        if def_line_exact:\n            cid = (\n                def_id\n                if (def_id is not None)\n                else (citations[0].get(\"id\") if citations else None)\n            )\n            lines.append(f'Definition: \"{def_line_exact}\"{_fmt(cid)}')\n        else:\n            # Try to harvest a definition-like line from snippets\n            best_line = \"\"\n            best_cid: int | None = None\n            for c in citations:\n                sid = c.get(\"id\")\n                sn = snippets_by_id.get(int(sid) if sid is not None else -1) or \"\"\n                for ln in sn.splitlines():\n                    if asked_ident in ln and _re.search(r\"\\b=\\b|def |class \", ln):\n                        best_line = ln.strip()\n                        best_cid = sid\n                        break\n                if best_line:\n                    break\n            if best_line:\n                lines.append(f'Definition: \"{best_line}\"{_fmt(best_cid)}')\n\n        # Usage line when possible\n        use_line = \"\"\n        use_cid: int | None = None\n        if usage_id is not None:\n            sn = snippets_by_id.get(int(usage_id), \"\") or \"\"\n            for ln in sn.splitlines():\n                if asked_ident in ln and not _re.match(\n                    rf\"\\s*{_re.escape(asked_ident)}\\s*=\", ln\n                ):\n                    use_line = ln.strip()\n                    use_cid = usage_id\n                    break\n        if not use_line:\n            # fall back to first citation line mentioning the ident\n            for c in citations:\n                sid = c.get(\"id\")\n                sn = snippets_by_id.get(int(sid) if sid is not None else -1) or \"\"\n                for ln in sn.splitlines():\n                    if asked_ident in ln:\n                        use_line = ln.strip()\n                        use_cid = sid\n                        break\n                if use_line:\n                    break\n        if use_line:\n            lines.append(f\"Usage: {use_line}{_fmt(use_cid)}\")\n\n    # For non-identifier broad queries, provide a brief pointer to the most relevant snippet\n    if not lines:\n        if citations:\n            sid = citations[0].get(\"id\")\n            path = citations[0].get(\"path\")\n            sn = snippets_by_id.get(int(sid) if sid is not None else -1) or \"\"\n            first = next((ln.strip() for ln in sn.splitlines() if ln.strip()), \"\")\n            if first:\n                # Trim to a compact preview\n                if len(first) > 160:\n                    first = first[:160].rstrip() + \"\u2026\"\n                lines.append(f\"Summary: {first}{_fmt(sid)}\")\n            else:\n                lines.append(f\"Summary: See {path}{_fmt(sid)}\")\n        else:\n            lines.append(\"Summary: No code context available.\")\n\n    return \"\\n\".join([ln for ln in lines if ln]).strip()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__fmt_2389": {
      "name": "_fmt",
      "type": "function",
      "start_line": 2389,
      "end_line": 2390,
      "content_hash": "9088c1656171592f0684f60868d215e2f9d75963",
      "content": "    def _fmt(cid: int | None) -> str:\n        return f\" [{cid}]\" if cid is not None else \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__context_answer_impl_2470": {
      "name": "_context_answer_impl",
      "type": "function",
      "start_line": 2470,
      "end_line": 3133,
      "content_hash": "603c402576540f08497360ef66714eeade3086ab",
      "content": "async def _context_answer_impl(\n    query: Any = None,\n    limit: Any = None,\n    per_path: Any = None,\n    budget_tokens: Any = None,\n    include_snippet: Any = None,\n    collection: Any = None,\n    max_tokens: Any = None,\n    temperature: Any = None,\n    mode: Any = None,\n    expand: Any = None,\n    language: Any = None,\n    under: Any = None,\n    kind: Any = None,\n    symbol: Any = None,\n    ext: Any = None,\n    path_regex: Any = None,\n    path_glob: Any = None,\n    not_glob: Any = None,\n    case: Any = None,\n    not_: Any = None,\n    repo: Any = None,\n    kwargs: Any = None,\n    # Dependency injection\n    get_embedding_model_fn=None,\n    expand_query_fn=None,\n    env_lock=None,  # Threading lock for env var manipulation\n    prepare_filters_and_retrieve_fn=None,  # For testability\n) -> Dict[str, Any]:\n    \"\"\"Natural-language Q&A over the repo using retrieval + local LLM (llama.cpp).\n\n    Implementation extracted from mcp_indexer_server.py for testability.\n    The @mcp.tool() decorated wrapper in mcp_indexer_server.py calls this.\n    \"\"\"\n    import time\n    import asyncio\n\n    # Import logger utilities\n    try:\n        from scripts.logger import safe_bool, safe_float\n    except ImportError:\n        def safe_bool(val, default=False, **kw):\n            if val is None:\n                return default\n            if isinstance(val, bool):\n                return val\n            return str(val).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n        def safe_float(val, default=0.0, **kw):\n            try:\n                return float(val) if val is not None else default\n            except Exception:\n                return default\n\n    # Get embedding model function\n    if get_embedding_model_fn is None:\n        from scripts.mcp_impl.admin_tools import _get_embedding_model\n        get_embedding_model_fn = _get_embedding_model\n\n    # Use injected lock or fall back to module-level lock\n    _lock = env_lock if env_lock is not None else _CA_ENV_LOCK\n\n    # Use injected retrieval function or fall back to module function\n    _retrieve_fn = prepare_filters_and_retrieve_fn if prepare_filters_and_retrieve_fn is not None else _ca_prepare_filters_and_retrieve\n\n    # Normalize inputs and compute effective limits/flags\n    _cfg = _ca_unwrap_and_normalize(\n        query,\n        limit,\n        per_path,\n        budget_tokens,\n        include_snippet,\n        collection,\n        max_tokens,\n        temperature,\n        mode,\n        expand,\n        language,\n        under,\n        kind,\n        symbol,\n        ext,\n        path_regex,\n        path_glob,\n        not_glob,\n        case,\n        not_,\n        kwargs,\n    )\n    queries = _cfg[\"queries\"]\n    lim = _cfg[\"limit\"]\n    ppath = _cfg[\"per_path\"]\n    include_snippet = _cfg[\"include_snippet\"]\n    collection = _cfg[\"collection\"]\n    budget_tokens = _cfg[\"budget_tokens\"]\n    max_tokens = _cfg[\"max_tokens\"]\n    temperature = _cfg[\"temperature\"]\n    mode = _cfg[\"mode\"]\n    expand = _cfg[\"expand\"]\n    _flt = _cfg[\"filters\"]\n    req_language = _flt.get(\"language\")\n    under = _flt.get(\"under\")\n    kind = _flt.get(\"kind\")\n    symbol = _flt.get(\"symbol\")\n    ext = _flt.get(\"ext\")\n    path_regex = _flt.get(\"path_regex\")\n    path_glob = _flt.get(\"path_glob\")\n    not_glob = _flt.get(\"not_glob\")\n    case = _flt.get(\"case\")\n    not_ = _flt.get(\"not_\")\n\n    # Enforce sane minimums to avoid empty span selection\n    try:\n        lim = int(lim)\n    except Exception:\n        lim = 15\n    if lim <= 0:\n        lim = 1\n    try:\n        ppath = int(ppath)\n    except Exception:\n        ppath = 5\n    if ppath <= 0:\n        ppath = 1\n\n    # Soft per-call deadline to avoid client-side 60s timeouts\n    _ca_start_ts = time.time()\n\n    if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n        logger.debug(\n            \"ARG_SHAPE\",\n            extra={\"normalized_queries\": queries, \"limit\": lim, \"per_path\": ppath},\n        )\n\n    # Broad-query budget bump (gated)\n    try:\n        _qtext = \" \".join([q for q in (queries or []) if isinstance(q, str)]).lower()\n        _broad_tokens = (\n            \"how\", \"explain\", \"overview\", \"architecture\", \"design\",\n            \"work\", \"works\", \"guide\", \"readme\",\n        )\n        _broad = any(t in _qtext for t in _broad_tokens)\n    except Exception:\n        _broad = False\n    if _broad:\n        try:\n            _factor = float(os.environ.get(\"CTX_BROAD_BUDGET_FACTOR\", \"1.4\"))\n        except Exception:\n            _factor = 1.0\n        if _factor > 1.0:\n            if budget_tokens is not None and str(budget_tokens).strip() != \"\":\n                try:\n                    budget_tokens = int(max(128, int(float(budget_tokens) * _factor)))\n                except Exception:\n                    pass\n            else:\n                try:\n                    _base = int(float(os.environ.get(\"MICRO_BUDGET_TOKENS\", \"5000\")))\n                    budget_tokens = int(max(128, int(_base * _factor)))\n                except Exception:\n                    pass\n\n    # Collection + model setup (reuse indexer defaults)\n    coll = (collection or _default_collection()) or \"\"\n    model_name = os.environ.get(\"EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\")\n    model = get_embedding_model_fn(model_name)\n\n    # Prepare environment toggles for ReFRAG gate-first and budgeting\n    if not _lock.acquire(timeout=30.0):\n        logger.warning(\"env_lock timeout, potential deadlock detected\")\n    prev = {\n        \"REFRAG_MODE\": os.environ.get(\"REFRAG_MODE\"),\n        \"REFRAG_GATE_FIRST\": os.environ.get(\"REFRAG_GATE_FIRST\"),\n        \"REFRAG_CANDIDATES\": os.environ.get(\"REFRAG_CANDIDATES\"),\n        \"COLLECTION_NAME\": os.environ.get(\"COLLECTION_NAME\"),\n        \"MICRO_BUDGET_TOKENS\": os.environ.get(\"MICRO_BUDGET_TOKENS\"),\n    }\n    err: Optional[str] = None\n    items = []\n    eff_language = None\n    eff_path_glob = None\n    eff_not_glob = None\n    override_under = None\n    sym_arg = None\n    cwd_root = None\n    spans = []\n    did_local_expand = False\n    original_queries = list(queries)\n\n    try:\n        # Enable ReFRAG gate-first for context compression\n        os.environ[\"REFRAG_MODE\"] = \"1\"\n        os.environ[\"REFRAG_GATE_FIRST\"] = os.environ.get(\"REFRAG_GATE_FIRST\", \"1\") or \"1\"\n        os.environ[\"COLLECTION_NAME\"] = coll\n        if budget_tokens is not None and str(budget_tokens).strip() != \"\":\n            os.environ[\"MICRO_BUDGET_TOKENS\"] = str(budget_tokens)\n\n        # Track original queries - expansion adds alternates for retrieval only\n        queries = list(queries)\n\n        # For LLM answering, default to include snippets\n        if include_snippet in (None, \"\"):\n            include_snippet = True\n\n        do_expand = safe_bool(\n            expand, default=False, logger=logger, context=\"expand\"\n        ) or safe_bool(\n            os.environ.get(\"HYBRID_EXPAND\", \"0\"),\n            default=False,\n            logger=logger,\n            context=\"HYBRID_EXPAND\",\n        )\n\n        if do_expand and expand_query_fn is not None:\n            try:\n                expand_result = await expand_query_fn(query=queries, max_new=2)\n                if expand_result.get(\"ok\") and expand_result.get(\"alternates\"):\n                    alts = expand_result[\"alternates\"][:2]\n                    if alts:\n                        queries.extend(alts)\n                        did_local_expand = True\n                        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                            logger.debug(\n                                \"Query expansion via expand_query\",\n                                extra={\"original\": original_queries, \"expanded\": queries},\n                            )\n            except Exception as e:\n                logger.debug(\"Query expansion failed\", exc_info=e)\n\n        try:\n            # Refactored retrieval pipeline (filters + hybrid search)\n            _retr = _retrieve_fn(\n                queries=queries,\n                lim=lim,\n                ppath=ppath,\n                filters=_cfg[\"filters\"],\n                model=model,\n                did_local_expand=did_local_expand,\n                kwargs={\n                    \"language\": _cfg[\"filters\"].get(\"language\"),\n                    \"under\": _cfg[\"filters\"].get(\"under\"),\n                    \"path_glob\": _cfg[\"filters\"].get(\"path_glob\"),\n                    \"not_glob\": _cfg[\"filters\"].get(\"not_glob\"),\n                    \"path_regex\": _cfg[\"filters\"].get(\"path_regex\"),\n                    \"ext\": _cfg[\"filters\"].get(\"ext\"),\n                    \"kind\": _cfg[\"filters\"].get(\"kind\"),\n                    \"case\": _cfg[\"filters\"].get(\"case\"),\n                    \"symbol\": _cfg[\"filters\"].get(\"symbol\"),\n                },\n                repo=repo,\n            )\n            items = _retr[\"items\"]\n            eff_language = _retr[\"eff_language\"]\n            eff_path_glob = _retr[\"eff_path_glob\"]\n            eff_not_glob = _retr[\"eff_not_glob\"]\n            override_under = _retr[\"override_under\"]\n            sym_arg = _retr[\"sym_arg\"]\n            cwd_root = _retr[\"cwd_root\"]\n            path_regex = _retr[\"path_regex\"]\n            ext = _retr[\"ext\"]\n            kind = _retr[\"kind\"]\n            case = _retr[\"case\"]\n            req_language = eff_language\n\n            fallback_kwargs = dict(kwargs or {})\n            for key in (\"path_glob\", \"language\", \"under\"):\n                fallback_kwargs.pop(key, None)\n\n            spans = _ca_fallback_and_budget(\n                items=items,\n                queries=queries,\n                lim=lim,\n                ppath=ppath,\n                eff_language=eff_language,\n                eff_path_glob=eff_path_glob,\n                eff_not_glob=eff_not_glob,\n                path_regex=path_regex,\n                sym_arg=sym_arg,\n                ext=ext,\n                kind=kind,\n                override_under=override_under,\n                did_local_expand=did_local_expand,\n                model=model,\n                req_language=req_language,\n                not_=not_,\n                case=case,\n                cwd_root=cwd_root,\n                include_snippet=bool(include_snippet),\n                kwargs=fallback_kwargs,\n                repo=repo,\n            )\n        except Exception as e:\n            err = str(e)\n            spans = []\n            if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n                logger.debug(\"EXCEPTION\", exc_info=e, extra={\"error\": err})\n    finally:\n        for k, v in prev.items():\n            if v is None:\n                try:\n                    del os.environ[k]\n                except KeyError:\n                    pass  # Already unset, fine\n                except Exception as e:\n                    logger.error(f\"Failed to restore env var {k}: {e}\")\n            else:\n                os.environ[k] = v\n        _lock.release()\n\n    if err is not None:\n        return {\n            \"error\": f\"hybrid search failed: {err}\",\n            \"citations\": [],\n            \"query\": original_queries,\n        }\n\n    # Ensure final retrieval call reflects Tier-2 relaxed filters\n    try:\n        from scripts.hybrid_search import run_hybrid_search as _rh\n        _ = _rh(\n            queries=queries,\n            limit=int(max(lim, 1)),\n            per_path=int(max(ppath, 1)),\n        )\n    except Exception:\n        pass\n\n    # Build citations and context payload for the decoder\n    (\n        citations,\n        context_blocks,\n        snippets_by_id,\n        asked_ident,\n        _def_line_exact,\n        _def_id,\n        _usage_id,\n    ) = _ca_build_citations_and_context(\n        spans=spans,\n        include_snippet=bool(include_snippet),\n        queries=queries,\n    )\n\n    # Salvage: if citations are empty but we have items, rebuild from raw items\n    if not citations:\n        try:\n            (\n                citations2,\n                context_blocks2,\n                snippets_by_id2,\n                asked_ident2,\n                _def_line_exact2,\n                _def_id2,\n                _usage_id2,\n            ) = _ca_build_citations_and_context(\n                spans=(items or []),\n                include_snippet=bool(include_snippet),\n                queries=queries,\n            )\n            if citations2:\n                citations = citations2\n                context_blocks = context_blocks2\n                snippets_by_id = snippets_by_id2\n                asked_ident = asked_ident2\n                _def_line_exact = _def_line_exact2\n                _def_id = _def_id2\n                _usage_id = _usage_id2\n        except Exception:\n            pass\n\n    # If still no citations, return an explicit insufficient-context answer\n    if not citations:\n        return {\n            \"answer\": \"insufficient context\",\n            \"citations\": [],\n            \"query\": original_queries,\n            \"used\": {\"gate_first\": True, \"refrag\": True, \"no_citations\": True},\n        }\n\n    # FS supplement for identifier definitions\n    if asked_ident and not _def_line_exact:\n        cand_paths: list[str] = []\n        for it in items or []:\n            p = it.get(\"path\") or it.get(\"host_path\") or it.get(\"container_path\")\n            if p and str(p) not in cand_paths:\n                cand_paths.append(str(p))\n        try:\n            qj3 = \" \".join(queries)\n            import re as _re\n            m = _re.search(r\"in\\s+([\\w./-]+\\.py)\\b\", qj3)\n            if m:\n                fp = m.group(1)\n                if fp not in cand_paths:\n                    cand_paths.append(fp)\n        except Exception:\n            pass\n        supplements = []\n        if str(os.environ.get(\"CTX_TIER3_FS\", \"0\")).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}:\n            supplements = _ca_ident_supplement(\n                cand_paths,\n                asked_ident,\n                include_snippet=bool(include_snippet),\n                max_hits=3,\n            )\n        if supplements:\n            def _k(s: Dict[str, Any]):\n                return (\n                    str(s.get(\"path\") or \"\"),\n                    int(s.get(\"start_line\") or 0),\n                    int(s.get(\"end_line\") or 0),\n                )\n            seen_keys = {_k(s) for s in spans}\n            new_spans = []\n            for s in supplements:\n                k = _k(s)\n                if k not in seen_keys:\n                    new_spans.append(s)\n                    seen_keys.add(k)\n            if new_spans:\n                spans = new_spans + spans\n                (\n                    citations,\n                    context_blocks,\n                    snippets_by_id,\n                    asked_ident,\n                    _def_line_exact,\n                    _def_id,\n                    _usage_id,\n                ) = _ca_build_citations_and_context(\n                    spans=spans,\n                    include_snippet=bool(include_snippet),\n                    queries=queries,\n                )\n\n    # Debug: log span details\n    if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n        logger.debug(\n            \"CONTEXT_BLOCKS\",\n            extra={\n                \"spans\": len(spans),\n                \"context_blocks\": len(context_blocks),\n                \"previews\": [block[:300] for block in context_blocks[:3]],\n            },\n        )\n\n    # Decoder params and stops\n    mtok, temp, top_k, top_p, stops = _ca_decoder_params(max_tokens)\n\n    # Deadline-aware decode budgeting\n    _client_deadline_sec = safe_float(\n        os.environ.get(\"CTX_CLIENT_DEADLINE_SEC\", \"178\"),\n        default=178.0, logger=logger, context=\"CTX_CLIENT_DEADLINE_SEC\",\n    )\n    _tokens_per_sec = safe_float(\n        os.environ.get(\"DECODER_TOKENS_PER_SEC\", \"\"),\n        default=10.0, logger=logger, context=\"DECODER_TOKENS_PER_SEC\",\n    )\n    _decoder_timeout_cap = safe_float(\n        os.environ.get(\"CTX_DECODER_TIMEOUT_CAP\", \"170\"),\n        default=170.0, logger=logger, context=\"CTX_DECODER_TIMEOUT_CAP\",\n    )\n    _deadline_margin = safe_float(\n        os.environ.get(\"CTX_DEADLINE_MARGIN_SEC\", \"6\"),\n        default=6.0, logger=logger, context=\"CTX_DEADLINE_MARGIN_SEC\",\n    )\n\n    # Call llama.cpp decoder (requires REFRAG_DECODER=1)\n    try:\n        from scripts.refrag_llamacpp import is_decoder_enabled\n\n        if not is_decoder_enabled():\n            logger.info(\"Decoder disabled; returning extractive fallback with citations\")\n            _fallback_txt = _ca_postprocess_answer(\n                \"\",\n                citations,\n                asked_ident=asked_ident,\n                def_line_exact=_def_line_exact,\n                def_id=_def_id,\n                usage_id=_usage_id,\n                snippets_by_id=snippets_by_id,\n            )\n            return {\n                \"error\": \"decoder disabled: set REFRAG_DECODER=1 and start llamacpp\",\n                \"answer\": _fallback_txt.strip(),\n                \"citations\": citations,\n                \"query\": original_queries,\n                \"used\": {\"decoder\": False, \"extractive_fallback\": True},\n            }\n\n        # Build prompt and decode (deadline-aware)\n        prompt = _ca_build_prompt(context_blocks, citations, original_queries)\n        if os.environ.get(\"DEBUG_CONTEXT_ANSWER\"):\n            logger.debug(\"LLM_PROMPT\", extra={\"length\": len(prompt)})\n\n        _elapsed = time.time() - _ca_start_ts\n        _remain = float(_client_deadline_sec) - _elapsed\n        if _remain <= float(_deadline_margin):\n            _fallback_txt = _ca_postprocess_answer(\n                \"\",\n                citations,\n                asked_ident=asked_ident,\n                def_line_exact=_def_line_exact,\n                def_id=_def_id,\n                usage_id=_usage_id,\n                snippets_by_id=snippets_by_id,\n            )\n            return {\n                \"answer\": _fallback_txt.strip(),\n                \"citations\": citations,\n                \"query\": original_queries,\n                \"used\": {\"gate_first\": True, \"refrag\": True, \"deadline_fallback\": True},\n            }\n\n        # Tighten max_tokens and decoder HTTP timeout to fit remaining time\n        try:\n            _allow_tokens = int(\n                max(\n                    16.0,\n                    min(\n                        float(mtok),\n                        max(0.0, _remain - max(0.0, float(_deadline_margin) - 2.0))\n                        * float(_tokens_per_sec),\n                    ),\n                )\n            )\n        except Exception:\n            _allow_tokens = int(max(16, int(mtok)))\n        mtok = int(_allow_tokens)\n        _llama_timeout = int(max(5.0, min(_decoder_timeout_cap, max(1.0, _remain - 1.0))))\n\n        with _env_overrides({\"LLAMACPP_TIMEOUT_SEC\": str(_llama_timeout)}):\n            answer = _ca_decode(\n                prompt,\n                mtok=mtok,\n                temp=temp,\n                top_k=top_k,\n                top_p=top_p,\n                stops=stops,\n                timeout=_llama_timeout,\n            )\n\n        # Post-process and validate\n        answer = _ca_postprocess_answer(\n            answer,\n            citations,\n            asked_ident=asked_ident,\n            def_line_exact=_def_line_exact,\n            def_id=_def_id,\n            usage_id=_usage_id,\n            snippets_by_id=snippets_by_id,\n        )\n\n    except Exception as e:\n        return {\n            \"error\": f\"decoder call failed: {e}\",\n            \"citations\": citations,\n            \"query\": original_queries,\n        }\n\n    # Final introspection call\n    try:\n        from scripts.hybrid_search import run_hybrid_search as _rh2\n        _ = _rh2(\n            queries=queries,\n            limit=int(max(lim, 1)),\n            per_path=int(max(ppath, 1)),\n        )\n    except Exception:\n        pass\n\n    # Optional: provide per-query answers/citations for pack mode\n    answers_by_query = None\n    try:\n        if len(original_queries) > 1 and str(_cfg.get(\"mode\") or \"\").strip().lower() == \"pack\":\n            import re as _re\n\n            def _tok2(s: str) -> list[str]:\n                try:\n                    return [\n                        w.lower()\n                        for w in _re.split(r\"[^A-Za-z0-9_]+\", str(s or \"\"))\n                        if len(w) >= 3\n                    ]\n                except Exception:\n                    return []\n\n            id_to_cit = {\n                int(c.get(\"id\") or 0): c\n                for c in (citations or [])\n                if int(c.get(\"id\") or 0) > 0\n            }\n            id_to_block = {idx + 1: blk for idx, blk in enumerate(context_blocks or [])}\n\n            answers_by_query = []\n            for q in original_queries:\n                try:\n                    toks = set(_tok2(q))\n                    picked_ids: list[int] = []\n                    if toks:\n                        for cid, c in id_to_cit.items():\n                            path_l = str(c.get(\"path\") or \"\").lower()\n                            sn = (snippets_by_id.get(cid) or \"\").lower()\n                            if any(t in sn or t in path_l for t in toks):\n                                picked_ids.append(cid)\n                                if len(picked_ids) >= 6:\n                                    break\n                    if not picked_ids:\n                        picked_ids = [c.get(\"id\") for c in (citations or [])[:2] if c.get(\"id\")]\n\n                    cits_i = [id_to_cit[cid] for cid in picked_ids if cid in id_to_cit]\n                    ctx_blocks_i = [id_to_block[cid] for cid in picked_ids if cid in id_to_block]\n\n                    if not cits_i:\n                        answers_by_query.append({\n                            \"query\": q,\n                            \"answer\": \"insufficient context\",\n                            \"citations\": [],\n                        })\n                        continue\n\n                    prompt_i = _ca_build_prompt(ctx_blocks_i, cits_i, [q])\n                    ans_raw_i = _ca_decode(\n                        prompt_i,\n                        mtok=mtok,\n                        temp=temp,\n                        top_k=top_k,\n                        top_p=top_p,\n                        stops=stops,\n                        timeout=_llama_timeout,\n                    )\n\n                    asked_ident_i = _primary_identifier_from_queries([q])\n                    ans_i = _ca_postprocess_answer(\n                        ans_raw_i,\n                        cits_i,\n                        asked_ident=asked_ident_i,\n                        def_line_exact=None,\n                        def_id=None,\n                        usage_id=None,\n                        snippets_by_id={cid: snippets_by_id.get(cid, \"\") for cid in picked_ids},\n                    )\n\n                    answers_by_query.append({\n                        \"query\": q,\n                        \"answer\": ans_i,\n                        \"citations\": cits_i,\n                    })\n                except Exception as _e:\n                    answers_by_query.append({\n                        \"query\": q,\n                        \"answer\": \"\",\n                        \"citations\": [],\n                        \"error\": str(_e),\n                    })\n    except Exception:\n        answers_by_query = None\n\n    out = {\n        \"answer\": answer.strip(),\n        \"citations\": citations,\n        \"query\": original_queries,\n        \"used\": {\"gate_first\": True, \"refrag\": True},\n    }\n    if answers_by_query:\n        out[\"answers_by_query\"] = answers_by_query\n    return out",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_safe_bool_2511": {
      "name": "safe_bool",
      "type": "function",
      "start_line": 2511,
      "end_line": 2516,
      "content_hash": "4a110050f4b1457316eb31ea1cced8e5ae17f06f",
      "content": "        def safe_bool(val, default=False, **kw):\n            if val is None:\n                return default\n            if isinstance(val, bool):\n                return val\n            return str(val).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_safe_float_2517": {
      "name": "safe_float",
      "type": "function",
      "start_line": 2517,
      "end_line": 2521,
      "content_hash": "92ed9c840b7dbfa8a13a286057b779a34e294d16",
      "content": "        def safe_float(val, default=0.0, **kw):\n            try:\n                return float(val) if val is not None else default\n            except Exception:\n                return default",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__k_2873": {
      "name": "_k",
      "type": "function",
      "start_line": 2873,
      "end_line": 2878,
      "content_hash": "ba78fbd8d177a797e5a004942e1a39f492781404",
      "content": "            def _k(s: Dict[str, Any]):\n                return (\n                    str(s.get(\"path\") or \"\"),\n                    int(s.get(\"start_line\") or 0),\n                    int(s.get(\"end_line\") or 0),\n                )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__tok2_3044": {
      "name": "_tok2",
      "type": "function",
      "start_line": 3044,
      "end_line": 3052,
      "content_hash": "f448a58a8f1993f73abcd828245c17c1e5d862e9",
      "content": "            def _tok2(s: str) -> list[str]:\n                try:\n                    return [\n                        w.lower()\n                        for w in _re.split(r\"[^A-Za-z0-9_]+\", str(s or \"\"))\n                        if len(w) >= 3\n                    ]\n                except Exception:\n                    return []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}