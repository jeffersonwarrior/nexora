{
  "file_path": "/work/external-deps/Context-Engine/scripts/refrag_glm.py",
  "file_hash": "ebb5f1622814aeb9c6cdb4c076d0071da94cfc23",
  "updated_at": "2025-12-26T17:34:23.882429",
  "symbols": {
    "function_get_model_config_64": {
      "name": "get_model_config",
      "type": "function",
      "start_line": 64,
      "end_line": 92,
      "content_hash": "49405d84071d9993bda0aa8a2ee4b6cddbdde237",
      "content": "def get_model_config(model: str) -> dict[str, Any]:\n    \"\"\"Get configuration for a GLM model version with backwards compatibility.\n    \n    Matches model names like 'glm-4.7', 'glm-4.6-air', 'glm-4.5-flash', etc.\n    Falls back to default config for unknown models.\n    \"\"\"\n    model_lower = model.lower()\n    # Try exact match first\n    if model_lower in GLM_MODEL_CONFIGS:\n        return GLM_MODEL_CONFIGS[model_lower]\n    # Try matching base version (e.g., 'glm-4.7-air' -> 'glm-4.7')\n    for base_model, config in GLM_MODEL_CONFIGS.items():\n        if model_lower.startswith(base_model):\n            return config\n    # Check for version pattern (glm-4.X)\n    # Note: This assumes GLM uses glm-4.X versioning. Future major versions\n    # (e.g., glm-5.X) will fall through to GLM_DEFAULT_CONFIG until explicitly added.\n    match = re.match(r\"glm-4\\.(\\d+)\", model_lower)\n    if match:\n        version = int(match.group(1))\n        if version >= 7:\n            return GLM_MODEL_CONFIGS[\"glm-4.7\"]\n        elif version == 6:\n            return GLM_MODEL_CONFIGS[\"glm-4.6\"]\n        elif version == 5:\n            return GLM_MODEL_CONFIGS[\"glm-4.5\"]\n        else:\n            return GLM_DEFAULT_CONFIG\n    return GLM_DEFAULT_CONFIG",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_detect_glm_runtime_95": {
      "name": "detect_glm_runtime",
      "type": "function",
      "start_line": 95,
      "end_line": 110,
      "content_hash": "bd38faa9b811d89de10a973e46bd9a1054d2d74f",
      "content": "def detect_glm_runtime() -> bool:\n    \"\"\"Detect whether the GLM runtime should be considered active.\n\n    Shared helper to centralise the environment-variable logic used to decide\n    if GLM-based behaviour should be enabled.\n\n    Returns:\n        True only if REFRAG_RUNTIME is explicitly set to 'glm' (case-insensitive).\n        False otherwise (e.g., REFRAG_RUNTIME='llamacpp', 'minimax', or unset).\n\n    Note:\n        The presence of GLM_API_KEY alone does NOT auto-enable GLM.\n        You must explicitly set REFRAG_RUNTIME=glm to use the GLM runtime.\n    \"\"\"\n    runtime = os.environ.get(\"REFRAG_RUNTIME\", \"\").strip().lower()\n    return runtime == \"glm\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_glm_model_name_113": {
      "name": "get_glm_model_name",
      "type": "function",
      "start_line": 113,
      "end_line": 119,
      "content_hash": "fc966859089297f0497de132cd94a4ae2608938a",
      "content": "def get_glm_model_name() -> str:\n    \"\"\"Get the active GLM model name with consistent fallback.\n    \n    Returns GLM_MODEL env var if set and non-empty, otherwise 'glm-4.6'.\n    \"\"\"\n    model = os.environ.get(\"GLM_MODEL\", \"\").strip()\n    return model if model else \"glm-4.6\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_GLMRefragClient_122": {
      "name": "GLMRefragClient",
      "type": "class",
      "start_line": 122,
      "end_line": 456,
      "content_hash": "047f68cbd60c7bab4e96414662489c031dc2bf28",
      "content": "class GLMRefragClient:\n    \"\"\"GLM client exposing generate_with_soft_embeddings(prompt, ...).\n\n    Notes:\n    - soft_embeddings are ignored (GLM does not support KV/soft-embed injection)\n    - prompt-mode only; mirrors llama.cpp adapter surface\n    - Uses OpenAI SDK with custom base_url for GLM API\n    - Backwards compatible with GLM 4.5, 4.6, and 4.7\n    \"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None) -> None:\n        self.api_key = api_key or os.environ.get(\"GLM_API_KEY\", \"\").strip()\n        if not self.api_key:\n            raise ValueError(\"GLM_API_KEY is required when using REFRAG_RUNTIME=glm\")\n        self.base_url = base_url or os.environ.get(\"GLM_API_BASE\", \"https://api.z.ai/api/paas/v4/\")\n        from openai import OpenAI\n        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n\n    def _parse_prompt_to_messages(self, prompt: str) -> list[dict[str, str]]:\n        \"\"\"Parse Granite-style prompt into proper OpenAI messages array.\n\n        Handles prompts like:\n        <|start_of_role|>system<|end_of_role|>...<|end_of_text|>\n        <|start_of_role|>user<|end_of_role|>...<|end_of_text|>\n        <|start_of_role|>assistant<|end_of_role|>\n        \"\"\"\n        import re\n        messages: list[dict[str, str]] = []\n\n        # Pattern to extract role blocks\n        pattern = r'<\\|start_of_role\\|>(\\w+)<\\|end_of_role\\|>(.*?)(?:<\\|end_of_text\\|>|$)'\n        matches = re.findall(pattern, prompt, re.DOTALL)\n\n        if matches:\n            for role, content in matches:\n                content = content.strip()\n                if content:  # Skip empty content (like trailing assistant role)\n                    messages.append({\"role\": role, \"content\": content})\n\n        # Fallback: if no matches, treat entire prompt as user message\n        if not messages:\n            messages = [{\"role\": \"user\", \"content\": prompt}]\n\n        return messages\n\n    def generate_with_soft_embeddings(\n        self,\n        prompt: str,\n        soft_embeddings: Optional[list[list[float]]] = None,  # unused\n        max_tokens: int = 256,\n        **gen_kwargs: Any,\n    ) -> str:\n        # Model selection priority:\n        # 1. Explicit model= parameter\n        # 2. disable_thinking=True -> GLM_MODEL_FAST (backwards compat for expand_query)\n        # 3. GLM_MODEL env var\n        # 4. Default: glm-4.6\n        disable_thinking = bool(gen_kwargs.pop(\"disable_thinking\", False))\n        explicit_model = gen_kwargs.pop(\"model\", None)\n        if explicit_model:\n            model = explicit_model\n        elif disable_thinking:\n            model = os.environ.get(\"GLM_MODEL_FAST\", \"glm-4.5\")\n        else:\n            model = os.environ.get(\"GLM_MODEL\", \"glm-4.6\")\n\n        # no_thinking: disable thinking on current model WITHOUT switching models\n        # Useful when you want to use GLM-4.7 but skip reasoning for simple tasks\n        no_thinking = bool(gen_kwargs.pop(\"no_thinking\", False))\n\n        # Get model-specific configuration for backwards compatibility\n        model_config = get_model_config(model)\n\n        # Use model-specific defaults, allow override via gen_kwargs\n        # For GLM-4.7: temp=1.0, top_p=0.95 (per migration guide)\n        # For backwards compat: caller can still override with lower values for stable output\n        temperature = float(gen_kwargs.get(\"temperature\", model_config[\"temperature\"]))\n        top_p = float(gen_kwargs.get(\"top_p\", model_config[\"top_p\"]))\n\n        # Cap max_tokens to model's output limit\n        requested_max = int(gen_kwargs.get(\"max_tokens\", max_tokens))\n        effective_max = min(requested_max, model_config[\"max_output_tokens\"])\n        stop = gen_kwargs.get(\"stop\")\n        timeout = gen_kwargs.pop(\"timeout\", None)\n        force_json = bool(gen_kwargs.pop(\"force_json\", False))\n\n        # Streaming options (GLM-4.7+ supports tool_stream)\n        stream = bool(gen_kwargs.pop(\"stream\", False))\n        tool_stream = bool(gen_kwargs.pop(\"tool_stream\", False))\n        tools = gen_kwargs.pop(\"tools\", None)\n\n        # Thinking/reasoning options\n        enable_thinking = gen_kwargs.pop(\"enable_thinking\", None)\n        # Combine disable_thinking and no_thinking flags for thinking control\n        should_disable_thinking = disable_thinking or no_thinking\n        \n        try:\n            timeout_val = float(timeout) if timeout is not None else None\n        except Exception:\n            timeout_val = None\n\n        try:\n            create_kwargs: dict[str, Any] = {\n                \"model\": model,\n                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n                \"max_tokens\": effective_max,\n                \"temperature\": temperature,\n                \"top_p\": top_p,\n                \"stop\": stop if stop else None,\n                \"timeout\": timeout_val,\n            }\n            \n            # Streaming support\n            if stream:\n                create_kwargs[\"stream\"] = True\n            \n            # Tool calling support\n            if tools:\n                create_kwargs[\"tools\"] = tools\n                # GLM-4.7+ supports streaming tool call parameters\n                if tool_stream and model_config[\"supports_tool_stream\"]:\n                    create_kwargs[\"extra_body\"] = create_kwargs.get(\"extra_body\", {})\n                    create_kwargs[\"extra_body\"][\"tool_stream\"] = True\n            \n            # When explicitly requested and supported by the backend, ask for\n            # JSON-only responses. If the provider rejects this parameter, the\n            # API call will raise and the caller will handle the failure.\n            if force_json:\n                create_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n            \n            # Thinking/deep reasoning control (GLM-4.5/4.6/4.7 all support thinking param)\n            # All three models support thinking: {type: \"disabled\"} for fast responses\n            if model_config[\"supports_thinking\"]:\n                if should_disable_thinking:\n                    create_kwargs[\"extra_body\"] = create_kwargs.get(\"extra_body\", {})\n                    create_kwargs[\"extra_body\"][\"thinking\"] = {\"type\": \"disabled\"}\n                elif enable_thinking:\n                    create_kwargs[\"extra_body\"] = create_kwargs.get(\"extra_body\", {})\n                    create_kwargs[\"extra_body\"][\"thinking\"] = {\"type\": \"enabled\"}\n\n            response = self.client.chat.completions.create(**create_kwargs)\n            \n            # Handle streaming response\n            if stream:\n                return self._handle_streaming_response(response)\n            \n            msg = response.choices[0].message\n            # GLM models may use either content or reasoning_content\n            content = msg.content or \"\"\n            # Fallback to reasoning_content if content is empty (thinking models)\n            if not content.strip():\n                content = getattr(msg, 'reasoning_content', None) or \"\"\n            return content.strip()\n        except Exception as e:\n            raise RuntimeError(f\"GLM completion failed: {e}\")\n    \n    def _handle_streaming_response(\n        self,\n        response: Any,\n    ) -> str:\n        \"\"\"Handle streaming response, accumulating content and reasoning.\n        \n        Supports:\n        - delta.content: Regular content tokens\n        - delta.reasoning_content: Thinking/reasoning tokens (GLM-4.6+)\n        \n        Note: Tool call streaming (GLM-4.7+) is parsed but not currently used\n        since context_answer returns text, not tool calls.\n        \"\"\"\n        content_parts: list[str] = []\n        reasoning_parts: list[str] = []\n        \n        for chunk in response:\n            if not chunk.choices:\n                continue\n            delta = chunk.choices[0].delta\n            \n            # Accumulate reasoning content (thinking process)\n            if hasattr(delta, 'reasoning_content') and delta.reasoning_content:\n                reasoning_parts.append(delta.reasoning_content)\n            \n            # Accumulate regular content\n            if hasattr(delta, 'content') and delta.content:\n                content_parts.append(delta.content)\n        \n        # Return content, fallback to reasoning if content is empty\n        content = \"\".join(content_parts).strip()\n        if not content:\n            content = \"\".join(reasoning_parts).strip()\n        return content\n    \n    def generate_with_streaming(\n        self,\n        prompt: str,\n        max_tokens: int = 256,\n        on_content: Optional[Any] = None,\n        on_reasoning: Optional[Any] = None,\n        on_tool_call: Optional[Any] = None,\n        **gen_kwargs: Any,\n    ) -> dict[str, Any]:\n        \"\"\"Generate with streaming, providing callbacks for real-time output.\n        \n        Callbacks:\n        - on_content(token: str): Called for each content token\n        - on_reasoning(token: str): Called for each reasoning/thinking token\n        - on_tool_call(idx: int, name: str, args: str): Called for tool call updates\n        \n        Returns:\n            Dict with 'content', 'reasoning', and 'tool_calls' accumulated results\n        \"\"\"\n        disable_thinking = bool(gen_kwargs.pop(\"disable_thinking\", False))\n        if disable_thinking:\n            model = os.environ.get(\"GLM_MODEL_FAST\", \"glm-4.5\")\n        else:\n            model = os.environ.get(\"GLM_MODEL\", \"glm-4.6\")\n        \n        model_config = get_model_config(model)\n        \n        temperature = float(gen_kwargs.get(\"temperature\", model_config[\"temperature\"]))\n        top_p = float(gen_kwargs.get(\"top_p\", model_config[\"top_p\"]))\n        requested_max = int(gen_kwargs.get(\"max_tokens\", max_tokens))\n        effective_max = min(requested_max, model_config[\"max_output_tokens\"])\n        \n        stop = gen_kwargs.get(\"stop\")\n        tools = gen_kwargs.pop(\"tools\", None)\n        tool_stream = bool(gen_kwargs.pop(\"tool_stream\", False))\n        enable_thinking = gen_kwargs.pop(\"enable_thinking\", None)\n        \n        create_kwargs: dict[str, Any] = {\n            \"model\": model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"max_tokens\": effective_max,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"stop\": stop if stop else None,\n            \"stream\": True,\n        }\n        \n        if tools:\n            create_kwargs[\"tools\"] = tools\n            if tool_stream and model_config[\"supports_tool_stream\"]:\n                create_kwargs[\"extra_body\"] = create_kwargs.get(\"extra_body\", {})\n                create_kwargs[\"extra_body\"][\"tool_stream\"] = True\n        \n        if model_config[\"supports_thinking\"]:\n            if disable_thinking:\n                create_kwargs[\"extra_body\"] = create_kwargs.get(\"extra_body\", {})\n                create_kwargs[\"extra_body\"][\"thinking\"] = {\"type\": \"disabled\"}\n            elif enable_thinking:\n                create_kwargs[\"extra_body\"] = create_kwargs.get(\"extra_body\", {})\n                create_kwargs[\"extra_body\"][\"thinking\"] = {\"type\": \"enabled\"}\n        \n        content_parts: list[str] = []\n        reasoning_parts: list[str] = []\n        tool_calls: dict[int, dict[str, str]] = {}\n        \n        response = self.client.chat.completions.create(**create_kwargs)\n        \n        for chunk in response:\n            if not chunk.choices:\n                continue\n            delta = chunk.choices[0].delta\n            \n            if hasattr(delta, 'reasoning_content') and delta.reasoning_content:\n                reasoning_parts.append(delta.reasoning_content)\n                if on_reasoning:\n                    on_reasoning(delta.reasoning_content)\n            \n            if hasattr(delta, 'content') and delta.content:\n                content_parts.append(delta.content)\n                if on_content:\n                    on_content(delta.content)\n            \n            if hasattr(delta, 'tool_calls') and delta.tool_calls:\n                for tool_call in delta.tool_calls:\n                    idx = tool_call.index\n                    if idx not in tool_calls:\n                        tool_calls[idx] = {\n                            \"name\": getattr(tool_call.function, 'name', '') or '',\n                            \"arguments\": getattr(tool_call.function, 'arguments', '') or '',\n                        }\n                    else:\n                        if hasattr(tool_call.function, 'arguments') and tool_call.function.arguments:\n                            tool_calls[idx][\"arguments\"] += tool_call.function.arguments\n                    if on_tool_call:\n                        on_tool_call(idx, tool_calls[idx][\"name\"], tool_calls[idx][\"arguments\"])\n        \n        return {\n            \"content\": \"\".join(content_parts).strip(),\n            \"reasoning\": \"\".join(reasoning_parts).strip(),\n            \"tool_calls\": tool_calls,\n        }\n\n    async def generate_batch_async(\n        self,\n        prompts: list[str],\n        max_tokens: int = 96,\n        concurrency: int = 4,\n        **gen_kwargs: Any,\n    ) -> list[str]:\n        \"\"\"Run multiple prompts concurrently using asyncio + ThreadPoolExecutor.\n\n        Args:\n            prompts: List of prompts to process\n            max_tokens: Max tokens per response\n            concurrency: Number of concurrent requests (default 4)\n            **gen_kwargs: Additional args passed to generate_with_soft_embeddings\n\n        Returns:\n            List of responses in same order as prompts\n        \"\"\"\n        import asyncio\n        from concurrent.futures import ThreadPoolExecutor\n\n        if not prompts:\n            return []\n\n        # Always use fast model for batch operations (indexing)\n        gen_kwargs[\"disable_thinking\"] = True\n        gen_kwargs[\"max_tokens\"] = max_tokens\n\n        async def run_one(prompt: str) -> str:\n            loop = asyncio.get_event_loop()\n            try:\n                return await loop.run_in_executor(\n                    executor,\n                    lambda: self.generate_with_soft_embeddings(prompt, **gen_kwargs)\n                )\n            except Exception:\n                return \"\"\n\n        with ThreadPoolExecutor(max_workers=concurrency) as executor:\n            results = await asyncio.gather(*[run_one(p) for p in prompts])\n\n        return list(results)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___132": {
      "name": "__init__",
      "type": "method",
      "start_line": 132,
      "end_line": 138,
      "content_hash": "bcb166d7b84f75337c07a1c6fd4dc269fea15f75",
      "content": "    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None) -> None:\n        self.api_key = api_key or os.environ.get(\"GLM_API_KEY\", \"\").strip()\n        if not self.api_key:\n            raise ValueError(\"GLM_API_KEY is required when using REFRAG_RUNTIME=glm\")\n        self.base_url = base_url or os.environ.get(\"GLM_API_BASE\", \"https://api.z.ai/api/paas/v4/\")\n        from openai import OpenAI\n        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__parse_prompt_to_messages_140": {
      "name": "_parse_prompt_to_messages",
      "type": "method",
      "start_line": 140,
      "end_line": 165,
      "content_hash": "f2079eaf09c5ebfa078645bb803055819de9e1fc",
      "content": "    def _parse_prompt_to_messages(self, prompt: str) -> list[dict[str, str]]:\n        \"\"\"Parse Granite-style prompt into proper OpenAI messages array.\n\n        Handles prompts like:\n        <|start_of_role|>system<|end_of_role|>...<|end_of_text|>\n        <|start_of_role|>user<|end_of_role|>...<|end_of_text|>\n        <|start_of_role|>assistant<|end_of_role|>\n        \"\"\"\n        import re\n        messages: list[dict[str, str]] = []\n\n        # Pattern to extract role blocks\n        pattern = r'<\\|start_of_role\\|>(\\w+)<\\|end_of_role\\|>(.*?)(?:<\\|end_of_text\\|>|$)'\n        matches = re.findall(pattern, prompt, re.DOTALL)\n\n        if matches:\n            for role, content in matches:\n                content = content.strip()\n                if content:  # Skip empty content (like trailing assistant role)\n                    messages.append({\"role\": role, \"content\": content})\n\n        # Fallback: if no matches, treat entire prompt as user message\n        if not messages:\n            messages = [{\"role\": \"user\", \"content\": prompt}]\n\n        return messages",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_generate_with_soft_embeddings_167": {
      "name": "generate_with_soft_embeddings",
      "type": "method",
      "start_line": 167,
      "end_line": 276,
      "content_hash": "cacc9061ad778875b6eb63f30a7740399632334a",
      "content": "    def generate_with_soft_embeddings(\n        self,\n        prompt: str,\n        soft_embeddings: Optional[list[list[float]]] = None,  # unused\n        max_tokens: int = 256,\n        **gen_kwargs: Any,\n    ) -> str:\n        # Model selection priority:\n        # 1. Explicit model= parameter\n        # 2. disable_thinking=True -> GLM_MODEL_FAST (backwards compat for expand_query)\n        # 3. GLM_MODEL env var\n        # 4. Default: glm-4.6\n        disable_thinking = bool(gen_kwargs.pop(\"disable_thinking\", False))\n        explicit_model = gen_kwargs.pop(\"model\", None)\n        if explicit_model:\n            model = explicit_model\n        elif disable_thinking:\n            model = os.environ.get(\"GLM_MODEL_FAST\", \"glm-4.5\")\n        else:\n            model = os.environ.get(\"GLM_MODEL\", \"glm-4.6\")\n\n        # no_thinking: disable thinking on current model WITHOUT switching models\n        # Useful when you want to use GLM-4.7 but skip reasoning for simple tasks\n        no_thinking = bool(gen_kwargs.pop(\"no_thinking\", False))\n\n        # Get model-specific configuration for backwards compatibility\n        model_config = get_model_config(model)\n\n        # Use model-specific defaults, allow override via gen_kwargs\n        # For GLM-4.7: temp=1.0, top_p=0.95 (per migration guide)\n        # For backwards compat: caller can still override with lower values for stable output\n        temperature = float(gen_kwargs.get(\"temperature\", model_config[\"temperature\"]))\n        top_p = float(gen_kwargs.get(\"top_p\", model_config[\"top_p\"]))\n\n        # Cap max_tokens to model's output limit\n        requested_max = int(gen_kwargs.get(\"max_tokens\", max_tokens))\n        effective_max = min(requested_max, model_config[\"max_output_tokens\"])\n        stop = gen_kwargs.get(\"stop\")\n        timeout = gen_kwargs.pop(\"timeout\", None)\n        force_json = bool(gen_kwargs.pop(\"force_json\", False))\n\n        # Streaming options (GLM-4.7+ supports tool_stream)\n        stream = bool(gen_kwargs.pop(\"stream\", False))\n        tool_stream = bool(gen_kwargs.pop(\"tool_stream\", False))\n        tools = gen_kwargs.pop(\"tools\", None)\n\n        # Thinking/reasoning options\n        enable_thinking = gen_kwargs.pop(\"enable_thinking\", None)\n        # Combine disable_thinking and no_thinking flags for thinking control\n        should_disable_thinking = disable_thinking or no_thinking\n        \n        try:\n            timeout_val = float(timeout) if timeout is not None else None\n        except Exception:\n            timeout_val = None\n\n        try:\n            create_kwargs: dict[str, Any] = {\n                \"model\": model,\n                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n                \"max_tokens\": effective_max,\n                \"temperature\": temperature,\n                \"top_p\": top_p,\n                \"stop\": stop if stop else None,\n                \"timeout\": timeout_val,\n            }\n            \n            # Streaming support\n            if stream:\n                create_kwargs[\"stream\"] = True\n            \n            # Tool calling support\n            if tools:\n                create_kwargs[\"tools\"] = tools\n                # GLM-4.7+ supports streaming tool call parameters\n                if tool_stream and model_config[\"supports_tool_stream\"]:\n                    create_kwargs[\"extra_body\"] = create_kwargs.get(\"extra_body\", {})\n                    create_kwargs[\"extra_body\"][\"tool_stream\"] = True\n            \n            # When explicitly requested and supported by the backend, ask for\n            # JSON-only responses. If the provider rejects this parameter, the\n            # API call will raise and the caller will handle the failure.\n            if force_json:\n                create_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n            \n            # Thinking/deep reasoning control (GLM-4.5/4.6/4.7 all support thinking param)\n            # All three models support thinking: {type: \"disabled\"} for fast responses\n            if model_config[\"supports_thinking\"]:\n                if should_disable_thinking:\n                    create_kwargs[\"extra_body\"] = create_kwargs.get(\"extra_body\", {})\n                    create_kwargs[\"extra_body\"][\"thinking\"] = {\"type\": \"disabled\"}\n                elif enable_thinking:\n                    create_kwargs[\"extra_body\"] = create_kwargs.get(\"extra_body\", {})\n                    create_kwargs[\"extra_body\"][\"thinking\"] = {\"type\": \"enabled\"}\n\n            response = self.client.chat.completions.create(**create_kwargs)\n            \n            # Handle streaming response\n            if stream:\n                return self._handle_streaming_response(response)\n            \n            msg = response.choices[0].message\n            # GLM models may use either content or reasoning_content\n            content = msg.content or \"\"\n            # Fallback to reasoning_content if content is empty (thinking models)\n            if not content.strip():\n                content = getattr(msg, 'reasoning_content', None) or \"\"\n            return content.strip()\n        except Exception as e:\n            raise RuntimeError(f\"GLM completion failed: {e}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__handle_streaming_response_278": {
      "name": "_handle_streaming_response",
      "type": "method",
      "start_line": 278,
      "end_line": 311,
      "content_hash": "ef8880337aabbaf451df8c1d0c6ea69ae58c059b",
      "content": "    def _handle_streaming_response(\n        self,\n        response: Any,\n    ) -> str:\n        \"\"\"Handle streaming response, accumulating content and reasoning.\n        \n        Supports:\n        - delta.content: Regular content tokens\n        - delta.reasoning_content: Thinking/reasoning tokens (GLM-4.6+)\n        \n        Note: Tool call streaming (GLM-4.7+) is parsed but not currently used\n        since context_answer returns text, not tool calls.\n        \"\"\"\n        content_parts: list[str] = []\n        reasoning_parts: list[str] = []\n        \n        for chunk in response:\n            if not chunk.choices:\n                continue\n            delta = chunk.choices[0].delta\n            \n            # Accumulate reasoning content (thinking process)\n            if hasattr(delta, 'reasoning_content') and delta.reasoning_content:\n                reasoning_parts.append(delta.reasoning_content)\n            \n            # Accumulate regular content\n            if hasattr(delta, 'content') and delta.content:\n                content_parts.append(delta.content)\n        \n        # Return content, fallback to reasoning if content is empty\n        content = \"\".join(content_parts).strip()\n        if not content:\n            content = \"\".join(reasoning_parts).strip()\n        return content",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_generate_with_streaming_313": {
      "name": "generate_with_streaming",
      "type": "method",
      "start_line": 313,
      "end_line": 413,
      "content_hash": "d7bdcf20a2be818e4a37f687bd45e7965cf1179d",
      "content": "    def generate_with_streaming(\n        self,\n        prompt: str,\n        max_tokens: int = 256,\n        on_content: Optional[Any] = None,\n        on_reasoning: Optional[Any] = None,\n        on_tool_call: Optional[Any] = None,\n        **gen_kwargs: Any,\n    ) -> dict[str, Any]:\n        \"\"\"Generate with streaming, providing callbacks for real-time output.\n        \n        Callbacks:\n        - on_content(token: str): Called for each content token\n        - on_reasoning(token: str): Called for each reasoning/thinking token\n        - on_tool_call(idx: int, name: str, args: str): Called for tool call updates\n        \n        Returns:\n            Dict with 'content', 'reasoning', and 'tool_calls' accumulated results\n        \"\"\"\n        disable_thinking = bool(gen_kwargs.pop(\"disable_thinking\", False))\n        if disable_thinking:\n            model = os.environ.get(\"GLM_MODEL_FAST\", \"glm-4.5\")\n        else:\n            model = os.environ.get(\"GLM_MODEL\", \"glm-4.6\")\n        \n        model_config = get_model_config(model)\n        \n        temperature = float(gen_kwargs.get(\"temperature\", model_config[\"temperature\"]))\n        top_p = float(gen_kwargs.get(\"top_p\", model_config[\"top_p\"]))\n        requested_max = int(gen_kwargs.get(\"max_tokens\", max_tokens))\n        effective_max = min(requested_max, model_config[\"max_output_tokens\"])\n        \n        stop = gen_kwargs.get(\"stop\")\n        tools = gen_kwargs.pop(\"tools\", None)\n        tool_stream = bool(gen_kwargs.pop(\"tool_stream\", False))\n        enable_thinking = gen_kwargs.pop(\"enable_thinking\", None)\n        \n        create_kwargs: dict[str, Any] = {\n            \"model\": model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"max_tokens\": effective_max,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"stop\": stop if stop else None,\n            \"stream\": True,\n        }\n        \n        if tools:\n            create_kwargs[\"tools\"] = tools\n            if tool_stream and model_config[\"supports_tool_stream\"]:\n                create_kwargs[\"extra_body\"] = create_kwargs.get(\"extra_body\", {})\n                create_kwargs[\"extra_body\"][\"tool_stream\"] = True\n        \n        if model_config[\"supports_thinking\"]:\n            if disable_thinking:\n                create_kwargs[\"extra_body\"] = create_kwargs.get(\"extra_body\", {})\n                create_kwargs[\"extra_body\"][\"thinking\"] = {\"type\": \"disabled\"}\n            elif enable_thinking:\n                create_kwargs[\"extra_body\"] = create_kwargs.get(\"extra_body\", {})\n                create_kwargs[\"extra_body\"][\"thinking\"] = {\"type\": \"enabled\"}\n        \n        content_parts: list[str] = []\n        reasoning_parts: list[str] = []\n        tool_calls: dict[int, dict[str, str]] = {}\n        \n        response = self.client.chat.completions.create(**create_kwargs)\n        \n        for chunk in response:\n            if not chunk.choices:\n                continue\n            delta = chunk.choices[0].delta\n            \n            if hasattr(delta, 'reasoning_content') and delta.reasoning_content:\n                reasoning_parts.append(delta.reasoning_content)\n                if on_reasoning:\n                    on_reasoning(delta.reasoning_content)\n            \n            if hasattr(delta, 'content') and delta.content:\n                content_parts.append(delta.content)\n                if on_content:\n                    on_content(delta.content)\n            \n            if hasattr(delta, 'tool_calls') and delta.tool_calls:\n                for tool_call in delta.tool_calls:\n                    idx = tool_call.index\n                    if idx not in tool_calls:\n                        tool_calls[idx] = {\n                            \"name\": getattr(tool_call.function, 'name', '') or '',\n                            \"arguments\": getattr(tool_call.function, 'arguments', '') or '',\n                        }\n                    else:\n                        if hasattr(tool_call.function, 'arguments') and tool_call.function.arguments:\n                            tool_calls[idx][\"arguments\"] += tool_call.function.arguments\n                    if on_tool_call:\n                        on_tool_call(idx, tool_calls[idx][\"name\"], tool_calls[idx][\"arguments\"])\n        \n        return {\n            \"content\": \"\".join(content_parts).strip(),\n            \"reasoning\": \"\".join(reasoning_parts).strip(),\n            \"tool_calls\": tool_calls,\n        }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_generate_batch_async_415": {
      "name": "generate_batch_async",
      "type": "method",
      "start_line": 415,
      "end_line": 456,
      "content_hash": "38fd7dba96aa02565ccfb2ed2df1306ac7c862c9",
      "content": "    async def generate_batch_async(\n        self,\n        prompts: list[str],\n        max_tokens: int = 96,\n        concurrency: int = 4,\n        **gen_kwargs: Any,\n    ) -> list[str]:\n        \"\"\"Run multiple prompts concurrently using asyncio + ThreadPoolExecutor.\n\n        Args:\n            prompts: List of prompts to process\n            max_tokens: Max tokens per response\n            concurrency: Number of concurrent requests (default 4)\n            **gen_kwargs: Additional args passed to generate_with_soft_embeddings\n\n        Returns:\n            List of responses in same order as prompts\n        \"\"\"\n        import asyncio\n        from concurrent.futures import ThreadPoolExecutor\n\n        if not prompts:\n            return []\n\n        # Always use fast model for batch operations (indexing)\n        gen_kwargs[\"disable_thinking\"] = True\n        gen_kwargs[\"max_tokens\"] = max_tokens\n\n        async def run_one(prompt: str) -> str:\n            loop = asyncio.get_event_loop()\n            try:\n                return await loop.run_in_executor(\n                    executor,\n                    lambda: self.generate_with_soft_embeddings(prompt, **gen_kwargs)\n                )\n            except Exception:\n                return \"\"\n\n        with ThreadPoolExecutor(max_workers=concurrency) as executor:\n            results = await asyncio.gather(*[run_one(p) for p in prompts])\n\n        return list(results)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_run_one_443": {
      "name": "run_one",
      "type": "method",
      "start_line": 443,
      "end_line": 451,
      "content_hash": "931a75854df066fc45cd0149126f641cb1105663",
      "content": "        async def run_one(prompt: str) -> str:\n            loop = asyncio.get_event_loop()\n            try:\n                return await loop.run_in_executor(\n                    executor,\n                    lambda: self.generate_with_soft_embeddings(prompt, **gen_kwargs)\n                )\n            except Exception:\n                return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_generate_pseudo_tags_batch_459": {
      "name": "generate_pseudo_tags_batch",
      "type": "function",
      "start_line": 459,
      "end_line": 527,
      "content_hash": "b407912aba6b324cb9f77956a91642efb3a7123f",
      "content": "def generate_pseudo_tags_batch(\n    texts: list[str],\n    concurrency: int = 4,\n) -> list[tuple[str, list[str]]]:\n    \"\"\"Batch generate pseudo+tags for multiple code chunks concurrently.\n\n    Args:\n        texts: List of code snippets to process\n        concurrency: Number of concurrent GLM calls (default 4)\n\n    Returns:\n        List of (pseudo, tags) tuples in same order as input texts\n    \"\"\"\n    import asyncio\n    import json as _json\n\n    if not texts:\n        return []\n\n    # Build prompts\n    prompts = []\n    for text in texts:\n        prompt = (\n            \"You are a JSON-only function that labels code spans for search enrichment.\\n\"\n            \"Respond with a single JSON object and nothing else (no prose, no markdown).\\n\"\n            \"Exact format: {\\\"pseudo\\\": string (<=20 tokens), \\\"tags\\\": [3-6 short strings]}.\\n\"\n            \"Code:\\n\" + text[:2000]\n        )\n        prompts.append(prompt)\n\n    # Run batch\n    client = GLMRefragClient()\n\n    try:\n        loop = asyncio.get_event_loop()\n    except RuntimeError:\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n    results = loop.run_until_complete(\n        client.generate_batch_async(\n            prompts,\n            max_tokens=int(os.environ.get(\"PSEUDO_MAX_TOKENS\", \"96\") or 96),\n            concurrency=concurrency,\n            temperature=float(os.environ.get(\"PSEUDO_TEMPERATURE\", \"0.10\") or 0.10),\n            top_p=float(os.environ.get(\"PSEUDO_TOP_P\", \"0.9\") or 0.9),\n            stop=[\"\\n\\n\"],\n            force_json=True,\n        )\n    )\n\n    # Parse JSON responses\n    parsed: list[tuple[str, list[str]]] = []\n    for out in results:\n        pseudo, tags = \"\", []\n        try:\n            obj = _json.loads(out)\n            if isinstance(obj, dict):\n                p = obj.get(\"pseudo\")\n                t = obj.get(\"tags\")\n                if isinstance(p, str):\n                    pseudo = p.strip()[:256]\n                if isinstance(t, list):\n                    tags = [str(x).strip() for x in t if str(x).strip()][:6]\n        except Exception:\n            pass\n        parsed.append((pseudo, tags))\n\n    return parsed",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}