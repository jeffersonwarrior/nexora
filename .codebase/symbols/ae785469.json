{
  "file_path": "/work/external-deps/Context-Engine/scripts/mcp_router.py",
  "file_hash": "e762bc7b73945640e16408ea39196c76fa3ae469",
  "updated_at": "2025-12-26T17:34:22.409270",
  "symbols": {
    "function__classify_intent_rules_61": {
      "name": "_classify_intent_rules",
      "type": "function",
      "start_line": 61,
      "end_line": 98,
      "content_hash": "be45e1d701c8198633f77bfba9e4fa3ae9329ec2",
      "content": "def _classify_intent_rules(q: str) -> str | None:\n    s = q.lower()\n    # Admin / maintenance first\n    if any(w in s for w in [\"reindex\", \"reset\", \"recreate\", \"index now\", \"fresh index\"]):\n        return INTENT_INDEX\n    if any(w in s for w in [\"prune\", \"pruning\", \"cleanup\", \"clean up\"]):\n        return INTENT_PRUNE\n    if any(w in s for w in [\"status\", \"health\", \"points\", \"stats\"]):\n        return INTENT_STATUS\n    if any(w in s for w in [\"list collections\", \"collections\", \"list qdrant\"]):\n        return INTENT_LIST\n\n    # Intent wrappers\n    if any(w in s for w in [\"tests\", \"pytest\", \"unit test\", \"test file\", \"where are tests\"]):\n        return INTENT_SEARCH_TESTS\n    # Memory intents\n    if any(w in s for w in [\n        \"remember this\", \"save memory\", \"store memory\", \"remember that\", \"save preference\", \"remember preference\"\n    ]):\n        return INTENT_MEMORY_STORE\n    if any(w in s for w in [\n        \"find memory\", \"recall\", \"retrieve memory\", \"memory search\", \"what did we save\"\n    ]):\n        return INTENT_MEMORY_FIND\n\n    if any(w in s for w in [\"config\", \"yaml\", \"toml\", \"ini\", \"settings file\", \"configuration\"]):\n        return INTENT_SEARCH_CONFIG\n    if any(w in s for w in [\"who calls\", \"callers\", \"used by\", \"usage sites\", \"references this function\"]):\n        return INTENT_SEARCH_CALLERS\n    if any(w in s for w in [\"importers\", \"who imports\", \"imports this\", \"importing modules\"]):\n        return INTENT_SEARCH_IMPORTERS\n\n    # Q&A-like prompts\n    if re.match(r\"^(what|how|why|explain|describe|summarize)(\\b|\\s)\", s):\n        return INTENT_ANSWER\n    if any(w in s for w in [\"recap\", \"design doc\", \"architecture\", \"adr\", \"retrospective\", \"postmortem\", \"summary of\", \"summarize the design\"]):\n        return INTENT_ANSWER\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__intent_prototypes_101": {
      "name": "_intent_prototypes",
      "type": "function",
      "start_line": 101,
      "end_line": 129,
      "content_hash": "c01067583c5cd47f069ce63c79d9a8dafe637af2",
      "content": "def _intent_prototypes() -> Dict[str, List[str]]:\n    return {\n        INTENT_ANSWER: [\n            \"explain, describe, summarize, recap, design, architecture, ADR, why/how\",\n            \"summarize design decisions and architecture rationale\",\n        ],\n        INTENT_SEARCH: [\n            \"find code references, search repository, locate files\",\n            \"code search in repo, general lookup\",\n        ],\n        INTENT_MEMORY_STORE: [\n            \"remember this, save preference, store memory\",\n        ],\n        INTENT_MEMORY_FIND: [\n            \"what did we save, recall saved notes, retrieve memory\",\n        ],\n        INTENT_SEARCH_TESTS: [\n            \"find unit tests, test files, pytest\",\n        ],\n        INTENT_SEARCH_CONFIG: [\n            \"config files, configuration changes, yaml toml ini settings\",\n        ],\n        INTENT_SEARCH_CALLERS: [\n            \"who calls this function, callers, usage sites\",\n        ],\n        INTENT_SEARCH_IMPORTERS: [\n            \"who imports this module, importers, importing modules\",\n        ],\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__classify_intent_ml_132": {
      "name": "_classify_intent_ml",
      "type": "function",
      "start_line": 132,
      "end_line": 170,
      "content_hash": "dfb1c22427f78d74e5142f10a597c94c00cd5c01",
      "content": "def _classify_intent_ml(q: str) -> str:\n    global _LAST_INTENT_DEBUG\n    protos = _intent_prototypes()\n    labels = list(protos.keys())\n    texts = [q] + [\"\\n\".join(protos[l]) for l in labels]\n    vecs = _embed_texts(texts)\n    if not vecs or len(vecs) < len(texts):\n        _LAST_INTENT_DEBUG = {\n            \"strategy\": \"ml\",\n            \"intent\": INTENT_SEARCH,\n            \"confidence\": 0.0,\n            \"query\": q,\n            \"top_candidate\": INTENT_SEARCH,\n            \"top_score\": 0.0,\n            \"threshold\": 0.25,\n            \"candidates\": [],\n            \"reason\": \"embed_failed\",\n        }\n        return INTENT_SEARCH\n    qv = vecs[0]\n    sims = []\n    for i, lab in enumerate(labels):\n        sims.append((lab, _cosine(qv, vecs[1 + i])))\n    sims.sort(key=lambda x: x[1], reverse=True)\n    top, score = sims[0]\n    # light threshold: if nothing is clearly better, default to search\n    picked = top if score >= 0.25 else INTENT_SEARCH\n    _LAST_INTENT_DEBUG = {\n        \"strategy\": \"ml\",\n        \"intent\": picked,\n        \"confidence\": float(score),\n        \"query\": q,\n        \"top_candidate\": top,\n        \"top_score\": float(score),\n        \"threshold\": 0.25,\n        \"candidates\": [(name, float(val)) for name, val in sims[:5]],\n        \"fallback\": picked == INTENT_SEARCH and top != INTENT_SEARCH,\n    }\n    return picked",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_classify_intent_173": {
      "name": "classify_intent",
      "type": "function",
      "start_line": 173,
      "end_line": 193,
      "content_hash": "bb2c73c96fd9394ed720c12cabd101e975ccb18a",
      "content": "def classify_intent(q: str) -> str:\n    global _LAST_INTENT_DEBUG\n    # Prefer high-precision rules; fall back to embedding classifier\n    ruled = _classify_intent_rules(q)\n    if ruled is not None:\n        _LAST_INTENT_DEBUG = {\n            \"strategy\": \"rules\",\n            \"intent\": ruled,\n            \"confidence\": 1.0,\n            \"query\": q,\n        }\n        return ruled\n    picked = _classify_intent_ml(q)\n    # Emit a debug hint when ML falls back to generic search intent\n    try:\n        if os.environ.get(\"DEBUG_ROUTER\") and isinstance(_LAST_INTENT_DEBUG, dict):\n            if _LAST_INTENT_DEBUG.get(\"fallback\"):\n                print(json.dumps({\"router\": {\"intent_fallback\": _LAST_INTENT_DEBUG}}), file=sys.stderr)\n    except Exception:\n        pass\n    return picked",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__parse_memory_store_payload_210": {
      "name": "_parse_memory_store_payload",
      "type": "function",
      "start_line": 210,
      "end_line": 258,
      "content_hash": "70919009dac617285e9a813c71ddc3457224d884",
      "content": "def _parse_memory_store_payload(q: str) -> Tuple[str, Dict[str, Any]]:\n    raw = str(q or \"\").strip()\n    if not raw:\n        return \"\", {}\n    cleaned = _MEMORY_TRIGGER_RE.sub(\"\", raw, count=1).lstrip()\n    meta: Dict[str, Any] = {}\n\n    def _assign_meta(key: str, value: str) -> None:\n        k = key.lower()\n        v = value.strip().strip(\" \\t\\r\\n,;.\")\n        if not v:\n            return\n        if k in {\"tag\", \"tags\"}:\n            tags = [t.strip() for t in re.split(r\"[,\\s/]+\", v) if t.strip()]\n            if tags:\n                meta[\"tags\"] = tags\n        else:\n            meta[k] = v\n\n    if cleaned.startswith(\"[\"):\n        m = re.match(r\"\\[([^\\]]+)\\]\\s*(.*)\", cleaned, flags=re.S)\n        if m:\n            meta_block = m.group(1)\n            cleaned = m.group(2)\n            for key, val in re.findall(r\"(\\w+)\\s*=\\s*([^\\s,;]+(?:,[^\\s,;]+)*)\", meta_block):\n                if key.strip().lower() in _MEMORY_META_KEYS:\n                    _assign_meta(key, val)\n\n    while True:\n        m = re.match(\n            r\"^(?P<key>(?:priority|tag|tags|topic|category|owner))\\s*=\\s*(?P<val>[^\\s;:]+)\\s*[,;:]?\\s*(?P<rest>.*)$\",\n            cleaned,\n            flags=re.IGNORECASE | re.S,\n        )\n        if not m:\n            break\n        _assign_meta(m.group(\"key\"), m.group(\"val\"))\n        cleaned = m.group(\"rest\")\n\n    cleaned = cleaned.lstrip(\":- \").lstrip()\n\n    split = _MEMORY_INTENT_SPLIT_RE.search(cleaned)\n    if split:\n        cleaned = cleaned[: split.start()].rstrip(\" ,;.\")\n\n    cleaned = cleaned.strip().strip('\"').strip()\n    if not cleaned:\n        cleaned = raw\n    return cleaned, meta",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__assign_meta_217": {
      "name": "_assign_meta",
      "type": "function",
      "start_line": 217,
      "end_line": 227,
      "content_hash": "56d19a4b3e244899579ef695d79a2ab09d280c76",
      "content": "    def _assign_meta(key: str, value: str) -> None:\n        k = key.lower()\n        v = value.strip().strip(\" \\t\\r\\n,;.\")\n        if not v:\n            return\n        if k in {\"tag\", \"tags\"}:\n            tags = [t.strip() for t in re.split(r\"[,\\s/]+\", v) if t.strip()]\n            if tags:\n                meta[\"tags\"] = tags\n        else:\n            meta[k] = v",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_build_plan_265": {
      "name": "build_plan",
      "type": "function",
      "start_line": 265,
      "end_line": 503,
      "content_hash": "4d918f1fe3a32ef87d629c7cf0d62fc9d22e7ac5",
      "content": "def build_plan(q: str) -> List[Tuple[str, Dict[str, Any]]]:\n    intent = classify_intent(q)\n    include_snippet = str(os.environ.get(\"ROUTER_INCLUDE_SNIPPET\", \"1\")).lower() in {\"1\", \"true\", \"yes\", \"on\"}\n    search_limit = int(os.environ.get(\"ROUTER_SEARCH_LIMIT\", \"8\") or 8)\n    max_tokens_env = os.environ.get(\"ROUTER_MAX_TOKENS\", \"\").strip()\n\n    def _reuse_last_filters(args: Dict[str, Any]) -> None:\n        \"\"\"Optionally hydrate `args` with cached filters when user asks for reuse.\"\"\"\n        try:\n            if _looks_like_same_filters(q):\n                sp = _load_scratchpad()\n                lf = sp.get(\"last_filters\") if isinstance(sp, dict) else None\n                if isinstance(lf, dict):\n                    for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n                        if k not in args and lf.get(k) not in (None, \"\"):\n                            args[k] = lf.get(k)\n        except Exception:\n            pass\n\n    # Repeat/redo handling: reuse last plan if asked\n    try:\n        if _looks_like_repeat(q):\n            sp = _load_scratchpad()\n            lp = sp.get(\"last_plan\")\n            if isinstance(lp, list) and lp:\n                # Normalize to list of (tool, args) tuples\n                norm: list[tuple] = []\n                for it in lp:\n                    if isinstance(it, (list, tuple)) and len(it) == 2:\n                        norm.append((it[0], it[1]))\n                if norm:\n                    return norm\n    except Exception:\n        pass\n\n    # Multi-intent: memory store + reindex in a single prompt\n    lowq = q.lower()\n    if any(w in lowq for w in [\"remember this\", \"store memory\", \"save memory\", \"remember that\"]) and any(w in lowq for w in [\"reindex\", \"index now\", \"recreate\", \"fresh index\"]):\n        idx_args: Dict[str, Any] = {}\n        if any(w in lowq for w in [\"recreate\", \"fresh\", \"from scratch\", \"fresh index\"]):\n            idx_args[\"recreate\"] = True\n        info, meta = _parse_memory_store_payload(q)\n        store_args: Dict[str, Any] = {\"information\": info or q.strip()}\n        if meta:\n            allowed = {\"priority\", \"tags\", \"topic\", \"category\", \"owner\"}\n            cleaned = {k: v for k, v in meta.items() if k in allowed and v not in (None, \"\", [])}\n            if cleaned:\n                store_args[\"metadata\"] = cleaned\n        return [(\"store\", store_args), (\"qdrant_index_root\", idx_args)]\n\n    if intent == INTENT_INDEX:\n        # Zero-arg safe default; recreate only if user asked explicitly\n        recreate = True if any(w in q.lower() for w in [\"recreate\", \"fresh\", \"from scratch\"]) else None\n        args = {}\n        if recreate is True:\n            args[\"recreate\"] = True\n        return [(\"qdrant_index_root\", args)]\n\n    if intent == INTENT_PRUNE:\n        return [(\"qdrant_prune\", {})]\n\n\n    if intent == INTENT_STATUS:\n        return [(\"qdrant_status\", {})]\n\n    if intent == INTENT_LIST:\n        return [(\"qdrant_list\", {})]\n\n\n    if intent == INTENT_SEARCH:\n        # Parse lightweight repo hints and choose the best search tool via signature similarity\n        hints = _parse_repo_hints(q)\n        clean_q, dsl_filters = _clean_query_and_dsl(q)\n        args = {\"query\": clean_q}\n        if search_limit:\n            args[\"limit\"] = search_limit\n        if include_snippet:\n            args[\"include_snippet\"] = True\n        # Optionally reuse last filters if requested\n        _reuse_last_filters(args)\n\n        # Merge filters: DSL tokens take precedence over heuristic hints\n        for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n            v = dsl_filters.get(k)\n            if v not in (None, \"\") and k not in args:\n                args[k] = v\n        for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n            v = hints.get(k)\n            if v not in (None, \"\") and k not in args:\n                args[k] = v\n        try:\n            tool_servers = _discover_tool_endpoints(allow_network=False)\n            picked = _select_best_search_tool_by_signature(q, tool_servers, allow_network=False) or \"repo_search\"\n        except Exception:\n            picked = \"repo_search\"\n        return [(picked, args)]\n\n    if intent == INTENT_SEARCH_TESTS:\n        hints = _parse_repo_hints(q)\n        clean_q, dsl_filters = _clean_query_and_dsl(q)\n        args = {\"query\": clean_q}\n        if search_limit:\n            args[\"limit\"] = search_limit\n        if include_snippet:\n            args[\"include_snippet\"] = True\n        # Optionally reuse last filters if requested\n        _reuse_last_filters(args)\n\n        for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n            v = dsl_filters.get(k)\n            if v not in (None, \"\") and k not in args:\n                args[k] = v\n        for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n            v = hints.get(k)\n            if v not in (None, \"\") and k not in args:\n                args[k] = v\n        return [(\"search_tests_for\", args)]\n\n    if intent == INTENT_SEARCH_CONFIG:\n        hints = _parse_repo_hints(q)\n        clean_q, dsl_filters = _clean_query_and_dsl(q)\n        args = {\"query\": clean_q}\n        if search_limit:\n            args[\"limit\"] = search_limit\n        if include_snippet:\n            args[\"include_snippet\"] = True\n        # Optionally reuse last filters if requested\n        _reuse_last_filters(args)\n\n        for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n            v = dsl_filters.get(k)\n            if v not in (None, \"\") and k not in args:\n                args[k] = v\n        for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n            v = hints.get(k)\n            if v not in (None, \"\") and k not in args:\n                args[k] = v\n        return [(\"search_config_for\", args)]\n\n    if intent == INTENT_MEMORY_STORE:\n        info, meta = _parse_memory_store_payload(q)\n        payload: Dict[str, Any] = {\"information\": info or q.strip()}\n        if meta:\n            allowed = {\"priority\", \"tags\", \"topic\", \"category\", \"owner\"}\n            cleaned = {k: v for k, v in meta.items() if k in allowed and v not in (None, \"\", [])}\n            if cleaned:\n                payload[\"metadata\"] = cleaned\n        return [(\"store\", payload)]\n\n    if intent == INTENT_MEMORY_FIND:\n        args = {\"query\": q}\n        if search_limit:\n            args[\"limit\"] = max(5, search_limit)\n        return [(\"find\", args)]\n\n    if intent == INTENT_SEARCH_CALLERS:\n        hints = _parse_repo_hints(q)\n        clean_q, dsl_filters = _clean_query_and_dsl(q)\n        args = {\"query\": clean_q}\n        if search_limit:\n            args[\"limit\"] = search_limit\n        # Optionally reuse last filters if requested\n        _reuse_last_filters(args)\n\n        for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n            v = dsl_filters.get(k)\n            if v not in (None, \"\") and k not in args:\n                args[k] = v\n        for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n            v = hints.get(k)\n            if v not in (None, \"\") and k not in args:\n                args[k] = v\n        return [(\"search_callers_for\", args)]\n\n    if intent == INTENT_SEARCH_IMPORTERS:\n        hints = _parse_repo_hints(q)\n        clean_q, dsl_filters = _clean_query_and_dsl(q)\n        args = {\"query\": clean_q}\n        if search_limit:\n            args[\"limit\"] = search_limit\n        # Optionally reuse last filters if requested\n        _reuse_last_filters(args)\n        for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n            v = dsl_filters.get(k)\n            if v not in (None, \"\") and k not in args:\n                args[k] = v\n        for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n            v = hints.get(k)\n            if v not in (None, \"\") and k not in args:\n                args[k] = v\n        return [(\"search_importers_for\", args)]\n\n    if intent == INTENT_ANSWER:\n        # Primary: context_answer_compat, then context_answer; Fallback: repo_search\n        # If the prompt looks like a design/recap, preface plan with a memory find step\n        def _looks_like_design_recap(s: str) -> bool:\n            low = s.lower()\n            return any(w in low for w in [\"recap\", \"design doc\", \"architecture\", \"adr\", \"retrospective\", \"postmortem\"]) and any(w in low for w in [\"design\", \"summary\", \"recap\", \"explain\"])\n\n        # Start with base args and carry through parsed repo hints to answer tools\n        args: Dict[str, Any] = {\"query\": q}\n        if max_tokens_env:\n            try:\n                mt = int(max_tokens_env)\n                if mt > 0:\n                    args[\"max_tokens\"] = mt\n            except Exception:\n                pass\n\n        # Parse lightweight hints and tighten when the query mentions \"router\"\n        hints = _parse_repo_hints(q)\n        lowq = q.lower()\n        if \"router\" in lowq:\n            # Bias strongly toward router implementation\n            router_globs = [\"**/mcp_router.py\", \"**/*router*.py\"]\n            if not hints.get(\"path_glob\"):\n                hints[\"path_glob\"] = router_globs\n            # Default language bias to python unless explicitly provided\n            if not hints.get(\"language\"):\n                hints[\"language\"] = \"python\"\n        # Carry hints into args so context_answer and compat use them\n        for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n            v = hints.get(k)\n            if v not in (None, \"\"):\n                args[k] = v\n\n        plan: List[Tuple[str, Dict[str, Any]]] = []\n        if _looks_like_design_recap(q):\n            plan.append((\"find\", {\"query\": q, \"limit\": 3}))\n        # Answer first (compat then native), then fall back to a targeted search with the same hints\n        plan.extend([\n            (\"context_answer_compat\", dict(args)),\n            (\"context_answer\", dict(args)),\n            (\"repo_search\", {**{k: v for k, v in args.items() if k != \"max_tokens\"}, \"limit\": max(5, search_limit)}),\n        ])\n        return plan\n\n    # Fallback\n    return [(\"repo_search\", {\"query\": q, \"limit\": search_limit})]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__reuse_last_filters_271": {
      "name": "_reuse_last_filters",
      "type": "function",
      "start_line": 271,
      "end_line": 282,
      "content_hash": "be5018bf67e5f7d375219be1100c3b529052820c",
      "content": "    def _reuse_last_filters(args: Dict[str, Any]) -> None:\n        \"\"\"Optionally hydrate `args` with cached filters when user asks for reuse.\"\"\"\n        try:\n            if _looks_like_same_filters(q):\n                sp = _load_scratchpad()\n                lf = sp.get(\"last_filters\") if isinstance(sp, dict) else None\n                if isinstance(lf, dict):\n                    for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n                        if k not in args and lf.get(k) not in (None, \"\"):\n                            args[k] = lf.get(k)\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__looks_like_design_recap_460": {
      "name": "_looks_like_design_recap",
      "type": "function",
      "start_line": 460,
      "end_line": 462,
      "content_hash": "c2458bd94991cb10697970645ec7dbfcb8fa36f6",
      "content": "        def _looks_like_design_recap(s: str) -> bool:\n            low = s.lower()\n            return any(w in low for w in [\"recap\", \"design doc\", \"architecture\", \"adr\", \"retrospective\", \"postmortem\"]) and any(w in low for w in [\"design\", \"summary\", \"recap\", \"explain\"])",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__post_raw_510": {
      "name": "_post_raw",
      "type": "function",
      "start_line": 510,
      "end_line": 519,
      "content_hash": "04f09ab3fb41e7501d801df9a43a76934f7b6db1",
      "content": "def _post_raw(url: str, payload: Dict[str, Any], headers: Dict[str, str], timeout: float = 60.0) -> Tuple[Dict[str, str], bytes]:\n    req = request.Request(url, method=\"POST\")\n    for k, v in headers.items():\n        req.add_header(k, v)\n    data = json.dumps(payload).encode(\"utf-8\")\n    with request.urlopen(req, data=data, timeout=timeout) as resp:\n        body = resp.read()\n        # Lowercase header keys for ease\n        hdrs = {k.lower(): v for k, v in resp.headers.items()}\n    return hdrs, body",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__parse_stream_or_json_522": {
      "name": "_parse_stream_or_json",
      "type": "function",
      "start_line": 522,
      "end_line": 536,
      "content_hash": "bc816c93caae3621f244ddfd0855177520f93712",
      "content": "def _parse_stream_or_json(body: bytes) -> Dict[str, Any]:\n    txt = body.decode(\"utf-8\", errors=\"ignore\")\n    # If it looks like SSE/streamable-http (data: ...), pick the last data: line\n    if \"data:\" in txt and (\"event:\" in txt or txt.strip().startswith(\"data:\")):\n        last = None\n        for line in txt.splitlines():\n            if line.startswith(\"data:\"):\n                last = line[len(\"data:\"):].strip()\n        if last:\n            try:\n                return json.loads(last)\n            except Exception:\n                pass\n    # Fallback: parse as JSON\n    return json.loads(txt)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__filter_args_539": {
      "name": "_filter_args",
      "type": "function",
      "start_line": 539,
      "end_line": 540,
      "content_hash": "e279adfc8a8ea330f462f41cfb66e50b2c1e5a0c",
      "content": "def _filter_args(d: Dict[str, Any]) -> Dict[str, Any]:\n    return {k: v for k, v in d.items() if v not in (None, \"\")}  # omit null/empty",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__mcp_handshake_543": {
      "name": "_mcp_handshake",
      "type": "function",
      "start_line": 543,
      "end_line": 577,
      "content_hash": "a087cd172b5e37a4ea23b81460bb5327326217f9",
      "content": "def _mcp_handshake(base_url: str, timeout: float = 30.0) -> Dict[str, str]:\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Accept\": \"application/json, text/event-stream\",\n    }\n    init_payload = {\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"initialize\",\n        \"params\": {\n            \"protocolVersion\": \"2024-11-05\",\n            \"capabilities\": {},\n            \"clientInfo\": {\"name\": \"router\", \"version\": \"0.1.0\"},\n        },\n        \"id\": 1,\n    }\n    hdrs, body = _post_raw_retry(base_url, init_payload, headers, timeout=timeout)\n    sid = hdrs.get(\"mcp-session-id\") or hdrs.get(\"Mcp-Session-Id\")\n    if not sid:\n        # Some servers may return JSON with sessionId; best-effort parse\n        try:\n            j = _parse_stream_or_json(body)\n            sid = j.get(\"sessionId\")\n        except Exception:\n            sid = None\n    # Tolerate servers (e.g., streamable-http bridge) that do not emit a session id header.\n    # In that case, proceed without attaching Mcp-Session-Id; downstream calls will still work\n    # for bridges that manage their own session lifecycle.\n    if sid:\n        headers[\"Mcp-Session-Id\"] = sid\n    # Send initialized notification (no id required)\n    try:\n        _post_raw_retry(base_url, {\"jsonrpc\": \"2.0\", \"method\": \"notifications/initialized\"}, headers, timeout=timeout)\n    except Exception:\n        pass\n    return headers",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_iserror_text_580": {
      "name": "_extract_iserror_text",
      "type": "function",
      "start_line": 580,
      "end_line": 590,
      "content_hash": "7ce0cfcd220fa77387edbc6bde32e25910125532",
      "content": "def _extract_iserror_text(resp: Dict[str, Any]) -> str | None:\n    try:\n        r = resp.get(\"result\") or {}\n        if isinstance(r, dict) and r.get(\"isError\"):\n            content = r.get(\"content\")\n            if isinstance(content, list) and content and isinstance(content[0], dict):\n                if content[0].get(\"type\") == \"text\":\n                    return content[0].get(\"text\")\n    except Exception:\n        pass\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_call_tool_http_593": {
      "name": "call_tool_http",
      "type": "function",
      "start_line": 593,
      "end_line": 658,
      "content_hash": "e5c9676da27b02adf5ce048d6f259cae272a4221",
      "content": "def call_tool_http(base_url: str, tool_name: str, args: Dict[str, Any], timeout: float = 120.0) -> Dict[str, Any]:\n    # Handshake per fastmcp streamable-http\n    headers = _mcp_handshake(base_url, timeout=min(timeout, 30.0))\n\n    def _do_call(arguments: Dict[str, Any]) -> Dict[str, Any]:\n        payload = {\n            \"jsonrpc\": \"2.0\",\n            \"id\": \"router-1\",\n            \"method\": \"tools/call\",\n            \"params\": {\n                \"name\": tool_name,\n                \"arguments\": arguments,\n            },\n        }\n        _, body = _post_raw_retry(base_url, payload, headers, timeout=timeout)\n        return _parse_stream_or_json(body)\n\n    # First attempt: choose wrapper based on tool shape\n    args1 = _filter_args(args)\n    if tool_name.endswith(\"_compat\"):\n        # Compat tools expose a single 'arguments' parameter; call with nested wrapper first\n        resp = _do_call({\"arguments\": args1})\n    else:\n        resp = _do_call(args1)\n\n    def _get_structured_error(r: Dict[str, Any]) -> str | None:\n        try:\n            rr = r.get(\"result\") or {}\n            sc = rr.get(\"structuredContent\") or {}\n            rs = sc.get(\"result\") or {}\n            err = rs.get(\"error\")\n            if isinstance(err, str):\n                return err\n        except Exception:\n            pass\n        return None\n\n    msg = _extract_iserror_text(resp)\n    serr = _get_structured_error(resp)\n    if msg:\n        low = msg.lower()\n        if (\"kwargs\" in low) and (\"field required\" in low or \"missing\" in low):\n            # Retry with kwargs wrapper for servers that expose **kwargs in schema\n            resp2 = _do_call({\"kwargs\": args1})\n            return resp2\n        if (\"arguments\" in low) and (\"field required\" in low or \"missing\" in low):\n            # Retry with single-blob arguments wrapper for **arguments schemas\n            resp3 = _do_call({\"arguments\": args1})\n            return resp3\n    # Heuristic: some tools accept the call but return {\"error\": \"query required\"}\n    # when kwargs weren't expanded. If we provided a query, retry with kwargs wrapper.\n    if (serr and serr.strip().lower() == \"query required\") and (\"query\" in args1 or \"queries\" in args1):\n        # Attempt 1: top-level kwargs\n        resp4 = _do_call({\"kwargs\": args1})\n        serr2 = _get_structured_error(resp4)\n        if not (serr2 and serr2.strip().lower() == \"query required\"):\n            return resp4\n        # Attempt 2: nested kwargs under arguments (for **arguments wrappers)\n        resp5 = _do_call({\"arguments\": {\"kwargs\": args1}})\n        serr3 = _get_structured_error(resp5)\n        if not (serr3 and serr3.strip().lower() == \"query required\"):\n            return resp5\n        # Attempt 3: nested direct args under arguments\n        resp6 = _do_call({\"arguments\": args1})\n        return resp6\n    return resp",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__do_call_597": {
      "name": "_do_call",
      "type": "function",
      "start_line": 597,
      "end_line": 608,
      "content_hash": "4942a89dc73119fec2ad845eced28ad65b8a06ca",
      "content": "    def _do_call(arguments: Dict[str, Any]) -> Dict[str, Any]:\n        payload = {\n            \"jsonrpc\": \"2.0\",\n            \"id\": \"router-1\",\n            \"method\": \"tools/call\",\n            \"params\": {\n                \"name\": tool_name,\n                \"arguments\": arguments,\n            },\n        }\n        _, body = _post_raw_retry(base_url, payload, headers, timeout=timeout)\n        return _parse_stream_or_json(body)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_structured_error_618": {
      "name": "_get_structured_error",
      "type": "function",
      "start_line": 618,
      "end_line": 628,
      "content_hash": "526a510300cf60e647e9038269904e306e472366",
      "content": "    def _get_structured_error(r: Dict[str, Any]) -> str | None:\n        try:\n            rr = r.get(\"result\") or {}\n            sc = rr.get(\"structuredContent\") or {}\n            rs = sc.get(\"result\") or {}\n            err = rs.get(\"error\")\n            if isinstance(err, str):\n                return err\n        except Exception:\n            pass\n        return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__is_failure_response_660": {
      "name": "_is_failure_response",
      "type": "function",
      "start_line": 660,
      "end_line": 671,
      "content_hash": "a59fc84d1d4171b1d87a230ca425a49aa6befcca",
      "content": "def _is_failure_response(resp: Dict[str, Any]) -> bool:\n    try:\n        r = resp.get(\"result\") or {}\n        if r.get(\"isError\") is True:\n            return True\n        sc = r.get(\"structuredContent\") or {}\n        rs = sc.get(\"result\") or {}\n        if isinstance(rs, dict) and isinstance(rs.get(\"error\"), str):\n            return True\n    except Exception:\n        return False\n    return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__tools_describe_from_health_686": {
      "name": "_tools_describe_from_health",
      "type": "function",
      "start_line": 686,
      "end_line": 714,
      "content_hash": "2193b6cf079e5856693a7baa303878800ac8040d",
      "content": "def _tools_describe_from_health(base_url: str, timeout: float = 3.0) -> list[dict]:\n    \"\"\"Best-effort: fetch tool descriptors from health /tools endpoint.\n    Only attempts for known default HTTP_URL_INDEXER/HTTP_URL_MEMORY.\n    \"\"\"\n    try:\n        import urllib.request\n        if base_url == HTTP_URL_INDEXER:\n            url = f\"http://localhost:{_HEALTH_PORT_INDEXER}/tools\"\n        elif base_url == HTTP_URL_MEMORY:\n            url = f\"http://localhost:{_HEALTH_PORT_MEMORY}/tools\"\n        else:\n            return []\n        with urllib.request.urlopen(url, timeout=timeout) as r:\n            if getattr(r, \"status\", 200) != 200:\n                return []\n            body = r.read()\n            j = _parse_stream_or_json(body)\n            tools = (j.get(\"tools\") if isinstance(j, dict) else None) or []\n            out = []\n            for t in tools:\n                if not isinstance(t, dict):\n                    continue\n                nm = t.get(\"name\")\n                if not nm:\n                    continue\n                out.append({\"name\": nm, \"description\": (t.get(\"description\") or \"\").strip()})\n            return out\n    except Exception:\n        return []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__post_raw_retry_718": {
      "name": "_post_raw_retry",
      "type": "function",
      "start_line": 718,
      "end_line": 731,
      "content_hash": "b08cede9799384bced6aa24647c4fea070a7506b",
      "content": "def _post_raw_retry(url: str, payload: Dict[str, Any], headers: Dict[str, str], timeout: float = 60.0, retries: int = 2, backoff: float = 0.5) -> Tuple[Dict[str, str], bytes]:\n    last_exc: Exception | None = None\n    for i in range(max(0, retries) + 1):\n        try:\n            return _post_raw(url, payload, headers, timeout=timeout)\n        except Exception as e:\n            last_exc = e\n            if i < retries:\n                try:\n                    time.sleep(backoff * (2 ** i))\n                except Exception:\n                    pass\n            else:\n                raise last_exc",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_BatchingContextAnswerClient_740": {
      "name": "BatchingContextAnswerClient",
      "type": "class",
      "start_line": 740,
      "end_line": 1080,
      "content_hash": "e2dfa6fac17f5a6c7cb4b5d882cec6f7e113befb",
      "content": "class BatchingContextAnswerClient:\n    \"\"\"Lightweight in-memory batching for context_answer calls.\n\n    - Queues short-lived requests keyed by (base_url, collection, filters_fingerprint)\n    - Flushes after a small window or when batch size cap is hit\n    - For multi-item batches, sends query=[...] with mode=\"pack\"\n    - Shares the same response with all enqueued requests\n    \"\"\"\n\n    def __init__(self, call_func=None, enable: bool | None = None, window_ms: int | None = None,\n                 max_batch: int | None = None, budget_ms: int | None = None):\n        self._call = call_func or call_tool_http\n        # Enabled flag (default off for back-compat)\n        if enable is None:\n            env_enabled = os.environ.get(\"ROUTER_BATCH_ENABLED\")\n            if env_enabled is None:\n                env_enabled = os.environ.get(\"ROUTER_BATCH_ENABLE\", \"0\")\n            self.enabled = str(env_enabled).strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}\n        else:\n            self.enabled = bool(enable)\n        # Window and caps\n        self.window_ms = int(os.environ.get(\"ROUTER_BATCH_WINDOW_MS\", str(window_ms if window_ms is not None else 100)) or 100)\n        env_max = os.environ.get(\"ROUTER_BATCH_MAX_SIZE\")\n        if env_max is None:\n            env_max = os.environ.get(\"ROUTER_BATCH_MAX\")\n        self.max_batch = int(env_max or (max_batch if max_batch is not None else 8))\n        env_budget = os.environ.get(\"ROUTER_BATCH_LATENCY_BUDGET_MS\")\n        if env_budget is None:\n            env_budget = os.environ.get(\"ROUTER_BATCH_BUDGET_MS\")\n        self.budget_ms = int(env_budget or (budget_ms if budget_ms is not None else 2000))\n        self._lock = threading.RLock()\n        self._groups: dict[str, dict[str, any]] = {}\n\n    def _should_bypass(self, args: Dict[str, Any]) -> bool:\n        # 1) explicit flag in args\n        try:\n            if isinstance(args, dict):\n                v = args.get(\"immediate\")\n                if v is not None and str(v).strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}:\n                    return True\n        except Exception:\n            pass\n        # 2) env-level bypass\n        if str(os.environ.get(\"ROUTER_BATCH_BYPASS\", \"0\")).strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}:\n            return True\n        # 3) free-text hint in the query\n        try:\n            q = str((args or {}).get(\"query\") or \"\")\n            if \"immediate answer\" in q.lower():\n                return True\n        except Exception:\n            pass\n        return False\n\n    def _norm_query(self, q: str) -> str:\n        try:\n            return re.sub(r\"\\s+\", \" \", str(q or \"\").strip())\n        except Exception:\n            return str(q)\n\n    def _filters_fingerprint(self, args: Dict[str, Any]) -> str:\n        keep = {\n            \"collection\", \"language\", \"under\", \"kind\", \"symbol\", \"ext\",\n            \"path_regex\", \"path_glob\", \"not_glob\", \"not_\", \"case\",\n            \"limit\", \"per_path\", \"include_snippet\",\n        }\n        try:\n            filt = {k: args.get(k) for k in keep if k in args}\n            # Normalize list-like values\n            def _norm(v):\n                if v is None:\n                    return None\n                if isinstance(v, (list, tuple)):\n                    return [str(x) for x in v]\n                return v\n            clean = {k: _norm(v) for k, v in filt.items()}\n            return json.dumps(clean, sort_keys=True, ensure_ascii=False)\n        except Exception:\n            return \"{}\"\n\n    def _group_key(self, base_url: str, args: Dict[str, Any]) -> str:\n        coll = str(args.get(\"collection\") or \"\")\n        fp = self._filters_fingerprint(args)\n        repo = os.getcwd()\n        return f\"{base_url}|{coll}|answer|{fp}|{repo}\"\n\n    def call_or_enqueue(self, base_url: str, tool: str, args: Dict[str, Any], timeout: float = 120.0) -> Dict[str, Any]:\n        # Passthrough conditions\n        if not self.enabled:\n            return self._call(base_url, tool, args, timeout=timeout)\n        if self._should_bypass(args):\n            return self._call(base_url, tool, args, timeout=timeout)\n\n        start_ts = time.time()\n        key = self._group_key(base_url, args or {})\n        norm_q = self._norm_query((args or {}).get(\"query\") or \"\")\n        ev = threading.Event()\n        slot = {\"event\": ev, \"result\": None, \"error\": None, \"query\": norm_q, \"args\": dict(args or {})}\n\n        with self._lock:\n            g = self._groups.get(key)\n            if not g:\n                g = {\n                    \"created\": time.time(),\n                    \"items\": [],  # list of slots\n                    \"timer\": None,\n                }\n                self._groups[key] = g\n            # Dedup within the window: if same normalized query already present, just add another waiter\n            g[\"items\"].append(slot)\n            # Start timer if not set\n            if g[\"timer\"] is None:\n                delay = max(0.0, float(self.window_ms) / 1000.0)\n                t = threading.Timer(delay, self._flush, args=(key,))\n                g[\"timer\"] = t\n                t.daemon = True\n                t.start()\n            # Flush early if cap reached\n            if len(g[\"items\"]) >= self.max_batch:\n                t = g.get(\"timer\")\n                if t:\n                    try:\n                        t.cancel()\n                    except Exception:\n                        pass\n                    g[\"timer\"] = None\n                # flush outside lock\n                threading.Thread(target=self._flush, args=(key,), daemon=True).start()\n\n        # Wait with a hard budget\n        remain = max(0.05, self.budget_ms / 1000.0)\n        ev.wait(timeout=min(timeout, remain))\n        # If not signaled yet (e.g., server slow), fall back to direct call\n        if not ev.is_set():\n            try:\n                res = self._call(base_url, tool, args, timeout=timeout)\n                slot[\"result\"] = res\n                ev.set()\n                # Remove this slot from the batch group to prevent duplicate flush calls\n                try:\n                    with self._lock:\n                        gg = self._groups.get(key)\n                        if gg:\n                            lst = gg.get(\"items\") or []\n                            if slot in lst:\n                                try:\n                                    lst.remove(slot)\n                                except Exception:\n                                    pass\n                            if not lst:\n                                # If group is empty, cancel timer and cleanup\n                                t2 = gg.get(\"timer\")\n                                if t2:\n                                    try:\n                                        t2.cancel()\n                                    except Exception:\n                                        pass\n                                self._groups.pop(key, None)\n                except Exception:\n                    pass\n                # Metrics: bypass due to budget\n                try:\n                    print(json.dumps({\"router\": {\"batch_fallback\": True, \"elapsed_ms\": int((time.time()-start_ts)*1000)}}), file=sys.stderr)\n                except Exception:\n                    pass\n                return res\n            except Exception as e:\n                slot[\"error\"] = e\n                ev.set()\n                raise\n\n        # Return shared result\n        if slot.get(\"error\") is not None:\n            raise slot[\"error\"]\n        return slot.get(\"result\") or {}\n\n    def _flush(self, key: str) -> None:\n        with self._lock:\n            g = self._groups.get(key)\n            if not g:\n                return\n            items = g.get(\"items\") or []\n            g[\"items\"] = []\n            g[\"timer\"] = None\n            if not items:\n                self._groups.pop(key, None)\n                return\n        # Build queries and merged args\n        unique_q: list[str] = []\n        seen_q = set()\n        for it in items:\n            q = it.get(\"query\") or \"\"\n            if q not in seen_q:\n                seen_q.add(q)\n                unique_q.append(q)\n        first_args = dict(items[0].get(\"args\") or {})\n        forward = {k: v for k, v in first_args.items() if k not in {\"query\", \"queries\"}}\n        base_url = None\n        try:\n            # Rebuild base_url from key prefix\n            base_url = key.split(\"|\")[0]\n        except Exception:\n            base_url = HTTP_URL_INDEXER\n\n        started = time.time()\n        results_by_q: Dict[str, Any] = {}\n        errors_by_q: Dict[str, Exception] = {}\n        calls = 0\n        try:\n            import copy as _copy\n        except Exception:\n            _copy = None  # type: ignore\n\n        # Single aggregated call when multiple unique queries\n        if len(unique_q) > 1:\n            args_all = dict(forward)\n            args_all[\"query\"] = list(unique_q)\n            args_all[\"mode\"] = args_all.get(\"mode\") or \"pack\"\n            try:\n                agg_res = self._call(base_url, \"context_answer\", args_all, timeout=120.0)\n                calls = 1\n                # Demultiplex: carve per-query replies\n                try:\n                    payload = ((agg_res or {}).get(\"result\") or {}).get(\"structuredContent\") or {}\n                    body = (payload.get(\"result\") or {})\n                    ans = str(body.get(\"answer\") or \"\")\n                    cits = body.get(\"citations\") or []\n                except Exception:\n                    payload, body, ans, cits = {}, {}, \"\", []\n\n                # Prefer exact demux from indexer if available\n                abq = None\n                try:\n                    abq = body.get(\"answers_by_query\")\n                except Exception:\n                    abq = None\n                if isinstance(abq, list) and abq:\n                    _map: Dict[str, Any] = {}\n                    # Build by query string when present, else by index\n                    by_idx = (len(abq) >= len(unique_q))\n                    for i, entry in enumerate(abq):\n                        try:\n                            qv = entry.get(\"query\")\n                            qk = None\n                            if isinstance(qv, list) and qv:\n                                qk = str(qv[0])\n                            elif isinstance(qv, str):\n                                qk = qv\n                        except Exception:\n                            qk = None\n                        # Fallback to index if query key not present\n                        key = qk if qk else (unique_q[i] if by_idx and i < len(unique_q) else None)\n                        if not key:\n                            continue\n                        # Build per result by overriding answer/citations/query\n                        per = _copy.deepcopy(agg_res) if _copy else json.loads(json.dumps(agg_res))\n                        try:\n                            per_body = (per.get(\"result\") or {}).get(\"structuredContent\", {}).get(\"result\", {})\n                        except Exception:\n                            per_body = None\n                        try:\n                            ans_i = str(entry.get(\"answer\") or \"\")\n                            cits_i = entry.get(\"citations\") or []\n                            if per_body is not None:\n                                per_body[\"answer\"] = ans_i\n                                per_body[\"citations\"] = cits_i\n                                per_body[\"query\"] = [key]\n                            else:\n                                if \"result\" in per and isinstance(per[\"result\"], dict):\n                                    sc = per[\"result\"].get(\"structuredContent\") or {}\n                                    if \"result\" in sc and isinstance(sc[\"result\"], dict):\n                                        sc[\"result\"][\"answer\"] = ans_i\n                                        sc[\"result\"][\"citations\"] = cits_i\n                                        sc[\"result\"][\"query\"] = [key]\n                        except Exception:\n                            pass\n                        _map[str(key)] = per\n                    for uq in unique_q:\n                        if str(uq) in _map:\n                            results_by_q[uq] = _map[str(uq)]\n                    # Fill missing via heuristics below if necessary\n                    remaining = [uq for uq in unique_q if uq not in results_by_q]\n                else:\n                    remaining = list(unique_q)\n\n                # No structured per-query answers from server: fall back to one call per query for correctness\n                if remaining:\n                    for uq in remaining:\n                        args_i = dict(forward)\n                        args_i[\"query\"] = uq\n                        try:\n                            results_by_q[uq] = self._call(base_url, \"context_answer\", args_i, timeout=120.0)\n                        except Exception as e:\n                            errors_by_q[uq] = e\n                    calls += len(remaining)\n            except Exception as e:\n                # If aggregated call fails, assign error to every query\n                for uq in unique_q:\n                    errors_by_q[uq] = e\n                calls = 1\n        else:\n            # Single query passthrough\n            args1 = dict(forward)\n            args1[\"query\"] = unique_q[0] if unique_q else \"\"\n            try:\n                results_by_q[args1[\"query\"]] = self._call(base_url, \"context_answer\", args1, timeout=120.0)\n            except Exception as e:\n                errors_by_q[args1[\"query\"]] = e\n            calls = 1\n\n        elapsed_ms = int((time.time() - started) * 1000)\n        try:\n            print(json.dumps({\n                \"router\": {\n                    \"batch_flushed\": True,\n                    \"n_items\": len(items),\n                    \"unique_q\": len(unique_q),\n                    \"calls\": int(calls),\n                    \"elapsed_ms\": elapsed_ms,\n                    \"ok\": (len(errors_by_q) == 0),\n                }\n            }), file=sys.stderr)\n        except Exception:\n            pass\n\n        # Fan out per-query results/errors to all waiters\n        for it in items:\n            q = it.get(\"query\") or \"\"\n            it[\"result\"] = results_by_q.get(q)\n            it[\"error\"] = errors_by_q.get(q)\n            ev = it.get(\"event\")\n            try:\n                if hasattr(ev, \"set\"):\n                    ev.set()\n            except Exception:\n                pass\n        with self._lock:\n            # Cleanup empty group\n            gg = self._groups.get(key)\n            if gg and not gg.get(\"items\"):\n                self._groups.pop(key, None)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___749": {
      "name": "__init__",
      "type": "method",
      "start_line": 749,
      "end_line": 771,
      "content_hash": "71604f74d442216277e25cbb620077901396dfff",
      "content": "    def __init__(self, call_func=None, enable: bool | None = None, window_ms: int | None = None,\n                 max_batch: int | None = None, budget_ms: int | None = None):\n        self._call = call_func or call_tool_http\n        # Enabled flag (default off for back-compat)\n        if enable is None:\n            env_enabled = os.environ.get(\"ROUTER_BATCH_ENABLED\")\n            if env_enabled is None:\n                env_enabled = os.environ.get(\"ROUTER_BATCH_ENABLE\", \"0\")\n            self.enabled = str(env_enabled).strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}\n        else:\n            self.enabled = bool(enable)\n        # Window and caps\n        self.window_ms = int(os.environ.get(\"ROUTER_BATCH_WINDOW_MS\", str(window_ms if window_ms is not None else 100)) or 100)\n        env_max = os.environ.get(\"ROUTER_BATCH_MAX_SIZE\")\n        if env_max is None:\n            env_max = os.environ.get(\"ROUTER_BATCH_MAX\")\n        self.max_batch = int(env_max or (max_batch if max_batch is not None else 8))\n        env_budget = os.environ.get(\"ROUTER_BATCH_LATENCY_BUDGET_MS\")\n        if env_budget is None:\n            env_budget = os.environ.get(\"ROUTER_BATCH_BUDGET_MS\")\n        self.budget_ms = int(env_budget or (budget_ms if budget_ms is not None else 2000))\n        self._lock = threading.RLock()\n        self._groups: dict[str, dict[str, any]] = {}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__should_bypass_773": {
      "name": "_should_bypass",
      "type": "method",
      "start_line": 773,
      "end_line": 792,
      "content_hash": "bed2a09a39af7f0d8dcf3f1336c1a7714354e6d0",
      "content": "    def _should_bypass(self, args: Dict[str, Any]) -> bool:\n        # 1) explicit flag in args\n        try:\n            if isinstance(args, dict):\n                v = args.get(\"immediate\")\n                if v is not None and str(v).strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}:\n                    return True\n        except Exception:\n            pass\n        # 2) env-level bypass\n        if str(os.environ.get(\"ROUTER_BATCH_BYPASS\", \"0\")).strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}:\n            return True\n        # 3) free-text hint in the query\n        try:\n            q = str((args or {}).get(\"query\") or \"\")\n            if \"immediate answer\" in q.lower():\n                return True\n        except Exception:\n            pass\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__norm_query_794": {
      "name": "_norm_query",
      "type": "method",
      "start_line": 794,
      "end_line": 798,
      "content_hash": "18e447036569484376d52c8207c2bc874243b59e",
      "content": "    def _norm_query(self, q: str) -> str:\n        try:\n            return re.sub(r\"\\s+\", \" \", str(q or \"\").strip())\n        except Exception:\n            return str(q)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__filters_fingerprint_800": {
      "name": "_filters_fingerprint",
      "type": "method",
      "start_line": 800,
      "end_line": 818,
      "content_hash": "5c9f524f332cec367bde57700062a3d6406018c7",
      "content": "    def _filters_fingerprint(self, args: Dict[str, Any]) -> str:\n        keep = {\n            \"collection\", \"language\", \"under\", \"kind\", \"symbol\", \"ext\",\n            \"path_regex\", \"path_glob\", \"not_glob\", \"not_\", \"case\",\n            \"limit\", \"per_path\", \"include_snippet\",\n        }\n        try:\n            filt = {k: args.get(k) for k in keep if k in args}\n            # Normalize list-like values\n            def _norm(v):\n                if v is None:\n                    return None\n                if isinstance(v, (list, tuple)):\n                    return [str(x) for x in v]\n                return v\n            clean = {k: _norm(v) for k, v in filt.items()}\n            return json.dumps(clean, sort_keys=True, ensure_ascii=False)\n        except Exception:\n            return \"{}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__norm_809": {
      "name": "_norm",
      "type": "method",
      "start_line": 809,
      "end_line": 814,
      "content_hash": "5774ae3c1f6d1ff819a800e129e36ecf22eef7ae",
      "content": "            def _norm(v):\n                if v is None:\n                    return None\n                if isinstance(v, (list, tuple)):\n                    return [str(x) for x in v]\n                return v",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__group_key_820": {
      "name": "_group_key",
      "type": "method",
      "start_line": 820,
      "end_line": 824,
      "content_hash": "b1f8ab04266c15b2ec9ee4135539a1b23c462b9c",
      "content": "    def _group_key(self, base_url: str, args: Dict[str, Any]) -> str:\n        coll = str(args.get(\"collection\") or \"\")\n        fp = self._filters_fingerprint(args)\n        repo = os.getcwd()\n        return f\"{base_url}|{coll}|answer|{fp}|{repo}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_call_or_enqueue_826": {
      "name": "call_or_enqueue",
      "type": "method",
      "start_line": 826,
      "end_line": 914,
      "content_hash": "f342885a2eb5259d76bc8db396943fef50298829",
      "content": "    def call_or_enqueue(self, base_url: str, tool: str, args: Dict[str, Any], timeout: float = 120.0) -> Dict[str, Any]:\n        # Passthrough conditions\n        if not self.enabled:\n            return self._call(base_url, tool, args, timeout=timeout)\n        if self._should_bypass(args):\n            return self._call(base_url, tool, args, timeout=timeout)\n\n        start_ts = time.time()\n        key = self._group_key(base_url, args or {})\n        norm_q = self._norm_query((args or {}).get(\"query\") or \"\")\n        ev = threading.Event()\n        slot = {\"event\": ev, \"result\": None, \"error\": None, \"query\": norm_q, \"args\": dict(args or {})}\n\n        with self._lock:\n            g = self._groups.get(key)\n            if not g:\n                g = {\n                    \"created\": time.time(),\n                    \"items\": [],  # list of slots\n                    \"timer\": None,\n                }\n                self._groups[key] = g\n            # Dedup within the window: if same normalized query already present, just add another waiter\n            g[\"items\"].append(slot)\n            # Start timer if not set\n            if g[\"timer\"] is None:\n                delay = max(0.0, float(self.window_ms) / 1000.0)\n                t = threading.Timer(delay, self._flush, args=(key,))\n                g[\"timer\"] = t\n                t.daemon = True\n                t.start()\n            # Flush early if cap reached\n            if len(g[\"items\"]) >= self.max_batch:\n                t = g.get(\"timer\")\n                if t:\n                    try:\n                        t.cancel()\n                    except Exception:\n                        pass\n                    g[\"timer\"] = None\n                # flush outside lock\n                threading.Thread(target=self._flush, args=(key,), daemon=True).start()\n\n        # Wait with a hard budget\n        remain = max(0.05, self.budget_ms / 1000.0)\n        ev.wait(timeout=min(timeout, remain))\n        # If not signaled yet (e.g., server slow), fall back to direct call\n        if not ev.is_set():\n            try:\n                res = self._call(base_url, tool, args, timeout=timeout)\n                slot[\"result\"] = res\n                ev.set()\n                # Remove this slot from the batch group to prevent duplicate flush calls\n                try:\n                    with self._lock:\n                        gg = self._groups.get(key)\n                        if gg:\n                            lst = gg.get(\"items\") or []\n                            if slot in lst:\n                                try:\n                                    lst.remove(slot)\n                                except Exception:\n                                    pass\n                            if not lst:\n                                # If group is empty, cancel timer and cleanup\n                                t2 = gg.get(\"timer\")\n                                if t2:\n                                    try:\n                                        t2.cancel()\n                                    except Exception:\n                                        pass\n                                self._groups.pop(key, None)\n                except Exception:\n                    pass\n                # Metrics: bypass due to budget\n                try:\n                    print(json.dumps({\"router\": {\"batch_fallback\": True, \"elapsed_ms\": int((time.time()-start_ts)*1000)}}), file=sys.stderr)\n                except Exception:\n                    pass\n                return res\n            except Exception as e:\n                slot[\"error\"] = e\n                ev.set()\n                raise\n\n        # Return shared result\n        if slot.get(\"error\") is not None:\n            raise slot[\"error\"]\n        return slot.get(\"result\") or {}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__flush_916": {
      "name": "_flush",
      "type": "method",
      "start_line": 916,
      "end_line": 1080,
      "content_hash": "95766e24ebf5351f284a4dff29e0ed133a69146d",
      "content": "    def _flush(self, key: str) -> None:\n        with self._lock:\n            g = self._groups.get(key)\n            if not g:\n                return\n            items = g.get(\"items\") or []\n            g[\"items\"] = []\n            g[\"timer\"] = None\n            if not items:\n                self._groups.pop(key, None)\n                return\n        # Build queries and merged args\n        unique_q: list[str] = []\n        seen_q = set()\n        for it in items:\n            q = it.get(\"query\") or \"\"\n            if q not in seen_q:\n                seen_q.add(q)\n                unique_q.append(q)\n        first_args = dict(items[0].get(\"args\") or {})\n        forward = {k: v for k, v in first_args.items() if k not in {\"query\", \"queries\"}}\n        base_url = None\n        try:\n            # Rebuild base_url from key prefix\n            base_url = key.split(\"|\")[0]\n        except Exception:\n            base_url = HTTP_URL_INDEXER\n\n        started = time.time()\n        results_by_q: Dict[str, Any] = {}\n        errors_by_q: Dict[str, Exception] = {}\n        calls = 0\n        try:\n            import copy as _copy\n        except Exception:\n            _copy = None  # type: ignore\n\n        # Single aggregated call when multiple unique queries\n        if len(unique_q) > 1:\n            args_all = dict(forward)\n            args_all[\"query\"] = list(unique_q)\n            args_all[\"mode\"] = args_all.get(\"mode\") or \"pack\"\n            try:\n                agg_res = self._call(base_url, \"context_answer\", args_all, timeout=120.0)\n                calls = 1\n                # Demultiplex: carve per-query replies\n                try:\n                    payload = ((agg_res or {}).get(\"result\") or {}).get(\"structuredContent\") or {}\n                    body = (payload.get(\"result\") or {})\n                    ans = str(body.get(\"answer\") or \"\")\n                    cits = body.get(\"citations\") or []\n                except Exception:\n                    payload, body, ans, cits = {}, {}, \"\", []\n\n                # Prefer exact demux from indexer if available\n                abq = None\n                try:\n                    abq = body.get(\"answers_by_query\")\n                except Exception:\n                    abq = None\n                if isinstance(abq, list) and abq:\n                    _map: Dict[str, Any] = {}\n                    # Build by query string when present, else by index\n                    by_idx = (len(abq) >= len(unique_q))\n                    for i, entry in enumerate(abq):\n                        try:\n                            qv = entry.get(\"query\")\n                            qk = None\n                            if isinstance(qv, list) and qv:\n                                qk = str(qv[0])\n                            elif isinstance(qv, str):\n                                qk = qv\n                        except Exception:\n                            qk = None\n                        # Fallback to index if query key not present\n                        key = qk if qk else (unique_q[i] if by_idx and i < len(unique_q) else None)\n                        if not key:\n                            continue\n                        # Build per result by overriding answer/citations/query\n                        per = _copy.deepcopy(agg_res) if _copy else json.loads(json.dumps(agg_res))\n                        try:\n                            per_body = (per.get(\"result\") or {}).get(\"structuredContent\", {}).get(\"result\", {})\n                        except Exception:\n                            per_body = None\n                        try:\n                            ans_i = str(entry.get(\"answer\") or \"\")\n                            cits_i = entry.get(\"citations\") or []\n                            if per_body is not None:\n                                per_body[\"answer\"] = ans_i\n                                per_body[\"citations\"] = cits_i\n                                per_body[\"query\"] = [key]\n                            else:\n                                if \"result\" in per and isinstance(per[\"result\"], dict):\n                                    sc = per[\"result\"].get(\"structuredContent\") or {}\n                                    if \"result\" in sc and isinstance(sc[\"result\"], dict):\n                                        sc[\"result\"][\"answer\"] = ans_i\n                                        sc[\"result\"][\"citations\"] = cits_i\n                                        sc[\"result\"][\"query\"] = [key]\n                        except Exception:\n                            pass\n                        _map[str(key)] = per\n                    for uq in unique_q:\n                        if str(uq) in _map:\n                            results_by_q[uq] = _map[str(uq)]\n                    # Fill missing via heuristics below if necessary\n                    remaining = [uq for uq in unique_q if uq not in results_by_q]\n                else:\n                    remaining = list(unique_q)\n\n                # No structured per-query answers from server: fall back to one call per query for correctness\n                if remaining:\n                    for uq in remaining:\n                        args_i = dict(forward)\n                        args_i[\"query\"] = uq\n                        try:\n                            results_by_q[uq] = self._call(base_url, \"context_answer\", args_i, timeout=120.0)\n                        except Exception as e:\n                            errors_by_q[uq] = e\n                    calls += len(remaining)\n            except Exception as e:\n                # If aggregated call fails, assign error to every query\n                for uq in unique_q:\n                    errors_by_q[uq] = e\n                calls = 1\n        else:\n            # Single query passthrough\n            args1 = dict(forward)\n            args1[\"query\"] = unique_q[0] if unique_q else \"\"\n            try:\n                results_by_q[args1[\"query\"]] = self._call(base_url, \"context_answer\", args1, timeout=120.0)\n            except Exception as e:\n                errors_by_q[args1[\"query\"]] = e\n            calls = 1\n\n        elapsed_ms = int((time.time() - started) * 1000)\n        try:\n            print(json.dumps({\n                \"router\": {\n                    \"batch_flushed\": True,\n                    \"n_items\": len(items),\n                    \"unique_q\": len(unique_q),\n                    \"calls\": int(calls),\n                    \"elapsed_ms\": elapsed_ms,\n                    \"ok\": (len(errors_by_q) == 0),\n                }\n            }), file=sys.stderr)\n        except Exception:\n            pass\n\n        # Fan out per-query results/errors to all waiters\n        for it in items:\n            q = it.get(\"query\") or \"\"\n            it[\"result\"] = results_by_q.get(q)\n            it[\"error\"] = errors_by_q.get(q)\n            ev = it.get(\"event\")\n            try:\n                if hasattr(ev, \"set\"):\n                    ev.set()\n            except Exception:\n                pass\n        with self._lock:\n            # Cleanup empty group\n            gg = self._groups.get(key)\n            if gg and not gg.get(\"items\"):\n                self._groups.pop(key, None)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__mcp_tools_list_1086": {
      "name": "_mcp_tools_list",
      "type": "function",
      "start_line": 1086,
      "end_line": 1103,
      "content_hash": "0f8fc1dd611a19a339f344c5b577520fd9e3766b",
      "content": "def _mcp_tools_list(base_url: str, timeout: float = 30.0) -> List[str]:\n    try:\n        headers = _mcp_handshake(base_url, timeout=min(timeout, 15.0))\n        payload = {\"jsonrpc\": \"2.0\", \"id\": \"router-list\", \"method\": \"tools/list\"}\n        _, body = _post_raw_retry(base_url, payload, headers, timeout=timeout)\n        j = _parse_stream_or_json(body)\n        tools = ((j.get(\"result\") or {}).get(\"tools\") or [])\n        names: List[str] = []\n        for t in tools:\n            try:\n                n = t.get(\"name\") if isinstance(t, dict) else None\n                if isinstance(n, str) and n:\n                    names.append(n)\n            except Exception:\n                continue\n        return names\n    except Exception:\n        return []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cosine_1114": {
      "name": "_cosine",
      "type": "function",
      "start_line": 1114,
      "end_line": 1129,
      "content_hash": "4a763297a69d78912a14f12cea8047ceb87f1bd5",
      "content": "def _cosine(a: list[float], b: list[float]) -> float:\n    try:\n        s = 0.0\n        na = 0.0\n        nb = 0.0\n        for i in range(min(len(a), len(b))):\n            va = float(a[i])\n            vb = float(b[i])\n            s += va * vb\n            na += va * va\n            nb += vb * vb\n        na = (na or 1.0) ** 0.5\n        nb = (nb or 1.0) ** 0.5\n        return s / (na * nb)\n    except Exception:\n        return 0.0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__mcp_tools_describe_1132": {
      "name": "_mcp_tools_describe",
      "type": "function",
      "start_line": 1132,
      "end_line": 1150,
      "content_hash": "a54671cadc169058a40b34d8423d488c42b3a76b",
      "content": "def _mcp_tools_describe(base_url: str, timeout: float = 20.0) -> list[dict]:\n    \"\"\"Return tool dicts from tools/list (includes name/description/schema if provided).\"\"\"\n    try:\n        headers = _mcp_handshake(base_url, timeout=min(timeout, 10.0))\n        payload = {\"jsonrpc\": \"2.0\", \"id\": \"router-list2\", \"method\": \"tools/list\"}\n        _, body = _post_raw_retry(base_url, payload, headers, timeout=timeout)\n        j = _parse_stream_or_json(body)\n        tools = ((j.get(\"result\") or {}).get(\"tools\") or [])\n        out = []\n        for t in tools:\n            if not isinstance(t, dict):\n                continue\n            name = (t.get(\"name\") or \"\").strip()\n            if not name:\n                continue\n            out.append(t)\n        return out\n    except Exception:\n        return []",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cache_ttl_sec_1161": {
      "name": "_cache_ttl_sec",
      "type": "function",
      "start_line": 1161,
      "end_line": 1165,
      "content_hash": "3b316c84c0e9c7f1deda8d40a0bebb9b2bee717e",
      "content": "def _cache_ttl_sec() -> int:\n    try:\n        return int(os.environ.get(\"ROUTER_TOOLS_CACHE_TTL_SEC\", \"60\") or 60)\n    except Exception:\n        return 60",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__scratchpad_path_1171": {
      "name": "_scratchpad_path",
      "type": "function",
      "start_line": 1171,
      "end_line": 1177,
      "content_hash": "49f70f9f85c403ff84d48bf11174d09f0982a0bb",
      "content": "def _scratchpad_path() -> str:\n    base = os.path.join(os.getcwd(), \".codebase\")\n    try:\n        os.makedirs(base, exist_ok=True)\n    except Exception:\n        pass\n    return os.path.join(base, \"router_scratchpad.json\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__load_scratchpad_1180": {
      "name": "_load_scratchpad",
      "type": "function",
      "start_line": 1180,
      "end_line": 1225,
      "content_hash": "5d71a002be67d1b3c8f59b5c99b3abdca63d2ee3",
      "content": "def _load_scratchpad() -> Dict[str, Any]:\n    p = _scratchpad_path()\n    try:\n        with open(p, \"r\", encoding=\"utf-8\") as f:\n            j = json.load(f)\n            if isinstance(j, dict):\n                try:\n                    ts = float(j.get(\"timestamp\") or 0.0)\n                except Exception:\n                    ts = 0.0\n                ttl = _scratchpad_ttl_sec()\n                if ts and ttl >= 0 and (time.time() - ts) > ttl:\n                    stale_keys = (\n                        \"last_plan\",\n                        \"last_filters\",\n                        \"mem_snippets\",\n                        \"last_answer\",\n                        \"last_citations\",\n                        \"last_paths\",\n                        \"last_metrics\",\n                    )\n                    removed = False\n                    for stale_key in stale_keys:\n                        if stale_key in j:\n                            j.pop(stale_key, None)\n                            removed = True\n                    if removed:\n                        j[\"timestamp\"] = 0.0\n                        try:\n                            print(\n                                json.dumps(\n                                    {\n                                        \"router\": {\n                                            \"scratchpad\": \"stale_cleared\",\n                                            \"age_sec\": round(time.time() - ts, 2),\n                                        }\n                                    }\n                                ),\n                                file=sys.stderr,\n                            )\n                        except Exception:\n                            pass\n                return j\n    except Exception:\n        pass\n    return {}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__save_scratchpad_1228": {
      "name": "_save_scratchpad",
      "type": "function",
      "start_line": 1228,
      "end_line": 1245,
      "content_hash": "ace292cce3f2da06269f69132ab7612177bcc247",
      "content": "def _save_scratchpad(d: Dict[str, Any]) -> None:\n    p = _scratchpad_path()\n    tmp = p + \".tmp\"\n    try:\n        with open(tmp, \"w\", encoding=\"utf-8\") as f:\n            json.dump(d, f)\n            try:\n                f.flush()\n                os.fsync(f.fileno())\n            except Exception:\n                pass\n        os.replace(tmp, p)\n    except Exception:\n        try:\n            if os.path.exists(tmp):\n                os.unlink(tmp)\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__looks_like_repeat_1248": {
      "name": "_looks_like_repeat",
      "type": "function",
      "start_line": 1248,
      "end_line": 1253,
      "content_hash": "c693e2b15c4c0931b340fc44dd402e933a2228c0",
      "content": "def _looks_like_repeat(q: str) -> bool:\n    s = q.strip().lower()\n    pats = [\n        \"repeat\", \"again\", \"same thing\", \"do that again\", \"rerun\", \"run it again\", \"same as before\",\n    ]\n    return any(p in s for p in pats)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__looks_like_same_filters_1256": {
      "name": "_looks_like_same_filters",
      "type": "function",
      "start_line": 1256,
      "end_line": 1258,
      "content_hash": "4d433a5d21c3867f66e6313f9f42f6e3a3078f81",
      "content": "def _looks_like_same_filters(q: str) -> bool:\n    s = q.strip().lower()\n    return any(p in s for p in [\"same filters\", \"reuse filters\", \"previous filters\"])",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__tools_describe_cached_1259": {
      "name": "_tools_describe_cached",
      "type": "function",
      "start_line": 1259,
      "end_line": 1270,
      "content_hash": "e22d8832b4c1286ac2f613b2669abd942fe67440",
      "content": "def _tools_describe_cached(base_url: str, allow_network: bool = True, timeout: float = 20.0) -> list[dict]:\n    now = time.time()\n    ts = _TOOLS_DESCR_TS.get(base_url, 0.0)\n    if base_url in _TOOLS_DESCR_CACHE and (now - ts) <= _cache_ttl_sec():\n        return _TOOLS_DESCR_CACHE[base_url]\n    if not allow_network:\n        return _TOOLS_DESCR_CACHE.get(base_url, [])\n    # Prefer /tools on health port; fallback to MCP tools/list\n    desc = _tools_describe_from_health(base_url, timeout=min(timeout, 3.0)) or _mcp_tools_describe(base_url, timeout=timeout)\n    _TOOLS_DESCR_CACHE[base_url] = desc\n    _TOOLS_DESCR_TS[base_url] = now\n    return desc",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__scratchpad_ttl_sec_1273": {
      "name": "_scratchpad_ttl_sec",
      "type": "function",
      "start_line": 1273,
      "end_line": 1277,
      "content_hash": "87062c144a575412927575fc9170ba3cd696d2f8",
      "content": "def _scratchpad_ttl_sec() -> int:\n    try:\n        return int(os.environ.get(\"ROUTER_SCRATCHPAD_TTL_SEC\", \"300\") or 300)\n    except Exception:\n        return 300",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__looks_like_expand_1280": {
      "name": "_looks_like_expand",
      "type": "function",
      "start_line": 1280,
      "end_line": 1286,
      "content_hash": "a2e4f4ad20b79d0a9cfc2bc7e073073c00a8ebf2",
      "content": "def _looks_like_expand(q: str) -> bool:\n    s = q.strip().lower()\n    pats = [\n        \"expand on\", \"expand that\", \"expand the summary\", \"elaborate\",\n        \"more detail\", \"more details\", \"go deeper\", \"add details\",\n    ]\n    return any(p in s for p in pats)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__default_tool_endpoints_1289": {
      "name": "_default_tool_endpoints",
      "type": "function",
      "start_line": 1289,
      "end_line": 1302,
      "content_hash": "14ca9bb91f5f66f4dcc9f012830ab005ccc7beab",
      "content": "def _default_tool_endpoints() -> Dict[str, str]:\n    idx = HTTP_URL_INDEXER\n    mem = HTTP_URL_MEMORY\n    mapping: Dict[str, str] = {}\n    for n in [\n        \"repo_search\",\"context_answer\",\"context_answer_compat\",\"expand_query\",\n        \"search_tests_for\",\"search_config_for\",\"search_callers_for\",\"search_importers_for\",\n        \"qdrant_index_root\",\"qdrant_prune\",\"qdrant_status\",\"qdrant_list\",\n        \"workspace_info\",\"list_workspaces\",\"change_history_for_path\",\"code_search\",\"context_search\",\n    ]:\n        mapping[n] = idx\n    mapping[\"store\"] = mem\n    mapping[\"find\"] = mem\n    return mapping",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__signature_text_1306": {
      "name": "_signature_text",
      "type": "function",
      "start_line": 1306,
      "end_line": 1318,
      "content_hash": "ae65b0ddeca20c54f8344b0593a75f785ba131d4",
      "content": "def _signature_text(t: dict) -> str:\n    name = (t.get(\"name\") or \"\").strip()\n    desc = (t.get(\"description\") or \"\").strip()\n    # Include simple param names if present\n    params = []\n    try:\n        schema = t.get(\"inputSchema\") or {}\n        props = (schema.get(\"properties\") or {}) if isinstance(schema, dict) else {}\n        params = [k for k in props.keys()]\n    except Exception:\n        params = []\n    ptxt = (\" params:\" + \",\".join(params)) if params else \"\"\n    return (name + \"\\n\" + desc + ptxt).strip()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__embed_texts_1325": {
      "name": "_embed_texts",
      "type": "function",
      "start_line": 1325,
      "end_line": 1360,
      "content_hash": "bc9030660aca760cba605b223a776e8a648bf886",
      "content": "def _embed_texts(texts: list[str]) -> list[list[float]]:\n    if not texts:\n        return []\n\n    # Try centralized embedder factory first (supports Qwen3 feature flag)\n    try:\n        from scripts.embedder import get_embedding_model\n        model_name = os.environ.get(\"EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\")\n        em = get_embedding_model(model_name)\n        raw = list(em.embed(texts))\n        vecs = [v.tolist() if hasattr(v, \"tolist\") else list(v) for v in raw]\n        return vecs\n    except ImportError:\n        pass\n\n    # Try dense embedding (fastembed) directly as fallback\n    if _FE_Embedding is not None:\n        try:\n            model_name = os.environ.get(\"EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\")\n            em = _embedder_singleton.get(\"model\")\n            if em is None or getattr(em, \"_name\", \"\") != model_name:\n                em = _FE_Embedding(model_name=model_name)\n                em._name = model_name  # type: ignore\n                _embedder_singleton[\"model\"] = em\n            raw = list(em.embed(texts))\n            vecs = [v.tolist() if hasattr(v, \"tolist\") else list(v) for v in raw]\n            return vecs\n        except Exception:\n            pass\n    # Fallback to lexical vectors\n    try:\n        from scripts.utils import lex_hash_vector_text  # type: ignore\n    except Exception:\n        # absolute minimal fallback: 1D length proxy\n        return [[float(len(t))] for t in texts]\n    return [lex_hash_vector_text(t, dim=4096) for t in texts]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__select_best_search_tool_by_signature_1363": {
      "name": "_select_best_search_tool_by_signature",
      "type": "function",
      "start_line": 1363,
      "end_line": 1408,
      "content_hash": "f5224fa99757fd61a9df07a82572bd5f22e807ab",
      "content": "def _select_best_search_tool_by_signature(q: str, tool_dict: dict[str, str], allow_network: bool = True) -> str | None:\n    \"\"\"Among available search tools, pick the best match by signature similarity.\n    tool_dict maps tool_name -> base_url; we try to fetch descriptions and score.\n    Falls back to repo_search unless a specific search_* clearly wins by a small margin.\n    \"\"\"\n    # Consider only search_* and repo_search\n    candidates = [n for n in tool_dict.keys() if n == \"repo_search\" or n.startswith(\"search_\")]\n    if not candidates:\n        return None\n    # Fetch descriptions per server (avoid duplicate calls) with cache and offline support\n    per_server: dict[str, list[dict]] = {}\n    for base in set(tool_dict[t] for t in candidates):\n        try:\n            per_server[base] = _tools_describe_cached(base, allow_network=allow_network)\n        except Exception:\n            per_server[base] = []\n    sig_map: dict[str, str] = {}\n    for tname in candidates:\n        base = tool_dict.get(tname)\n        descs = per_server.get(base, [])\n        # find matching tool object\n        obj = None\n        for td in descs:\n            if (td.get(\"name\") or \"\").strip() == tname:\n                obj = td\n                break\n        sig_map[tname] = _signature_text(obj or {\"name\": tname, \"description\": \"\"})\n    texts = [q] + [sig_map[n] for n in candidates]\n    vecs = _embed_texts(texts)\n    if not vecs or len(vecs) < 1 + len(candidates):\n        return None\n    qv = vecs[0]\n    scores: list[tuple[str, float]] = []\n    for i, name in enumerate(candidates):\n        sv = vecs[1 + i]\n        scores.append((name, _cosine(qv, sv)))\n    scores.sort(key=lambda x: x[1], reverse=True)\n    best, best_s = scores[0]\n    # Prefer repo_search unless a specific search_* beats it by margin\n    repo_s = next((s for n, s in scores if n == \"repo_search\"), None)\n    margin = 0.02\n    if best == \"repo_search\" or repo_s is None:\n        return best\n    if best != \"repo_search\" and best_s >= (repo_s + margin):\n        return best\n    return \"repo_search\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__parse_repo_hints_1414": {
      "name": "_parse_repo_hints",
      "type": "function",
      "start_line": 1414,
      "end_line": 1464,
      "content_hash": "1d285ae72983515160a525ccbfc7c9b504e2879b",
      "content": "def _parse_repo_hints(q: str) -> Dict[str, Any]:\n    \"\"\"Extract light filters from the query: language, under (path), symbol, ext, path_glob, not_glob.\"\"\"\n    s = q.strip()\n    low = s.lower()\n    out: Dict[str, Any] = {}\n    # language\n    for lang in sorted(_LANGS, key=len, reverse=True):\n        if re.search(rf\"\\b{re.escape(lang)}\\b\", low):\n            # normalize a couple common aliases\n            out[\"language\"] = {\"javascript\":\"js\",\"typescript\":\"ts\",\"c++\":\"cpp\",\"c#\":\"csharp\"}.get(lang, lang)\n            break\n    # under / in folder\n    # Prefer explicit \"under <path>\"; else fallback to \"in/inside <path>\"\n    m_under = re.search(r\"\\bunder\\s+([\\w./-]+)\", low)\n    m_in = re.search(r\"\\b(?:in|inside)\\s+([\\w./-]+)\", low)\n    m = m_under or m_in\n    if m:\n        cand = m.group(1)\n        # avoid treating language mention as a path (e.g., \"in python\")\n        if len(cand) >= 2 and cand not in _LANGS:\n            out[\"under\"] = cand\n    # symbol-like tokens: foo(), Foo.bar, pkg::Sym\n    m2 = re.search(r\"([A-Za-z_][A-Za-z0-9_]*\\s*\\(\\))|([A-Za-z_][\\w]*\\.[A-Za-z_][\\w]*)|([A-Za-z_][\\w]*::[A-Za-z_][\\w]*)\", s)\n    if m2:\n        sym = m2.group(0)\n        sym = re.sub(r\"\\s*\\(\\)\\s*$\", \"\", sym)\n        out[\"symbol\"] = sym\n    # file extension mention\n    m3 = re.search(r\"\\.(py|ts|tsx|js|jsx|go|java|rs|kt|rb|php|scala|swift)$\", s)\n    if m3:\n        out[\"ext\"] = m3.group(1)\n    # glob inclusions: \"only *.py\" or \"only py files\"\n    globs: List[str] = []\n    if re.search(r\"\\bonly\\b\", low):\n        # *.ext\n        m_glob = re.search(r\"\\*\\.[A-Za-z0-9]+\", s)\n        if m_glob:\n            globs.append(\"**/\" + m_glob.group(0))\n        # 'python files' style\n        if \"python\" in low and \"*.py\" not in \" \".join(globs):\n            globs.append(\"**/*.py\")\n    if globs:\n        out[\"path_glob\"] = globs\n    # exclusions: \"exclude vendor\", \"exclude tests\"\n    not_glob: List[str] = []\n    for ex in [\"vendor\", \"node_modules\", \"dist\", \"build\", \"tests\", \"__pycache__\"]:\n        if re.search(rf\"\\bexclude\\s+{re.escape(ex)}\\b\", low):\n            not_glob.append(f\"**/{ex}/**\")\n    if not_glob:\n        out[\"not_glob\"] = not_glob\n    return out",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__clean_query_and_dsl_1467": {
      "name": "_clean_query_and_dsl",
      "type": "function",
      "start_line": 1467,
      "end_line": 1477,
      "content_hash": "2e2b43d107c642f820b074feb09d9b4d754cc543",
      "content": "def _clean_query_and_dsl(q: str) -> Tuple[str, Dict[str, Any]]:\n    \"\"\"Strip DSL tokens from the natural-language query and return (clean_query, dsl_filters).\n    Falls back to the raw query if parse_query_dsl is unavailable.\n    \"\"\"\n    try:\n        # Lazy import to avoid heavy dependencies at module import time\n        from scripts.hybrid_search import parse_query_dsl  # type: ignore\n        clean, extracted = parse_query_dsl([q])\n        return (clean[0] if clean else \"\"), (extracted or {})\n    except Exception:\n        return q, {}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__discover_tool_endpoints_1482": {
      "name": "_discover_tool_endpoints",
      "type": "function",
      "start_line": 1482,
      "end_line": 1512,
      "content_hash": "4fa6ead6f5fec8172ac223d6af66ff097ccef948",
      "content": "def _discover_tool_endpoints(force: bool = False, allow_network: bool = True) -> Dict[str, str]:\n    \"\"\"Return mapping of tool_name -> base_url.\n    Uses a small TTL cache; optionally avoids network (offline planning).\n    Indexer HTTP has priority if both expose the same tool name.\n    \"\"\"\n    global _TOOL_ENDPOINTS_CACHE_TS, _TOOL_ENDPOINTS_CACHE_MAP\n    now = time.time()\n    ttl = _cache_ttl_sec()\n    if not force and _TOOL_ENDPOINTS_CACHE_MAP and (now - _TOOL_ENDPOINTS_CACHE_TS) <= ttl:\n        return _TOOL_ENDPOINTS_CACHE_MAP\n    if not allow_network:\n        # Use cached if present; otherwise a conservative default\n        return _TOOL_ENDPOINTS_CACHE_MAP or _default_tool_endpoints()\n    mapping: Dict[str, str] = {}\n    # Indexer first (priority) \u2014 prefer health /tools registry when available\n    idx_desc = _tools_describe_cached(HTTP_URL_INDEXER, allow_network=allow_network)\n    for t in idx_desc:\n        n = t.get(\"name\") if isinstance(t, dict) else None\n        if n:\n            mapping[n] = HTTP_URL_INDEXER\n    # Memory server next\n    mem_desc = _tools_describe_cached(HTTP_URL_MEMORY, allow_network=allow_network)\n    for t in mem_desc:\n        n = t.get(\"name\") if isinstance(t, dict) else None\n        if n and n not in mapping:\n            mapping[n] = HTTP_URL_MEMORY\n    if mapping:\n        _TOOL_ENDPOINTS_CACHE_MAP.clear()\n        _TOOL_ENDPOINTS_CACHE_MAP.update(mapping)\n        _TOOL_ENDPOINTS_CACHE_TS = now\n    return mapping or (_TOOL_ENDPOINTS_CACHE_MAP or _default_tool_endpoints())",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__is_result_good_1515": {
      "name": "_is_result_good",
      "type": "function",
      "start_line": 1515,
      "end_line": 1548,
      "content_hash": "1d41577db83d53b1df96d05a11a7d17159fb39f7",
      "content": "def _is_result_good(tool: str, resp: Dict[str, Any]) -> bool:\n    \"\"\"Lightweight validation to decide if we should stop or try fallbacks.\"\"\"\n    try:\n        r = resp.get(\"result\") or {}\n        sc = r.get(\"structuredContent\") or {}\n        rs = sc.get(\"result\") or {}\n        # Answer tools: require a non-empty answer and avoid obvious \"no context\" replies\n        if tool in {\"context_answer\", \"context_answer_compat\"}:\n            ans = rs.get(\"answer\") if isinstance(rs, dict) else None\n            if isinstance(ans, str):\n                s = ans.strip()\n                if s and not any(p in s.lower() for p in [\n                    \"insufficient context\", \"not enough context\", \"no relevant\", \"don't know\", \"cannot answer\"\n                ]):\n                    return True\n            # If empty or unclear text, accept if we have citations\n            cites = rs.get(\"citations\") if isinstance(rs, dict) else None\n            if isinstance(cites, list) and len(cites) > 0:\n                return True\n            return False\n        # Search tools: require at least one hit\n        if tool.startswith(\"search_\") or tool == \"repo_search\":\n            total = rs.get(\"total\") if isinstance(rs, dict) else None\n            if isinstance(total, int) and total > 0:\n                return True\n            results = rs.get(\"results\") if isinstance(rs, dict) else None\n            if isinstance(results, list) and len(results) > 0:\n                return True\n            return False\n        # Admin/status tools: treat non-error responses as success\n        return not _is_failure_response(resp)\n    except Exception:\n        # Be conservative: if we can't parse, require non-error\n        return not _is_failure_response(resp)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__extract_metric_from_resp_1556": {
      "name": "_extract_metric_from_resp",
      "type": "function",
      "start_line": 1556,
      "end_line": 1584,
      "content_hash": "c84a77ccf99a4f5d7fc01e83426f8fa0e7cab8c4",
      "content": "def _extract_metric_from_resp(tool: str, resp: Dict[str, Any]) -> tuple[str, float] | None:\n    try:\n        r = resp.get(\"result\") or {}\n        sc = r.get(\"structuredContent\") or {}\n        rs = sc.get(\"result\") or {}\n        # Search-like tools\n        if tool in {\"repo_search\", \"code_search\", \"context_search\", \"search_tests_for\", \"search_config_for\", \"search_callers_for\", \"search_importers_for\"}:\n            tot = rs.get(\"total\")\n            if isinstance(tot, (int, float)):\n                return (\"total_results\", float(tot))\n            results = rs.get(\"results\")\n            if isinstance(results, list):\n                return (\"total_results\", float(len(results)))\n            return None\n        # Answer tools: track citations count\n        if tool in {\"context_answer\", \"context_answer_compat\"}:\n            cites = rs.get(\"citations\")\n            if isinstance(cites, list):\n                return (\"citations\", float(len(cites)))\n            return (\"citations\", 0.0)\n        # Admin/status: qdrant_status \u2192 point count\n        if tool == \"qdrant_status\":\n            cnt = rs.get(\"count\")\n            if isinstance(cnt, (int, float)):\n                return (\"points\", float(cnt))\n            return None\n    except Exception:\n        return None\n    return None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__divergence_thresholds_1587": {
      "name": "_divergence_thresholds",
      "type": "function",
      "start_line": 1587,
      "end_line": 1596,
      "content_hash": "598d769fb24d07d5c8d7aa3a39300ccbf702dc50",
      "content": "def _divergence_thresholds() -> tuple[float, int]:\n    try:\n        drop_frac = float(os.environ.get(\"ROUTER_DIVERGENCE_DROP_FRAC\", \"0.5\") or 0.5)\n    except Exception:\n        drop_frac = 0.5\n    try:\n        min_base = int(os.environ.get(\"ROUTER_DIVERGENCE_MIN_BASE\", \"3\") or 3)\n    except Exception:\n        min_base = 3\n    return drop_frac, min_base",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__material_drop_1599": {
      "name": "_material_drop",
      "type": "function",
      "start_line": 1599,
      "end_line": 1607,
      "content_hash": "097064a45bf2081f2e13184f803b38da374f7411",
      "content": "def _material_drop(prev: float | None, curr: float, drop_frac: float, min_base: int) -> bool:\n    try:\n        if prev is None:\n            return False\n        if prev < float(min_base):\n            return False\n        return curr < (float(prev) * float(drop_frac))\n    except Exception:\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__divergence_is_fatal_for_1612": {
      "name": "_divergence_is_fatal_for",
      "type": "function",
      "start_line": 1612,
      "end_line": 1623,
      "content_hash": "2302ed9b694089e25dbc0533500ca2c6ced1582e",
      "content": "def _divergence_is_fatal_for(tool: str) -> bool:\n    try:\n        s = (os.environ.get(\"ROUTER_DIVERGENCE_FATAL_TOOLS\", \"\") or \"\").strip()\n        if not s:\n            return False\n        low = s.lower()\n        if low in {\"*\", \"all\", \"1\", \"true\"}:\n            return True\n        names = {t.strip().lower() for t in s.split(\",\") if t.strip()}\n        return tool.strip().lower() in names\n    except Exception:\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_main_1629": {
      "name": "main",
      "type": "function",
      "start_line": 1629,
      "end_line": 1893,
      "content_hash": "780c8f5c27b85e98610b59cb04d0cecc384e15ee",
      "content": "def main(argv: List[str]) -> int:\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"query\", help=\"User query to route\")\n    ap.add_argument(\"--plan\", action=\"store_true\", help=\"Only print routing plan (no execution)\")\n    ap.add_argument(\"--run\", action=\"store_true\", help=\"Execute the routed tool(s) over HTTP\")\n    ap.add_argument(\"--timeout\", type=float, default=180.0, help=\"HTTP timeout for tool calls\")\n    args = ap.parse_args(argv)\n\n    plan = build_plan(args.query)\n    print(json.dumps({\"router\": {\"url\": HTTP_URL_INDEXER, \"plan\": plan}}, indent=2))\n\n    if args.plan and not args.run:\n        return 0\n\n    # Load scratchpad for prior context and TTL freshness\n    sp = {}\n    fresh = False\n    prior_answer = None\n    prior_citations = None\n    prior_paths = None\n    try:\n        sp = _load_scratchpad()\n        ts = float(sp.get(\"timestamp\") or 0.0)\n        fresh = bool(ts and (time.time() - ts) <= _scratchpad_ttl_sec())\n        if fresh:\n            prior_answer = sp.get(\"last_answer\")\n            prior_citations = sp.get(\"last_citations\")\n            prior_paths = sp.get(\"last_paths\")\n    except Exception:\n        pass\n\n    # Execute sequentially until one succeeds\n    last_err = None\n    last = None\n    tool_servers = _discover_tool_endpoints()\n    # Simple scratchpad for memory\u2192answer workflow\n    mem_snippets: list[str] = list(sp.get(\"mem_snippets\") or []) if fresh else []\n    for idx, (tool, targs) in enumerate(plan):\n        base_url = tool_servers.get(tool, HTTP_URL_INDEXER)\n        # Skip memory.find if we already have fresh snippets and this is a repeat/expand\n        if (tool.lower().endswith(\"find\") or tool.lower() in {\"find\", \"memory.find\"}) and mem_snippets and fresh and (_looks_like_repeat(args.query) or _looks_like_expand(args.query)):\n            try:\n                print(json.dumps({\"tool\": tool, \"skipped\": \"scratchpad_fresh\"}))\n            except Exception:\n                pass\n            continue\n\n        # If we have memory/prior summary and are about to answer, augment the query text\n        if tool in {\"context_answer\", \"context_answer_compat\"} and (mem_snippets or (fresh and (prior_answer or prior_citations or prior_paths))):\n            try:\n                tq = str((targs or {}).get(\"query\") or args.query)\n                sections = [tq]\n                # Memory snippets \u2192 bullets\n                if mem_snippets:\n                    bullets = []\n                    for s in mem_snippets[:3]:\n                        ss = re.sub(r\"\\s+\", \" \", str(s)).strip()\n                        if len(ss) > 200:\n                            ss = ss[:197] + \"...\"\n                        bullets.append(f\"- {ss}\")\n                    sections.append(\"Memory context:\\n\" + \"\\n\".join(bullets))\n                # Prior summary/citations if asked to expand or repeat and still fresh\n                if fresh and (_looks_like_expand(args.query) or _looks_like_repeat(args.query)):\n                    if isinstance(prior_answer, str) and prior_answer.strip():\n                        pa = re.sub(r\"\\s+\", \" \", prior_answer).strip()\n                        if len(pa) > 400:\n                            pa = pa[:397] + \"...\"\n                        sections.append(\"Prior summary:\\n\" + pa)\n                    paths_list = []\n                    if isinstance(prior_paths, list) and prior_paths:\n                        paths_list = [str(p) for p in prior_paths[:5]]\n                    elif isinstance(prior_citations, list) and prior_citations:\n                        uniq = []\n                        for c in prior_citations:\n                            if isinstance(c, dict) and c.get(\"path\") and c[\"path\"] not in uniq:\n                                uniq.append(c[\"path\"])\n                        paths_list = uniq[:5]\n                    if paths_list:\n                        sections.append(\"Citations context:\\n\" + \"\\n\".join(f\"- {p}\" for p in paths_list))\n                aug = \"\\n\\n\".join(sections)\n                targs = {**(targs or {}), \"query\": aug}\n            except Exception:\n                pass\n        try:\n            if tool in {\"context_answer\", \"context_answer_compat\"}:\n                res = _BATCH_CLIENT.call_or_enqueue(base_url, tool, targs, timeout=args.timeout)\n            else:\n                res = call_tool_http(base_url, tool, targs, timeout=args.timeout)\n            print(json.dumps({\"tool\": tool, \"result\": res}, indent=2))\n            last = res\n            # If this was a memory.find step, capture snippets for later augmentation\n            try:\n                if tool.lower().endswith(\"find\") or tool.lower() in {\"find\", \"memory.find\"}:\n                    r = res.get(\"result\") or {}\n                    items = []\n                    sc = r.get(\"structuredContent\")\n                    if isinstance(sc, dict):\n                        rs0 = sc.get(\"result\") or sc\n                        if isinstance(rs0, dict):\n                            items = rs0.get(\"results\") or rs0.get(\"hits\") or []\n                    if not items:\n                        content = r.get(\"content\")\n                        if isinstance(content, list):\n                            for c in content:\n                                if not isinstance(c, dict):\n                                    continue\n                                # Prefer native JSON payloads if present\n                                if \"json\" in c:\n                                    j = c.get(\"json\")\n                                    if isinstance(j, (dict, list)):\n                                        container = j.get(\"result\") if isinstance(j, dict) and \"result\" in j else j\n                                        if isinstance(container, dict):\n                                            items = container.get(\"results\") or container.get(\"hits\") or []\n                                            if items:\n                                                break\n                                # Fallback: text field containing JSON\n                                if c.get(\"type\") == \"text\":\n                                    ttxt = c.get(\"text\")\n                                    if isinstance(ttxt, str) and ttxt.strip():\n                                        try:\n                                            j = json.loads(ttxt)\n                                        except Exception:\n                                            continue\n                                        container = j.get(\"result\") if isinstance(j, dict) and \"result\" in j else j\n                                        if isinstance(container, dict):\n                                            items = container.get(\"results\") or container.get(\"hits\") or []\n                                            if items:\n                                                break\n                    for it in items:\n                        if isinstance(it, dict):\n                            txt = it.get(\"information\") or it.get(\"content\") or it.get(\"text\")\n                            if isinstance(txt, str) and txt.strip():\n                                mem_snippets.append(txt.strip())\n            except Exception:\n                pass\n            # Determine if we should treat this step as terminal\n            has_future_answer = any(tn in {\"context_answer\", \"context_answer_compat\"} for (tn, _) in plan[idx + 1 :])\n            if (not _is_failure_response(res)) and _is_result_good(tool, res):\n                if tool.lower() in {\"find\", \"memory.find\"} and has_future_answer:\n                    # Don't stop; proceed to answer step with augmented query\n                    continue\n                # Persist scratchpad: filters, memory, prior answer/citations, and success criteria\n                try:\n                    last_filters: Dict[str, Any] = {}\n                    for (tn, ta) in plan:\n                        if tn == \"repo_search\" or tn.startswith(\"search_\"):\n                            if isinstance(ta, dict):\n                                for k in (\"language\", \"under\", \"symbol\", \"ext\", \"path_glob\", \"not_glob\"):\n                                    if ta.get(k) not in (None, \"\"):\n                                        last_filters[k] = ta.get(k)\n                            break\n                    # Extract prior answer and citations if this was an answer tool\n                    last_answer_text = None\n                    last_citations_list = None\n                    last_paths_list: list[str] | None = None\n                    if tool in {\"context_answer\", \"context_answer_compat\"}:\n                        try:\n                            r0 = res.get(\"result\") or {}\n                            sc0 = r0.get(\"structuredContent\") or {}\n                            rs0 = sc0.get(\"result\") or sc0\n                            if isinstance(rs0, dict):\n                                ans0 = rs0.get(\"answer\")\n                                if isinstance(ans0, str):\n                                    last_answer_text = ans0\n                                cites0 = rs0.get(\"citations\")\n                                if isinstance(cites0, list):\n                                    last_citations_list = cites0\n                                    uniqp: list[str] = []\n                                    for c in cites0:\n                                        if isinstance(c, dict) and c.get(\"path\") and c[\"path\"] not in uniqp:\n                                            uniqp.append(c[\"path\"])\n                                    last_paths_list = uniqp\n                        except Exception:\n                            pass\n                    # Divergence detection against last-known-good metrics\n                    divergence_should_abort = False\n                    last_metrics_prev = {}\n                    try:\n                        last_metrics_prev = sp.get(\"last_metrics\") or {}\n                        if not isinstance(last_metrics_prev, dict):\n                            last_metrics_prev = {}\n                    except Exception:\n                        last_metrics_prev = {}\n                    metric = _extract_metric_from_resp(tool, res)\n                    last_metrics_map = dict(last_metrics_prev)\n                    if metric is not None:\n                        mname, mval = metric\n                        # Compare to previous metric for this tool\n                        prev_val = None\n                        try:\n                            prev_val = last_metrics_prev.get(tool, {}).get(mname)\n                            if prev_val is not None:\n                                prev_val = float(prev_val)\n                        except Exception:\n                            prev_val = None\n                        drop_frac, min_base = _divergence_thresholds()\n                        if _material_drop(prev_val, float(mval), drop_frac, min_base):\n                            fatal = _divergence_is_fatal_for(tool)\n                            try:\n                                print(json.dumps({\n                                    \"divergence\": {\n                                        \"tool\": tool,\n                                        \"metric\": mname,\n                                        \"previous\": prev_val,\n                                        \"current\": float(mval),\n                                        \"drop_frac\": drop_frac,\n                                        \"fatal\": fatal,\n                                    }\n                                }))\n                            except Exception:\n                                pass\n                            if fatal:\n                                divergence_should_abort = True\n                        # Update metrics for persistence\n                        try:\n                            last_metrics_map.setdefault(tool, {})[mname] = float(mval)\n                        except Exception:\n                            pass\n                    else:\n                        last_metrics_map = last_metrics_prev\n\n                    success_criteria = {\n                        \"context_answer\": {\"expected_fields\": [\"answer\"], \"min_citations\": 0},\n                        \"context_answer_compat\": {\"expected_fields\": [\"answer\"], \"min_citations\": 0},\n                        \"repo_search\": {\"min_results\": 1},\n                        \"search_config_for\": {\"min_results\": 1},\n                        \"search_tests_for\": {\"min_results\": 1},\n                        \"search_callers_for\": {\"min_results\": 1},\n                        \"search_importers_for\": {\"min_results\": 1},\n                        \"find\": {\"min_results\": 1},\n                    }\n                    sp = {\n                        \"last_query\": args.query,\n                        \"last_plan\": plan,\n                        \"last_filters\": last_filters or None,\n                        \"mem_snippets\": mem_snippets[:5],\n                        \"last_answer\": last_answer_text,\n                        \"last_citations\": last_citations_list,\n                        \"last_paths\": last_paths_list,\n                        \"success_criteria\": success_criteria,\n                        \"last_metrics\": last_metrics_map,\n                        \"timestamp\": time.time(),\n                    }\n                    _save_scratchpad(sp)\n                except Exception:\n                    pass\n\n                if divergence_should_abort:\n                    # Treat as failure for this tool and try next in plan\n                    continue\n\n                return 0\n            # else try next tool in plan\n        except Exception as e:\n            last_err = e\n            # Print a concise hint about which server failed\n            try:\n                print(json.dumps({\"tool\": tool, \"server\": base_url, \"error\": str(e)}), file=sys.stderr)\n            except Exception:\n                pass\n            continue\n    if last_err:\n        print(f\"Router: all attempts failed: {last_err}\", file=sys.stderr)\n    # If we have a last response but it was a failure, return non-zero\n    return 1 if last is not None else 2",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}