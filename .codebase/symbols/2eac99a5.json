{
  "file_path": "/work/context-engine/tests/test_cache_deduplication.py",
  "file_hash": "089c40250ad6845551b6f07b03325077383f3ea0",
  "updated_at": "2025-12-26T17:34:20.717169",
  "symbols": {
    "class_TestUnifiedCache_27": {
      "name": "TestUnifiedCache",
      "type": "class",
      "start_line": 27,
      "end_line": 172,
      "content_hash": "f7715ecfb8a318ec09a721562e4b17022b30a985",
      "content": "class TestUnifiedCache(unittest.TestCase):\n    \"\"\"Test unified caching functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test environment.\"\"\"\n        clear_all_caches()\n\n    def test_cache_creation(self):\n        \"\"\"Test cache creation with different policies.\"\"\"\n        # Test LRU cache\n        lru_cache = UnifiedCache(\"test_lru\", max_size=10, eviction_policy=EvictionPolicy.LRU)\n        self.assertEqual(lru_cache.name, \"test_lru\")\n        self.assertEqual(lru_cache.eviction_policy, EvictionPolicy.LRU)\n\n        # Test LFU cache\n        lfu_cache = UnifiedCache(\"test_lfu\", max_size=10, eviction_policy=EvictionPolicy.LFU)\n        self.assertEqual(lfu_cache.eviction_policy, EvictionPolicy.LFU)\n\n        # Test TTL cache\n        ttl_cache = UnifiedCache(\"test_ttl\", max_size=10, eviction_policy=EvictionPolicy.TTL, default_ttl=1.0)\n        self.assertEqual(ttl_cache.eviction_policy, EvictionPolicy.TTL)\n\n    def test_cache_basic_operations(self):\n        \"\"\"Test basic cache set/get operations.\"\"\"\n        cache = UnifiedCache(\"test_basic\", max_size=5)\n\n        # Test set and get\n        self.assertTrue(cache.set(\"key1\", \"value1\"))\n        self.assertEqual(cache.get(\"key1\"), \"value1\")\n\n        # Test non-existent key\n        self.assertIsNone(cache.get(\"nonexistent\"))\n\n        # Test cache contains\n        self.assertIn(\"key1\", cache)\n        self.assertNotIn(\"nonexistent\", cache)\n\n        # Test cache size\n        self.assertEqual(len(cache), 1)\n        self.assertEqual(cache.size(), 1)\n\n    def test_cache_eviction(self):\n        \"\"\"Test cache eviction policies.\"\"\"\n        cache = UnifiedCache(\"test_eviction\", max_size=3, eviction_policy=EvictionPolicy.LRU)\n\n        # Fill cache beyond capacity\n        cache.set(\"key1\", \"value1\")\n        cache.set(\"key2\", \"value2\")\n        cache.set(\"key3\", \"value3\")\n        self.assertEqual(len(cache), 3)\n\n        # Add one more to trigger eviction\n        cache.set(\"key4\", \"value4\")\n        self.assertEqual(len(cache), 3)  # Should still be at max size\n\n        # Check that oldest was evicted (key1)\n        self.assertIsNone(cache.get(\"key1\"))\n        self.assertEqual(cache.get(\"key4\"), \"value4\")\n\n    def test_cache_ttl_expiration(self):\n        \"\"\"Test TTL-based expiration.\"\"\"\n        cache = UnifiedCache(\"test_ttl\", max_size=10, eviction_policy=EvictionPolicy.TTL, default_ttl=0.1)\n\n        # Set value with short TTL\n        cache.set(\"key1\", \"value1\", ttl=0.1)\n        self.assertEqual(cache.get(\"key1\"), \"value1\")\n\n        # Wait for expiration\n        time.sleep(0.2)\n        self.assertIsNone(cache.get(\"key1\"))  # Should be expired\n\n    def test_cache_statistics(self):\n        \"\"\"Test cache statistics tracking.\"\"\"\n        cache = UnifiedCache(\"test_stats\", max_size=5)\n\n        # Perform operations\n        cache.set(\"key1\", \"value1\")\n        cache.get(\"key1\")  # Hit\n        cache.get(\"key2\")  # Miss\n\n        stats = cache.get_stats()\n        self.assertEqual(stats['hits'], 1)\n        self.assertEqual(stats['misses'], 1)\n        self.assertEqual(stats['current_size'], 1)\n        self.assertGreater(stats['hit_rate'], 0)\n\n    def test_cache_complex_keys(self):\n        \"\"\"Test caching with complex key types.\"\"\"\n        cache = UnifiedCache(\"test_complex\", max_size=10)\n\n        # Test different key types\n        self.assertTrue(cache.set(\"string_key\", \"value1\"))\n        self.assertTrue(cache.set((\"tuple\", \"key\"), \"value2\"))\n        self.assertTrue(cache.set([\"list\", \"key\"], \"value3\"))\n        self.assertTrue(cache.set({\"dict\": \"key\"}, \"value4\"))\n\n        # Verify retrieval\n        self.assertEqual(cache.get(\"string_key\"), \"value1\")\n        self.assertEqual(cache.get((\"tuple\", \"key\")), \"value2\")\n        self.assertEqual(cache.get([\"list\", \"key\"]), \"value3\")\n        self.assertEqual(cache.get({\"dict\": \"key\"}), \"value4\")\n\n    def test_cache_memory_management(self):\n        \"\"\"Test cache memory limits.\"\"\"\n        cache = UnifiedCache(\"test_memory\", max_size=5, max_memory_mb=0.001)  # 1KB limit\n\n        # Add values until memory limit is reached\n        large_value = \"x\" * 100  # ~100 bytes\n        cache.set(\"key1\", large_value)\n        cache.set(\"key2\", large_value)\n        cache.set(\"key3\", large_value)\n        cache.set(\"key4\", large_value)\n        cache.set(\"key5\", large_value)\n\n        # Should trigger eviction due to memory limit\n        cache.set(\"key6\", large_value)\n\n        stats = cache.get_stats()\n        self.assertLessEqual(stats['current_memory_bytes'], 1024)  # Should be under 1KB\n        self.assertLessEqual(len(cache), 5)  # Should be at max size\n\n    def test_cached_decorator(self):\n        \"\"\"Test the cached decorator.\"\"\"\n        call_count = 0\n\n        @cached(\"test_decorator\", ttl=1.0)\n        def expensive_function(x):\n            nonlocal call_count\n            call_count += 1\n            return x * 2\n\n        # First call should compute\n        result1 = expensive_function(5)\n        self.assertEqual(result1, 10)\n        self.assertEqual(call_count, 1)\n\n        # Second call should use cache\n        result2 = expensive_function(5)\n        self.assertEqual(result2, 10)\n        self.assertEqual(call_count, 1)  # Should not increase\n\n        # Wait for expiration and call again\n        time.sleep(1.1)\n        result3 = expensive_function(5)\n        self.assertEqual(result3, 10)\n        self.assertEqual(call_count, 2)  # Should recompute",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_setUp_30": {
      "name": "setUp",
      "type": "method",
      "start_line": 30,
      "end_line": 32,
      "content_hash": "1c5b63e7698dc4d0a215692da9c8da85efc34fd1",
      "content": "    def setUp(self):\n        \"\"\"Set up test environment.\"\"\"\n        clear_all_caches()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_cache_creation_34": {
      "name": "test_cache_creation",
      "type": "method",
      "start_line": 34,
      "end_line": 47,
      "content_hash": "5743f30577bf7ac839b12d97a63dca7bd08f28ad",
      "content": "    def test_cache_creation(self):\n        \"\"\"Test cache creation with different policies.\"\"\"\n        # Test LRU cache\n        lru_cache = UnifiedCache(\"test_lru\", max_size=10, eviction_policy=EvictionPolicy.LRU)\n        self.assertEqual(lru_cache.name, \"test_lru\")\n        self.assertEqual(lru_cache.eviction_policy, EvictionPolicy.LRU)\n\n        # Test LFU cache\n        lfu_cache = UnifiedCache(\"test_lfu\", max_size=10, eviction_policy=EvictionPolicy.LFU)\n        self.assertEqual(lfu_cache.eviction_policy, EvictionPolicy.LFU)\n\n        # Test TTL cache\n        ttl_cache = UnifiedCache(\"test_ttl\", max_size=10, eviction_policy=EvictionPolicy.TTL, default_ttl=1.0)\n        self.assertEqual(ttl_cache.eviction_policy, EvictionPolicy.TTL)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_cache_basic_operations_49": {
      "name": "test_cache_basic_operations",
      "type": "method",
      "start_line": 49,
      "end_line": 66,
      "content_hash": "b2ed64d8646697e5f3a2ea52d0c466439763fd85",
      "content": "    def test_cache_basic_operations(self):\n        \"\"\"Test basic cache set/get operations.\"\"\"\n        cache = UnifiedCache(\"test_basic\", max_size=5)\n\n        # Test set and get\n        self.assertTrue(cache.set(\"key1\", \"value1\"))\n        self.assertEqual(cache.get(\"key1\"), \"value1\")\n\n        # Test non-existent key\n        self.assertIsNone(cache.get(\"nonexistent\"))\n\n        # Test cache contains\n        self.assertIn(\"key1\", cache)\n        self.assertNotIn(\"nonexistent\", cache)\n\n        # Test cache size\n        self.assertEqual(len(cache), 1)\n        self.assertEqual(cache.size(), 1)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_cache_eviction_68": {
      "name": "test_cache_eviction",
      "type": "method",
      "start_line": 68,
      "end_line": 84,
      "content_hash": "66015852f10bac47cf298f8c8aa75d747b73b7b3",
      "content": "    def test_cache_eviction(self):\n        \"\"\"Test cache eviction policies.\"\"\"\n        cache = UnifiedCache(\"test_eviction\", max_size=3, eviction_policy=EvictionPolicy.LRU)\n\n        # Fill cache beyond capacity\n        cache.set(\"key1\", \"value1\")\n        cache.set(\"key2\", \"value2\")\n        cache.set(\"key3\", \"value3\")\n        self.assertEqual(len(cache), 3)\n\n        # Add one more to trigger eviction\n        cache.set(\"key4\", \"value4\")\n        self.assertEqual(len(cache), 3)  # Should still be at max size\n\n        # Check that oldest was evicted (key1)\n        self.assertIsNone(cache.get(\"key1\"))\n        self.assertEqual(cache.get(\"key4\"), \"value4\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_cache_ttl_expiration_86": {
      "name": "test_cache_ttl_expiration",
      "type": "method",
      "start_line": 86,
      "end_line": 96,
      "content_hash": "56e4276e7cd3ca41fc5fe3fb09a319e4c25bf0b2",
      "content": "    def test_cache_ttl_expiration(self):\n        \"\"\"Test TTL-based expiration.\"\"\"\n        cache = UnifiedCache(\"test_ttl\", max_size=10, eviction_policy=EvictionPolicy.TTL, default_ttl=0.1)\n\n        # Set value with short TTL\n        cache.set(\"key1\", \"value1\", ttl=0.1)\n        self.assertEqual(cache.get(\"key1\"), \"value1\")\n\n        # Wait for expiration\n        time.sleep(0.2)\n        self.assertIsNone(cache.get(\"key1\"))  # Should be expired",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_cache_statistics_98": {
      "name": "test_cache_statistics",
      "type": "method",
      "start_line": 98,
      "end_line": 111,
      "content_hash": "e4998eb9eb0c2e1d209a6f2b3463bb35b6facdd4",
      "content": "    def test_cache_statistics(self):\n        \"\"\"Test cache statistics tracking.\"\"\"\n        cache = UnifiedCache(\"test_stats\", max_size=5)\n\n        # Perform operations\n        cache.set(\"key1\", \"value1\")\n        cache.get(\"key1\")  # Hit\n        cache.get(\"key2\")  # Miss\n\n        stats = cache.get_stats()\n        self.assertEqual(stats['hits'], 1)\n        self.assertEqual(stats['misses'], 1)\n        self.assertEqual(stats['current_size'], 1)\n        self.assertGreater(stats['hit_rate'], 0)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_cache_complex_keys_113": {
      "name": "test_cache_complex_keys",
      "type": "method",
      "start_line": 113,
      "end_line": 127,
      "content_hash": "2e96f73261946a378a812e14a45a74c54c5cd2cb",
      "content": "    def test_cache_complex_keys(self):\n        \"\"\"Test caching with complex key types.\"\"\"\n        cache = UnifiedCache(\"test_complex\", max_size=10)\n\n        # Test different key types\n        self.assertTrue(cache.set(\"string_key\", \"value1\"))\n        self.assertTrue(cache.set((\"tuple\", \"key\"), \"value2\"))\n        self.assertTrue(cache.set([\"list\", \"key\"], \"value3\"))\n        self.assertTrue(cache.set({\"dict\": \"key\"}, \"value4\"))\n\n        # Verify retrieval\n        self.assertEqual(cache.get(\"string_key\"), \"value1\")\n        self.assertEqual(cache.get((\"tuple\", \"key\")), \"value2\")\n        self.assertEqual(cache.get([\"list\", \"key\"]), \"value3\")\n        self.assertEqual(cache.get({\"dict\": \"key\"}), \"value4\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_cache_memory_management_129": {
      "name": "test_cache_memory_management",
      "type": "method",
      "start_line": 129,
      "end_line": 146,
      "content_hash": "62c6787d8f7ca80d3e4f8b54d92e97ab637e8df1",
      "content": "    def test_cache_memory_management(self):\n        \"\"\"Test cache memory limits.\"\"\"\n        cache = UnifiedCache(\"test_memory\", max_size=5, max_memory_mb=0.001)  # 1KB limit\n\n        # Add values until memory limit is reached\n        large_value = \"x\" * 100  # ~100 bytes\n        cache.set(\"key1\", large_value)\n        cache.set(\"key2\", large_value)\n        cache.set(\"key3\", large_value)\n        cache.set(\"key4\", large_value)\n        cache.set(\"key5\", large_value)\n\n        # Should trigger eviction due to memory limit\n        cache.set(\"key6\", large_value)\n\n        stats = cache.get_stats()\n        self.assertLessEqual(stats['current_memory_bytes'], 1024)  # Should be under 1KB\n        self.assertLessEqual(len(cache), 5)  # Should be at max size",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_cached_decorator_148": {
      "name": "test_cached_decorator",
      "type": "method",
      "start_line": 148,
      "end_line": 172,
      "content_hash": "2bf5a3152efd6612a956caf6a27dd2138841831d",
      "content": "    def test_cached_decorator(self):\n        \"\"\"Test the cached decorator.\"\"\"\n        call_count = 0\n\n        @cached(\"test_decorator\", ttl=1.0)\n        def expensive_function(x):\n            nonlocal call_count\n            call_count += 1\n            return x * 2\n\n        # First call should compute\n        result1 = expensive_function(5)\n        self.assertEqual(result1, 10)\n        self.assertEqual(call_count, 1)\n\n        # Second call should use cache\n        result2 = expensive_function(5)\n        self.assertEqual(result2, 10)\n        self.assertEqual(call_count, 1)  # Should not increase\n\n        # Wait for expiration and call again\n        time.sleep(1.1)\n        result3 = expensive_function(5)\n        self.assertEqual(result3, 10)\n        self.assertEqual(call_count, 2)  # Should recompute",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_expensive_function_153": {
      "name": "expensive_function",
      "type": "method",
      "start_line": 153,
      "end_line": 156,
      "content_hash": "3c83942d6b0756458674b7ad6ef8917259f1aafd",
      "content": "        def expensive_function(x):\n            nonlocal call_count\n            call_count += 1\n            return x * 2",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_TestRequestDeduplication_175": {
      "name": "TestRequestDeduplication",
      "type": "class",
      "start_line": 175,
      "end_line": 342,
      "content_hash": "2740720454baed52ad7e571bb2ccfdf4e7ea3ceb",
      "content": "class TestRequestDeduplication(unittest.TestCase):\n    \"\"\"Test request deduplication functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test environment.\"\"\"\n        clear_deduplication_cache()\n\n    def test_fingerprint_generation(self):\n        \"\"\"Test request fingerprint generation.\"\"\"\n        request1 = {\n            'queries': ['test', 'query'],\n            'limit': 10,\n            'language': 'python'\n        }\n\n        request2 = {\n            'queries': ['query', 'test'],  # Different order\n            'limit': 10,\n            'language': 'python'\n        }\n\n        request3 = {\n            'queries': ['different', 'query'],\n            'limit': 10,\n            'language': 'python'\n        }\n\n        fp1 = RequestFingerprint(request1)\n        fp2 = RequestFingerprint(request2)\n        fp3 = RequestFingerprint(request3)\n\n        # Same requests should have same fingerprint\n        self.assertEqual(fp1.fingerprint, fp2.fingerprint)\n\n        # Different requests should have different fingerprints\n        self.assertNotEqual(fp1.fingerprint, fp3.fingerprint)\n\n        # Fingerprints should be consistent\n        self.assertEqual(len(fp1.fingerprint), 64)  # SHA256 hash length\n\n    def test_exact_match_deduplication(self):\n        \"\"\"Test exact match deduplication.\"\"\"\n        deduplicator = RequestDeduplicator(\n            \"test_exact\",\n            max_cache_size=10,\n            exact_match=True,\n            similarity_threshold=1.0\n        )\n\n        request = {\n            'queries': ['test', 'query'],\n            'limit': 10\n        }\n\n        # First request should be unique\n        is_dup1, fp1 = deduplicator.is_duplicate(request)\n        self.assertFalse(is_dup1)\n        self.assertIsNone(fp1)\n\n        # Second identical request should be duplicate\n        is_dup2, fp2 = deduplicator.is_duplicate(request)\n        self.assertTrue(is_dup2)\n        self.assertIsNotNone(fp2)\n\n    def test_similarity_match_deduplication(self):\n        \"\"\"Test similarity-based deduplication.\"\"\"\n        deduplicator = RequestDeduplicator(\n            \"test_similarity\",\n            max_cache_size=10,\n            exact_match=False,\n            similarity_threshold=0.8\n        )\n\n        request1 = {\n            'queries': ['test', 'query'],\n            'limit': 10\n        }\n\n        request2 = {\n            'queries': ['test', 'queries'],  # Very similar\n            'limit': 10\n        }\n\n        request3 = {\n            'queries': ['completely', 'different'],\n            'limit': 10\n        }\n\n        # First request should be unique\n        is_dup1, fp1 = deduplicator.is_duplicate(request1)\n        self.assertFalse(is_dup1)\n        self.assertIsNone(fp1)\n\n        # Second similar request should be duplicate\n        is_dup2, fp2 = deduplicator.is_duplicate(request2)\n        self.assertTrue(is_dup2)\n        self.assertIsNotNone(fp2)\n\n        # Third different request should be unique\n        is_dup3, fp3 = deduplicator.is_duplicate(request3)\n        self.assertFalse(is_dup3)\n        self.assertIsNone(fp3)\n\n    def test_deduplication_statistics(self):\n        \"\"\"Test deduplication statistics tracking.\"\"\"\n        deduplicator = RequestDeduplicator(\"test_stats\", max_cache_size=5)\n\n        request1 = {'queries': ['test'], 'limit': 10}\n        request2 = {'queries': ['test'], 'limit': 10}\n        request3 = {'queries': ['different'], 'limit': 10}\n\n        # Process requests\n        deduplicator.is_duplicate(request1)  # Unique\n        deduplicator.is_duplicate(request2)  # Duplicate\n        deduplicator.is_duplicate(request3)  # Unique\n\n        stats = deduplicator.get_stats()\n        self.assertEqual(stats['total_requests'], 3)\n        self.assertEqual(stats['unique_requests'], 2)\n        self.assertEqual(stats['deduped_requests'], 1)\n        self.assertEqual(stats['dedup_rate'], 33.33)  # 1/3 * 100\n\n    def test_deduplication_ttl(self):\n        \"\"\"Test TTL-based expiration in deduplication.\"\"\"\n        deduplicator = RequestDeduplicator(\n            \"test_ttl\",\n            max_cache_size=5,\n            dedup_window_seconds=0.1\n        )\n\n        request = {'queries': ['test'], 'limit': 10}\n\n        # First request should be unique\n        is_dup1, fp1 = deduplicator.is_duplicate(request)\n        self.assertFalse(is_dup1)\n\n        # Wait for expiration\n        time.sleep(0.2)\n\n        # Same request should be unique again after expiration\n        is_dup2, fp2 = deduplicator.is_duplicate(request)\n        self.assertFalse(is_dup2)\n\n    def test_deduplicate_request_decorator(self):\n        \"\"\"Test the deduplicate_request decorator.\"\"\"\n        call_count = 0\n\n        @deduplicate_request(ttl=1.0)\n        def expensive_search(query):\n            nonlocal call_count\n            call_count += 1\n            return f\"search_result_for_{query}\"\n\n        # First call should execute\n        result1 = expensive_search(\"test\")\n        self.assertEqual(result1, \"search_result_for_test\")\n        self.assertEqual(call_count, 1)\n\n        # Second identical call should be deduplicated\n        result2 = expensive_search(\"test\")\n        self.assertIsNone(result2)  # Decorator returns None for duplicates\n        self.assertEqual(call_count, 1)  # Should not increase\n\n        # Wait for expiration and call again\n        time.sleep(1.1)\n        result3 = expensive_search(\"test\")\n        self.assertEqual(result3, \"search_result_for_test\")\n        self.assertEqual(call_count, 2)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_setUp_178": {
      "name": "setUp",
      "type": "method",
      "start_line": 178,
      "end_line": 180,
      "content_hash": "4adbcb5faf54696f1355437cb538fec148183aff",
      "content": "    def setUp(self):\n        \"\"\"Set up test environment.\"\"\"\n        clear_deduplication_cache()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_fingerprint_generation_182": {
      "name": "test_fingerprint_generation",
      "type": "method",
      "start_line": 182,
      "end_line": 213,
      "content_hash": "e8d5f30bda292422c589cd5c980049a15053ce5f",
      "content": "    def test_fingerprint_generation(self):\n        \"\"\"Test request fingerprint generation.\"\"\"\n        request1 = {\n            'queries': ['test', 'query'],\n            'limit': 10,\n            'language': 'python'\n        }\n\n        request2 = {\n            'queries': ['query', 'test'],  # Different order\n            'limit': 10,\n            'language': 'python'\n        }\n\n        request3 = {\n            'queries': ['different', 'query'],\n            'limit': 10,\n            'language': 'python'\n        }\n\n        fp1 = RequestFingerprint(request1)\n        fp2 = RequestFingerprint(request2)\n        fp3 = RequestFingerprint(request3)\n\n        # Same requests should have same fingerprint\n        self.assertEqual(fp1.fingerprint, fp2.fingerprint)\n\n        # Different requests should have different fingerprints\n        self.assertNotEqual(fp1.fingerprint, fp3.fingerprint)\n\n        # Fingerprints should be consistent\n        self.assertEqual(len(fp1.fingerprint), 64)  # SHA256 hash length",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_exact_match_deduplication_215": {
      "name": "test_exact_match_deduplication",
      "type": "method",
      "start_line": 215,
      "end_line": 237,
      "content_hash": "d4ac7e1b065525897cdd3c35463a58dadc071ad5",
      "content": "    def test_exact_match_deduplication(self):\n        \"\"\"Test exact match deduplication.\"\"\"\n        deduplicator = RequestDeduplicator(\n            \"test_exact\",\n            max_cache_size=10,\n            exact_match=True,\n            similarity_threshold=1.0\n        )\n\n        request = {\n            'queries': ['test', 'query'],\n            'limit': 10\n        }\n\n        # First request should be unique\n        is_dup1, fp1 = deduplicator.is_duplicate(request)\n        self.assertFalse(is_dup1)\n        self.assertIsNone(fp1)\n\n        # Second identical request should be duplicate\n        is_dup2, fp2 = deduplicator.is_duplicate(request)\n        self.assertTrue(is_dup2)\n        self.assertIsNotNone(fp2)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_similarity_match_deduplication_239": {
      "name": "test_similarity_match_deduplication",
      "type": "method",
      "start_line": 239,
      "end_line": 276,
      "content_hash": "cec733844ae216b923c247d02363c3048c20e38e",
      "content": "    def test_similarity_match_deduplication(self):\n        \"\"\"Test similarity-based deduplication.\"\"\"\n        deduplicator = RequestDeduplicator(\n            \"test_similarity\",\n            max_cache_size=10,\n            exact_match=False,\n            similarity_threshold=0.8\n        )\n\n        request1 = {\n            'queries': ['test', 'query'],\n            'limit': 10\n        }\n\n        request2 = {\n            'queries': ['test', 'queries'],  # Very similar\n            'limit': 10\n        }\n\n        request3 = {\n            'queries': ['completely', 'different'],\n            'limit': 10\n        }\n\n        # First request should be unique\n        is_dup1, fp1 = deduplicator.is_duplicate(request1)\n        self.assertFalse(is_dup1)\n        self.assertIsNone(fp1)\n\n        # Second similar request should be duplicate\n        is_dup2, fp2 = deduplicator.is_duplicate(request2)\n        self.assertTrue(is_dup2)\n        self.assertIsNotNone(fp2)\n\n        # Third different request should be unique\n        is_dup3, fp3 = deduplicator.is_duplicate(request3)\n        self.assertFalse(is_dup3)\n        self.assertIsNone(fp3)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_deduplication_statistics_278": {
      "name": "test_deduplication_statistics",
      "type": "method",
      "start_line": 278,
      "end_line": 295,
      "content_hash": "e80160ae397d40adb69d0fdc7b08a4ed399f2392",
      "content": "    def test_deduplication_statistics(self):\n        \"\"\"Test deduplication statistics tracking.\"\"\"\n        deduplicator = RequestDeduplicator(\"test_stats\", max_cache_size=5)\n\n        request1 = {'queries': ['test'], 'limit': 10}\n        request2 = {'queries': ['test'], 'limit': 10}\n        request3 = {'queries': ['different'], 'limit': 10}\n\n        # Process requests\n        deduplicator.is_duplicate(request1)  # Unique\n        deduplicator.is_duplicate(request2)  # Duplicate\n        deduplicator.is_duplicate(request3)  # Unique\n\n        stats = deduplicator.get_stats()\n        self.assertEqual(stats['total_requests'], 3)\n        self.assertEqual(stats['unique_requests'], 2)\n        self.assertEqual(stats['deduped_requests'], 1)\n        self.assertEqual(stats['dedup_rate'], 33.33)  # 1/3 * 100",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_deduplication_ttl_297": {
      "name": "test_deduplication_ttl",
      "type": "method",
      "start_line": 297,
      "end_line": 316,
      "content_hash": "d565180332a15fc23a60bd6eb198dd882bd0c960",
      "content": "    def test_deduplication_ttl(self):\n        \"\"\"Test TTL-based expiration in deduplication.\"\"\"\n        deduplicator = RequestDeduplicator(\n            \"test_ttl\",\n            max_cache_size=5,\n            dedup_window_seconds=0.1\n        )\n\n        request = {'queries': ['test'], 'limit': 10}\n\n        # First request should be unique\n        is_dup1, fp1 = deduplicator.is_duplicate(request)\n        self.assertFalse(is_dup1)\n\n        # Wait for expiration\n        time.sleep(0.2)\n\n        # Same request should be unique again after expiration\n        is_dup2, fp2 = deduplicator.is_duplicate(request)\n        self.assertFalse(is_dup2)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_deduplicate_request_decorator_318": {
      "name": "test_deduplicate_request_decorator",
      "type": "method",
      "start_line": 318,
      "end_line": 342,
      "content_hash": "3f854a91c23a5310281c370ff393c382113c021e",
      "content": "    def test_deduplicate_request_decorator(self):\n        \"\"\"Test the deduplicate_request decorator.\"\"\"\n        call_count = 0\n\n        @deduplicate_request(ttl=1.0)\n        def expensive_search(query):\n            nonlocal call_count\n            call_count += 1\n            return f\"search_result_for_{query}\"\n\n        # First call should execute\n        result1 = expensive_search(\"test\")\n        self.assertEqual(result1, \"search_result_for_test\")\n        self.assertEqual(call_count, 1)\n\n        # Second identical call should be deduplicated\n        result2 = expensive_search(\"test\")\n        self.assertIsNone(result2)  # Decorator returns None for duplicates\n        self.assertEqual(call_count, 1)  # Should not increase\n\n        # Wait for expiration and call again\n        time.sleep(1.1)\n        result3 = expensive_search(\"test\")\n        self.assertEqual(result3, \"search_result_for_test\")\n        self.assertEqual(call_count, 2)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_expensive_search_323": {
      "name": "expensive_search",
      "type": "method",
      "start_line": 323,
      "end_line": 326,
      "content_hash": "2e1098d2f91e05959673e9c7f6bd9a367ca33a5f",
      "content": "        def expensive_search(query):\n            nonlocal call_count\n            call_count += 1\n            return f\"search_result_for_{query}\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_TestCacheIntegration_345": {
      "name": "TestCacheIntegration",
      "type": "class",
      "start_line": 345,
      "end_line": 410,
      "content_hash": "db58b35dba3981b3d705df2005e3e4598ffe3319",
      "content": "class TestCacheIntegration(unittest.TestCase):\n    \"\"\"Test integration between caching systems.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test environment.\"\"\"\n        clear_all_caches()\n        clear_deduplication_cache()\n\n    def test_predefined_caches(self):\n        \"\"\"Test predefined cache configurations.\"\"\"\n        # Test embedding cache\n        embed_cache = get_embedding_cache()\n        self.assertEqual(embed_cache.name, \"embeddings\")\n\n        # Test search cache\n        search_cache = get_search_cache()\n        self.assertEqual(search_cache.name, \"search_results\")\n\n        # Test expansion cache\n        expansion_cache = get_expansion_cache()\n        self.assertEqual(expansion_cache.name, \"expansions\")\n\n    def test_global_cache_operations(self):\n        \"\"\"Test global cache management functions.\"\"\"\n        # Test getting all stats\n        all_stats = get_all_cache_stats()\n        self.assertIsInstance(all_stats, dict)\n        self.assertIn(\"embeddings\", all_stats)\n        self.assertIn(\"search_results\", all_stats)\n        self.assertIn(\"expansions\", all_stats)\n\n        # Test clearing all caches\n        embed_cache = get_embedding_cache()\n        embed_cache.set(\"test_key\", \"test_value\")\n\n        self.assertEqual(len(embed_cache), 1)\n\n        clear_all_caches()\n\n        # Caches should be empty after clear\n        self.assertEqual(len(embed_cache), 0)\n\n    def test_deduplicator_global(self):\n        \"\"\"Test global deduplicator functions.\"\"\"\n        # Test getting deduplicator\n        deduplicator = get_deduplicator()\n        self.assertIsInstance(deduplicator, RequestDeduplicator)\n\n        # Test global deduplication functions\n        request = {'queries': ['test'], 'limit': 10}\n\n        is_dup1, fp1 = is_duplicate_request(request)\n        self.assertFalse(is_dup1)\n\n        is_dup2, fp2 = is_duplicate_request(request)\n        self.assertTrue(is_dup2)  # Should be duplicate now\n\n        # Test getting stats\n        stats = get_deduplication_stats()\n        self.assertGreater(stats['total_requests'], 0)\n\n        # Test clearing cache\n        clear_deduplication_cache()\n\n        stats_after_clear = get_deduplication_stats()\n        self.assertEqual(stats_after_clear['total_requests'], 0)  # Should reset",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_setUp_348": {
      "name": "setUp",
      "type": "method",
      "start_line": 348,
      "end_line": 351,
      "content_hash": "861f8dc8191ef674822f90c8da4794aecc85813e",
      "content": "    def setUp(self):\n        \"\"\"Set up test environment.\"\"\"\n        clear_all_caches()\n        clear_deduplication_cache()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_predefined_caches_353": {
      "name": "test_predefined_caches",
      "type": "method",
      "start_line": 353,
      "end_line": 365,
      "content_hash": "7fe6139c10f403b16476c6357727520133b561f9",
      "content": "    def test_predefined_caches(self):\n        \"\"\"Test predefined cache configurations.\"\"\"\n        # Test embedding cache\n        embed_cache = get_embedding_cache()\n        self.assertEqual(embed_cache.name, \"embeddings\")\n\n        # Test search cache\n        search_cache = get_search_cache()\n        self.assertEqual(search_cache.name, \"search_results\")\n\n        # Test expansion cache\n        expansion_cache = get_expansion_cache()\n        self.assertEqual(expansion_cache.name, \"expansions\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_global_cache_operations_367": {
      "name": "test_global_cache_operations",
      "type": "method",
      "start_line": 367,
      "end_line": 385,
      "content_hash": "7450f5b47f412b603c6567798efaa5b70b4e204b",
      "content": "    def test_global_cache_operations(self):\n        \"\"\"Test global cache management functions.\"\"\"\n        # Test getting all stats\n        all_stats = get_all_cache_stats()\n        self.assertIsInstance(all_stats, dict)\n        self.assertIn(\"embeddings\", all_stats)\n        self.assertIn(\"search_results\", all_stats)\n        self.assertIn(\"expansions\", all_stats)\n\n        # Test clearing all caches\n        embed_cache = get_embedding_cache()\n        embed_cache.set(\"test_key\", \"test_value\")\n\n        self.assertEqual(len(embed_cache), 1)\n\n        clear_all_caches()\n\n        # Caches should be empty after clear\n        self.assertEqual(len(embed_cache), 0)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_deduplicator_global_387": {
      "name": "test_deduplicator_global",
      "type": "method",
      "start_line": 387,
      "end_line": 410,
      "content_hash": "f5ddcc611e29fac0e8b3a182367b0e08fd91dde3",
      "content": "    def test_deduplicator_global(self):\n        \"\"\"Test global deduplicator functions.\"\"\"\n        # Test getting deduplicator\n        deduplicator = get_deduplicator()\n        self.assertIsInstance(deduplicator, RequestDeduplicator)\n\n        # Test global deduplication functions\n        request = {'queries': ['test'], 'limit': 10}\n\n        is_dup1, fp1 = is_duplicate_request(request)\n        self.assertFalse(is_dup1)\n\n        is_dup2, fp2 = is_duplicate_request(request)\n        self.assertTrue(is_dup2)  # Should be duplicate now\n\n        # Test getting stats\n        stats = get_deduplication_stats()\n        self.assertGreater(stats['total_requests'], 0)\n\n        # Test clearing cache\n        clear_deduplication_cache()\n\n        stats_after_clear = get_deduplication_stats()\n        self.assertEqual(stats_after_clear['total_requests'], 0)  # Should reset",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_TestPerformanceCharacteristics_413": {
      "name": "TestPerformanceCharacteristics",
      "type": "class",
      "start_line": 413,
      "end_line": 469,
      "content_hash": "7900f8217021caf2ce2413631d9e76df0d9f0146",
      "content": "class TestPerformanceCharacteristics(unittest.TestCase):\n    \"\"\"Test performance characteristics of caching and deduplication.\"\"\"\n\n    def test_cache_performance(self):\n        \"\"\"Test cache performance with large datasets.\"\"\"\n        cache = UnifiedCache(\"perf_test\", max_size=1000)\n\n        # Measure time for bulk operations\n        start_time = time.time()\n\n        # Bulk insert\n        for i in range(500):\n            cache.set(f\"key_{i}\", f\"value_{i}\")\n\n        insert_time = time.time() - start_time\n\n        # Measure retrieval time\n        start_time = time.time()\n        for i in range(500):\n            cache.get(f\"key_{i}\")\n\n        retrieval_time = time.time() - start_time\n\n        # Performance should be reasonable\n        self.assertLess(insert_time, 1.0)  # Should complete within 1 second\n        self.assertLess(retrieval_time, 0.5)  # Should complete within 0.5 seconds\n\n        stats = cache.get_stats()\n        self.assertEqual(stats['current_size'], 500)\n\n    def test_deduplication_performance(self):\n        \"\"\"Test deduplication performance with many requests.\"\"\"\n        deduplicator = RequestDeduplicator(\"perf_test\", max_cache_size=1000)\n\n        # Measure time for bulk duplicate checking\n        start_time = time.time()\n\n        # Create a mix of unique and duplicate requests (50 unique repeated twice)\n        requests = [\n            {'queries': [f'query_{i % 50}'], 'limit': 10}\n            for i in range(100)\n        ]\n\n        duplicate_count = 0\n        for request in requests:\n            is_dup, _ = deduplicator.is_duplicate(request)\n            if is_dup:\n                duplicate_count += 1\n\n        processing_time = time.time() - start_time\n\n        # Performance should be reasonable\n        self.assertLess(processing_time, 1.0)  # Should complete within 1 second\n        self.assertGreater(duplicate_count, 0)  # Should detect duplicates\n\n        stats = deduplicator.get_stats()\n        self.assertEqual(stats['total_requests'], 100)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_cache_performance_416": {
      "name": "test_cache_performance",
      "type": "method",
      "start_line": 416,
      "end_line": 441,
      "content_hash": "a46bcf15d8c852c764d4680df8c086c0bf5ddd48",
      "content": "    def test_cache_performance(self):\n        \"\"\"Test cache performance with large datasets.\"\"\"\n        cache = UnifiedCache(\"perf_test\", max_size=1000)\n\n        # Measure time for bulk operations\n        start_time = time.time()\n\n        # Bulk insert\n        for i in range(500):\n            cache.set(f\"key_{i}\", f\"value_{i}\")\n\n        insert_time = time.time() - start_time\n\n        # Measure retrieval time\n        start_time = time.time()\n        for i in range(500):\n            cache.get(f\"key_{i}\")\n\n        retrieval_time = time.time() - start_time\n\n        # Performance should be reasonable\n        self.assertLess(insert_time, 1.0)  # Should complete within 1 second\n        self.assertLess(retrieval_time, 0.5)  # Should complete within 0.5 seconds\n\n        stats = cache.get_stats()\n        self.assertEqual(stats['current_size'], 500)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_test_deduplication_performance_443": {
      "name": "test_deduplication_performance",
      "type": "method",
      "start_line": 443,
      "end_line": 469,
      "content_hash": "56bd3b12ef2c63e5b8733986095bdf9d699980a4",
      "content": "    def test_deduplication_performance(self):\n        \"\"\"Test deduplication performance with many requests.\"\"\"\n        deduplicator = RequestDeduplicator(\"perf_test\", max_cache_size=1000)\n\n        # Measure time for bulk duplicate checking\n        start_time = time.time()\n\n        # Create a mix of unique and duplicate requests (50 unique repeated twice)\n        requests = [\n            {'queries': [f'query_{i % 50}'], 'limit': 10}\n            for i in range(100)\n        ]\n\n        duplicate_count = 0\n        for request in requests:\n            is_dup, _ = deduplicator.is_duplicate(request)\n            if is_dup:\n                duplicate_count += 1\n\n        processing_time = time.time() - start_time\n\n        # Performance should be reasonable\n        self.assertLess(processing_time, 1.0)  # Should complete within 1 second\n        self.assertGreater(duplicate_count, 0)  # Should detect duplicates\n\n        stats = deduplicator.get_stats()\n        self.assertEqual(stats['total_requests'], 100)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_test_cache_update_respects_memory_cap_481": {
      "name": "test_cache_update_respects_memory_cap",
      "type": "function",
      "start_line": 481,
      "end_line": 499,
      "content_hash": "5e018db5a28189dc638ab2b3aa19c1dbba4e20c4",
      "content": "def test_cache_update_respects_memory_cap():\n    \"\"\"Updating an existing key with a much larger value should not exceed memory cap.\n    If the updated entry alone violates the cap, the set should fail and the key be removed.\n    \"\"\"\n    cache = UnifiedCache(\"test_update_mem\", max_size=5, max_memory_mb=0.001)  # ~1KB\n\n    small = \"x\" * 100  # ~100 bytes\n    assert cache.set(\"k\", small)\n\n    large = \"y\" * 2000  # ~2KB, exceeds cap on its own\n    ok = cache.set(\"k\", large)\n\n    # Should fail to retain the oversized update and not exceed memory cap\n    assert ok is False\n    # Prior small value should be retained on failed oversized update\n    assert cache.get(\"k\") == small\n\n    stats = cache.get_stats()\n    assert stats[\"current_memory_bytes\"] <= 1024",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}