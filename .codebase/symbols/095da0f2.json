{
  "file_path": "/work/external-deps/Context-Engine/scripts/mcp_indexer_server.py",
  "file_hash": "f1e4e036c062720bbb3f26d892240aced0c65607",
  "updated_at": "2025-12-26T17:34:21.862166",
  "symbols": {
    "function__json_dumps_36": {
      "name": "_json_dumps",
      "type": "function",
      "start_line": 36,
      "end_line": 37,
      "content_hash": "36caebe4ddbf8ee5bf54070bcea2d394d918c131",
      "content": "    def _json_dumps(obj) -> str:\n        return orjson.dumps(obj).decode(\"utf-8\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__json_dumps_bytes_38": {
      "name": "_json_dumps_bytes",
      "type": "function",
      "start_line": 38,
      "end_line": 39,
      "content_hash": "9e7481da0e3d3129bf67181631a4388d5948802f",
      "content": "    def _json_dumps_bytes(obj) -> bytes:\n        return orjson.dumps(obj)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__json_dumps_42": {
      "name": "_json_dumps",
      "type": "function",
      "start_line": 42,
      "end_line": 43,
      "content_hash": "6485aaaa9a480fcdcc16b5861fdca2ffff0bd7d5",
      "content": "    def _json_dumps(obj) -> str:\n        return json.dumps(obj)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__json_dumps_bytes_44": {
      "name": "_json_dumps_bytes",
      "type": "function",
      "start_line": 44,
      "end_line": 45,
      "content_hash": "0bc69f9958e8c83cef174115ec52b0e8c12870e4",
      "content": "    def _json_dumps_bytes(obj) -> bytes:\n        return json.dumps(obj).encode(\"utf-8\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__highlight_snippet_181": {
      "name": "_highlight_snippet",
      "type": "function",
      "start_line": 181,
      "end_line": 184,
      "content_hash": "07c11aa5de2450963c5f14223f254f4d42f65350",
      "content": "    def _highlight_snippet(snippet, tokens):  # type: ignore\n        return (\n            _do_highlight_snippet(snippet, tokens) if _do_highlight_snippet else snippet\n        )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__highlight_snippet_187": {
      "name": "_highlight_snippet",
      "type": "function",
      "start_line": 187,
      "end_line": 188,
      "content_hash": "e702beef4f00977bae93d8b77a9a9e2de6fc7ad7",
      "content": "    def _highlight_snippet(snippet, tokens):  # type: ignore\n        return snippet",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__tool_capture_wrapper_301": {
      "name": "_tool_capture_wrapper",
      "type": "function",
      "start_line": 301,
      "end_line": 316,
      "content_hash": "b9d9eb2c2f3ba1d5c23d61eb84740a3d04e5742c",
      "content": "    def _tool_capture_wrapper(*dargs, **dkwargs):\n        orig_deco = _orig_tool(*dargs, **dkwargs)\n\n        def _inner(fn):\n            try:\n                _TOOLS_REGISTRY.append(\n                    {\n                        \"name\": dkwargs.get(\"name\") or getattr(fn, \"__name__\", \"\"),\n                        \"description\": (getattr(fn, \"__doc__\", None) or \"\").strip(),\n                    }\n                )\n            except (AttributeError, TypeError) as e:\n                logger.warning(f\"Failed to capture tool metadata for {fn}\", exc_info=e)\n            return orig_deco(fn)\n\n        return _inner",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__inner_304": {
      "name": "_inner",
      "type": "function",
      "start_line": 304,
      "end_line": 314,
      "content_hash": "cd6458fb725490b7234d4e4503bb889ad3d89d5b",
      "content": "        def _inner(fn):\n            try:\n                _TOOLS_REGISTRY.append(\n                    {\n                        \"name\": dkwargs.get(\"name\") or getattr(fn, \"__name__\", \"\"),\n                        \"description\": (getattr(fn, \"__doc__\", None) or \"\").strip(),\n                    }\n                )\n            except (AttributeError, TypeError) as e:\n                logger.warning(f\"Failed to capture tool metadata for {fn}\", exc_info=e)\n            return orig_deco(fn)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__relax_var_kwarg_defaults_323": {
      "name": "_relax_var_kwarg_defaults",
      "type": "function",
      "start_line": 323,
      "end_line": 367,
      "content_hash": "62d7cd67c2cc8213c8a0e9cb6464fe080ceb9aae",
      "content": "def _relax_var_kwarg_defaults() -> None:\n    \"\"\"Allow tools that rely on **kwargs compatibility shims to be invoked without\n    callers supplying an explicit 'kwargs' or 'arguments' field.\"\"\"\n    try:\n        from pydantic_core import PydanticUndefined as _PydanticUndefined  # type: ignore\n    except Exception:  # pragma: no cover - defensive\n\n        class _Sentinel:  # type: ignore\n            pass\n\n        _PydanticUndefined = _Sentinel()  # type: ignore\n\n    try:\n        tool_manager = getattr(mcp, \"_tool_manager\", None)\n        tools = getattr(tool_manager, \"_tools\", {}) if tool_manager is not None else {}\n    except Exception:\n        tools = {}\n\n    for tool in tools.values():\n        try:\n            model = getattr(tool.fn_metadata, \"arg_model\", None)\n            if model is None:\n                continue\n            fields = getattr(model, \"model_fields\", {})\n            changed = False\n            for key in (\"kwargs\", \"arguments\"):\n                fld = fields.get(key)\n                if fld is None:\n                    continue\n                default = getattr(fld, \"default\", None)\n                default_factory = getattr(fld, \"default_factory\", None)\n                if default is _PydanticUndefined and default_factory is None:\n                    try:\n                        fld.default_factory = dict  # type: ignore[attr-defined]\n                    except Exception:\n                        fld.default_factory = lambda: {}  # type: ignore\n                    fld.default = None\n                    changed = True\n            if changed:\n                try:\n                    model.model_rebuild(force=True)\n                except Exception:\n                    pass\n        except Exception:\n            continue",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class__Sentinel_330": {
      "name": "_Sentinel",
      "type": "class",
      "start_line": 330,
      "end_line": 331,
      "content_hash": "22320923e6992c08176ec87e401e3cb7b4136f2b",
      "content": "        class _Sentinel:  # type: ignore\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__start_readyz_server_380": {
      "name": "_start_readyz_server",
      "type": "function",
      "start_line": 380,
      "end_line": 433,
      "content_hash": "4867b3f512a5d566d682cbd88153fc042cdded6e",
      "content": "def _start_readyz_server():\n    try:\n        from http.server import BaseHTTPRequestHandler, HTTPServer\n\n        class H(BaseHTTPRequestHandler):\n            def do_GET(self):\n                try:\n                    if self.path == \"/readyz\":\n                        self.send_response(200)\n                        self.send_header(\"Content-Type\", \"application/json\")\n                        self.end_headers()\n                        payload = {\"ok\": True, \"app\": APP_NAME}\n                        self.wfile.write(_json_dumps_bytes(payload))\n                    elif self.path == \"/tools\":\n                        self.send_response(200)\n                        self.send_header(\"Content-Type\", \"application/json\")\n                        self.end_headers()\n                        # Hide expand_query when decoder is disabled\n                        tools = _TOOLS_REGISTRY\n                        try:\n                            from scripts.refrag_llamacpp import is_decoder_enabled  # type: ignore\n                        except Exception:\n                            is_decoder_enabled = lambda: False  # type: ignore\n                        try:\n                            if not is_decoder_enabled():\n                                tools = [\n                                    t\n                                    for t in tools\n                                    if (t.get(\"name\") or \"\") != \"expand_query\"\n                                ]\n                        except Exception:\n                            pass\n                        payload = {\"ok\": True, \"tools\": tools}\n                        self.wfile.write(_json_dumps_bytes(payload))\n                    else:\n                        self.send_response(404)\n                        self.end_headers()\n                except Exception:\n                    try:\n                        self.send_response(500)\n                        self.end_headers()\n                    except Exception:\n                        pass\n\n            def log_message(self, *args, **kwargs):\n                # Quiet health server logs\n                return\n\n        srv = HTTPServer((HOST, HEALTH_PORT), H)\n        th = threading.Thread(target=srv.serve_forever, daemon=True)\n        th.start()\n        return True\n    except Exception:\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_H_384": {
      "name": "H",
      "type": "class",
      "start_line": 384,
      "end_line": 426,
      "content_hash": "7054a02924017104c54f6cc4645bd85d37edf436",
      "content": "        class H(BaseHTTPRequestHandler):\n            def do_GET(self):\n                try:\n                    if self.path == \"/readyz\":\n                        self.send_response(200)\n                        self.send_header(\"Content-Type\", \"application/json\")\n                        self.end_headers()\n                        payload = {\"ok\": True, \"app\": APP_NAME}\n                        self.wfile.write(_json_dumps_bytes(payload))\n                    elif self.path == \"/tools\":\n                        self.send_response(200)\n                        self.send_header(\"Content-Type\", \"application/json\")\n                        self.end_headers()\n                        # Hide expand_query when decoder is disabled\n                        tools = _TOOLS_REGISTRY\n                        try:\n                            from scripts.refrag_llamacpp import is_decoder_enabled  # type: ignore\n                        except Exception:\n                            is_decoder_enabled = lambda: False  # type: ignore\n                        try:\n                            if not is_decoder_enabled():\n                                tools = [\n                                    t\n                                    for t in tools\n                                    if (t.get(\"name\") or \"\") != \"expand_query\"\n                                ]\n                        except Exception:\n                            pass\n                        payload = {\"ok\": True, \"tools\": tools}\n                        self.wfile.write(_json_dumps_bytes(payload))\n                    else:\n                        self.send_response(404)\n                        self.end_headers()\n                except Exception:\n                    try:\n                        self.send_response(500)\n                        self.end_headers()\n                    except Exception:\n                        pass\n\n            def log_message(self, *args, **kwargs):\n                # Quiet health server logs\n                return",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_do_GET_385": {
      "name": "do_GET",
      "type": "method",
      "start_line": 385,
      "end_line": 422,
      "content_hash": "5a6ecddae312d93ce596f829452ec058116c6f8c",
      "content": "            def do_GET(self):\n                try:\n                    if self.path == \"/readyz\":\n                        self.send_response(200)\n                        self.send_header(\"Content-Type\", \"application/json\")\n                        self.end_headers()\n                        payload = {\"ok\": True, \"app\": APP_NAME}\n                        self.wfile.write(_json_dumps_bytes(payload))\n                    elif self.path == \"/tools\":\n                        self.send_response(200)\n                        self.send_header(\"Content-Type\", \"application/json\")\n                        self.end_headers()\n                        # Hide expand_query when decoder is disabled\n                        tools = _TOOLS_REGISTRY\n                        try:\n                            from scripts.refrag_llamacpp import is_decoder_enabled  # type: ignore\n                        except Exception:\n                            is_decoder_enabled = lambda: False  # type: ignore\n                        try:\n                            if not is_decoder_enabled():\n                                tools = [\n                                    t\n                                    for t in tools\n                                    if (t.get(\"name\") or \"\") != \"expand_query\"\n                                ]\n                        except Exception:\n                            pass\n                        payload = {\"ok\": True, \"tools\": tools}\n                        self.wfile.write(_json_dumps_bytes(payload))\n                    else:\n                        self.send_response(404)\n                        self.end_headers()\n                except Exception:\n                    try:\n                        self.send_response(500)\n                        self.end_headers()\n                    except Exception:\n                        pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_log_message_424": {
      "name": "log_message",
      "type": "method",
      "start_line": 424,
      "end_line": 426,
      "content_hash": "8814b1b25511dd32580a05a193b6a05e71a018fc",
      "content": "            def log_message(self, *args, **kwargs):\n                # Quiet health server logs\n                return",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_run_subprocess_async_443": {
      "name": "run_subprocess_async",
      "type": "function",
      "start_line": 443,
      "end_line": 507,
      "content_hash": "b1835983e3ac674e7d85d28afc0f8570a0378f7d",
      "content": "    async def run_subprocess_async(\n        cmd: List[str],\n        timeout: Optional[float] = None,\n        env: Optional[Dict[str, str]] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Fallback subprocess runner if subprocess_manager is not available.\"\"\"\n        proc: Optional[asyncio.subprocess.Process] = None\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                *cmd,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n                env=env,\n            )\n            # Default timeout from env if not provided by caller\n            if timeout is None:\n                timeout = MCP_TOOL_TIMEOUT_SECS\n            try:\n                stdout_b, stderr_b = await asyncio.wait_for(\n                    proc.communicate(), timeout=timeout\n                )\n                code = proc.returncode\n            except asyncio.TimeoutError:\n                try:\n                    proc.kill()\n                except Exception:\n                    pass\n                return {\n                    \"ok\": False,\n                    \"code\": -1,\n                    \"stdout\": \"\",\n                    \"stderr\": f\"Command timed out after {timeout}s\",\n                }\n            stdout = (stdout_b or b\"\").decode(\"utf-8\", errors=\"ignore\")\n            stderr = (stderr_b or b\"\").decode(\"utf-8\", errors=\"ignore\")\n\n            def _cap_tail(s: str) -> str:\n                if not s:\n                    return s\n                return (\n                    s\n                    if len(s) <= MAX_LOG_TAIL\n                    else (\"...[tail truncated]\\n\" + s[-MAX_LOG_TAIL:])\n                )\n\n            return {\n                \"ok\": code == 0,\n                \"code\": code,\n                \"stdout\": _cap_tail(stdout),\n                \"stderr\": _cap_tail(stderr),\n            }\n        except Exception as e:\n            return {\"ok\": False, \"code\": -2, \"stdout\": \"\", \"stderr\": str(e)}\n        finally:\n            try:\n                if proc is not None:\n                    if proc.stdout is not None:\n                        proc.stdout.close()\n                    if proc.stderr is not None:\n                        proc.stderr.close()\n                    # Ensure the process is reaped\n                    with contextlib.suppress(Exception):\n                        await proc.wait()\n            except Exception:\n                pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cap_tail_479": {
      "name": "_cap_tail",
      "type": "function",
      "start_line": 479,
      "end_line": 486,
      "content_hash": "adcf285b541b6c552af2dd4f7d1f5fec1d393270",
      "content": "            def _cap_tail(s: str) -> str:\n                if not s:\n                    return s\n                return (\n                    s\n                    if len(s) <= MAX_LOG_TAIL\n                    else (\"...[tail truncated]\\n\" + s[-MAX_LOG_TAIL:])\n                )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_qdrant_index_root_535": {
      "name": "qdrant_index_root",
      "type": "function",
      "start_line": 535,
      "end_line": 607,
      "content_hash": "3698b0ebddfadfd0eb9b8be145bfc401dbf1f8c9",
      "content": "async def qdrant_index_root(\n    recreate: Optional[bool] = None, collection: Optional[str] = None, session: Optional[str] = None\n) -> Dict[str, Any]:\n    \"\"\"Initialize or refresh the vector index for the workspace root (/work).\n\n    When to use:\n    - First-time setup for a repo, or to reindex the whole workspace\n    - After large refactors or schema changes (set recreate=true)\n    - If you want a clean collection or to switch the target collection\n\n    Parameters:\n    - recreate: bool (default: false). Drop/recreate the collection before indexing.\n    - collection: str (optional). Target collection; defaults to workspace state or env COLLECTION_NAME.\n\n    Returns: subprocess result from ingest_code.py with args echoed. On success code==0.\n    Notes:\n    - Omit fields instead of sending null values.\n    - Safe to call repeatedly; unchanged files are skipped by the indexer.\n    \"\"\"\n    sess = _require_auth_session(session)\n\n    # Leniency: if clients embed JSON in 'collection' (and include 'recreate'), parse it\n    try:\n        if _looks_jsonish_string(collection):\n            _parsed = _maybe_parse_jsonish(collection)\n            if isinstance(_parsed, dict):\n                collection = _parsed.get(\"collection\", collection)\n                if recreate is None and \"recreate\" in _parsed:\n                    recreate = _coerce_bool(_parsed.get(\"recreate\"), False)\n    except Exception:\n        pass\n\n    # Resolve collection: prefer explicit value; otherwise use workspace state\n    try:\n        _c = (collection or \"\").strip()\n    except Exception:\n        _c = \"\"\n    # Empty string means use workspace state default (codebase)\n    if _c:\n        coll = _c\n    else:\n        try:\n            from scripts.workspace_state import (\n                get_collection_name as _ws_get_collection_name,\n                is_multi_repo_mode as _ws_is_multi_repo_mode,\n            )  # type: ignore\n\n            if _ws_is_multi_repo_mode():\n                coll = _ws_get_collection_name(\"/work\") or _default_collection()\n            else:\n                coll = _ws_get_collection_name(None) or _default_collection()\n        except Exception:\n            coll = _default_collection()\n\n    _require_collection_access((sess or {}).get(\"user_id\") if sess else None, coll, \"write\")\n\n    env = os.environ.copy()\n    env[\"QDRANT_URL\"] = QDRANT_URL\n    env[\"COLLECTION_NAME\"] = coll\n\n    cmd = [\"python\", _work_script(\"ingest_code.py\"), \"--root\", \"/work\"]\n    if recreate:\n        cmd.append(\"--recreate\")\n\n    res = await _run_async(cmd, env=env)\n    ret = {\"args\": {\"root\": \"/work\", \"collection\": coll, \"recreate\": recreate}, **res}\n    try:\n        if ret.get(\"ok\") and int(ret.get(\"code\", 1)) == 0:\n            if _invalidate_router_scratchpad(\"/work\"):\n                ret[\"invalidated_router_scratchpad\"] = True\n    except Exception:\n        pass\n    return ret",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_qdrant_list_611": {
      "name": "qdrant_list",
      "type": "function",
      "start_line": 611,
      "end_line": 637,
      "content_hash": "be3e56b0f90bdbfe16b158441e77ed57f4ca10ec",
      "content": "async def qdrant_list(kwargs: Any = None) -> Dict[str, Any]:\n    \"\"\"List available Qdrant collections.\n\n    When to use:\n    - Inspect which collections exist before indexing/searching\n    - Debug collection naming in multi-workspace setups\n\n    Parameters:\n    - (none). Extra params are ignored.\n\n    Returns:\n    - {\"collections\": [str, ...]} or {\"error\": \"...\"}\n    \"\"\"\n    try:\n        from qdrant_client import QdrantClient\n\n        client = QdrantClient(\n            url=QDRANT_URL,\n            api_key=os.environ.get(\"QDRANT_API_KEY\"),\n            timeout=float(os.environ.get(\"QDRANT_TIMEOUT\", \"20\") or 20),\n        )\n        cols_info = await asyncio.to_thread(client.get_collections)\n        return {\"collections\": [c.name for c in cols_info.collections]}\n    except ImportError:\n        return {\"error\": \"qdrant_client is not installed in this container\"}\n    except Exception as e:\n        return {\"error\": str(e)}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_workspace_info_641": {
      "name": "workspace_info",
      "type": "function",
      "start_line": 641,
      "end_line": 671,
      "content_hash": "bdfc15a16b07cae1eb75609d6d3c46b651380480",
      "content": "async def workspace_info(\n    workspace_path: Optional[str] = None, kwargs: Any = None\n) -> Dict[str, Any]:\n    \"\"\"Read .codebase/state.json for the current workspace and resolve defaults.\n\n    When to use:\n    - Determine the default collection used by this workspace\n    - Inspect indexing status and metadata saved by indexer/watch\n\n    Parameters:\n    - workspace_path: str (optional). Defaults to \"/work\".\n\n    Returns:\n    - {\"workspace_path\": str, \"default_collection\": str, \"source\": \"state_file\"|\"env\", \"state\": dict}\n    \"\"\"\n    ws_path = (workspace_path or \"/work\").strip() or \"/work\"\n\n\n    st = _read_ws_state(ws_path) or {}\n    coll = (\n        (st.get(\"qdrant_collection\") if isinstance(st, dict) else None)\n        or os.environ.get(\"DEFAULT_COLLECTION\")\n        or os.environ.get(\"COLLECTION_NAME\")\n        or DEFAULT_COLLECTION\n    )\n    return {\n        \"workspace_path\": ws_path,\n        \"default_collection\": coll,\n        \"source\": (\"state_file\" if st else \"env\"),\n        \"state\": st or {},\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_list_workspaces_675": {
      "name": "list_workspaces",
      "type": "function",
      "start_line": 675,
      "end_line": 693,
      "content_hash": "35677f9306b76708e84724cb5c22c8c6b0584016",
      "content": "async def list_workspaces(search_root: Optional[str] = None) -> Dict[str, Any]:\n    \"\"\"Scan search_root recursively for .codebase/state.json and summarize workspaces.\n\n    When to use:\n    - Multi-repo environments; pick a workspace/collection to operate on\n\n    Parameters:\n    - search_root: str (optional). Directory to scan; defaults to parent of /work.\n\n    Returns:\n    - {\"workspaces\": [{\"workspace_path\": str, \"collection_name\": str, \"last_updated\": str|int, \"indexing_state\": str}, ...]}\n    \"\"\"\n    try:\n        from scripts.workspace_state import list_workspaces as _lw  # type: ignore\n\n        items = await asyncio.to_thread(lambda: _lw(search_root))\n        return {\"workspaces\": items}\n    except Exception as e:\n        return {\"error\": str(e)}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_collection_map_700": {
      "name": "collection_map",
      "type": "function",
      "start_line": 700,
      "end_line": 715,
      "content_hash": "1e392786a6936c03132bedab76de18b3b6745cef",
      "content": "async def collection_map(\n    search_root: Optional[str] = None,\n    collection: Optional[str] = None,\n    repo_name: Optional[str] = None,\n    include_samples: Optional[bool] = None,\n    limit: Optional[int] = None,\n) -> Dict[str, Any]:\n    \"\"\"Return collection\u2194repo mappings with optional Qdrant payload samples.\"\"\"\n    return await _collection_map_impl(\n        search_root=search_root,\n        collection=collection,\n        repo_name=repo_name,\n        include_samples=include_samples,\n        limit=limit,\n        coerce_bool_fn=_coerce_bool,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_memory_store_722": {
      "name": "memory_store",
      "type": "function",
      "start_line": 722,
      "end_line": 739,
      "content_hash": "71a4a6091509ebf462e9c2af75ad994e065d1b63",
      "content": "async def memory_store(\n    information: str,\n    metadata: Optional[Dict[str, Any]] = None,\n    collection: Optional[str] = None,\n) -> Dict[str, Any]:\n    \"\"\"Store a free-form memory entry in Qdrant using the active collection.\n\n    - Embeds the text and writes both dense and lexical vectors (plus mini vector in ReFRAG mode).\n    - Honors explicit collection overrides; otherwise falls back to workspace/env defaults.\n    - Returns a payload compatible with context-aware tools.\n    \"\"\"\n    return await _memory_store_impl(\n        information=information,\n        metadata=metadata,\n        collection=collection,\n        default_collection_fn=_default_collection,\n        get_embedding_model_fn=_get_embedding_model,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_qdrant_status_743": {
      "name": "qdrant_status",
      "type": "function",
      "start_line": 743,
      "end_line": 857,
      "content_hash": "e90010bf548baec714b6a78684c36fc9652f4cde",
      "content": "async def qdrant_status(\n    collection: Optional[str] = None,\n    max_points: Optional[int] = None,\n    batch: Optional[int] = None,\n    kwargs: Any = None,\n) -> Dict[str, Any]:\n    \"\"\"Summarize collection size and recent index timestamps.\n\n    When to use:\n    - Check whether indexing ran recently and overall point count\n\n    Parameters:\n    - collection: str (optional). Defaults to env COLLECTION_NAME.\n    - max_points: int. Cap scanned points when estimating timestamps (default 5000).\n    - batch: int. Scroll page size (default 1000).\n\n    Returns:\n    - {\"collection\": str, \"count\": int, \"scanned_points\": int,\n       \"last_ingested_at\": {\"unix\": int, \"iso\": str},\n       \"last_modified_at\": {\"unix\": int, \"iso\": str}}\n    - or {\"error\": \"...\"}\n    \"\"\"\n    # Leniency: absorb 'kwargs' JSON payload some clients send instead of top-level args\n    try:\n        _extra = _extract_kwargs_payload(kwargs)\n        if _extra and not collection:\n            collection = _extra.get(\"collection\", collection)\n        if _extra and max_points in (None, \"\") and _extra.get(\"max_points\") is not None:\n            max_points = _coerce_int(_extra.get(\"max_points\"), None)\n        if _extra and batch in (None, \"\") and _extra.get(\"batch\") is not None:\n            batch = _coerce_int(_extra.get(\"batch\"), None)\n    except Exception:\n        pass\n    coll = collection or _default_collection()\n    try:\n        from qdrant_client import QdrantClient\n        import datetime as _dt\n\n        client = QdrantClient(\n            url=QDRANT_URL,\n            api_key=os.environ.get(\"QDRANT_API_KEY\"),\n            timeout=float(os.environ.get(\"QDRANT_TIMEOUT\", \"20\") or 20),\n        )\n        # Count points\n        try:\n            cnt_res = await asyncio.to_thread(\n                lambda: client.count(collection_name=coll, exact=True)\n            )\n            total = int(getattr(cnt_res, \"count\", 0))\n        except Exception:\n            total = 0\n        # Scan a limited number of points to estimate last timestamps\n        max_points = (\n            int(max_points)\n            if max_points not in (None, \"\")\n            else int(os.environ.get(\"MCP_STATUS_MAX_POINTS\", \"5000\"))\n        )\n        batch = int(batch) if batch not in (None, \"\") else 1000\n        scanned = 0\n        last_ing = None\n        last_mod = None\n        next_page = None\n        while scanned < max_points:\n            limit = min(batch, max_points - scanned)\n            try:\n                pts, next_page = await asyncio.to_thread(\n                    lambda: client.scroll(\n                        collection_name=coll,\n                        limit=limit,\n                        offset=next_page,\n                        with_payload=True,\n                        with_vectors=False,\n                    )\n                )\n            except Exception:\n                # Fallback without offset keyword (older clients)\n                pts, next_page = await asyncio.to_thread(\n                    lambda: client.scroll(\n                        collection_name=coll,\n                        limit=limit,\n                        with_payload=True,\n                        with_vectors=False,\n                    )\n                )\n            if not pts:\n                break\n            scanned += len(pts)\n            for p in pts:\n                md = (p.payload or {}).get(\"metadata\") or {}\n                ti = md.get(\"ingested_at\")\n                tm = md.get(\"last_modified_at\")\n                if isinstance(ti, int):\n                    last_ing = ti if last_ing is None else max(last_ing, ti)\n                if isinstance(tm, int):\n                    last_mod = tm if last_mod is None else max(last_mod, tm)\n            if not next_page:\n                break\n\n        def _iso(ts):\n            if isinstance(ts, int) and ts > 0:\n                try:\n                    return _dt.datetime.fromtimestamp(ts, _dt.timezone.utc).isoformat()\n                except Exception:\n                    return \"\"\n            return \"\"\n\n        return {\n            \"collection\": coll,\n            \"count\": total,\n            \"scanned_points\": scanned,\n            \"last_ingested_at\": {\"unix\": last_ing or 0, \"iso\": _iso(last_ing)},\n            \"last_modified_at\": {\"unix\": last_mod or 0, \"iso\": _iso(last_mod)},\n        }\n    except Exception as e:\n        return {\"collection\": coll, \"error\": str(e)}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__iso_841": {
      "name": "_iso",
      "type": "function",
      "start_line": 841,
      "end_line": 847,
      "content_hash": "6b1c34d3b78064214e5a6ce29c4c69d7545e4c26",
      "content": "        def _iso(ts):\n            if isinstance(ts, int) and ts > 0:\n                try:\n                    return _dt.datetime.fromtimestamp(ts, _dt.timezone.utc).isoformat()\n                except Exception:\n                    return \"\"\n            return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_qdrant_index_861": {
      "name": "qdrant_index",
      "type": "function",
      "start_line": 861,
      "end_line": 957,
      "content_hash": "012c4bee2c87412e2ae951602eea9ea994fd84cc",
      "content": "async def qdrant_index(\n    subdir: Optional[str] = None,\n    recreate: Optional[bool] = None,\n    collection: Optional[str] = None,\n    session: Optional[str] = None,\n) -> Dict[str, Any]:\n    \"\"\"Index the workspace (/work) or a specific subdirectory.\n\n    Use this when you want to index only part of the repo (e.g., \"scripts\" or \"backend/api\").\n    For full-repo indexing, prefer qdrant_index_root.\n\n    Parameters:\n    - subdir: str. \"\" or omit to index the root; or a relative path under /work (e.g., \"scripts\").\n    - recreate: bool (default: false). Drop/recreate the collection before indexing.\n    - collection: str (optional). Target collection; defaults to workspace state or env COLLECTION_NAME.\n\n    Returns: subprocess result from ingest_code.py with args echoed. On success code==0.\n    Notes:\n    - Paths are sandboxed to /work; attempts to escape will be rejected.\n    - Omit fields rather than sending null values.\n    \"\"\"\n    sess = _require_auth_session(session)\n\n    # Leniency: parse JSON-ish payloads mistakenly sent in 'collection' or 'subdir'\n    try:\n        if _looks_jsonish_string(collection):\n            _parsed = _maybe_parse_jsonish(collection)\n            if isinstance(_parsed, dict):\n                subdir = _parsed.get(\"subdir\", subdir)\n                collection = _parsed.get(\"collection\", collection)\n                if recreate is None and \"recreate\" in _parsed:\n                    recreate = _coerce_bool(_parsed.get(\"recreate\"), False)\n        if _looks_jsonish_string(subdir):\n            _parsed2 = _maybe_parse_jsonish(subdir)\n            if isinstance(_parsed2, dict):\n                subdir = _parsed2.get(\"subdir\", subdir)\n                collection = _parsed2.get(\"collection\", collection)\n                if recreate is None and \"recreate\" in _parsed2:\n                    recreate = _coerce_bool(_parsed2.get(\"recreate\"), False)\n    except Exception:\n        pass\n\n    root = \"/work\"\n    if subdir:\n        subdir = subdir.lstrip(\"/\")\n        root = os.path.join(root, subdir)\n    # Enforce /work sandbox\n    real_root = os.path.realpath(root)\n    if not (real_root == \"/work\" or real_root.startswith(\"/work/\")):\n        return {\"ok\": False, \"error\": \"subdir escapes /work sandbox\"}\n    root = real_root\n    # Resolve collection: prefer explicit value; otherwise use workspace state (use workspace root)\n    try:\n        _c2 = (collection or \"\").strip()\n    except Exception:\n        _c2 = \"\"\n    # Empty string means use workspace state default (codebase)\n    if _c2:\n        coll = _c2\n    else:\n        try:\n            from scripts.workspace_state import (\n                get_collection_name as _ws_get_collection_name,\n                is_multi_repo_mode as _ws_is_multi_repo_mode,\n            )  # type: ignore\n\n            if _ws_is_multi_repo_mode():\n                coll = _ws_get_collection_name(root) or _default_collection()\n            else:\n                coll = _ws_get_collection_name(None) or _default_collection()\n        except Exception:\n            coll = _default_collection()\n\n    _require_collection_access((sess or {}).get(\"user_id\") if sess else None, coll, \"write\")\n\n    env = os.environ.copy()\n    env[\"QDRANT_URL\"] = QDRANT_URL\n    env[\"COLLECTION_NAME\"] = coll\n\n    cmd = [\n        \"python\",\n        _work_script(\"ingest_code.py\"),\n        \"--root\",\n        root,\n    ]\n    if recreate:\n        cmd.append(\"--recreate\")\n\n    res = await _run_async(cmd, env=env)\n    ret = {\"args\": {\"root\": root, \"collection\": coll, \"recreate\": recreate}, **res}\n    try:\n        if ret.get(\"ok\") and int(ret.get(\"code\", 1)) == 0:\n            if _invalidate_router_scratchpad(\"/work\"):\n                ret[\"invalidated_router_scratchpad\"] = True\n    except Exception:\n        pass\n    return ret",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_set_session_defaults_961": {
      "name": "set_session_defaults",
      "type": "function",
      "start_line": 961,
      "end_line": 1035,
      "content_hash": "8605fe9fe33ec5d99aace5427995efc4ccb76184",
      "content": "async def set_session_defaults(\n    collection: Any = None,\n    mode: Any = None,\n    under: Any = None,\n    language: Any = None,\n    session: Any = None,\n    ctx: Context = None,\n    **kwargs,\n) -> Dict[str, Any]:\n    \"\"\"Set defaults (e.g., collection, mode, under) for subsequent calls.\n\n    Behavior:\n    - If request Context is available, persist defaults per-connection so later calls on\n      the same MCP session automatically use them (no token required).\n    - Optionally also stores token-scoped defaults for cross-connection reuse.\n    \"\"\"\n    try:\n        _extra = _extract_kwargs_payload(kwargs)\n        if _extra:\n            if (collection is None or (isinstance(collection, str) and collection.strip() == \"\")) and _extra.get(\"collection\") is not None:\n                collection = _extra.get(\"collection\")\n            if (mode is None or (isinstance(mode, str) and str(mode).strip() == \"\")) and _extra.get(\"mode\") is not None:\n                mode = _extra.get(\"mode\")\n            if (under is None or (isinstance(under, str) and str(under).strip() == \"\")) and _extra.get(\"under\") is not None:\n                under = _extra.get(\"under\")\n            if (language is None or (isinstance(language, str) and str(language).strip() == \"\")) and _extra.get(\"language\") is not None:\n                language = _extra.get(\"language\")\n            if (session is None or (isinstance(session, str) and str(session).strip() == \"\")) and _extra.get(\"session\") is not None:\n                session = _extra.get(\"session\")\n    except Exception:\n        pass\n\n    defaults: Dict[str, Any] = {}\n    unset_keys: set[str] = set()\n    for _key, _val in ((\"collection\", collection), (\"mode\", mode), (\"under\", under), (\"language\", language)):\n        if isinstance(_val, str):\n            _s = _val.strip()\n            if _s:\n                defaults[_key] = _s\n            else:\n                unset_keys.add(_key)\n\n    # Per-connection storage (preferred)\n    try:\n        if ctx is not None and getattr(ctx, \"session\", None) is not None and (defaults or unset_keys):\n            with _SESSION_CTX_LOCK:\n                existing2 = SESSION_DEFAULTS_BY_SESSION.get(ctx.session) or {}\n                for _k in unset_keys:\n                    existing2.pop(_k, None)\n                existing2.update(defaults)\n                SESSION_DEFAULTS_BY_SESSION[ctx.session] = existing2\n    except Exception:\n        pass\n\n    # Optional token storage\n    sid = str(session).strip() if session is not None else \"\"\n    if not sid:\n        sid = uuid.uuid4().hex[:12]\n    try:\n        if defaults or unset_keys:\n            with _SESSION_LOCK:\n                existing = SESSION_DEFAULTS.get(sid) or {}\n                for _k in unset_keys:\n                    existing.pop(_k, None)\n                existing.update(defaults)\n                SESSION_DEFAULTS[sid] = existing\n    except Exception:\n        pass\n\n    return {\n        \"ok\": True,\n        \"session\": sid,\n        \"defaults\": SESSION_DEFAULTS.get(sid, {}),\n        \"applied\": (\"connection\" if (ctx is not None and getattr(ctx, \"session\", None) is not None) else \"token\"),\n    }",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_qdrant_prune_1038": {
      "name": "qdrant_prune",
      "type": "function",
      "start_line": 1038,
      "end_line": 1049,
      "content_hash": "6e87c1ef35a335f81a6a26f0a489635c1814474b",
      "content": "async def qdrant_prune(kwargs: Any = None, **ignored: Any) -> Dict[str, Any]:\n    \"\"\"Remove stale points for /work (files deleted/moved but still in the index).\n\n    Extra arguments are accepted for forward compatibility but ignored.\n    Returns the subprocess result from ``prune.py`` with status information.\n    \"\"\"\n    env = os.environ.copy()\n    env[\"PRUNE_ROOT\"] = \"/work\"\n\n    cmd = [\"python\", _work_script(\"prune.py\")]\n    res = await _run_async(cmd, env=env)\n    return res",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_repo_search_1072": {
      "name": "repo_search",
      "type": "function",
      "start_line": 1072,
      "end_line": 1160,
      "content_hash": "2f8b1121d8da20a9c43a132aaf76708bfe5ae386",
      "content": "async def repo_search(\n    query: Any = None,\n    queries: Any = None,\n    limit: Any = None,\n    per_path: Any = None,\n    include_snippet: Any = None,\n    context_lines: Any = None,\n    rerank_enabled: Any = None,\n    rerank_top_n: Any = None,\n    rerank_return_m: Any = None,\n    rerank_timeout_ms: Any = None,\n    highlight_snippet: Any = None,\n    collection: Any = None,\n    workspace_path: Any = None,\n    mode: Any = None,\n    session: Any = None,\n    ctx: Context = None,\n    language: Any = None,\n    under: Any = None,\n    kind: Any = None,\n    symbol: Any = None,\n    path_regex: Any = None,\n    path_glob: Any = None,\n    not_glob: Any = None,\n    ext: Any = None,\n    not_: Any = None,\n    case: Any = None,\n    repo: Any = None,\n    compact: Any = None,\n    output_format: Any = None,\n    args: Any = None,\n    kwargs: Any = None,\n) -> Dict[str, Any]:\n    \"\"\"Zero-config code search over repositories (hybrid: vector + lexical RRF, rerank ON by default).\n\n    When to use:\n    - Find relevant code spans quickly; prefer this over embedding-only search.\n    - Use context_answer when you need a synthesized explanation; use context_search to blend with memory notes.\n\n    Key parameters:\n    - query: str or list[str]. Multiple queries are fused; accepts \"queries\" alias.\n    - limit: int (default 10). Total results across files.\n    - per_path: int (default 2). Max results per file.\n    - include_snippet/context_lines: return inline snippets near hits when true.\n    - rerank_*: ONNX reranker is ON by default for best relevance; timeouts fall back to hybrid.\n    - output_format: \"json\" (default) or \"toon\" for token-efficient TOON format.\n    - collection: str. Target collection; defaults to workspace state or env COLLECTION_NAME.\n    - repo: str or list[str]. Filter by repo name(s). Use \"*\" to search all repos.\n\n    Returns:\n    - Dict with keys: results, total, used_rerank, rerank_counters\n    \"\"\"\n    return await _repo_search_impl(\n        query=query,\n        queries=queries,\n        limit=limit,\n        per_path=per_path,\n        include_snippet=include_snippet,\n        context_lines=context_lines,\n        rerank_enabled=rerank_enabled,\n        rerank_top_n=rerank_top_n,\n        rerank_return_m=rerank_return_m,\n        rerank_timeout_ms=rerank_timeout_ms,\n        highlight_snippet=highlight_snippet,\n        collection=collection,\n        workspace_path=workspace_path,\n        mode=mode,\n        session=session,\n        ctx=ctx,\n        language=language,\n        under=under,\n        kind=kind,\n        symbol=symbol,\n        path_regex=path_regex,\n        path_glob=path_glob,\n        not_glob=not_glob,\n        ext=ext,\n        not_=not_,\n        case=case,\n        repo=repo,\n        compact=compact,\n        output_format=output_format,\n        args=args,\n        kwargs=kwargs,\n        get_embedding_model_fn=_get_embedding_model,\n        require_auth_session_fn=_require_auth_session,\n        do_highlight_snippet_fn=_do_highlight_snippet,\n        run_async_fn=_run_async,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_repo_search_compat_1164": {
      "name": "repo_search_compat",
      "type": "function",
      "start_line": 1164,
      "end_line": 1227,
      "content_hash": "1fcae663cc10be87399656afcc9e93d9d9d3021e",
      "content": "async def repo_search_compat(**arguments) -> Dict[str, Any]:\n    \"\"\"Compatibility wrapper for repo_search (lenient argument handling).\n\n    When to use:\n    - Clients that only send a single dict payload or use aliases (q/text/top_k)\n    - Avoids schema errors by normalizing and forwarding to repo_search\n\n    Returns: same shape as repo_search.\n    Note: Prefer calling repo_search directly when possible.\n    \"\"\"\n    try:\n        args = arguments or {}\n        # Core query: prefer explicit query, else q/text; allow queries list passthrough\n        query = args.get(\"query\") or args.get(\"q\") or args.get(\"text\")\n        queries = args.get(\"queries\")\n        # top_k alias for limit\n        limit = args.get(\"limit\")\n        if (\n            limit is None or (isinstance(limit, str) and str(limit).strip() == \"\")\n        ) and (\"top_k\" in args):\n            limit = args.get(\"top_k\")\n        # not/ not_ normalization\n        not_value = args.get(\"not_\") if (\"not_\" in args) else args.get(\"not\")\n\n        # Build forward kwargs; pass alias keys too so repo_search's leniency picks them up\n        forward = {\n            \"query\": query,\n            \"limit\": limit,\n            \"per_path\": args.get(\"per_path\"),\n            \"include_snippet\": args.get(\"include_snippet\"),\n            \"context_lines\": args.get(\"context_lines\"),\n            \"rerank_enabled\": args.get(\"rerank_enabled\"),\n            \"rerank_top_n\": args.get(\"rerank_top_n\"),\n            \"rerank_return_m\": args.get(\"rerank_return_m\"),\n            \"rerank_timeout_ms\": args.get(\"rerank_timeout_ms\"),\n            \"highlight_snippet\": args.get(\"highlight_snippet\"),\n            \"collection\": args.get(\"collection\"),\n            \"session\": args.get(\"session\"),\n            \"workspace_path\": args.get(\"workspace_path\"),\n            \"language\": args.get(\"language\"),\n            \"under\": args.get(\"under\"),\n            \"kind\": args.get(\"kind\"),\n            \"symbol\": args.get(\"symbol\"),\n            \"path_regex\": args.get(\"path_regex\"),\n            \"path_glob\": args.get(\"path_glob\"),\n            \"not_glob\": args.get(\"not_glob\"),\n            \"ext\": args.get(\"ext\"),\n            \"not_\": not_value,\n            \"case\": args.get(\"case\"),\n            \"compact\": args.get(\"compact\"),\n            \"mode\": args.get(\"mode\"),\n            \"repo\": args.get(\"repo\"),  # Cross-codebase isolation\n            \"output_format\": args.get(\"output_format\"),  # \"json\" or \"toon\"\n            # Alias passthroughs captured by repo_search(**kwargs)\n            \"queries\": queries,\n            \"q\": args.get(\"q\"),\n            \"text\": args.get(\"text\"),\n            \"top_k\": args.get(\"top_k\"),\n        }\n        # Drop Nones to avoid overriding repo_search defaults unnecessarily\n        clean = {k: v for k, v in forward.items() if v is not None}\n        return await repo_search(**clean)\n    except Exception as e:\n        return {\"error\": f\"repo_search_compat failed: {e}\"}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_context_answer_compat_1231": {
      "name": "context_answer_compat",
      "type": "function",
      "start_line": 1231,
      "end_line": 1271,
      "content_hash": "dc577b1031dfab1fe70c9ab9ffc5ebc4fe5f09d1",
      "content": "async def context_answer_compat(arguments: Any = None) -> Dict[str, Any]:\n    \"\"\"Compatibility wrapper for context_answer (lenient argument handling).\n\n    When to use:\n    - Clients that send a single 'arguments' dict or alternate keys (q/text)\n    - Avoids schema errors by normalizing/forwarding to context_answer\n\n    Returns: same shape as context_answer.\n    Note: Prefer calling context_answer directly when possible.\n    \"\"\"\n    try:\n        args = arguments or {}\n        query = args.get(\"query\") or args.get(\"q\") or args.get(\"text\")\n        forward = {\n            \"query\": query,\n            \"limit\": args.get(\"limit\"),\n            \"per_path\": args.get(\"per_path\"),\n            \"budget_tokens\": args.get(\"budget_tokens\"),\n            \"include_snippet\": args.get(\"include_snippet\"),\n            \"collection\": args.get(\"collection\"),\n            \"max_tokens\": args.get(\"max_tokens\"),\n            \"temperature\": args.get(\"temperature\"),\n            \"mode\": args.get(\"mode\"),\n            \"expand\": args.get(\"expand\"),\n            # ---- Forward retrieval filters so router hints are honored ----\n            \"language\": args.get(\"language\"),\n            \"under\": args.get(\"under\"),\n            \"kind\": args.get(\"kind\"),\n            \"symbol\": args.get(\"symbol\"),\n            \"ext\": args.get(\"ext\"),\n            \"path_regex\": args.get(\"path_regex\"),\n            \"path_glob\": args.get(\"path_glob\"),\n            \"not_glob\": args.get(\"not_glob\"),\n            \"case\": args.get(\"case\"),\n            # pass through NOT filter under either key\n            \"not_\": args.get(\"not_\") or args.get(\"not\"),\n        }\n        clean = {k: v for k, v in forward.items() if v is not None}\n        return await context_answer(**clean)\n    except Exception as e:\n        return {\"error\": f\"context_answer_compat failed: {e}\"}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_search_tests_for_1278": {
      "name": "search_tests_for",
      "type": "function",
      "start_line": 1278,
      "end_line": 1313,
      "content_hash": "a460a8d6f9bb68f1c97f8e24670b74173db2407d",
      "content": "async def search_tests_for(\n    query: Any = None,\n    limit: Any = None,\n    include_snippet: Any = None,\n    context_lines: Any = None,\n    under: Any = None,\n    language: Any = None,\n    session: Any = None,\n    compact: Any = None,\n    kwargs: Any = None,\n    ctx: Context = None,\n) -> Dict[str, Any]:\n    \"\"\"Find test files related to a query.\n\n    What it does:\n    - Presets common test file globs and forwards to repo_search\n    - Accepts extra filters via kwargs (e.g., language, under, case)\n\n    Parameters:\n    - query: str or list[str]; limit; include_snippet/context_lines; under; language; compact\n\n    Returns: repo_search result shape.\n    \"\"\"\n    return await _search_tests_for_impl(\n        query=query,\n        limit=limit,\n        include_snippet=include_snippet,\n        context_lines=context_lines,\n        under=under,\n        language=language,\n        session=session,\n        compact=compact,\n        kwargs=kwargs,\n        ctx=ctx,\n        repo_search_fn=repo_search,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_search_config_for_1317": {
      "name": "search_config_for",
      "type": "function",
      "start_line": 1317,
      "end_line": 1347,
      "content_hash": "e20ec311c436df81cd222acc35fa257b28aefbd4",
      "content": "async def search_config_for(\n    query: Any = None,\n    limit: Any = None,\n    include_snippet: Any = None,\n    context_lines: Any = None,\n    under: Any = None,\n    session: Any = None,\n    compact: Any = None,\n    kwargs: Any = None,\n    ctx: Context = None,\n) -> Dict[str, Any]:\n    \"\"\"Find likely configuration files for a service/query.\n\n    What it does:\n    - Presets config file globs (yaml/json/toml/etc.) and forwards to repo_search\n    - Accepts extra filters via kwargs\n\n    Returns: repo_search result shape.\n    \"\"\"\n    return await _search_config_for_impl(\n        query=query,\n        limit=limit,\n        include_snippet=include_snippet,\n        context_lines=context_lines,\n        under=under,\n        session=session,\n        compact=compact,\n        kwargs=kwargs,\n        ctx=ctx,\n        repo_search_fn=repo_search,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_search_callers_for_1351": {
      "name": "search_callers_for",
      "type": "function",
      "start_line": 1351,
      "end_line": 1376,
      "content_hash": "a29e628a029ae2529a639b19ff1010ffa0f6065e",
      "content": "async def search_callers_for(\n    query: Any = None,\n    limit: Any = None,\n    language: Any = None,\n    session: Any = None,\n    kwargs: Any = None,\n    ctx: Context = None,\n) -> Dict[str, Any]:\n    \"\"\"Heuristic search for callers/usages of a symbol.\n\n    When to use:\n    - You want files that reference/invoke a function/class\n\n    Notes:\n    - Thin wrapper over repo_search today; pass language or path_glob to narrow\n    - Returns repo_search result shape\n    \"\"\"\n    return await _search_callers_for_impl(\n        query=query,\n        limit=limit,\n        language=language,\n        session=session,\n        kwargs=kwargs,\n        ctx=ctx,\n        repo_search_fn=repo_search,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_search_importers_for_1380": {
      "name": "search_importers_for",
      "type": "function",
      "start_line": 1380,
      "end_line": 1404,
      "content_hash": "30da66853b90b2e82c88ade6847c16009de40af7",
      "content": "async def search_importers_for(\n    query: Any = None,\n    limit: Any = None,\n    language: Any = None,\n    session: Any = None,\n    kwargs: Any = None,\n    ctx: Context = None,\n) -> Dict[str, Any]:\n    \"\"\"Find files likely importing or referencing a module/symbol.\n\n    What it does:\n    - Presets code globs across common languages; forwards to repo_search\n    - Accepts additional filters via kwargs (e.g., under, case)\n\n    Returns: repo_search result shape.\n    \"\"\"\n    return await _search_importers_for_impl(\n        query=query,\n        limit=limit,\n        language=language,\n        session=session,\n        kwargs=kwargs,\n        ctx=ctx,\n        repo_search_fn=repo_search,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_search_commits_for_1408": {
      "name": "search_commits_for",
      "type": "function",
      "start_line": 1408,
      "end_line": 1440,
      "content_hash": "874c766ea9911fb059025add28aa82a0e7f7a3d1",
      "content": "async def search_commits_for(\n    query: Any = None,\n    path: Any = None,\n    collection: Any = None,\n    limit: Any = None,\n    max_points: Any = None,\n) -> Dict[str, Any]:\n    \"\"\"Search git commit history indexed in Qdrant.\n\n    What it does:\n    - Queries commit documents ingested by scripts/ingest_history.py\n    - Filters by optional file path (metadata.files contains path)\n\n    Parameters:\n    - query: str or list[str]; matched lexically against commit message/text\n    - path: str (optional). Relative path under /work; filters commits that touched this file\n    - collection: str (optional). Defaults to env/WS collection\n    - limit: int (optional, default 10). Max commits to return\n    - max_points: int (optional). Safety cap on scanned points (default 1000)\n\n    Returns:\n    - {\"ok\": true, \"results\": [{\"commit_id\", \"author_name\", \"authored_date\", \"message\", \"files\"}, ...], \"scanned\": int}\n    - On error: {\"ok\": false, \"error\": \"...\"}\n    \"\"\"\n    return await _search_commits_for_impl(\n        query=query,\n        path=path,\n        collection=collection,\n        limit=limit,\n        max_points=max_points,\n        default_collection_fn=_default_collection,\n        get_embedding_model_fn=_get_embedding_model,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_change_history_for_path_1444": {
      "name": "change_history_for_path",
      "type": "function",
      "start_line": 1444,
      "end_line": 1469,
      "content_hash": "007e6b912ae4694d604f48989b96e40e0e42ed01",
      "content": "async def change_history_for_path(\n    path: Any,\n    collection: Any = None,\n    max_points: Any = None,\n    include_commits: Any = None,\n) -> Dict[str, Any]:\n    \"\"\"Summarize recent change metadata for a file path from the index.\n\n    Parameters:\n    - path: str. Relative path under /work.\n    - collection: str (optional). Defaults to env/WS default.\n    - max_points: int (optional). Safety cap on scanned points.\n    - include_commits: bool (optional). If true, attach a small list of recent commits\n      touching this path based on the commit index.\n\n    Returns:\n    - {\"ok\": true, \"summary\": {...}} or {\"ok\": false, \"error\": \"...\"}.\n    \"\"\"\n    return await _change_history_for_path_impl(\n        path=path,\n        collection=collection,\n        max_points=max_points,\n        include_commits=include_commits,\n        default_collection_fn=_default_collection,\n        search_commits_fn=search_commits_for,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_context_answer_1492": {
      "name": "context_answer",
      "type": "function",
      "start_line": 1492,
      "end_line": 1576,
      "content_hash": "d8934ec3884790bc65d44f3359aee865eb74e403",
      "content": "async def context_answer(\n    query: Any = None,\n    limit: Any = None,\n    per_path: Any = None,\n    budget_tokens: Any = None,\n    include_snippet: Any = None,\n    collection: Any = None,\n    max_tokens: Any = None,\n    temperature: Any = None,\n    mode: Any = None,  # \"stitch\" (default) or \"pack\"\n    expand: Any = None,  # whether to LLM-expand queries (up to 2 alternates)\n    # Retrieval filter parameters (passed through to hybrid_search)\n    language: Any = None,\n    under: Any = None,\n    kind: Any = None,\n    symbol: Any = None,\n    ext: Any = None,\n    path_regex: Any = None,\n    path_glob: Any = None,\n    not_glob: Any = None,\n    case: Any = None,\n    not_: Any = None,\n    # Repo scoping (cross-codebase isolation)\n    repo: Any = None,  # str, list[str], or \"*\" to search all repos\n    kwargs: Any = None,\n) -> Dict[str, Any]:\n    \"\"\"Natural-language Q&A over the repo using retrieval + local LLM (llama.cpp).\n\n    What it does:\n    - Retrieves relevant code (hybrid vector+lexical with reranking enabled by default).\n    - Budgets/merges micro-spans, builds citations, and asks the LLM to answer.\n    - Returns a concise answer plus file/line citations.\n\n    When to use:\n    - You need an explanation or \"how to\" grounded in code.\n    - Prefer repo_search for raw hits; prefer context_search to blend code + memory.\n\n    Key parameters:\n    - query: str or list[str]; may be expanded if expand=true.\n    - budget_tokens: int. Token budget across code spans (defaults from MICRO_BUDGET_TOKENS).\n    - include_snippet: bool (default true). Include code snippets sent to the LLM and return them when requested.\n    - max_tokens, temperature: decoding controls.\n    - mode: \"stitch\" (default) or \"pack\" for prompt assembly.\n    - expand: bool. Use tiny local LLM to propose up to 2 alternate queries.\n    - Filters: language, under, kind, symbol, ext, path_regex, path_glob, not_glob, not_, case.\n    - repo: str or list[str]. Filter by repo name(s). Use \"*\" to search all repos (disable auto-filter).\n      By default, auto-detects current repo from CURRENT_REPO env and filters to it.\n\n    Returns:\n    - {\"answer\": str, \"citations\": [{\"path\": str, \"start_line\": int, \"end_line\": int}], \"query\": list[str], \"used\": {...}}\n    - On decoder disabled/error, returns {\"error\": \"...\", \"citations\": [...], \"query\": [...]}\n\n    Notes:\n    - Reranking is enabled by default for optimal retrieval quality.\n    - Honors env knobs such as REFRAG_MODE, REFRAG_GATE_FIRST, MICRO_BUDGET_TOKENS, DECODER_*.\n    - Keeps answers brief (2\u20134 sentences) and grounded; rejects ungrounded output.\n    \"\"\"\n    return await _context_answer_impl(\n        query=query,\n        limit=limit,\n        per_path=per_path,\n        budget_tokens=budget_tokens,\n        include_snippet=include_snippet,\n        collection=collection,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        mode=mode,\n        expand=expand,\n        language=language,\n        under=under,\n        kind=kind,\n        symbol=symbol,\n        ext=ext,\n        path_regex=path_regex,\n        path_glob=path_glob,\n        not_glob=not_glob,\n        case=case,\n        not_=not_,\n        repo=repo,\n        kwargs=kwargs,\n        get_embedding_model_fn=_get_embedding_model,\n        expand_query_fn=expand_query,\n        env_lock=_ENV_LOCK,\n        prepare_filters_and_retrieve_fn=_ca_prepare_filters_and_retrieve,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_code_search_1579": {
      "name": "code_search",
      "type": "function",
      "start_line": 1579,
      "end_line": 1636,
      "content_hash": "e9a75f537eff71d79c56229679ef6ac9add76d8e",
      "content": "async def code_search(\n    query: Any = None,\n    limit: Any = None,\n    per_path: Any = None,\n    include_snippet: Any = None,\n    context_lines: Any = None,\n    rerank_enabled: Any = None,\n    rerank_top_n: Any = None,\n    rerank_return_m: Any = None,\n    rerank_timeout_ms: Any = None,\n    highlight_snippet: Any = None,\n    collection: Any = None,\n    language: Any = None,\n    under: Any = None,\n    kind: Any = None,\n    symbol: Any = None,\n    path_regex: Any = None,\n    path_glob: Any = None,\n    not_glob: Any = None,\n    ext: Any = None,\n    not_: Any = None,\n    case: Any = None,\n    session: Any = None,\n    compact: Any = None,\n    kwargs: Any = None,\n) -> Dict[str, Any]:\n    \"\"\"Exact alias of repo_search (hybrid code search with reranking enabled by default).\n\n    Prefer repo_search; this name exists for discoverability in some IDEs/agents.\n    Same parameters and return shape as repo_search.\n    Reranking (rerank_enabled=true) is ON by default for optimal result quality.\n    \"\"\"\n    return await repo_search(\n        query=query,\n        limit=limit,\n        per_path=per_path,\n        include_snippet=include_snippet,\n        context_lines=context_lines,\n        rerank_enabled=rerank_enabled,\n        rerank_top_n=rerank_top_n,\n        rerank_return_m=rerank_return_m,\n        rerank_timeout_ms=rerank_timeout_ms,\n        highlight_snippet=highlight_snippet,\n        collection=collection,\n        language=language,\n        under=under,\n        kind=kind,\n        symbol=symbol,\n        path_regex=path_regex,\n        path_glob=path_glob,\n        not_glob=not_glob,\n        ext=ext,\n        not_=not_,\n        case=case,\n        session=session,\n        compact=compact,\n        kwargs=kwargs,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_info_request_1644": {
      "name": "info_request",
      "type": "function",
      "start_line": 1644,
      "end_line": 1844,
      "content_hash": "6ddd5f019e1d25692052c4ca26ab07489367ac9f",
      "content": "async def info_request(\n    # Primary parameter\n    info_request: str = None,\n    information_request: str = None,  # Alias\n    # Explanation mode\n    include_explanation: bool = None,\n    # Relationship mapping\n    include_relationships: bool = None,\n    # Auth/session (passed through to repo_search)\n    session: str = None,\n    # Optional filters (pass-through to repo_search)\n    limit: int = None,\n    language: str = None,\n    under: str = None,\n    repo: Any = None,\n    path_glob: Any = None,\n    # Additional options\n    include_snippet: bool = None,\n    context_lines: int = None,\n    # Output format\n    output_format: Any = None,  # \"json\" (default) or \"toon\" for token-efficient format\n    kwargs: Any = None,\n) -> Dict[str, Any]:\n    \"\"\"Simplified codebase retrieval with optional explanation mode.\n\n    When to use:\n    - Simple, single-parameter code search with human-readable descriptions\n    - When you want optional explanation mode for richer context\n    - Drop-in replacement for basic codebase retrieval tools\n\n    Key parameters:\n    - info_request: str. Natural language description of the code you're looking for.\n    - information_request: str. Alias for info_request.\n    - include_explanation: bool (default false). Add summary, primary_locations, related_concepts.\n    - include_relationships: bool (default false). Add imports_from, calls, related_paths to results.\n    - limit: int (default 10). Maximum results to return.\n    - language: str. Filter by programming language.\n    - under: str. Limit search to specific directory.\n    - repo: str or list[str]. Filter by repository name(s).\n    - output_format: \"json\" (default) or \"toon\" for token-efficient TOON format.\n\n    Returns:\n    - Compact mode (default): results with information field and relevance_score alias\n    - Explanation mode: adds summary, primary_locations, related_concepts, query_understanding\n\n    Example:\n    - {\"info_request\": \"database connection pooling\"}\n    - {\"info_request\": \"authentication middleware\", \"include_explanation\": true}\n    \"\"\"\n    # Resolve query from either parameter\n    query = info_request or information_request\n    if not query or not str(query).strip():\n        return {\"ok\": False, \"error\": \"info_request parameter is required\", \"results\": []}\n    query = str(query).strip()\n\n    # Resolve defaults from env\n    _default_limit = safe_int(\n        os.environ.get(\"INFO_REQUEST_LIMIT\", \"10\"), default=10, logger=logger\n    )\n    _default_context = safe_int(\n        os.environ.get(\"INFO_REQUEST_CONTEXT_LINES\", \"5\"), default=5, logger=logger\n    )\n    _default_explain = str(\n        os.environ.get(\"INFO_REQUEST_EXPLAIN_DEFAULT\", \"0\")\n    ).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n    _default_relationships = str(\n        os.environ.get(\"INFO_REQUEST_RELATIONSHIPS\", \"0\")\n    ).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n\n    # Apply defaults\n    eff_limit = limit if limit is not None else _default_limit\n    eff_context = context_lines if context_lines is not None else _default_context\n    eff_snippet = include_snippet if include_snippet is not None else True\n    eff_explain = include_explanation if include_explanation is not None else _default_explain\n    eff_relationships = include_relationships if include_relationships is not None else _default_relationships\n\n    # Smart limits based on query characteristics (only if user didn't override)\n    if limit is None:\n        query_words = len(query.split())\n        query_lower = query.lower()\n        if query_words <= 2:  # Short query like \"auth handler\"\n            eff_limit = 15  # More results for broad queries\n        elif \"how does\" in query_lower or \"what is\" in query_lower:\n            eff_limit = 8   # Questions need focused results\n\n    # Call repo_search (always JSON - we format TOON ourselves after enhancement)\n    search_result = await repo_search(\n        query=query,\n        limit=eff_limit,\n        per_path=3,  # Better default for info requests\n        session=session,\n        include_snippet=eff_snippet,\n        context_lines=eff_context,\n        language=language,\n        under=under,\n        repo=repo,\n        path_glob=path_glob,\n        output_format=\"json\",  # Always get JSON to iterate results\n        kwargs=kwargs,\n    )\n\n    # Extract results\n    results = search_result.get(\"results\", [])\n    total = search_result.get(\"total\", len(results))\n    used_rerank = search_result.get(\"used_rerank\", False)\n\n    # Enhance each result with information field and optional relationships\n    enhanced_results = []\n    for r in results:\n        enhanced = dict(r)\n        enhanced[\"information\"] = _format_information_field(r)\n        enhanced[\"relevance_score\"] = r.get(\"score\", 0.0)  # Alias\n        # Add relationships if requested\n        if eff_relationships:\n            enhanced[\"relationships\"] = _extract_relationships(r)\n        enhanced_results.append(enhanced)\n\n    # Build better search strategy string\n    strategy_parts = [\"hybrid\"]\n    if used_rerank:\n        strategy_parts.append(\"rerank\")\n    if repo:\n        strategy_parts.append(\"repo_filtered\")\n    if language:\n        strategy_parts.append(f\"lang:{language}\")\n    if under:\n        strategy_parts.append(\"path_filtered\")\n    search_strategy = \"+\".join(strategy_parts)\n\n    # Build response\n    response: Dict[str, Any] = {\n        \"ok\": True,\n        \"results\": enhanced_results,\n        \"total\": total,\n        \"search_strategy\": search_strategy,\n    }\n\n    # Add explanation if requested\n    if eff_explain:\n        # Primary locations: unique file paths\n        seen_paths = set()\n        primary_locations = []\n        for r in results:\n            p = r.get(\"path\", \"\")\n            if p and p not in seen_paths:\n                seen_paths.add(p)\n                primary_locations.append(p)\n                if len(primary_locations) >= 5:\n                    break\n\n        # Related concepts\n        related_concepts = _extract_related_concepts(query, results)\n\n        # Detected symbols from query\n        detected_symbols = _extract_symbols_from_query(query)\n\n        # Summary\n        n_files = len(seen_paths)\n        summary = f\"Found {total} results related to '{query}' across {n_files} file{'s' if n_files != 1 else ''}\"\n\n        # Group results by file\n        files_map: Dict[str, list] = {}\n        for r in enhanced_results:\n            p = r.get(\"path\", \"\")\n            if p not in files_map:\n                files_map[p] = []\n            files_map[p].append({\n                \"symbol\": r.get(\"symbol\", \"\"),\n                \"line\": r.get(\"start_line\", 0),\n                \"score\": r.get(\"score\", 0.0),\n            })\n\n        grouped_results = {\n            \"by_file\": {\n                path: {\n                    \"count\": len(items),\n                    \"top_symbols\": [i[\"symbol\"] for i in sorted(items, key=lambda x: -x[\"score\"])[:3] if i[\"symbol\"]],\n                }\n                for path, items in files_map.items()\n            }\n        }\n\n        # Calculate confidence\n        confidence = _calculate_confidence(query, enhanced_results)\n\n        response[\"summary\"] = summary\n        response[\"primary_locations\"] = primary_locations\n        response[\"related_concepts\"] = related_concepts\n        response[\"grouped_results\"] = grouped_results\n        response[\"confidence\"] = confidence\n        response[\"query_understanding\"] = {\n            \"intent\": \"search_for_code\",\n            \"detected_language\": language or None,\n            \"detected_symbols\": detected_symbols,\n            \"search_strategy\": search_strategy,\n        }\n\n    # Apply TOON formatting if requested or enabled globally\n    if _should_use_toon(output_format):\n        return _format_results_as_toon(response, compact=False)  # Keep info_request fields\n    return response",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_context_search_1851": {
      "name": "context_search",
      "type": "function",
      "start_line": 1851,
      "end_line": 1938,
      "content_hash": "0fe4b5c7a36975c26f4b3714760e8949e8095718",
      "content": "async def context_search(\n    query: Any = None,\n    limit: Any = None,\n    per_path: Any = None,\n    include_memories: Any = None,\n    memory_weight: Any = None,\n    per_source_limits: Any = None,\n    include_snippet: Any = None,\n    context_lines: Any = None,\n    rerank_enabled: Any = None,\n    rerank_top_n: Any = None,\n    rerank_return_m: Any = None,\n    rerank_timeout_ms: Any = None,\n    highlight_snippet: Any = None,\n    collection: Any = None,\n    language: Any = None,\n    under: Any = None,\n    kind: Any = None,\n    symbol: Any = None,\n    path_regex: Any = None,\n    path_glob: Any = None,\n    not_glob: Any = None,\n    ext: Any = None,\n    not_: Any = None,\n    case: Any = None,\n    session: Any = None,\n    compact: Any = None,\n    repo: Any = None,\n    output_format: Any = None,\n    kwargs: Any = None,\n) -> Dict[str, Any]:\n    \"\"\"Blend code search results with memory-store entries (notes, docs) for richer context.\n\n    When to use:\n    - You want code spans plus relevant memories in one response.\n    - Prefer repo_search for code-only; use context_answer when you need an LLM-written answer.\n\n    Key parameters:\n    - query: str or list[str]\n    - include_memories: bool (opt-in). If true, queries the memory collection and merges with code results.\n    - memory_weight: float (default 1.0). Scales memory scores relative to code.\n    - per_source_limits: dict, e.g. {\"code\": 5, \"memory\": 3}\n    - All repo_search filters are supported and passed through.\n    - output_format: \"json\" (default) or \"toon\" for token-efficient TOON format.\n    - rerank_enabled: bool (default true). ONNX reranker is ON by default for better relevance.\n    - repo: str or list[str]. Filter by repo name(s). Use \"*\" to search all repos (disable auto-filter).\n      By default, auto-detects current repo from CURRENT_REPO env and filters to it.\n\n    Returns:\n    - {\"results\": [{\"source\": \"code\"| \"memory\", ...}, ...], \"total\": N[, \"memory_note\": str]}\n    - In compact mode, results are reduced to lightweight records.\n\n    Example:\n    - include_memories=true, per_source_limits={\"code\": 6, \"memory\": 2}, path_glob=\"docs/**\"\n    \"\"\"\n    return await _context_search_impl(\n        query=query,\n        limit=limit,\n        per_path=per_path,\n        include_memories=include_memories,\n        memory_weight=memory_weight,\n        per_source_limits=per_source_limits,\n        include_snippet=include_snippet,\n        context_lines=context_lines,\n        rerank_enabled=rerank_enabled,\n        rerank_top_n=rerank_top_n,\n        rerank_return_m=rerank_return_m,\n        rerank_timeout_ms=rerank_timeout_ms,\n        highlight_snippet=highlight_snippet,\n        collection=collection,\n        language=language,\n        under=under,\n        kind=kind,\n        symbol=symbol,\n        path_regex=path_regex,\n        path_glob=path_glob,\n        not_glob=not_glob,\n        ext=ext,\n        not_=not_,\n        case=case,\n        session=session,\n        compact=compact,\n        repo=repo,\n        output_format=output_format,\n        kwargs=kwargs,\n        repo_search_fn=repo_search,\n        get_embedding_model_fn=_get_embedding_model,\n    )",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_expand_query_1945": {
      "name": "expand_query",
      "type": "function",
      "start_line": 1945,
      "end_line": 1962,
      "content_hash": "3114f20e56a9a787b7db79cb2833c2f66feb439a",
      "content": "async def expand_query(\n    query: Any = None,\n    max_new: Any = None,\n    session: Optional[str] = None,\n) -> Dict[str, Any]:\n    \"\"\"LLM-assisted query expansion (local llama.cpp, if enabled).\n\n    When to use:\n    - Generate 1\u20132 compact alternates before repo_search/context_answer\n\n    Parameters:\n    - query: str or list[str]\n    - max_new: int in [0,5] (default 3)\n\n    Returns:\n    - {\"alternates\": list[str]} or {\"alternates\": [], \"hint\": \"...\"} if decoder disabled\n    \"\"\"\n    return await _expand_query_impl(query=query, max_new=max_new, session=session)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}