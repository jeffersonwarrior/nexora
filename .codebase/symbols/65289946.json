{
  "file_path": "/work/external-deps/Context-Engine/scripts/refrag_minimax.py",
  "file_hash": "ed453ec986d0ec324a981121319f543d59f540b2",
  "updated_at": "2025-12-26T17:34:21.548854",
  "symbols": {
    "class_MiniMaxRefragClient_15": {
      "name": "MiniMaxRefragClient",
      "type": "class",
      "start_line": 15,
      "end_line": 96,
      "content_hash": "5bc7c48f3a378e70b2457e993751e192d2c47bac",
      "content": "class MiniMaxRefragClient:\n    \"\"\"MiniMax M2 client exposing generate_with_soft_embeddings(prompt, ...).\n\n    Notes:\n    - soft_embeddings are ignored (MiniMax does not support KV/soft-embed injection)\n    - prompt-mode only; mirrors llama.cpp adapter surface\n    - Uses OpenAI SDK with custom base_url for MiniMax API\n    \"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None) -> None:\n        self.api_key = api_key or os.environ.get(\"MINIMAX_API_KEY\", \"\").strip()\n        if not self.api_key:\n            raise ValueError(\"MINIMAX_API_KEY is required when using REFRAG_RUNTIME=minimax\")\n        self.base_url = base_url or os.environ.get(\"MINIMAX_API_BASE\", \"https://api.minimax.io/v1\")\n        from openai import OpenAI\n        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n\n    def generate_with_soft_embeddings(\n        self,\n        prompt: str,\n        soft_embeddings: Optional[list[list[float]]] = None,  # unused\n        max_tokens: Optional[int] = None,\n        **gen_kwargs: Any,\n    ) -> str:\n        model = os.environ.get(\"MINIMAX_MODEL\", \"MiniMax-M2\")\n        if max_tokens is None:\n            try:\n                max_tokens = int(os.environ.get(\"DECODER_MAX_TOKENS\", \"4096\"))\n            except ValueError:\n                max_tokens = 4096\n        # MiniMax M2 API requires temperature in (0.0, 1.0] - exclusive of 0.0\n        # Per docs: \"The temperature parameter range is (0.0, 1.0], recommended value: 1.0,\n        # values outside this range will return an error\"\n        raw_temp = float(gen_kwargs.get(\"temperature\", 1.0))\n        temperature = max(0.01, min(1.0, raw_temp))  # MiniMax requires (0.0, 1.0]\n        top_p = float(gen_kwargs.get(\"top_p\", 0.95))\n        # Ignore stop sequences - MiniMax M2 thinking models can be cut off prematurely\n        gen_kwargs.pop(\"stop\", None)\n        gen_kwargs.pop(\"timeout\", None)\n        gen_kwargs.pop(\"force_json\", None)\n        system_prompt = gen_kwargs.pop(\"system\", None)\n\n        try:\n            import re\n            final_max_tokens = int(gen_kwargs.get(\"max_tokens\", max_tokens))\n            # Don't pass stop sequences to MiniMax - they can cut off thinking models\n            # before they output the final answer\n            messages = []\n            if system_prompt:\n                messages.append({\"role\": \"system\", \"content\": system_prompt})\n            messages.append({\"role\": \"user\", \"content\": prompt})\n            response = self.client.chat.completions.create(\n                model=model,\n                messages=messages,\n                max_tokens=final_max_tokens,\n                temperature=temperature,\n                top_p=top_p,\n            )\n            content = response.choices[0].message.content or \"\"\n            # MiniMax M2 wraps thinking in <think>...</think> blocks\n            # First try to get content after </think>, if empty extract from inside think block\n            after_think = re.sub(r'<think>[\\s\\S]*?</think>\\s*', '', content).strip()\n            if after_think:\n                return after_think\n            # If no content after think block, try to extract JSON from inside it\n            think_match = re.search(r'<think>([\\s\\S]*?)</think>', content)\n            if think_match:\n                think_content = think_match.group(1)\n                # Look for JSON array in the thinking\n                json_match = re.search(r'\\[[\\s\\S]*?\\]', think_content)\n                if json_match:\n                    return json_match.group(0)\n                # Fallback: extract quoted strings from numbered lists in thinking\n                # Pattern matches: 1. \"text\" or - \"text\" or \"text\" on its own line\n                quoted = re.findall(r'[\"\\']([^\"\\']{5,})[\"\\']', think_content)\n                if len(quoted) >= 2:\n                    # Return as JSON array\n                    import json\n                    return json.dumps(quoted[:2])\n            return content.strip()\n        except Exception as e:\n            raise RuntimeError(f\"MiniMax completion failed: {e}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___24": {
      "name": "__init__",
      "type": "method",
      "start_line": 24,
      "end_line": 30,
      "content_hash": "09c6abcfba959320518a9b5567b38b1037cc0d8e",
      "content": "    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None) -> None:\n        self.api_key = api_key or os.environ.get(\"MINIMAX_API_KEY\", \"\").strip()\n        if not self.api_key:\n            raise ValueError(\"MINIMAX_API_KEY is required when using REFRAG_RUNTIME=minimax\")\n        self.base_url = base_url or os.environ.get(\"MINIMAX_API_BASE\", \"https://api.minimax.io/v1\")\n        from openai import OpenAI\n        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_generate_with_soft_embeddings_32": {
      "name": "generate_with_soft_embeddings",
      "type": "method",
      "start_line": 32,
      "end_line": 96,
      "content_hash": "0bf291b46c9e3660c249cfc4c4ef63ee8cf1a089",
      "content": "    def generate_with_soft_embeddings(\n        self,\n        prompt: str,\n        soft_embeddings: Optional[list[list[float]]] = None,  # unused\n        max_tokens: Optional[int] = None,\n        **gen_kwargs: Any,\n    ) -> str:\n        model = os.environ.get(\"MINIMAX_MODEL\", \"MiniMax-M2\")\n        if max_tokens is None:\n            try:\n                max_tokens = int(os.environ.get(\"DECODER_MAX_TOKENS\", \"4096\"))\n            except ValueError:\n                max_tokens = 4096\n        # MiniMax M2 API requires temperature in (0.0, 1.0] - exclusive of 0.0\n        # Per docs: \"The temperature parameter range is (0.0, 1.0], recommended value: 1.0,\n        # values outside this range will return an error\"\n        raw_temp = float(gen_kwargs.get(\"temperature\", 1.0))\n        temperature = max(0.01, min(1.0, raw_temp))  # MiniMax requires (0.0, 1.0]\n        top_p = float(gen_kwargs.get(\"top_p\", 0.95))\n        # Ignore stop sequences - MiniMax M2 thinking models can be cut off prematurely\n        gen_kwargs.pop(\"stop\", None)\n        gen_kwargs.pop(\"timeout\", None)\n        gen_kwargs.pop(\"force_json\", None)\n        system_prompt = gen_kwargs.pop(\"system\", None)\n\n        try:\n            import re\n            final_max_tokens = int(gen_kwargs.get(\"max_tokens\", max_tokens))\n            # Don't pass stop sequences to MiniMax - they can cut off thinking models\n            # before they output the final answer\n            messages = []\n            if system_prompt:\n                messages.append({\"role\": \"system\", \"content\": system_prompt})\n            messages.append({\"role\": \"user\", \"content\": prompt})\n            response = self.client.chat.completions.create(\n                model=model,\n                messages=messages,\n                max_tokens=final_max_tokens,\n                temperature=temperature,\n                top_p=top_p,\n            )\n            content = response.choices[0].message.content or \"\"\n            # MiniMax M2 wraps thinking in <think>...</think> blocks\n            # First try to get content after </think>, if empty extract from inside think block\n            after_think = re.sub(r'<think>[\\s\\S]*?</think>\\s*', '', content).strip()\n            if after_think:\n                return after_think\n            # If no content after think block, try to extract JSON from inside it\n            think_match = re.search(r'<think>([\\s\\S]*?)</think>', content)\n            if think_match:\n                think_content = think_match.group(1)\n                # Look for JSON array in the thinking\n                json_match = re.search(r'\\[[\\s\\S]*?\\]', think_content)\n                if json_match:\n                    return json_match.group(0)\n                # Fallback: extract quoted strings from numbered lists in thinking\n                # Pattern matches: 1. \"text\" or - \"text\" or \"text\" on its own line\n                quoted = re.findall(r'[\"\\']([^\"\\']{5,})[\"\\']', think_content)\n                if len(quoted) >= 2:\n                    # Return as JSON array\n                    import json\n                    return json.dumps(quoted[:2])\n            return content.strip()\n        except Exception as e:\n            raise RuntimeError(f\"MiniMax completion failed: {e}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}