{
  "file_path": "/work/external-deps/Context-Engine/scripts/indexing_admin.py",
  "file_hash": "66e8172437b152b8791096f0c7af81192c84b046",
  "updated_at": "2025-12-26T17:34:24.235321",
  "symbols": {
    "function__staging_enabled_75": {
      "name": "_staging_enabled",
      "type": "function",
      "start_line": 75,
      "end_line": 76,
      "content_hash": "9898d985d25fce79428196c472cde662783f03d0",
      "content": "def _staging_enabled() -> bool:\n    return bool(is_staging_enabled() if callable(is_staging_enabled) else False)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__workspace_base_dir_79": {
      "name": "_workspace_base_dir",
      "type": "function",
      "start_line": 79,
      "end_line": 80,
      "content_hash": "95314c17ba934bec53d4db753c885d9c9384ceec",
      "content": "def _workspace_base_dir() -> Path:\n    return Path(os.environ.get(\"WORKSPACE_PATH\") or os.environ.get(\"WATCH_ROOT\") or \"/work\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__repo_state_dir_83": {
      "name": "_repo_state_dir",
      "type": "function",
      "start_line": 83,
      "end_line": 84,
      "content_hash": "df81e30387d18805884ae5addf1bd5292a6b1d7a",
      "content": "def _repo_state_dir(repo_name: str) -> Path:\n    return _workspace_base_dir() / \".codebase\" / \"repos\" / repo_name",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__copy_repo_state_for_clone_87": {
      "name": "_copy_repo_state_for_clone",
      "type": "function",
      "start_line": 87,
      "end_line": 135,
      "content_hash": "137c039a977b6a3d6e8ce37f3f2d722dabd2847b",
      "content": "def _copy_repo_state_for_clone(\n    *,\n    repo_name: str,\n    clone_repo_name: str,\n    src_workspace: str,\n    clone_workspace: str,\n) -> None:\n    src_dir = _repo_state_dir(repo_name)\n    if not src_dir.exists():\n        return\n    dst_dir = _repo_state_dir(clone_repo_name)\n    shutil.copytree(src_dir, dst_dir, dirs_exist_ok=True)\n\n    cache_path = dst_dir / \"cache.json\"\n    if not cache_path.exists():\n        return\n\n    try:\n        with cache_path.open(\"r\", encoding=\"utf-8\") as fh:\n            cache = json.load(fh)\n        file_hashes = cache.get(\"file_hashes\")\n        if not isinstance(file_hashes, dict):\n            return\n\n        src_root = str(Path(src_workspace).resolve())\n        clone_root = str(Path(clone_workspace).resolve())\n        updated = False\n        new_hashes: Dict[str, Any] = {}\n        for key, value in file_hashes.items():\n            new_key = key\n            if isinstance(key, str):\n                if key.startswith(src_root):\n                    new_key = clone_root + key[len(src_root) :]\n                else:\n                    token = f\"/{repo_name}\"\n                    repl = f\"/{clone_repo_name}\"\n                    if token in key:\n                        new_key = key.replace(token, repl, 1)\n                if new_key != key:\n                    updated = True\n            new_hashes[new_key] = value\n\n        if updated:\n            cache[\"file_hashes\"] = new_hashes\n            cache[\"updated_at\"] = datetime.now(timezone.utc).isoformat()\n            with cache_path.open(\"w\", encoding=\"utf-8\") as fh:\n                json.dump(cache, fh, ensure_ascii=False, indent=2, sort_keys=True)\n    except Exception as exc:\n        print(f\"[staging] Warning: failed to retarget cache.json for {clone_repo_name}: {exc}\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__probe_collection_schema_143": {
      "name": "_probe_collection_schema",
      "type": "function",
      "start_line": 143,
      "end_line": 215,
      "content_hash": "37ec15346253d9b120645175dff417b7a88a42e4",
      "content": "def _probe_collection_schema(collection: str) -> Optional[Dict[str, Any]]:\n    if not collection or QdrantClient is None:\n        return None\n    cached = _COLLECTION_SCHEMA_CACHE.get(collection)\n    if cached:\n        return cached\n\n    qdrant_url = os.environ.get(\"QDRANT_URL\", \"http://qdrant:6333\")\n    api_key = os.environ.get(\"QDRANT_API_KEY\") or None\n    try:\n        client = QdrantClient(url=qdrant_url, api_key=api_key)\n    except Exception:\n        return None\n\n    try:\n        info = client.get_collection(collection_name=collection)\n    except Exception:\n        try:\n            client.close()\n        except Exception:\n            pass\n        return None\n\n    try:\n        vectors: Dict[str, Optional[int]] = {}\n        raw_vectors = getattr(getattr(info, \"config\", None), \"params\", None)\n        raw_vectors = getattr(raw_vectors, \"vectors\", None)\n        if raw_vectors:\n            items = None\n            if isinstance(raw_vectors, dict):\n                items = raw_vectors.items()\n            else:\n                try:\n                    items = raw_vectors.items()\n                except Exception:\n                    items = None\n            if items:\n                for name, params in items:\n                    try:\n                        size = getattr(params, \"size\", None)\n                    except Exception:\n                        size = None\n                    vectors[str(name)] = size\n\n        sparse_vectors: set[str] = set()\n        raw_sparse = getattr(getattr(info.config, \"params\", None), \"sparse_vectors\", None)\n        if raw_sparse:\n            keys_iter = None\n            if isinstance(raw_sparse, dict):\n                keys_iter = raw_sparse.keys()\n            else:\n                try:\n                    keys_iter = raw_sparse.keys()\n                except Exception:\n                    keys_iter = None\n            if keys_iter:\n                for name in keys_iter:\n                    sparse_vectors.add(str(name))\n\n        schema = {\n            \"vectors\": vectors,\n            \"sparse_vectors\": sparse_vectors,\n            \"payload_indexes\": getattr(getattr(info.config, \"params\", None), \"payload_indexes\", None),\n        }\n        _COLLECTION_SCHEMA_CACHE[collection] = schema\n        return schema\n    except Exception:\n        return None\n    finally:\n        try:\n            client.close()\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__filter_snapshot_only_recreate_keys_218": {
      "name": "_filter_snapshot_only_recreate_keys",
      "type": "function",
      "start_line": 218,
      "end_line": 257,
      "content_hash": "ae9a6a4bfceb55f75c17bbcd52c28162d1fa51c3",
      "content": "def _filter_snapshot_only_recreate_keys(\n    collection: str,\n    drift_keys: List[str],\n    env_config: Dict[str, Any],\n) -> Tuple[List[str], List[str]]:\n    if not drift_keys or not collection or not env_config:\n        return drift_keys, []\n    schema = _probe_collection_schema(collection)\n    if not schema:\n        return drift_keys, []\n\n    vectors = schema.get(\"vectors\") or {}\n    sparse_vectors = schema.get(\"sparse_vectors\") or set()\n    mini_dim = vectors.get(_MINI_VECTOR_NAME)\n    has_sparse = _LEX_SPARSE_NAME in sparse_vectors\n\n    snapshot_only: List[str] = []\n    remaining: List[str] = []\n    for key in drift_keys:\n        suppress = False\n        if key == \"embedding_model\":\n            desired = env_config.get(\"embedding_model\")\n            if desired and vectors.get(_sanitize_vector_name(desired) if _sanitize_vector_name else desired):\n                suppress = True\n        elif key == \"embedding_provider\":\n            suppress = False  # provider changes require rebuild\n        if key == \"lex_sparse_mode\":\n            desired_sparse = bool(env_config.get(\"lex_sparse_mode\"))\n            if desired_sparse and has_sparse:\n                suppress = True\n        elif key == \"mini_vec_dim\":\n            desired_dim = env_config.get(\"mini_vec_dim\")\n            refrag_on = bool(env_config.get(\"refrag_mode\"))\n            if refrag_on and desired_dim and mini_dim == desired_dim:\n                suppress = True\n        if suppress:\n            snapshot_only.append(key)\n        else:\n            remaining.append(key)\n    return remaining, snapshot_only",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__auto_refresh_snapshot_if_needed_260": {
      "name": "_auto_refresh_snapshot_if_needed",
      "type": "function",
      "start_line": 260,
      "end_line": 372,
      "content_hash": "4cd5ad653a207d204fce577897c55e6da661f836",
      "content": "def _auto_refresh_snapshot_if_needed(\n    *,\n    collection: str,\n    workspace_path: Optional[str],\n    repo_name: Optional[str],\n    snapshot_only_keys: List[str],\n    staging_status: str = \"none\",\n    indexing_state: str = \"\",\n) -> bool:\n    if not snapshot_only_keys or not collection:\n        return False\n    if promote_pending_indexing_config is None:\n        return False\n    if persist_indexing_config is None:\n        return False\n    ws = (workspace_path or \"\").strip()\n    if not ws:\n        return False\n    cache_key = f\"{ws}:{repo_name or ''}\"\n    if cache_key in _SNAPSHOT_REFRESHED:\n        return False\n\n    # Safety: Do not auto-refresh during active staging rebuild.\n    # The old collection depends on the saved .env being accurate.\n    if staging_status == \"active\":\n        return False\n\n    # Skip when indexing is currently running; wait for a quiescent window.\n    try:\n        normalized_index_state = (indexing_state or \"\").strip().lower()\n    except Exception:\n        normalized_index_state = \"\"\n    if normalized_index_state in {\"initializing\", \"indexing\", \"running\"}:\n        return False\n\n    try:\n        if get_workspace_state is not None:\n            st = get_workspace_state(ws, repo_name) or {}\n            if isinstance(st, dict):\n                has_pending = bool(st.get(\"indexing_config_pending\") or st.get(\"indexing_env_pending\"))\n                applied_hash = str(st.get(\"indexing_config_hash\") or \"\")\n                pending_hash = str(st.get(\"indexing_config_pending_hash\") or \"\")\n            else:\n                has_pending = False\n                applied_hash = \"\"\n                pending_hash = \"\"\n        else:\n            has_pending = False\n            applied_hash = \"\"\n            pending_hash = \"\"\n    except Exception:\n        has_pending = False\n        applied_hash = \"\"\n        pending_hash = \"\"\n\n    # If the pending hash already matches applied, there's nothing to refresh.\n    if applied_hash and pending_hash and applied_hash == pending_hash:\n        return False\n\n    # Auto-capture current config as pending when safe:\n    # - No active staging rebuild (checked above)\n    # - No pending config exists (avoid clobbering existing pending)\n    # - Schema already validated (snapshot_only_keys non-empty)\n    if not has_pending:\n        try:\n            persist_indexing_config(\n                workspace_path=ws,\n                repo_name=repo_name,\n                pending=True,\n            )\n        except Exception as exc:\n            try:\n                print(\n                    f\"[snapshot_refresh] Failed to capture pending config for {collection}: {exc}\"\n                )\n            except Exception:\n                pass\n            return False\n\n    dry_run = str(os.environ.get(\"CTXCE_SNAPSHOT_REFRESH_DRY_RUN\", \"0\")).strip().lower() in {\n        \"1\",\n        \"true\",\n        \"yes\",\n        \"on\",\n    }\n\n    try:\n        if dry_run:\n            print(\n                f\"[snapshot_refresh] DRY RUN: would promote pending config for {collection} \"\n                f\"(workspace={ws}, repo={repo_name or 'default'}) after validating schema: \"\n                f\"{', '.join(snapshot_only_keys)}\"\n            )\n            return True\n        promote_pending_indexing_config(workspace_path=ws, repo_name=repo_name)\n        _SNAPSHOT_REFRESHED.add(cache_key)\n        try:\n            print(\n                f\"[snapshot_refresh] promoted pending indexing config for {collection} \"\n                f\"(workspace={ws}, repo={repo_name or 'default'}) after validating schema: \"\n                f\"{', '.join(snapshot_only_keys)}\"\n            )\n        except Exception:\n            pass\n        return True\n    except Exception as exc:\n        try:\n            print(\n                f\"[snapshot_refresh] Failed to promote indexing config for {collection}: {exc}\"\n            )\n        except Exception:\n            pass\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__delete_path_tree_375": {
      "name": "_delete_path_tree",
      "type": "function",
      "start_line": 375,
      "end_line": 408,
      "content_hash": "4dbc7abb4478b890857a5aaefa1f7777946c6fe8",
      "content": "def _delete_path_tree(p: Path) -> bool:\n    try:\n        if not p.exists():\n            return False\n        if p.is_dir():\n            try:\n                shutil.rmtree(p)\n                return True\n            except Exception:\n                # Best-effort permission fixup for shared volumes / root-owned files.\n                try:\n                    for sub in p.rglob(\"*\"):\n                        try:\n                            if sub.is_dir():\n                                os.chmod(sub, 0o777)\n                            else:\n                                os.chmod(sub, 0o666)\n                        except Exception:\n                            pass\n                    try:\n                        os.chmod(p, 0o777)\n                    except Exception:\n                        pass\n                except Exception:\n                    pass\n                try:\n                    shutil.rmtree(p)\n                    return True\n                except Exception:\n                    return False\n        p.unlink()\n        return True\n    except Exception:\n        return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__resolve_codebase_root_411": {
      "name": "_resolve_codebase_root",
      "type": "function",
      "start_line": 411,
      "end_line": 435,
      "content_hash": "243874ffbcaaa3e5fecc296fbaffb7850fd4d0a3",
      "content": "def _resolve_codebase_root(work_root: Path) -> Path:\n    env_root = (\n        os.environ.get(\"CTXCE_CODEBASE_ROOT\")\n        or os.environ.get(\"CODEBASE_ROOT\")\n        or \"\"\n    ).strip()\n\n    candidates: List[Path] = []\n    if env_root:\n        candidates.append(Path(env_root))\n    candidates.append(work_root)\n    candidates.append(work_root.parent)\n\n    for candidate in candidates:\n        try:\n            base = candidate.resolve()\n        except Exception:\n            base = candidate\n        try:\n            if (base / \".codebase\" / \"repos\").exists():\n                return base\n        except Exception:\n            continue\n\n    return work_root",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__cleanup_old_clone_438": {
      "name": "_cleanup_old_clone",
      "type": "function",
      "start_line": 438,
      "end_line": 553,
      "content_hash": "2d805de1b4a542be5e860363d134f7a27e2532e3",
      "content": "def _cleanup_old_clone(\n    *,\n    collection: str,\n    repo_name: Optional[str],\n    delete_collection: bool = True,\n    work_dir: Optional[str] = None,\n    workspace_root: Optional[str] = None,\n) -> None:\n    \"\"\"Best-effort cleanup of cloned *_old collection and workspace/meta.\"\"\"\n\n    old_collection = f\"{collection}_old\"\n    if delete_collection:\n        try:\n            delete_collection_qdrant(\n                qdrant_url=os.environ.get(\"QDRANT_URL\", \"http://qdrant:6333\"),\n                api_key=os.environ.get(\"QDRANT_API_KEY\") or None,\n                collection=old_collection,\n            )\n        except Exception:\n            pass\n\n    if not repo_name:\n        return\n\n    old_slug = f\"{repo_name}_old\"\n\n    def _resolve_base_work_root() -> Path:\n        env_work = work_dir or os.environ.get(\"WORK_DIR\") or os.environ.get(\"WORKDIR\") or \"/work\"\n        try:\n            return Path(env_work).resolve()\n        except Exception:\n            return Path(env_work)\n\n    base_work_root = _resolve_base_work_root()\n\n    def _clamp_to_base(candidate: Optional[Path]) -> Path:\n        if candidate is None:\n            return base_work_root\n        try:\n            resolved_candidate = candidate.resolve()\n        except Exception:\n            resolved_candidate = candidate\n        try:\n            resolved_base = base_work_root.resolve()\n        except Exception:\n            resolved_base = base_work_root\n        try:\n            if resolved_base == resolved_candidate or resolved_base in resolved_candidate.parents:\n                return resolved_candidate\n        except Exception:\n            pass\n        try:\n            print(\n                f\"[staging] refusing to treat {resolved_candidate} as work root; falling back to {resolved_base}\"\n            )\n        except Exception:\n            pass\n        return resolved_base\n\n    work_root: Optional[Path] = None\n    if workspace_root:\n        try:\n            work_root = Path(workspace_root).resolve().parent\n        except Exception:\n            try:\n                work_root = Path(workspace_root).parent\n            except Exception:\n                work_root = None\n    if work_root is None:\n        try:\n            if work_dir:\n                work_root = Path(work_dir).resolve()\n            else:\n                work_root = Path(os.environ.get(\"WORK_DIR\") or os.environ.get(\"WORKDIR\") or \"/work\").resolve()\n        except Exception:\n            work_root = Path(\"/work\")\n    work_root = _clamp_to_base(work_root)\n\n    codebase_root = _resolve_codebase_root(work_root)\n\n    def _safe_delete(target: Path) -> None:\n        try:\n            resolved_target = target.resolve()\n            resolved_base = work_root.resolve()\n        except Exception:\n            try:\n                resolved_target = target\n                resolved_base = work_root\n            except Exception:\n                return\n        try:\n            if resolved_base == resolved_target or resolved_base in resolved_target.parents:\n                _delete_path_tree(resolved_target)\n            else:\n                print(f\"[staging] refusing to delete {resolved_target}: outside work root {resolved_base}\")\n        except Exception:\n            pass\n\n    # Workspace clone dir\n    try:\n        _safe_delete((work_root / old_slug).resolve())\n    except Exception:\n        pass\n\n    # Repo metadata dir\n    try:\n        _safe_delete((codebase_root / \".codebase\" / \"repos\" / old_slug).resolve())\n    except Exception:\n        pass\n\n    # If codebase_root differs from work_root, also try under work_root for safety.\n    try:\n        if str(codebase_root.resolve()) != str(work_root.resolve()):\n            _safe_delete((work_root / \".codebase\" / \"repos\" / old_slug).resolve())\n    except Exception:\n        pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__resolve_base_work_root_464": {
      "name": "_resolve_base_work_root",
      "type": "function",
      "start_line": 464,
      "end_line": 469,
      "content_hash": "3814376450e5b8efa72734de6c23581c1d67d9bc",
      "content": "    def _resolve_base_work_root() -> Path:\n        env_work = work_dir or os.environ.get(\"WORK_DIR\") or os.environ.get(\"WORKDIR\") or \"/work\"\n        try:\n            return Path(env_work).resolve()\n        except Exception:\n            return Path(env_work)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__clamp_to_base_473": {
      "name": "_clamp_to_base",
      "type": "function",
      "start_line": 473,
      "end_line": 495,
      "content_hash": "bcd52a08759d0ac6d8536dac2f6af641556d0b76",
      "content": "    def _clamp_to_base(candidate: Optional[Path]) -> Path:\n        if candidate is None:\n            return base_work_root\n        try:\n            resolved_candidate = candidate.resolve()\n        except Exception:\n            resolved_candidate = candidate\n        try:\n            resolved_base = base_work_root.resolve()\n        except Exception:\n            resolved_base = base_work_root\n        try:\n            if resolved_base == resolved_candidate or resolved_base in resolved_candidate.parents:\n                return resolved_candidate\n        except Exception:\n            pass\n        try:\n            print(\n                f\"[staging] refusing to treat {resolved_candidate} as work root; falling back to {resolved_base}\"\n            )\n        except Exception:\n            pass\n        return resolved_base",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__safe_delete_518": {
      "name": "_safe_delete",
      "type": "function",
      "start_line": 518,
      "end_line": 534,
      "content_hash": "914a808f2febdc19532e3d88e00a534d660ec9ad",
      "content": "    def _safe_delete(target: Path) -> None:\n        try:\n            resolved_target = target.resolve()\n            resolved_base = work_root.resolve()\n        except Exception:\n            try:\n                resolved_target = target\n                resolved_base = work_root\n            except Exception:\n                return\n        try:\n            if resolved_base == resolved_target or resolved_base in resolved_target.parents:\n                _delete_path_tree(resolved_target)\n            else:\n                print(f\"[staging] refusing to delete {resolved_target}: outside work root {resolved_base}\")\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_current_env_indexing_hash_595": {
      "name": "current_env_indexing_hash",
      "type": "function",
      "start_line": 595,
      "end_line": 601,
      "content_hash": "16cb9d07e994d6966def4e485f23e5a68c77ef34",
      "content": "def current_env_indexing_hash() -> str:\n    try:\n        if get_indexing_config_snapshot and compute_indexing_config_hash:\n            return compute_indexing_config_hash(get_indexing_config_snapshot())\n    except Exception:\n        return \"\"\n    return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__current_env_indexing_config_and_hash_604": {
      "name": "_current_env_indexing_config_and_hash",
      "type": "function",
      "start_line": 604,
      "end_line": 618,
      "content_hash": "fab0dc47baef1a399d9e8271bd6cf2d22a325b8e",
      "content": "def _current_env_indexing_config_and_hash() -> Tuple[Dict[str, Any], str]:\n    cfg: Dict[str, Any] = {}\n    cfg_hash = \"\"\n    if get_indexing_config_snapshot and compute_indexing_config_hash:\n        try:\n            snapshot = get_indexing_config_snapshot()\n            if isinstance(snapshot, dict):\n                cfg = snapshot\n            cfg_hash = compute_indexing_config_hash(snapshot or {})\n        except Exception:\n            cfg = {}\n            cfg_hash = \"\"\n    if not cfg_hash:\n        cfg_hash = current_env_indexing_hash()\n    return cfg, cfg_hash",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__classify_indexing_drift_621": {
      "name": "_classify_indexing_drift",
      "type": "function",
      "start_line": 621,
      "end_line": 652,
      "content_hash": "ad39de55f309c5e693dfca6cd5bc7ee44cfe1d4f",
      "content": "def _classify_indexing_drift(\n    applied_config: Dict[str, Any],\n    current_config: Dict[str, Any],\n) -> Tuple[List[str], List[str]]:\n    recreate_keys: List[str] = []\n    reindex_keys: List[str] = []\n    if not applied_config or not current_config:\n        return recreate_keys, reindex_keys\n    keys = set(applied_config.keys()) | set(current_config.keys())\n    for key in sorted(keys):\n        applied_has = key in applied_config\n        current_has = key in current_config\n        applied_val = applied_config.get(key)\n        current_val = current_config.get(key)\n\n        # Back-compat: older state snapshots may not include newly added keys.\n        # Treat missing keys as their default values to avoid false drift.\n        if not applied_has and key in _INDEXING_CONFIG_DEFAULTS:\n            applied_val = _INDEXING_CONFIG_DEFAULTS.get(key)\n        if not current_has and key in _INDEXING_CONFIG_DEFAULTS:\n            current_val = _INDEXING_CONFIG_DEFAULTS.get(key)\n\n        if applied_val == current_val:\n            continue\n        drift_class = CONFIG_DRIFT_RULES.get(key, \"recreate\")\n        if drift_class == \"recreate\":\n            recreate_keys.append(key)\n        elif drift_class == \"reindex\":\n            reindex_keys.append(key)\n        else:\n            recreate_keys.append(key)\n    return recreate_keys, reindex_keys",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_collection_mapping_index_655": {
      "name": "collection_mapping_index",
      "type": "function",
      "start_line": 655,
      "end_line": 696,
      "content_hash": "7c9610f49b23af0f27d587076dfcf990fbb140b0",
      "content": "def collection_mapping_index(*, work_dir: str) -> Dict[str, List[Dict[str, Any]]]:\n    if not get_collection_mappings:\n        return {}\n    try:\n        ttl = float(os.environ.get(\"CTXCE_COLLECTION_MAPPING_INDEX_TTL_SECS\", \"5\") or 5)\n    except Exception:\n        ttl = 5.0\n    try:\n        now = time.time()\n    except Exception:\n        now = 0.0\n    try:\n        if (\n            ttl > 0\n            and _MAPPING_INDEX_CACHE.get(\"work_dir\") == work_dir\n            and (now - float(_MAPPING_INDEX_CACHE.get(\"ts\") or 0.0)) < ttl\n        ):\n            cached = _MAPPING_INDEX_CACHE.get(\"value\")\n            if isinstance(cached, dict):\n                return cached\n    except Exception:\n        pass\n    try:\n        mappings = get_collection_mappings(search_root=work_dir) or []\n    except Exception:\n        mappings = []\n    out: Dict[str, List[Dict[str, Any]]] = {}\n    for m in mappings:\n        try:\n            name = str(m.get(\"collection_name\") or \"\").strip()\n        except Exception:\n            name = \"\"\n        if not name:\n            continue\n        out.setdefault(name, []).append(m)\n    try:\n        _MAPPING_INDEX_CACHE[\"ts\"] = now\n        _MAPPING_INDEX_CACHE[\"work_dir\"] = work_dir\n        _MAPPING_INDEX_CACHE[\"value\"] = out\n    except Exception:\n        pass\n    return out",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_resolve_collection_root_699": {
      "name": "resolve_collection_root",
      "type": "function",
      "start_line": 699,
      "end_line": 720,
      "content_hash": "3b9125320dd575f119a1770de86d4cdde5c70f98",
      "content": "def resolve_collection_root(*, collection: str, work_dir: str) -> Tuple[Optional[str], Optional[str]]:\n    name = (collection or \"\").strip()\n    if not name:\n        return None, None\n    idx = collection_mapping_index(work_dir=work_dir)\n    candidates = idx.get(name) or []\n    if not candidates:\n        return None, None\n    chosen = candidates[0]\n    repo_name = chosen.get(\"repo_name\")\n    container_path = chosen.get(\"container_path\")\n    try:\n        repo_name = str(repo_name) if repo_name is not None else None\n    except Exception:\n        repo_name = None\n    try:\n        container_path = str(container_path) if container_path is not None else None\n    except Exception:\n        container_path = None\n    if container_path:\n        return container_path, repo_name\n    return work_dir, repo_name",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_workspace_state_safe_723": {
      "name": "_get_workspace_state_safe",
      "type": "function",
      "start_line": 723,
      "end_line": 732,
      "content_hash": "bc723353fc5ea68f198b735760ce3a1071db4cce",
      "content": "def _get_workspace_state_safe(workspace_path: str, repo_name: Optional[str]) -> Dict[str, Any]:\n    if not get_workspace_state:\n        return {}\n    try:\n        state = get_workspace_state(workspace_path, repo_name) or {}\n        if isinstance(state, dict):\n            return state\n        return {}\n    except Exception:\n        return {}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_get_indexing_state_735": {
      "name": "get_indexing_state",
      "type": "function",
      "start_line": 735,
      "end_line": 745,
      "content_hash": "f1807e3b9268783bb311e0a1fda0b61e11ce68ec",
      "content": "def get_indexing_state(*, workspace_path: str, repo_name: Optional[str]) -> str:\n    if not get_workspace_state:\n        return \"\"\n    try:\n        st = get_workspace_state(workspace_path, repo_name) or {}\n        idx_status = st.get(\"indexing_status\") or {}\n        if isinstance(idx_status, dict):\n            return str(idx_status.get(\"state\") or \"\")\n    except Exception:\n        return \"\"\n    return \"\"",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_build_admin_collections_view_748": {
      "name": "build_admin_collections_view",
      "type": "function",
      "start_line": 748,
      "end_line": 915,
      "content_hash": "2bdcc5b5e29aa30a3905650ec387ab76f669abb6",
      "content": "def build_admin_collections_view(*, collections: Any, work_dir: str) -> List[Dict[str, Any]]:\n    env_config, env_hash = _current_env_indexing_config_and_hash()\n    mapping_index = collection_mapping_index(work_dir=work_dir)\n\n    enriched: List[Dict[str, Any]] = []\n    for c in collections or []:\n        try:\n            coll_name = str(getattr(c, \"qdrant_collection\", None) or c.get(\"qdrant_collection\") or \"\").strip()  # type: ignore[union-attr]\n        except Exception:\n            coll_name = \"\"\n\n        applied_hash = \"\"\n        applied_config: Dict[str, Any] = {}\n        pending_hash = \"\"\n        indexing_state = \"\"\n        indexing_started_at = \"\"\n        progress_files_processed: Optional[int] = None\n        progress_total_files: Optional[int] = None\n        progress_current_file = \"\"\n        repo_name = None\n        container_path = None\n        mapping_count = 0\n\n        try:\n            matches = mapping_index.get(coll_name) or []\n            mapping_count = len(matches)\n            if matches:\n                m = matches[0]\n                repo_name = m.get(\"repo_name\")\n                container_path = m.get(\"container_path\")\n        except Exception:\n            mapping_count = 0\n\n        st: Dict[str, Any] = {}\n        if container_path and get_workspace_state:\n            try:\n                st_raw = get_workspace_state(str(container_path), repo_name) or {}\n                st = st_raw if isinstance(st_raw, dict) else {}\n            except Exception:\n                st = {}\n        if st:\n            try:\n                applied_hash = str(st.get(\"indexing_config_hash\") or \"\")\n                cfg = st.get(\"indexing_config\") or {}\n                applied_config = cfg if isinstance(cfg, dict) else {}\n                pending_hash = str(st.get(\"indexing_config_pending_hash\") or \"\")\n                idx_status = st.get(\"indexing_status\") or {}\n                if isinstance(idx_status, dict):\n                    indexing_state = str(idx_status.get(\"state\") or \"\")\n                    indexing_started_at = str(idx_status.get(\"started_at\") or \"\")\n                    progress = idx_status.get(\"progress\") or {}\n                    if isinstance(progress, dict):\n                        try:\n                            progress_files_processed = int(progress.get(\"files_processed\"))\n                        except Exception:\n                            progress_files_processed = None\n                        try:\n                            progress_total_files = int(progress.get(\"total_files\"))\n                        except Exception:\n                            progress_total_files = None\n                        try:\n                            progress_current_file = str(progress.get(\"current_file\") or \"\")\n                        except Exception:\n                            progress_current_file = \"\"\n            except Exception:\n                applied_hash = \"\"\n                applied_config = {}\n                pending_hash = \"\"\n                indexing_state = \"\"\n                indexing_started_at = \"\"\n                progress_files_processed = None\n                progress_total_files = None\n                progress_current_file = \"\"\n\n        # Add staging information\n        staging_status = \"none\"\n        staging_collection = \"\"\n        staging_state = \"\"\n        if st:\n            try:\n                staging_info = st.get(\"staging\") or {}\n                if isinstance(staging_info, dict) and staging_info.get(\"collection\"):\n                    staging_status = \"active\"\n                    staging_collection = str(staging_info.get(\"collection\") or \"\")\n                    staging_status_info = staging_info.get(\"status\") or {}\n                    if isinstance(staging_status_info, dict):\n                        staging_state = str(staging_status_info.get(\"state\") or \"\")\n                else:\n                    staging_status = \"none\"\n            except Exception:\n                staging_status = \"none\"\n\n        # \"maintenance needed\" should reflect actual config drift requiring a maintenance reindex.\n        # Pending hashes can exist during staging / queued rebuild flows; keep them visible in the UI\n        # but do not treat them as drift.\n        drift_detected = bool(env_hash and applied_hash and env_hash != applied_hash)\n        drift_recreate_keys: List[str] = []\n        drift_reindex_keys: List[str] = []\n        drift_unknown = False\n        snapshot_only_recreate_keys: List[str] = []\n        snapshot_refresh_triggered = False\n        if drift_detected:\n            if applied_config and env_config:\n                drift_recreate_keys, drift_reindex_keys = _classify_indexing_drift(applied_config, env_config)\n                if drift_recreate_keys:\n                    drift_recreate_keys, snapshot_only_recreate_keys = _filter_snapshot_only_recreate_keys(\n                        coll_name, drift_recreate_keys, env_config\n                    )\n            else:\n                drift_unknown = True\n\n        needs_recreate = bool(drift_recreate_keys)\n        if drift_unknown and not needs_recreate:\n            needs_recreate = True\n\n        needs_reindex_only = bool(not needs_recreate and drift_reindex_keys)\n        needs_snapshot_refresh = bool(\n            snapshot_only_recreate_keys and not needs_recreate and not needs_reindex_only\n        )\n        if needs_snapshot_refresh:\n            snapshot_refresh_triggered = _auto_refresh_snapshot_if_needed(\n                collection=coll_name,\n                workspace_path=container_path or work_dir,\n                repo_name=repo_name,\n                snapshot_only_keys=snapshot_only_recreate_keys,\n                staging_status=staging_status,\n                indexing_state=indexing_state,\n            )\n\n        needs_reindex = needs_recreate or needs_reindex_only\n\n        try:\n            cid = getattr(c, \"id\", None) if hasattr(c, \"id\") else c.get(\"id\")  # type: ignore[union-attr]\n        except Exception:\n            cid = None\n\n        enriched.append(\n            {\n                \"id\": cid,\n                \"qdrant_collection\": coll_name,\n                \"container_path\": str(container_path) if container_path else \"\",\n                \"repo_name\": str(repo_name) if repo_name else \"\",\n                \"mapping_count\": mapping_count,\n                \"indexing_state\": indexing_state,\n                \"indexing_started_at\": indexing_started_at,\n                \"progress_files_processed\": progress_files_processed,\n                \"progress_total_files\": progress_total_files,\n                \"progress_current_file\": progress_current_file,\n                \"applied_indexing_hash\": applied_hash,\n                \"pending_indexing_hash\": pending_hash,\n                \"current_indexing_hash\": env_hash,\n                \"needs_reindex\": needs_reindex,\n                \"needs_recreate\": needs_recreate,\n                \"needs_reindex_only\": needs_reindex_only,\n                \"needs_snapshot_refresh\": needs_snapshot_refresh,\n                \"snapshot_refresh_triggered\": snapshot_refresh_triggered,\n                \"drift_recreate_keys\": drift_recreate_keys,\n                \"drift_reindex_keys\": drift_reindex_keys,\n                \"snapshot_only_recreate_keys\": snapshot_only_recreate_keys,\n                \"drift_unknown\": drift_unknown,\n                \"has_mapping\": bool(container_path),\n                \"staging_status\": staging_status,\n                \"staging_collection\": staging_collection,\n                \"staging_state\": staging_state,\n            }\n        )\n\n    return enriched",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_delete_collection_qdrant_918": {
      "name": "delete_collection_qdrant",
      "type": "function",
      "start_line": 918,
      "end_line": 936,
      "content_hash": "8446334213e2eb4f47d29a31de9459060752792e",
      "content": "def delete_collection_qdrant(*, qdrant_url: str, api_key: Optional[str], collection: str) -> None:\n    if QdrantClient is None:\n        return\n    name = (collection or \"\").strip()\n    if not name:\n        return\n    try:\n        cli = QdrantClient(url=qdrant_url, api_key=api_key or None)\n    except Exception:\n        return\n    try:\n        cli.delete_collection(collection_name=name)\n    except Exception:\n        pass\n    finally:\n        try:\n            cli.close()\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_recreate_collection_qdrant_939": {
      "name": "recreate_collection_qdrant",
      "type": "function",
      "start_line": 939,
      "end_line": 958,
      "content_hash": "d3aed36245630ea90219f6b3c0201fa127d532cc",
      "content": "def recreate_collection_qdrant(*, qdrant_url: str, api_key: Optional[str], collection: str) -> None:\n    if QdrantClient is None:\n        return\n    name = (collection or \"\").strip()\n    if not name:\n        return\n    try:\n        cli = QdrantClient(url=qdrant_url, api_key=api_key or None)\n    except Exception as connect_error:\n        raise RuntimeError(f\"Failed to connect to Qdrant for collection '{name}': {connect_error}\") from connect_error\n    try:\n        try:\n            cli.delete_collection(collection_name=name)\n        except Exception as delete_error:\n            raise RuntimeError(f\"Failed to delete existing collection '{name}' in Qdrant: {delete_error}\") from delete_error\n    finally:\n        try:\n            cli.close()\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_spawn_ingest_code_961": {
      "name": "spawn_ingest_code",
      "type": "function",
      "start_line": 961,
      "end_line": 1021,
      "content_hash": "f8cffbc4c63f263294ff294be1c15bc65e19909e",
      "content": "def spawn_ingest_code(\n    *,\n    root: str,\n    work_dir: str,\n    collection: str,\n    recreate: bool,\n    repo_name: Optional[str],\n    env_overrides: Optional[Dict[str, Any]] = None,\n    clear_caches: bool = False,\n) -> None:\n    script_path = str((Path(__file__).resolve().parent / \"ingest_code.py\").resolve())\n    cmd = [sys.executable or \"python3\", script_path, \"--root\", root, \"--no-skip-unchanged\"]\n    if recreate:\n        cmd.append(\"--recreate\")\n    if clear_caches:\n        cmd.append(\"--clear-indexing-caches\")\n\n    env = os.environ.copy()\n    # Apply per-run env overrides (e.g. pending env snapshots for staging rebuild).\n    # Merge overrides on top of current env, removing keys for None values.\n    if isinstance(env_overrides, dict):\n        for k, v in env_overrides.items():\n            if v is None:\n                env.pop(k, None)\n            else:\n                env[str(k)] = str(v)\n        # When we provide env overrides for a run (e.g. staging rebuild), we also want to\n        # force ingest_code to honor the explicit COLLECTION_NAME instead of routing based\n        # on per-repo state/serving_collection in multi-repo mode.\n        # CTXCE_FORCE_COLLECTION_NAME is only used for these subprocess runs; normal watcher\n        # and indexer flows do not set it.\n        env[\"CTXCE_FORCE_COLLECTION_NAME\"] = \"1\"  # Force ingest_code to use COLLECTION_NAME for staging/pending env overrides\n    env[\"COLLECTION_NAME\"] = collection\n    env[\"WATCH_ROOT\"] = work_dir\n    env[\"WORKSPACE_PATH\"] = work_dir\n\n    try:\n        if update_indexing_status:\n            update_indexing_status(\n                workspace_path=root,\n                repo_name=repo_name,\n                status={\n                    \"state\": \"initializing\",\n                    \"started_at\": datetime.now(timezone.utc).isoformat(),\n                    \"progress\": {\"files_processed\": 0, \"total_files\": None, \"current_file\": None},\n                },\n            )\n    except Exception:\n        pass\n\n    # Spawn the ingest process and validate it started successfully\n    try:\n        proc = subprocess.Popen(cmd, env=env)\n        try:\n            poller = proc.poll  # type: ignore[attr-defined]\n        except AttributeError:\n            poller = None\n        if callable(poller) and poller() is not None:\n            raise RuntimeError(f\"Failed to start ingest process: {cmd}\")\n    except Exception as exc:\n        raise RuntimeError(f\"Failed to spawn ingest_code for {root}: {exc}\") from exc",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__determine_embedding_dim_1024": {
      "name": "_determine_embedding_dim",
      "type": "function",
      "start_line": 1024,
      "end_line": 1036,
      "content_hash": "d15f7d8a317cb525b29864ae0cfb84199bef9add",
      "content": "def _determine_embedding_dim(model_name: str) -> int:\n    if get_model_dimension:\n        try:\n            return int(get_model_dimension(model_name))\n        except Exception:\n            pass\n    try:\n        from fastembed import TextEmbedding  # type: ignore\n\n        model = TextEmbedding(model_name=model_name)\n        return len(next(model.embed([\"dimension probe\"])))\n    except Exception:\n        return 1536",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__normalize_cloned_collection_schema_1039": {
      "name": "_normalize_cloned_collection_schema",
      "type": "function",
      "start_line": 1039,
      "end_line": 1072,
      "content_hash": "728b80bc97910625dc772f6e3c217599b594a096",
      "content": "def _normalize_cloned_collection_schema(*, collection_name: str, qdrant_url: str) -> None:\n    if QdrantClient is None:\n        return\n    vector_name = None\n    model_name = os.environ.get(\"EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\")\n    dim = _determine_embedding_dim(model_name)\n    if _sanitize_vector_name is not None:\n        try:\n            vector_name = _sanitize_vector_name(model_name)\n        except Exception:\n            vector_name = None\n    try:\n        client = QdrantClient(url=qdrant_url, api_key=os.environ.get(\"QDRANT_API_KEY\") or None)\n    except Exception:\n        return\n    try:\n        # IMPORTANT: This function is called on a freshly cloned \"*_old\" collection which may\n        # be actively serving traffic during a migration. We must not recreate/modify the\n        # vector schema here, since Qdrant can't add vector names in-place and the fallback\n        # logic in ingest_code can delete+recreate collections (which would drop copied points).\n        #\n        # We only ensure payload indexes for query performance.\n        if ensure_payload_indexes is not None:\n            ensure_payload_indexes(client, collection_name)\n    except Exception as exc:\n        try:\n            print(f\"[staging] Warning: failed to normalize cloned collection {collection_name}: {exc}\")\n        except Exception:\n            pass\n    finally:\n        try:\n            client.close()\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_collection_point_count_1075": {
      "name": "_get_collection_point_count",
      "type": "function",
      "start_line": 1075,
      "end_line": 1092,
      "content_hash": "2b27f440d0eb8312b3afbe14ecfe299201510bd6",
      "content": "def _get_collection_point_count(*, collection_name: str, qdrant_url: str) -> Optional[int]:\n    if QdrantClient is None:\n        return None\n    try:\n        client = QdrantClient(url=qdrant_url, api_key=os.environ.get(\"QDRANT_API_KEY\") or None)\n    except Exception:\n        return None\n    try:\n        try:\n            result = client.count(collection_name=collection_name, exact=True)\n            return int(getattr(result, \"count\", 0))\n        except Exception:\n            return None\n    finally:\n        try:\n            client.close()\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__wait_for_clone_points_1095": {
      "name": "_wait_for_clone_points",
      "type": "function",
      "start_line": 1095,
      "end_line": 1144,
      "content_hash": "e227f5ff38ca97bd2f372c1c0c8a78391a4550e9",
      "content": "def _wait_for_clone_points(\n    *,\n    source_collection: str,\n    cloned_collection: str,\n    qdrant_url: str,\n    expected_count: Optional[int],\n    timeout_seconds: int = 60,\n) -> None:\n    if QdrantClient is None:\n        return\n    try:\n        client = QdrantClient(url=qdrant_url, api_key=os.environ.get(\"QDRANT_API_KEY\") or None)\n    except Exception:\n        return\n\n    try:\n        if expected_count is None or expected_count <= 0:\n            # Nothing to wait for (empty collection or unknown count)\n            return\n\n        deadline = time.time() + timeout_seconds\n        while True:\n            try:\n                result = client.count(collection_name=cloned_collection, exact=True)\n                clone_count = int(getattr(result, \"count\", 0))\n            except Exception:\n                clone_count = 0\n\n            if clone_count >= expected_count:\n                try:\n                    print(\n                        f\"[staging] Clone verification succeeded: {cloned_collection} has \"\n                        f\"{clone_count} points (expected >= {expected_count}).\"\n                    )\n                except Exception:\n                    pass\n                return\n\n            if time.time() > deadline:\n                raise RuntimeError(\n                    f\"Cloned collection {cloned_collection} only has {clone_count} points \"\n                    f\"(expected at least {expected_count})\"\n                )\n\n            time.sleep(2)\n    finally:\n        try:\n            client.close()\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_start_staging_rebuild_1147": {
      "name": "start_staging_rebuild",
      "type": "function",
      "start_line": 1147,
      "end_line": 1338,
      "content_hash": "2d1d29d7bd29e5609a44c6ca86f2d57964a0b386",
      "content": "def start_staging_rebuild(*, collection: str, work_dir: str) -> str:\n    if not _staging_enabled():\n        raise RuntimeError(\"Staging is disabled (set CTXCE_STAGING_ENABLED=1 to enable)\")\n    root, repo_name = resolve_collection_root(collection=collection, work_dir=work_dir)\n    if not root:\n        raise RuntimeError(\"No workspace mapping found for collection\")\n\n    state = _get_workspace_state_safe(root, repo_name)\n    current_staging = state.get(\"staging\") or {}\n    staging_status_state = \"\"\n    try:\n        staging_status_state = str(((current_staging.get(\"status\") or {}).get(\"state\") or \"\")).strip().lower()\n    except Exception:\n        staging_status_state = \"\"\n    queued_marker = staging_status_state in {\"\", \"queued\"}\n    if isinstance(current_staging, dict) and current_staging.get(\"collection\") and not queued_marker:\n        raise RuntimeError(\"A staging collection is already running for this workspace\")\n\n    # Copy current collection to <collection>_old\n    old_collection = f\"{collection}_old\"\n    qdrant_url = os.environ.get(\"QDRANT_URL\", \"http://qdrant:6333\")\n    source_point_count = _get_collection_point_count(collection_name=collection, qdrant_url=qdrant_url)\n\n    # Use local import for thread-safety and determinism\n    _copy_fn: Any = copy_collection_qdrant\n    if _copy_fn is None:\n        # Re-import for container environments where module-level import may have failed\n        from scripts.collection_admin import copy_collection_qdrant as _ccq\n        _copy_fn = _ccq\n\n    if not callable(_copy_fn):\n        raise RuntimeError(\"copy_collection_qdrant unavailable (import failed)\")\n\n    try:\n        print(f\"[staging] Copying collection {collection} -> {old_collection} (overwrite=True)\")\n        try:\n            print(\n                f\"[staging] copy_collection_qdrant callable={callable(_copy_fn)} type={type(_copy_fn)} module={getattr(_copy_fn, '__module__', '?')}\"\n            )\n        except Exception:\n            pass\n        _copy_fn(\n            source=collection,\n            target=old_collection,\n            qdrant_url=qdrant_url,\n            overwrite=True,\n        )\n        print(f\"[staging] Copy completed for {old_collection}\")\n    except Exception as exc:\n        print(f\"[staging] ERROR copying {collection} -> {old_collection}: {exc!r}\")\n        try:\n            print(\"[staging] TRACEBACK (copy)\")\n            print(traceback.format_exc())\n        except Exception:\n            pass\n        raise\n\n    try:\n        _wait_for_clone_points(\n            source_collection=collection,\n            cloned_collection=old_collection,\n            qdrant_url=qdrant_url,\n            expected_count=source_point_count,\n            timeout_seconds=90,\n        )\n    except Exception as exc:\n        print(f\"[staging] ERROR verifying clone {old_collection}: {exc}\")\n        raise\n\n    # IMPORTANT: switch serving to *_old as soon as the clone is verified.\n    # Large repos can make the filesystem copy below slow; don't block the traffic cutover.\n    try:\n        print(f\"[staging] Switching serving to clone {old_collection}\")\n        update_workspace_state(\n            workspace_path=root,\n            repo_name=repo_name,\n            updates={\n                \"serving_collection\": old_collection,\n                \"serving_repo_slug\": f\"{repo_name}_old\" if repo_name else \"\",\n                \"active_repo_slug\": repo_name,\n                \"qdrant_collection\": collection,\n            },\n        )\n    except Exception as exc:\n        print(f\"[staging] ERROR updating serving state to {old_collection}: {exc}\")\n        raise\n\n    # Best-effort: ensure payload indexes on the clone. Failures here must not break cutover.\n    try:\n        _normalize_cloned_collection_schema(collection_name=old_collection, qdrant_url=qdrant_url)\n    except Exception as exc:\n        print(f\"[staging] Warning: failed to normalize cloned collection {old_collection}: {exc}\")\n\n    # Duplicate workspace slug/state to <slug>_old so watcher/indexer can serve reads from the clone.\n    # This can be slow for large repos; treat as best-effort and do not block staging cutover.\n    if repo_name:\n        work_root = Path(os.environ.get(\"WORK_DIR\") or os.environ.get(\"WORKDIR\") or \"/work\")\n        canonical_dir = work_root / repo_name\n        old_dir = work_root / f\"{repo_name}_old\"\n        try:\n            if canonical_dir.exists():\n                t0 = time.time()\n                print(f\"[staging] Copying workspace tree {canonical_dir} -> {old_dir}\")\n                shutil.copytree(canonical_dir, old_dir, dirs_exist_ok=True)\n                print(f\"[staging] Workspace copy completed in {time.time() - t0:.1f}s\")\n        except Exception as exc:\n            print(f\"[staging] Warning: failed to copy workspace tree for {repo_name}: {exc}\")\n\n        try:\n            _copy_repo_state_for_clone(\n                repo_name=repo_name,\n                clone_repo_name=f\"{repo_name}_old\",\n                src_workspace=str(canonical_dir),\n                clone_workspace=str(old_dir),\n            )\n        except Exception as exc:\n            print(f\"[staging] Warning: failed to copy repo state for {repo_name}: {exc}\")\n\n        try:\n            old_state = state.copy()\n            # Preserve the env/config that was serving traffic; drop pending snapshots.\n            old_state[\"qdrant_collection\"] = old_collection\n            old_state[\"serving_collection\"] = old_collection\n            old_state[\"serving_repo_slug\"] = f\"{repo_name}_old\"\n            old_state[\"active_repo_slug\"] = old_state.get(\"active_repo_slug\") or repo_name\n            try:\n                old_state[\"indexing_status\"] = {\"state\": \"idle\"}\n            except Exception:\n                pass\n            old_state[\"indexing_config_pending\"] = None\n            old_state[\"indexing_config_pending_hash\"] = None\n            old_state[\"indexing_env_pending\"] = None\n            old_state[\"staging\"] = None\n            update_workspace_state(\n                workspace_path=str(old_dir),\n                repo_name=f\"{repo_name}_old\",\n                updates=old_state,\n            )\n        except Exception as exc:\n            print(f\"[staging] Warning: failed to write *_old state for {repo_name}: {exc}\")\n\n    # Prepare canonical slug for rebuild (pending env)\n    pending_cfg = state.get(\"indexing_config_pending\") or state.get(\"indexing_config\")\n    pending_hash = state.get(\"indexing_config_pending_hash\") or state.get(\"indexing_config_hash\")\n    pending_env = state.get(\"indexing_env_pending\") or dict(os.environ)\n    env_hash = pending_hash or current_env_indexing_hash()\n\n    if not pending_cfg and get_indexing_config_snapshot:\n        pending_cfg = get_indexing_config_snapshot() if callable(get_indexing_config_snapshot) else get_indexing_config_snapshot\n    if not pending_hash and pending_cfg and compute_indexing_config_hash:\n        pending_hash = compute_indexing_config_hash(pending_cfg)\n\n    if set_staging_state:\n        staging_info = {\n            \"collection\": old_collection,\n            \"started_at\": datetime.now(timezone.utc).isoformat(),\n            \"updated_at\": datetime.now(timezone.utc).isoformat(),\n            \"env_hash\": pending_hash or env_hash,\n            \"indexing_config\": pending_cfg,\n            \"indexing_config_hash\": pending_hash,\n            \"environment\": pending_env,\n            \"status\": {\"state\": \"initializing\"},\n            \"workspace_path\": root,\n            \"repo_name\": repo_name,\n        }\n        set_staging_state(workspace_path=root, repo_name=repo_name, staging=staging_info)\n\n    if update_staging_status:\n        update_staging_status(\n            workspace_path=root,\n            repo_name=repo_name,\n            status={\n                \"state\": \"initializing\",\n                \"started_at\": datetime.now(timezone.utc).isoformat(),\n                \"progress\": {\"files_processed\": 0, \"total_files\": None, \"current_file\": None},\n            },\n        )\n\n    recreate_collection_qdrant(\n        qdrant_url=os.environ.get(\"QDRANT_URL\", \"http://qdrant:6333\"),\n        api_key=os.environ.get(\"QDRANT_API_KEY\") or None,\n        collection=collection,\n    )\n    spawn_ingest_code(\n        root=root,\n        work_dir=work_dir,\n        collection=collection,\n        recreate=True,\n        repo_name=repo_name,\n        env_overrides=pending_env if isinstance(pending_env, dict) else None,\n    )\n    return collection",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_activate_staging_rebuild_1341": {
      "name": "activate_staging_rebuild",
      "type": "function",
      "start_line": 1341,
      "end_line": 1406,
      "content_hash": "46bcd9b3cdad02422430a639d93c4e04f7a1b39e",
      "content": "def activate_staging_rebuild(*, collection: str, work_dir: str) -> None:\n    if not _staging_enabled():\n        raise RuntimeError(\"Staging is disabled (set CTXCE_STAGING_ENABLED=1 to enable)\")\n    root, repo_name = resolve_collection_root(collection=collection, work_dir=work_dir)\n    if not root:\n        raise RuntimeError(\"No workspace mapping found for collection\")\n    state = _get_workspace_state_safe(root, repo_name)\n    staging = state.get(\"staging\") or {}\n\n    # Idempotent: allow Activate to run even if staging metadata was lost,\n    # as long as serving is still pointed at *_old (or the clone artifacts exist).\n    staging_active = False\n    try:\n        if isinstance(staging, dict) and staging.get(\"collection\"):\n            staging_active = True\n    except Exception:\n        staging_active = False\n    try:\n        if str(state.get(\"serving_collection\") or \"\").strip() == f\"{collection}_old\":\n            staging_active = True\n    except Exception:\n        pass\n    try:\n        if str(state.get(\"serving_repo_slug\") or \"\").strip().endswith(\"_old\"):\n            staging_active = True\n    except Exception:\n        pass\n\n    if not staging_active:\n        # Nothing to activate.\n        return\n    # This staging workflow serves reads from <collection>_old while recreating/reindexing\n    # the primary <collection>. \"Activate\" means: switch serving back to the rebuilt primary\n    # and remove the *_old clone.\n    cleanup_kwargs = {\n        \"collection\": collection,\n        \"repo_name\": repo_name,\n        \"delete_collection\": True,\n        \"work_dir\": work_dir,\n        \"workspace_root\": root,\n    }\n\n    try:\n        # Reset serving state back to the primary collection and clear staging metadata.\n        if update_workspace_state and repo_name:\n            try:\n                update_workspace_state(\n                    workspace_path=root,\n                    repo_name=repo_name,\n                    updates={\n                        \"serving_collection\": collection,\n                        \"serving_repo_slug\": repo_name,\n                        \"active_repo_slug\": repo_name,\n                        \"qdrant_collection\": collection,\n                    },\n                )\n            except Exception:\n                pass\n\n        if clear_staging_collection:\n            try:\n                clear_staging_collection(workspace_path=root, repo_name=repo_name)\n            except Exception:\n                pass\n    finally:\n        _cleanup_old_clone(**cleanup_kwargs)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_abort_staging_rebuild_1409": {
      "name": "abort_staging_rebuild",
      "type": "function",
      "start_line": 1409,
      "end_line": 1477,
      "content_hash": "1b35ad6fd8ca32bc944271345bd89b3e9643fd5a",
      "content": "def abort_staging_rebuild(\n    *,\n    collection: str,\n    work_dir: str,\n    delete_collection: bool = True,\n) -> None:\n    if not _staging_enabled():\n        raise RuntimeError(\"Staging is disabled (set CTXCE_STAGING_ENABLED=1 to enable)\")\n    if not clear_staging_collection:\n        raise RuntimeError(\"clear_staging_collection helper unavailable\")\n    root, repo_name = resolve_collection_root(collection=collection, work_dir=work_dir)\n    if not root:\n        raise RuntimeError(\"No workspace mapping found for collection\")\n    state = _get_workspace_state_safe(root, repo_name)\n    staging = state.get(\"staging\") or {}\n\n    # Idempotent: allow Abort to run even if staging metadata was lost,\n    # as long as serving is still pointed at *_old (or the clone artifacts exist).\n    staging_active = False\n    try:\n        if isinstance(staging, dict) and staging.get(\"collection\"):\n            staging_active = True\n    except Exception:\n        staging_active = False\n    try:\n        if str(state.get(\"serving_collection\") or \"\").strip() == f\"{collection}_old\":\n            staging_active = True\n    except Exception:\n        pass\n    try:\n        if str(state.get(\"serving_repo_slug\") or \"\").strip().endswith(\"_old\"):\n            staging_active = True\n    except Exception:\n        pass\n\n    if not staging_active:\n        # Nothing to abort.\n        return\n\n    # Staging rebuild serves traffic from <collection>_old while recreating <collection>.\n    # Abort should restore serving state to the primary collection and remove the *_old clone.\n    cleanup_kwargs = {\n        \"collection\": collection,\n        \"repo_name\": repo_name,\n        \"delete_collection\": delete_collection,\n        \"work_dir\": work_dir,\n        \"workspace_root\": root,\n    }\n\n    try:\n        # Reset serving state back to the primary collection.\n        if update_workspace_state and repo_name:\n            try:\n                update_workspace_state(\n                    workspace_path=root,\n                    repo_name=repo_name,\n                    updates={\n                        \"serving_collection\": collection,\n                        \"serving_repo_slug\": repo_name,\n                        \"active_repo_slug\": repo_name,\n                        \"qdrant_collection\": collection,\n                    },\n                )\n            except Exception:\n                pass\n\n        clear_staging_collection(workspace_path=root, repo_name=repo_name)\n    finally:\n        _cleanup_old_clone(**cleanup_kwargs)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}