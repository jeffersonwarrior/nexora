{
  "file_path": "/work/.local/tools/modelscan/providers/openai.go",
  "file_hash": "2db6d6c3f290b5b9c1627f670ab966c48406070e",
  "updated_at": "2025-12-26T17:34:22.464730",
  "symbols": {
    "struct_OpenAIProvider_14": {
      "name": "OpenAIProvider",
      "type": "struct",
      "start_line": 14,
      "end_line": 21,
      "content_hash": "e750fef6a2c6e3c4f9b8489cd37f50ec06f39e91",
      "content": "type OpenAIProvider struct {\n\tapiKey    string\n\tbaseURL   string\n\tclient    *openai.Client\n\tendpoints []Endpoint\n}\n\n// NewOpenAIProvider creates a new OpenAI provider instance using the official SDK",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_NewOpenAIProvider_22": {
      "name": "NewOpenAIProvider",
      "type": "function",
      "start_line": 22,
      "end_line": 31,
      "content_hash": "4dd1e9c442b2900b1a9bd88fbd75f2c6b6112b3a",
      "content": "func NewOpenAIProvider(apiKey string) Provider {\n\tclient := openai.NewClient(apiKey)\n\n\treturn &OpenAIProvider{\n\t\tapiKey:  apiKey,\n\t\tbaseURL: \"https://api.openai.com/v1\",\n\t\tclient:  client,\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_init_32": {
      "name": "init",
      "type": "function",
      "start_line": 32,
      "end_line": 35,
      "content_hash": "01f8921f6f9d4c720720d676cc1cd4905395d9ed",
      "content": "func init() {\n\tRegisterProvider(\"openai\", NewOpenAIProvider)\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_ValidateEndpoints_36": {
      "name": "ValidateEndpoints",
      "type": "method",
      "start_line": 36,
      "end_line": 80,
      "content_hash": "8ef8b030a63b4e83bc05e4a4c50a46da1b48bcb3",
      "content": "func (p *OpenAIProvider) ValidateEndpoints(ctx context.Context, verbose bool) error {\n\tendpoints := p.GetEndpoints()\n\n\t// Parallelize endpoint testing for better performance\n\tvar wg sync.WaitGroup\n\tvar mu sync.Mutex // Protect concurrent writes to endpoint status\n\n\tfor i := range endpoints {\n\t\twg.Add(1)\n\t\tgo func(endpoint *Endpoint) {\n\t\t\tdefer wg.Done()\n\n\t\t\tif verbose {\n\t\t\t\tmu.Lock()\n\t\t\t\tfmt.Printf(\"  Testing endpoint: %s %s\\n\", endpoint.Method, endpoint.Path)\n\t\t\t\tmu.Unlock()\n\t\t\t}\n\n\t\t\tstart := time.Now()\n\t\t\terr := p.testEndpoint(ctx, endpoint)\n\t\t\tlatency := time.Since(start)\n\n\t\t\tmu.Lock()\n\t\t\tendpoint.Latency = latency\n\t\t\tif err != nil {\n\t\t\t\tendpoint.Status = StatusFailed\n\t\t\t\tendpoint.Error = err.Error()\n\t\t\t\tif verbose {\n\t\t\t\t\tfmt.Printf(\"    \u2717 Failed: %v\\n\", err)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tendpoint.Status = StatusWorking\n\t\t\t\tif verbose {\n\t\t\t\t\tfmt.Printf(\"    \u2713 Working (%v)\\n\", latency)\n\t\t\t\t}\n\t\t\t}\n\t\t\tmu.Unlock()\n\t\t}(&endpoints[i])\n\t}\n\twg.Wait()\n\n\tp.endpoints = endpoints\n\treturn nil\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_ListModels_81": {
      "name": "ListModels",
      "type": "method",
      "start_line": 81,
      "end_line": 117,
      "content_hash": "33e234c39cee5fa37712a114cff6fe9f875685e7",
      "content": "func (p *OpenAIProvider) ListModels(ctx context.Context, verbose bool) ([]Model, error) {\n\tif verbose {\n\t\tfmt.Println(\"  Fetching available models from OpenAI API...\")\n\t}\n\n\t// Use the official SDK to list models\n\tmodelsList, err := p.client.ListModels(ctx)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to list models: %w\", err)\n\t}\n\n\tmodels := make([]Model, 0, len(modelsList.Models))\n\tfor _, apiModel := range modelsList.Models {\n\t\t// Only include models we can actually use (skip embeddings, whisper, tts, etc.)\n\t\tif !p.isUsableModel(apiModel.ID) {\n\t\t\tcontinue\n\t\t}\n\n\t\tmodel := Model{\n\t\t\tID:        apiModel.ID,\n\t\t\tName:      p.formatModelName(apiModel.ID),\n\t\t\tCreatedAt: time.Unix(apiModel.CreatedAt, 0).Format(time.RFC3339),\n\t\t}\n\n\t\t// Enrich with pricing and capabilities\n\t\tmodel = p.enrichModelDetails(model)\n\t\tmodels = append(models, model)\n\t}\n\n\tif verbose {\n\t\tfmt.Printf(\"  Found %d usable models\\n\", len(models))\n\t}\n\n\treturn models, nil\n}\n\n// isUsableModel filters out non-chat models",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_isUsableModel_118": {
      "name": "isUsableModel",
      "type": "method",
      "start_line": 118,
      "end_line": 144,
      "content_hash": "adf2795541f765b6fade1a7ce0a5f31dcbf7c4fa",
      "content": "func (p *OpenAIProvider) isUsableModel(modelID string) bool {\n\t// Skip embedding, whisper, TTS, moderation, and DALL-E models\n\tskipPrefixes := []string{\n\t\t\"text-embedding\", \"embedding\",\n\t\t\"whisper\", \"tts\",\n\t\t\"text-moderation\",\n\t\t\"dall-e\", \"davinci-edit\", \"babbage-edit\",\n\t}\n\n\tfor _, prefix := range skipPrefixes {\n\t\tif strings.HasPrefix(modelID, prefix) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\t// Skip old completion models that are deprecated\n\tif strings.Contains(modelID, \"davinci\") && !strings.Contains(modelID, \"gpt\") {\n\t\treturn false\n\t}\n\tif strings.Contains(modelID, \"curie\") || strings.Contains(modelID, \"babbage\") || strings.Contains(modelID, \"ada\") {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\n// formatModelName creates a human-readable name from model ID",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_formatModelName_145": {
      "name": "formatModelName",
      "type": "method",
      "start_line": 145,
      "end_line": 170,
      "content_hash": "b4e7a836158d40be91e837b58dcb01e56f85e170",
      "content": "func (p *OpenAIProvider) formatModelName(modelID string) string {\n\t// GPT-4 models\n\tif strings.HasPrefix(modelID, \"gpt-4o\") {\n\t\treturn \"GPT-4 Omni: \" + modelID\n\t}\n\tif strings.HasPrefix(modelID, \"gpt-4-turbo\") {\n\t\treturn \"GPT-4 Turbo: \" + modelID\n\t}\n\tif strings.HasPrefix(modelID, \"gpt-4\") {\n\t\treturn \"GPT-4: \" + modelID\n\t}\n\n\t// GPT-3.5 models\n\tif strings.HasPrefix(modelID, \"gpt-3.5\") {\n\t\treturn \"GPT-3.5: \" + modelID\n\t}\n\n\t// O-series models\n\tif strings.HasPrefix(modelID, \"o1\") || strings.HasPrefix(modelID, \"o3\") {\n\t\treturn \"O-Series Reasoning: \" + modelID\n\t}\n\n\treturn modelID\n}\n\n// enrichModelDetails adds pricing, context window, and capability information",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_enrichModelDetails_171": {
      "name": "enrichModelDetails",
      "type": "method",
      "start_line": 171,
      "end_line": 291,
      "content_hash": "a83b456e27d943ef771d78250b1fb5ba1e731bd6",
      "content": "func (p *OpenAIProvider) enrichModelDetails(model Model) Model {\n\t// Set common capabilities\n\tmodel.SupportsTools = true\n\tmodel.CanStream = true\n\n\t// Determine specific details based on model ID\n\tswitch {\n\t// GPT-4o models (latest, multimodal)\n\tcase strings.HasPrefix(model.ID, \"gpt-4o-mini\"):\n\t\tmodel.CostPer1MIn = 0.15\n\t\tmodel.CostPer1MOut = 0.60\n\t\tmodel.ContextWindow = 128000\n\t\tmodel.MaxTokens = 16384\n\t\tmodel.SupportsImages = true\n\t\tmodel.CanReason = false\n\t\tmodel.Categories = []string{\"chat\", \"fast\", \"cost-effective\", \"vision\"}\n\n\tcase strings.HasPrefix(model.ID, \"gpt-4o\"):\n\t\tmodel.CostPer1MIn = 2.50\n\t\tmodel.CostPer1MOut = 10.00\n\t\tmodel.ContextWindow = 128000\n\t\tmodel.MaxTokens = 16384\n\t\tmodel.SupportsImages = true\n\t\tmodel.CanReason = true\n\t\tmodel.Categories = []string{\"chat\", \"multimodal\", \"vision\", \"premium\"}\n\n\t// GPT-4 Turbo models\n\tcase strings.HasPrefix(model.ID, \"gpt-4-turbo\"):\n\t\tmodel.CostPer1MIn = 10.00\n\t\tmodel.CostPer1MOut = 30.00\n\t\tmodel.ContextWindow = 128000\n\t\tmodel.MaxTokens = 4096\n\t\tmodel.SupportsImages = strings.Contains(model.ID, \"vision\") || strings.Contains(model.ID, \"preview\")\n\t\tmodel.CanReason = true\n\t\tmodel.Categories = []string{\"chat\", \"premium\", \"legacy\"}\n\n\t// GPT-4 base models\n\tcase strings.HasPrefix(model.ID, \"gpt-4\"):\n\t\tmodel.CostPer1MIn = 30.00\n\t\tmodel.CostPer1MOut = 60.00\n\t\tmodel.ContextWindow = 8192\n\t\tmodel.MaxTokens = 4096\n\t\tmodel.SupportsImages = strings.Contains(model.ID, \"vision\")\n\t\tmodel.CanReason = true\n\t\tmodel.Categories = []string{\"chat\", \"premium\", \"legacy\"}\n\n\t// GPT-3.5 models\n\tcase strings.HasPrefix(model.ID, \"gpt-3.5-turbo\"):\n\t\tmodel.CostPer1MIn = 0.50\n\t\tmodel.CostPer1MOut = 1.50\n\t\tmodel.ContextWindow = 16385\n\t\tmodel.MaxTokens = 4096\n\t\tmodel.SupportsImages = false\n\t\tmodel.CanReason = false\n\t\tmodel.Categories = []string{\"chat\", \"cost-effective\", \"legacy\"}\n\n\tcase strings.HasPrefix(model.ID, \"gpt-3.5-turbo-instruct\"):\n\t\tmodel.CostPer1MIn = 1.50\n\t\tmodel.CostPer1MOut = 2.00\n\t\tmodel.ContextWindow = 4096\n\t\tmodel.MaxTokens = 4096\n\t\tmodel.SupportsImages = false\n\t\tmodel.CanReason = false\n\t\tmodel.Categories = []string{\"completion\", \"legacy\"}\n\n\t// O-series reasoning models\n\tcase strings.HasPrefix(model.ID, \"o1-mini\"):\n\t\tmodel.CostPer1MIn = 3.00\n\t\tmodel.CostPer1MOut = 12.00\n\t\tmodel.ContextWindow = 128000\n\t\tmodel.MaxTokens = 65536\n\t\tmodel.SupportsImages = false\n\t\tmodel.CanReason = true\n\t\tmodel.Categories = []string{\"reasoning\", \"problem-solving\", \"fast\"}\n\n\tcase strings.HasPrefix(model.ID, \"o1\"):\n\t\tmodel.CostPer1MIn = 15.00\n\t\tmodel.CostPer1MOut = 60.00\n\t\tmodel.ContextWindow = 128000\n\t\tmodel.MaxTokens = 100000\n\t\tmodel.SupportsImages = false\n\t\tmodel.CanReason = true\n\t\tmodel.Categories = []string{\"reasoning\", \"problem-solving\", \"premium\"}\n\n\tcase strings.HasPrefix(model.ID, \"o3\"):\n\t\tmodel.CostPer1MIn = 20.00\n\t\tmodel.CostPer1MOut = 80.00\n\t\tmodel.ContextWindow = 128000\n\t\tmodel.MaxTokens = 100000\n\t\tmodel.SupportsImages = true\n\t\tmodel.CanReason = true\n\t\tmodel.Categories = []string{\"reasoning\", \"multimodal\", \"premium\"}\n\n\tdefault:\n\t\t// Default values for unknown models\n\t\tmodel.CostPer1MIn = 1.00\n\t\tmodel.CostPer1MOut = 2.00\n\t\tmodel.ContextWindow = 8192\n\t\tmodel.MaxTokens = 4096\n\t\tmodel.SupportsImages = false\n\t\tmodel.CanReason = false\n\t\tmodel.Categories = []string{\"chat\"}\n\t}\n\n\t// Add capabilities metadata\n\tmodel.Capabilities = map[string]string{\n\t\t\"function_calling\": \"full\",\n\t\t\"json_mode\":        \"supported\",\n\t\t\"streaming\":        \"supported\",\n\t}\n\n\tif model.SupportsImages {\n\t\tmodel.Capabilities[\"vision\"] = \"high\"\n\t}\n\tif model.CanReason {\n\t\tmodel.Capabilities[\"reasoning\"] = \"advanced\"\n\t}\n\n\treturn model\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_GetCapabilities_292": {
      "name": "GetCapabilities",
      "type": "method",
      "start_line": 292,
      "end_line": 310,
      "content_hash": "602921ddd5afe962a374909959775b3034c3dce3",
      "content": "func (p *OpenAIProvider) GetCapabilities() ProviderCapabilities {\n\treturn ProviderCapabilities{\n\t\tSupportsChat:         true,\n\t\tSupportsFIM:          false,\n\t\tSupportsEmbeddings:   true,\n\t\tSupportsFineTuning:   true,\n\t\tSupportsAgents:       true,\n\t\tSupportsFileUpload:   true,\n\t\tSupportsStreaming:    true,\n\t\tSupportsJSONMode:     true,\n\t\tSupportsVision:       true,\n\t\tSupportsAudio:        true,\n\t\tSupportedParameters:  []string{\"temperature\", \"max_tokens\", \"top_p\", \"frequency_penalty\", \"presence_penalty\", \"stop\"},\n\t\tSecurityFeatures:     []string{\"moderation_endpoint\", \"content_filtering\"},\n\t\tMaxRequestsPerMinute: 500,\n\t\tMaxTokensPerRequest:  128000,\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_GetEndpoints_311": {
      "name": "GetEndpoints",
      "type": "method",
      "start_line": 311,
      "end_line": 330,
      "content_hash": "72a2fcdfee22afd3671197684fe4c7477e793083",
      "content": "func (p *OpenAIProvider) GetEndpoints() []Endpoint {\n\treturn []Endpoint{\n\t\t{\n\t\t\tPath:        \"/v1/chat/completions\",\n\t\t\tMethod:      \"POST\",\n\t\t\tDescription: \"Create a chat completion\",\n\t\t},\n\t\t{\n\t\t\tPath:        \"/v1/models\",\n\t\t\tMethod:      \"GET\",\n\t\t\tDescription: \"List available models\",\n\t\t},\n\t\t{\n\t\t\tPath:        \"/v1/embeddings\",\n\t\t\tMethod:      \"POST\",\n\t\t\tDescription: \"Create embeddings\",\n\t\t},\n\t}\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_TestModel_331": {
      "name": "TestModel",
      "type": "method",
      "start_line": 331,
      "end_line": 358,
      "content_hash": "804670fec850cf01bfebea14d1ec6902eb2100e4",
      "content": "func (p *OpenAIProvider) TestModel(ctx context.Context, modelID string, verbose bool) error {\n\tif verbose {\n\t\tfmt.Printf(\"  Testing model: %s\\n\", modelID)\n\t}\n\n\t// Use the official SDK to send a test message\n\tresp, err := p.client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{\n\t\tModel:     modelID,\n\t\tMaxTokens: 10,\n\t\tMessages: []openai.ChatCompletionMessage{\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleUser,\n\t\t\t\tContent: \"Say 'test successful' in 2 words\",\n\t\t\t},\n\t\t},\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"model test failed: %w\", err)\n\t}\n\n\tif verbose && len(resp.Choices) > 0 {\n\t\tfmt.Printf(\"    Response: %s\\n\", resp.Choices[0].Message.Content)\n\t\tfmt.Printf(\"    \u2713 Model is working\\n\")\n\t}\n\n\treturn nil\n}\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_testEndpoint_359": {
      "name": "testEndpoint",
      "type": "method",
      "start_line": 359,
      "end_line": 391,
      "content_hash": "32707b9c2b06687dfebf1a868671ceb10222fa3d",
      "content": "func (p *OpenAIProvider) testEndpoint(ctx context.Context, endpoint *Endpoint) error {\n\tswitch endpoint.Path {\n\tcase \"/v1/models\":\n\t\t// Test models list endpoint\n\t\t_, err := p.client.ListModels(ctx)\n\t\treturn err\n\n\tcase \"/v1/chat/completions\":\n\t\t// Test chat endpoint with minimal request\n\t\t_, err := p.client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{\n\t\t\tModel:     \"gpt-3.5-turbo\",\n\t\t\tMaxTokens: 5,\n\t\t\tMessages: []openai.ChatCompletionMessage{\n\t\t\t\t{\n\t\t\t\t\tRole:    openai.ChatMessageRoleUser,\n\t\t\t\t\tContent: \"Hi\",\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\treturn err\n\n\tcase \"/v1/embeddings\":\n\t\t// Test embeddings endpoint\n\t\t_, err := p.client.CreateEmbeddings(ctx, openai.EmbeddingRequest{\n\t\t\tModel: openai.AdaEmbeddingV2,\n\t\t\tInput: []string{\"test\"},\n\t\t})\n\t\treturn err\n\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown endpoint: %s\", endpoint.Path)\n\t}\n}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}