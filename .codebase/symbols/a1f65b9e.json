{
  "file_path": "/work/context-engine/scripts/watch_index.py",
  "file_hash": "6767812a2f01018def917b2fec69bdccb5ddcbe4",
  "updated_at": "2025-12-26T17:34:22.699438",
  "symbols": {
    "class__SkipUnchanged_71": {
      "name": "_SkipUnchanged",
      "type": "class",
      "start_line": 71,
      "end_line": 73,
      "content_hash": "08e1cfa281374725fd8967926aa6112b486c27f5",
      "content": "class _SkipUnchanged(Exception):\n    \"\"\"Sentinel exception to skip unchanged files in the watch loop.\"\"\"\n    pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__detect_repo_for_file_76": {
      "name": "_detect_repo_for_file",
      "type": "function",
      "start_line": 76,
      "end_line": 84,
      "content_hash": "6d0b26bea7c72b992714bef40d54a3d86f778385",
      "content": "def _detect_repo_for_file(file_path: Path) -> Optional[Path]:\n    \"\"\"Detect repository root for a file under WATCH root.\"\"\"\n    try:\n        rel_path = file_path.resolve().relative_to(ROOT.resolve())\n    except Exception:\n        return None\n    if not rel_path.parts:\n        return ROOT\n    return ROOT / rel_path.parts[0]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_collection_for_repo_87": {
      "name": "_get_collection_for_repo",
      "type": "function",
      "start_line": 87,
      "end_line": 160,
      "content_hash": "fe3b7374aaa34cebbf13ac29585cf822e9ce1d29",
      "content": "def _get_collection_for_repo(repo_path: Path) -> str:\n    \"\"\"Resolve Qdrant collection for a repo, with logical_repo_id-aware reuse.\n\n    In multi-repo mode, prefer reusing an existing canonical collection that has\n    already been associated with this logical repository (same git common dir)\n    by consulting workspace_state. Falls back to the legacy per-repo hashed\n    collection naming when no mapping exists.\n    \"\"\"\n    default_coll = os.environ.get(\"COLLECTION_NAME\", \"codebase\")\n    try:\n        repo_name = _extract_repo_name_from_path(str(repo_path))\n    except Exception:\n        repo_name = None\n\n    # Multi-repo: always honor explicit serving/qdrant collection from state when present.\n    # This is required for staging/migration workflows (e.g. *_old repos) even when\n    # logical-repo reuse is disabled.\n    if repo_name and is_multi_repo_mode():\n        workspace_root = os.environ.get(\"WORKSPACE_PATH\") or os.environ.get(\"WATCH_ROOT\") or \"/work\"\n        try:\n            ws_root_path = Path(workspace_root).resolve()\n        except Exception:\n            ws_root_path = Path(workspace_root)\n        ws_path = str((ws_root_path / repo_name).resolve())\n\n        state: Dict[str, Any]\n        try:\n            state = get_workspace_state(ws_path, repo_name) or {}\n        except Exception:\n            state = {}\n\n        if isinstance(state, dict):\n            serving_coll = state.get(\"serving_collection\") or state.get(\"qdrant_collection\")\n            if isinstance(serving_coll, str) and serving_coll:\n                return serving_coll\n\n        # Multi-repo: try to reuse a canonical collection based on logical_repo_id\n        if logical_repo_reuse_enabled():\n            try:\n                state = ensure_logical_repo_id(state, ws_path)\n            except Exception:\n                pass\n\n            lrid = state.get(\"logical_repo_id\")\n            if isinstance(lrid, str) and lrid:\n                coll: Optional[str]\n                try:\n                    coll = find_collection_for_logical_repo(lrid, search_root=str(ws_root_path))\n                except Exception:\n                    coll = None\n                if isinstance(coll, str) and coll:\n                    try:\n                        update_workspace_state(\n                            workspace_path=ws_path,\n                            updates={\"qdrant_collection\": coll, \"logical_repo_id\": lrid},\n                            repo_name=repo_name,\n                        )\n                    except Exception:\n                        pass\n                    return coll\n\n        # Legacy behaviour: derive per-repo collection name\n        try:\n            return get_collection_name(repo_name)\n        except Exception:\n            return default_coll\n\n    # Single-repo mode or repo_name detection failed: use existing helpers/env\n    try:\n        if repo_name:\n            return get_collection_name(repo_name)\n    except Exception:\n        pass\n    return default_coll",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_collection_for_file_163": {
      "name": "_get_collection_for_file",
      "type": "function",
      "start_line": 163,
      "end_line": 169,
      "content_hash": "76dfd94bb468531462b8ffbd09421a16e681e500",
      "content": "def _get_collection_for_file(file_path: Path) -> str:\n    if not is_multi_repo_mode():\n        return os.environ.get(\"COLLECTION_NAME\", \"codebase\")\n    repo_path = _detect_repo_for_file(file_path)\n    if repo_path is not None:\n        return _get_collection_for_repo(repo_path)\n    return os.environ.get(\"COLLECTION_NAME\", \"codebase\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_ChangeQueue_172": {
      "name": "ChangeQueue",
      "type": "class",
      "start_line": 172,
      "end_line": 235,
      "content_hash": "f978e5ece1de02546379208dd4287039100a04fa",
      "content": "class ChangeQueue:\n    def __init__(self, process_cb):\n        self._lock = threading.Lock()\n        self._paths: Set[Path] = set()\n        self._pending: Set[Path] = set()\n        self._timer: threading.Timer | None = None\n        self._process_cb = process_cb\n        # Serialize processing to avoid concurrent use of TextEmbedding/QdrantClient\n        self._processing_lock = threading.Lock()\n\n    def add(self, p: Path):\n        with self._lock:\n            self._paths.add(p)\n            if self._timer is not None:\n                try:\n                    self._timer.cancel()\n                except Exception as e:\n                    logger.error(\n                        \"Failed to cancel timer in ChangeQueue.add\",\n                        extra={\"error\": str(e)},\n                    )\n            self._timer = threading.Timer(DELAY_SECS, self._flush)\n            self._timer.daemon = True\n            self._timer.start()\n\n    def _flush(self):\n        # Grab current batch\n        with self._lock:\n            paths = list(self._paths)\n            self._paths.clear()\n            self._timer = None\n\n        # Try to run the processor exclusively; if busy, queue and return\n        if not self._processing_lock.acquire(blocking=False):\n            with self._lock:\n                self._pending.update(paths)\n                if self._timer is None:\n                    # schedule a follow-up flush to pick up pending when free\n                    self._timer = threading.Timer(DELAY_SECS, self._flush)\n                    self._timer.daemon = True\n                    self._timer.start()\n            return\n        try:\n            # Per-file locking in index_single_file handles indexer/watcher coordination\n            todo = paths\n            while True:\n                try:\n                    self._process_cb(list(todo))\n                except Exception as e:\n                    try:\n                        print(f\"[watcher_error] processing batch failed: {e}\")\n                    except Exception as inner_e:  # pragma: no cover - logging fallback\n                        logger.error(\n                            \"Exception in ChangeQueue._flush during batch processing\",\n                            extra={\"error\": str(inner_e)},\n                        )\n                # drain any pending accumulated during processing\n                with self._lock:\n                    if not self._pending:\n                        break\n                    todo = list(self._pending)\n                    self._pending.clear()\n        finally:\n            self._processing_lock.release()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___173": {
      "name": "__init__",
      "type": "method",
      "start_line": 173,
      "end_line": 180,
      "content_hash": "aba03eebdd3e44ddcd8363287261a6f168403855",
      "content": "    def __init__(self, process_cb):\n        self._lock = threading.Lock()\n        self._paths: Set[Path] = set()\n        self._pending: Set[Path] = set()\n        self._timer: threading.Timer | None = None\n        self._process_cb = process_cb\n        # Serialize processing to avoid concurrent use of TextEmbedding/QdrantClient\n        self._processing_lock = threading.Lock()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_add_182": {
      "name": "add",
      "type": "method",
      "start_line": 182,
      "end_line": 195,
      "content_hash": "498a6eaefde18e864526fc6f9348a29f241b9dfd",
      "content": "    def add(self, p: Path):\n        with self._lock:\n            self._paths.add(p)\n            if self._timer is not None:\n                try:\n                    self._timer.cancel()\n                except Exception as e:\n                    logger.error(\n                        \"Failed to cancel timer in ChangeQueue.add\",\n                        extra={\"error\": str(e)},\n                    )\n            self._timer = threading.Timer(DELAY_SECS, self._flush)\n            self._timer.daemon = True\n            self._timer.start()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__flush_197": {
      "name": "_flush",
      "type": "method",
      "start_line": 197,
      "end_line": 235,
      "content_hash": "78d2efc64dbc8a2cbea6fd52f40e43683ae7ad3a",
      "content": "    def _flush(self):\n        # Grab current batch\n        with self._lock:\n            paths = list(self._paths)\n            self._paths.clear()\n            self._timer = None\n\n        # Try to run the processor exclusively; if busy, queue and return\n        if not self._processing_lock.acquire(blocking=False):\n            with self._lock:\n                self._pending.update(paths)\n                if self._timer is None:\n                    # schedule a follow-up flush to pick up pending when free\n                    self._timer = threading.Timer(DELAY_SECS, self._flush)\n                    self._timer.daemon = True\n                    self._timer.start()\n            return\n        try:\n            # Per-file locking in index_single_file handles indexer/watcher coordination\n            todo = paths\n            while True:\n                try:\n                    self._process_cb(list(todo))\n                except Exception as e:\n                    try:\n                        print(f\"[watcher_error] processing batch failed: {e}\")\n                    except Exception as inner_e:  # pragma: no cover - logging fallback\n                        logger.error(\n                            \"Exception in ChangeQueue._flush during batch processing\",\n                            extra={\"error\": str(inner_e)},\n                        )\n                # drain any pending accumulated during processing\n                with self._lock:\n                    if not self._pending:\n                        break\n                    todo = list(self._pending)\n                    self._pending.clear()\n        finally:\n            self._processing_lock.release()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "class_IndexHandler_238": {
      "name": "IndexHandler",
      "type": "class",
      "start_line": 238,
      "end_line": 538,
      "content_hash": "70dcddbefbe62d6e1927b531d5c3afc2b0cf19ab",
      "content": "class IndexHandler(FileSystemEventHandler):\n    def __init__(\n        self,\n        root: Path,\n        queue: ChangeQueue,\n        client: Optional[QdrantClient],\n        default_collection: Optional[str] = None,\n        *,\n        collection: Optional[str] = None,\n    ):\n        super().__init__()\n        self.root = root\n        self.queue = queue\n        self.client = client\n        resolved_collection = collection if collection is not None else default_collection\n        self.default_collection = resolved_collection\n        # In multi-repo mode, per-file collections are resolved via _get_collection_for_file.\n        # Avoid using a root-level default collection (e.g., \"/work-<hash>\") for data ops.\n        if is_multi_repo_mode():\n            self.collection = None\n        else:\n            self.collection = resolved_collection\n        self.excl = idx._Excluder(root)\n        # Track ignore file for live reloads\n        try:\n            ig_name = os.environ.get(\"QDRANT_IGNORE_FILE\", \".qdrantignore\")\n            self._ignore_path = (self.root / ig_name).resolve()\n        except (OSError, ValueError) as e:\n            try:\n                print(f\"[ignore_file] Could not resolve ignore file path: {e}\")\n            except Exception:\n                pass\n            self._ignore_path = None\n        try:\n            self._ignore_mtime = (\n                self._ignore_path.stat().st_mtime\n                if self._ignore_path and self._ignore_path.exists()\n                else 0.0\n            )\n        except Exception:\n            self._ignore_mtime = 0.0\n\n    def _maybe_reload_excluder(self):\n        try:\n            if not self._ignore_path:\n                return\n            cur = (\n                self._ignore_path.stat().st_mtime if self._ignore_path.exists() else 0.0\n            )\n            if cur != self._ignore_mtime:\n                self.excl = idx._Excluder(self.root)\n                self._ignore_mtime = cur\n                try:\n                    print(f\"[ignore_reload] reloaded patterns from {self._ignore_path}\")\n                except Exception:\n                    pass\n        except Exception:\n            pass\n\n    def _maybe_enqueue(self, src_path: str):\n        # Refresh ignore patterns if the file changed\n        self._maybe_reload_excluder()\n        p = Path(src_path)\n        try:\n            p = p.resolve()\n        except Exception:\n            return\n        # skip directories\n        if p.is_dir():\n            return\n        # ensure file is under root\n        try:\n            rel = p.resolve().relative_to(self.root.resolve())\n        except ValueError:\n            return\n\n        try:\n            if _get_global_state_dir is not None:\n                global_state_dir = _get_global_state_dir()\n                if p.is_relative_to(global_state_dir):\n                    return\n        except (OSError, ValueError):\n            pass\n\n        if any(part == \".codebase\" for part in p.parts):\n            return\n\n        # Git history manifests are handled by a separate ingestion pipeline and should still\n        # be processed even when .remote-git is excluded from code indexing.\n        if any(part == \".remote-git\" for part in p.parts) and p.suffix.lower() == \".json\":\n            self.queue.add(p)\n            return\n\n        # directory-level excludes (parent dir)\n        rel_dir = \"/\" + str(rel.parent).replace(os.sep, \"/\")\n        if rel_dir == \"/.\":\n            rel_dir = \"/\"\n        if self.excl.exclude_dir(rel_dir):\n            return\n        # only code files (check extension AND extensionless files like Dockerfile)\n        if not idx.is_indexable_file(p):\n            return\n        # file-level excludes\n        relf = (rel_dir.rstrip(\"/\") + \"/\" + p.name).replace(\"//\", \"/\")\n        if self.excl.exclude_file(relf):\n            return\n        self.queue.add(p)\n\n    def on_modified(self, event):\n        if not event.is_directory:\n            self._maybe_enqueue(event.src_path)\n\n    def on_created(self, event):\n        if not event.is_directory:\n            self._maybe_enqueue(event.src_path)\n\n    def on_deleted(self, event):\n        if event.is_directory:\n            return\n        try:\n            p = Path(event.src_path).resolve()\n        except Exception:\n            return\n        if any(part == \".codebase\" for part in p.parts):\n            return\n        # Only attempt deletion for code files we would have indexed\n        if not idx.is_indexable_file(p):\n            return\n        if self.client is not None:\n            try:\n                if is_multi_repo_mode():\n                    collection = _get_collection_for_file(p)\n                else:\n                    collection = self.collection or _get_collection_for_file(p)\n                idx.delete_points_by_path(self.client, collection, str(p))\n                print(f\"[deleted] {p} -> {collection}\")\n            except Exception:\n                pass\n        else:\n            print(f\"File deletion detected: {p}\")\n\n        try:\n            repo_path = _detect_repo_for_file(p)\n            if repo_path:\n                repo_name = _extract_repo_name_from_path(str(repo_path))\n                remove_cached_file(str(p), repo_name)\n\n                # Remove symbol cache entry\n                try:\n                    from scripts.workspace_state import remove_cached_symbols\n                    remove_cached_symbols(str(p))\n                    print(f\"[deleted_symbol_cache] {p}\")\n                except Exception as e:\n                    print(f\"[symbol_cache_delete_error] {p}: {e}\")\n            else:\n                root_repo_name = _extract_repo_name_from_path(str(self.root))\n                remove_cached_file(str(p), root_repo_name)\n\n                # Remove symbol cache entry (single repo mode)\n                try:\n                    from scripts.workspace_state import remove_cached_symbols\n                    remove_cached_symbols(str(p))\n                    print(f\"[deleted_symbol_cache] {p}\")\n                except Exception as e:\n                    print(f\"[symbol_cache_delete_error] {p}: {e}\")\n        except Exception:\n            pass\n\n        try:\n            repo_path = _detect_repo_for_file(p) or self.root\n            _log_activity(str(repo_path), \"deleted\", p)\n        except Exception as e:\n            try:\n                print(f\"[delete_error] {p}: {e}\")\n            except Exception:\n                pass\n\n    def on_moved(self, event):\n        if event.is_directory:\n            return\n        # Attempt optimized rename when content unchanged; else fallback to reindex\n        try:\n            src = Path(event.src_path).resolve()\n            dest = Path(event.dest_path).resolve()\n        except Exception:\n            return\n        # Only react to code files (including extensionless like Dockerfile)\n        if not idx.is_indexable_file(dest) and not idx.is_indexable_file(src):\n            return\n        # If destination directory is ignored, treat as simple deletion\n        try:\n            rel_dir = \"/\" + str(\n                dest.parent.resolve().relative_to(self.root.resolve())\n            ).replace(os.sep, \"/\")\n            if rel_dir == \"/.\":\n                rel_dir = \"/\"\n            if self.excl.exclude_dir(rel_dir):\n                if idx.is_indexable_file(src):\n                    try:\n                        if is_multi_repo_mode():\n                            coll = _get_collection_for_file(src)\n                        else:\n                            coll = self.collection or _get_collection_for_file(src)\n                        idx.delete_points_by_path(\n                            self.client, coll, str(src)\n                        )\n                        print(f\"[moved:ignored_dest_deleted_src] {src} -> {dest}\")\n                        try:\n                            src_repo_path = _detect_repo_for_file(src)\n                            src_repo_name = (\n                                _extract_repo_name_from_path(str(src_repo_path))\n                                if src_repo_path is not None\n                                else None\n                            )\n                            remove_cached_file(str(src), src_repo_name)\n                        except Exception:\n                            pass\n\n                    except Exception:\n                        pass\n                return\n        except Exception:\n            pass\n        src_collection = _get_collection_for_file(src)\n        dest_collection = _get_collection_for_file(dest)\n        is_cross_collection = src_collection != dest_collection\n        if is_cross_collection:\n            print(f\"[cross_collection_move] {src} -> {dest}\")\n\n        moved_count = -1\n        renamed_hash: str | None = None\n        if self.client is not None:\n            try:\n                moved_count, renamed_hash = _rename_in_store(\n                    self.client, src_collection, src, dest, dest_collection\n                )\n            except Exception:\n                moved_count, renamed_hash = -1, None\n        if moved_count and moved_count > 0:\n            try:\n                print(\n                    f\"[moved] {src} -> {dest} ({moved_count} chunk(s) relinked)\"\n                )\n                src_repo_path = _detect_repo_for_file(src)\n                dest_repo_path = _detect_repo_for_file(dest)\n                src_repo_name = (\n                    _extract_repo_name_from_path(str(src_repo_path))\n                    if src_repo_path is not None\n                    else None\n                )\n                dest_repo_name = (\n                    _extract_repo_name_from_path(str(dest_repo_path))\n                    if dest_repo_path is not None\n                    else None\n                )\n                src_hash = \"\"\n                if src_repo_name:\n                    src_hash = get_cached_file_hash(str(src), src_repo_name)\n                    remove_cached_file(str(src), src_repo_name)\n                if not src_hash and renamed_hash:\n                    src_hash = renamed_hash\n                if dest_repo_name and src_hash:\n                    set_cached_file_hash(\n                        str(dest), src_hash, dest_repo_name\n                    )\n            except Exception:\n                pass\n            try:\n                _log_activity(\n                    str(dest_repo_path or self.root),\n                    \"moved\",\n                    dest,\n                    {\"from\": str(src), \"chunks\": int(moved_count)},\n                )\n            except Exception:\n                pass\n            return\n        if self.client is not None:\n            try:\n                if idx.is_indexable_file(src):\n                    try:\n                        idx.delete_points_by_path(self.client, src_collection, str(src))\n                    except Exception:\n                        # In multi-repo mode, avoid falling back to any root-level collection.\n                        if (not is_multi_repo_mode()) and self.collection:\n                            idx.delete_points_by_path(\n                                self.client,\n                                self.collection,\n                                str(src),\n                            )\n                        else:\n                            raise\n                    print(f\"[moved:deleted_src] {src}\")\n            except Exception:\n                pass\n        else:\n            print(f\"[remote_mode] Move detected: {src} -> {dest}\")\n        try:\n            self._maybe_enqueue(str(dest))\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method___init___239": {
      "name": "__init__",
      "type": "method",
      "start_line": 239,
      "end_line": 278,
      "content_hash": "f72c6da484e55da91de12452fd49ae2c3118ae82",
      "content": "    def __init__(\n        self,\n        root: Path,\n        queue: ChangeQueue,\n        client: Optional[QdrantClient],\n        default_collection: Optional[str] = None,\n        *,\n        collection: Optional[str] = None,\n    ):\n        super().__init__()\n        self.root = root\n        self.queue = queue\n        self.client = client\n        resolved_collection = collection if collection is not None else default_collection\n        self.default_collection = resolved_collection\n        # In multi-repo mode, per-file collections are resolved via _get_collection_for_file.\n        # Avoid using a root-level default collection (e.g., \"/work-<hash>\") for data ops.\n        if is_multi_repo_mode():\n            self.collection = None\n        else:\n            self.collection = resolved_collection\n        self.excl = idx._Excluder(root)\n        # Track ignore file for live reloads\n        try:\n            ig_name = os.environ.get(\"QDRANT_IGNORE_FILE\", \".qdrantignore\")\n            self._ignore_path = (self.root / ig_name).resolve()\n        except (OSError, ValueError) as e:\n            try:\n                print(f\"[ignore_file] Could not resolve ignore file path: {e}\")\n            except Exception:\n                pass\n            self._ignore_path = None\n        try:\n            self._ignore_mtime = (\n                self._ignore_path.stat().st_mtime\n                if self._ignore_path and self._ignore_path.exists()\n                else 0.0\n            )\n        except Exception:\n            self._ignore_mtime = 0.0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__maybe_reload_excluder_280": {
      "name": "_maybe_reload_excluder",
      "type": "method",
      "start_line": 280,
      "end_line": 295,
      "content_hash": "01d52f7fd40b7e4dc9c8264a3cc1c14eb89979d0",
      "content": "    def _maybe_reload_excluder(self):\n        try:\n            if not self._ignore_path:\n                return\n            cur = (\n                self._ignore_path.stat().st_mtime if self._ignore_path.exists() else 0.0\n            )\n            if cur != self._ignore_mtime:\n                self.excl = idx._Excluder(self.root)\n                self._ignore_mtime = cur\n                try:\n                    print(f\"[ignore_reload] reloaded patterns from {self._ignore_path}\")\n                except Exception:\n                    pass\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method__maybe_enqueue_297": {
      "name": "_maybe_enqueue",
      "type": "method",
      "start_line": 297,
      "end_line": 344,
      "content_hash": "caeedede3aef9652fc52c4f3abf73de54317b244",
      "content": "    def _maybe_enqueue(self, src_path: str):\n        # Refresh ignore patterns if the file changed\n        self._maybe_reload_excluder()\n        p = Path(src_path)\n        try:\n            p = p.resolve()\n        except Exception:\n            return\n        # skip directories\n        if p.is_dir():\n            return\n        # ensure file is under root\n        try:\n            rel = p.resolve().relative_to(self.root.resolve())\n        except ValueError:\n            return\n\n        try:\n            if _get_global_state_dir is not None:\n                global_state_dir = _get_global_state_dir()\n                if p.is_relative_to(global_state_dir):\n                    return\n        except (OSError, ValueError):\n            pass\n\n        if any(part == \".codebase\" for part in p.parts):\n            return\n\n        # Git history manifests are handled by a separate ingestion pipeline and should still\n        # be processed even when .remote-git is excluded from code indexing.\n        if any(part == \".remote-git\" for part in p.parts) and p.suffix.lower() == \".json\":\n            self.queue.add(p)\n            return\n\n        # directory-level excludes (parent dir)\n        rel_dir = \"/\" + str(rel.parent).replace(os.sep, \"/\")\n        if rel_dir == \"/.\":\n            rel_dir = \"/\"\n        if self.excl.exclude_dir(rel_dir):\n            return\n        # only code files (check extension AND extensionless files like Dockerfile)\n        if not idx.is_indexable_file(p):\n            return\n        # file-level excludes\n        relf = (rel_dir.rstrip(\"/\") + \"/\" + p.name).replace(\"//\", \"/\")\n        if self.excl.exclude_file(relf):\n            return\n        self.queue.add(p)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_on_modified_346": {
      "name": "on_modified",
      "type": "method",
      "start_line": 346,
      "end_line": 348,
      "content_hash": "5220a39176cc3a7e4c2c5f1235f53425d6094fd9",
      "content": "    def on_modified(self, event):\n        if not event.is_directory:\n            self._maybe_enqueue(event.src_path)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_on_created_350": {
      "name": "on_created",
      "type": "method",
      "start_line": 350,
      "end_line": 352,
      "content_hash": "d519b8d0385d3b54edf2423d0919a581598c7789",
      "content": "    def on_created(self, event):\n        if not event.is_directory:\n            self._maybe_enqueue(event.src_path)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_on_deleted_354": {
      "name": "on_deleted",
      "type": "method",
      "start_line": 354,
      "end_line": 413,
      "content_hash": "c5f8ce765204ba704e21355164669b002606b21b",
      "content": "    def on_deleted(self, event):\n        if event.is_directory:\n            return\n        try:\n            p = Path(event.src_path).resolve()\n        except Exception:\n            return\n        if any(part == \".codebase\" for part in p.parts):\n            return\n        # Only attempt deletion for code files we would have indexed\n        if not idx.is_indexable_file(p):\n            return\n        if self.client is not None:\n            try:\n                if is_multi_repo_mode():\n                    collection = _get_collection_for_file(p)\n                else:\n                    collection = self.collection or _get_collection_for_file(p)\n                idx.delete_points_by_path(self.client, collection, str(p))\n                print(f\"[deleted] {p} -> {collection}\")\n            except Exception:\n                pass\n        else:\n            print(f\"File deletion detected: {p}\")\n\n        try:\n            repo_path = _detect_repo_for_file(p)\n            if repo_path:\n                repo_name = _extract_repo_name_from_path(str(repo_path))\n                remove_cached_file(str(p), repo_name)\n\n                # Remove symbol cache entry\n                try:\n                    from scripts.workspace_state import remove_cached_symbols\n                    remove_cached_symbols(str(p))\n                    print(f\"[deleted_symbol_cache] {p}\")\n                except Exception as e:\n                    print(f\"[symbol_cache_delete_error] {p}: {e}\")\n            else:\n                root_repo_name = _extract_repo_name_from_path(str(self.root))\n                remove_cached_file(str(p), root_repo_name)\n\n                # Remove symbol cache entry (single repo mode)\n                try:\n                    from scripts.workspace_state import remove_cached_symbols\n                    remove_cached_symbols(str(p))\n                    print(f\"[deleted_symbol_cache] {p}\")\n                except Exception as e:\n                    print(f\"[symbol_cache_delete_error] {p}: {e}\")\n        except Exception:\n            pass\n\n        try:\n            repo_path = _detect_repo_for_file(p) or self.root\n            _log_activity(str(repo_path), \"deleted\", p)\n        except Exception as e:\n            try:\n                print(f\"[delete_error] {p}: {e}\")\n            except Exception:\n                pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_on_moved_415": {
      "name": "on_moved",
      "type": "method",
      "start_line": 415,
      "end_line": 538,
      "content_hash": "eba6dec345a3b7e96109f4d96cb64d2e429b33eb",
      "content": "    def on_moved(self, event):\n        if event.is_directory:\n            return\n        # Attempt optimized rename when content unchanged; else fallback to reindex\n        try:\n            src = Path(event.src_path).resolve()\n            dest = Path(event.dest_path).resolve()\n        except Exception:\n            return\n        # Only react to code files (including extensionless like Dockerfile)\n        if not idx.is_indexable_file(dest) and not idx.is_indexable_file(src):\n            return\n        # If destination directory is ignored, treat as simple deletion\n        try:\n            rel_dir = \"/\" + str(\n                dest.parent.resolve().relative_to(self.root.resolve())\n            ).replace(os.sep, \"/\")\n            if rel_dir == \"/.\":\n                rel_dir = \"/\"\n            if self.excl.exclude_dir(rel_dir):\n                if idx.is_indexable_file(src):\n                    try:\n                        if is_multi_repo_mode():\n                            coll = _get_collection_for_file(src)\n                        else:\n                            coll = self.collection or _get_collection_for_file(src)\n                        idx.delete_points_by_path(\n                            self.client, coll, str(src)\n                        )\n                        print(f\"[moved:ignored_dest_deleted_src] {src} -> {dest}\")\n                        try:\n                            src_repo_path = _detect_repo_for_file(src)\n                            src_repo_name = (\n                                _extract_repo_name_from_path(str(src_repo_path))\n                                if src_repo_path is not None\n                                else None\n                            )\n                            remove_cached_file(str(src), src_repo_name)\n                        except Exception:\n                            pass\n\n                    except Exception:\n                        pass\n                return\n        except Exception:\n            pass\n        src_collection = _get_collection_for_file(src)\n        dest_collection = _get_collection_for_file(dest)\n        is_cross_collection = src_collection != dest_collection\n        if is_cross_collection:\n            print(f\"[cross_collection_move] {src} -> {dest}\")\n\n        moved_count = -1\n        renamed_hash: str | None = None\n        if self.client is not None:\n            try:\n                moved_count, renamed_hash = _rename_in_store(\n                    self.client, src_collection, src, dest, dest_collection\n                )\n            except Exception:\n                moved_count, renamed_hash = -1, None\n        if moved_count and moved_count > 0:\n            try:\n                print(\n                    f\"[moved] {src} -> {dest} ({moved_count} chunk(s) relinked)\"\n                )\n                src_repo_path = _detect_repo_for_file(src)\n                dest_repo_path = _detect_repo_for_file(dest)\n                src_repo_name = (\n                    _extract_repo_name_from_path(str(src_repo_path))\n                    if src_repo_path is not None\n                    else None\n                )\n                dest_repo_name = (\n                    _extract_repo_name_from_path(str(dest_repo_path))\n                    if dest_repo_path is not None\n                    else None\n                )\n                src_hash = \"\"\n                if src_repo_name:\n                    src_hash = get_cached_file_hash(str(src), src_repo_name)\n                    remove_cached_file(str(src), src_repo_name)\n                if not src_hash and renamed_hash:\n                    src_hash = renamed_hash\n                if dest_repo_name and src_hash:\n                    set_cached_file_hash(\n                        str(dest), src_hash, dest_repo_name\n                    )\n            except Exception:\n                pass\n            try:\n                _log_activity(\n                    str(dest_repo_path or self.root),\n                    \"moved\",\n                    dest,\n                    {\"from\": str(src), \"chunks\": int(moved_count)},\n                )\n            except Exception:\n                pass\n            return\n        if self.client is not None:\n            try:\n                if idx.is_indexable_file(src):\n                    try:\n                        idx.delete_points_by_path(self.client, src_collection, str(src))\n                    except Exception:\n                        # In multi-repo mode, avoid falling back to any root-level collection.\n                        if (not is_multi_repo_mode()) and self.collection:\n                            idx.delete_points_by_path(\n                                self.client,\n                                self.collection,\n                                str(src),\n                            )\n                        else:\n                            raise\n                    print(f\"[moved:deleted_src] {src}\")\n            except Exception:\n                pass\n        else:\n            print(f\"[remote_mode] Move detected: {src} -> {dest}\")\n        try:\n            self._maybe_enqueue(str(dest))\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__set_status_indexing_542": {
      "name": "_set_status_indexing",
      "type": "function",
      "start_line": 542,
      "end_line": 554,
      "content_hash": "e0a6c0137f0dce53636e3e0f84e8d227e7af8acf",
      "content": "def _set_status_indexing(workspace_path: str, total_files: int) -> None:\n    try:\n        repo_name = _extract_repo_name_from_path(workspace_path)\n        update_indexing_status(\n            repo_name=repo_name,\n            status={\n                \"state\": \"indexing\",\n                \"started_at\": datetime.now().isoformat(),\n                \"progress\": {\"files_processed\": 0, \"total_files\": int(total_files)},\n            },\n        )\n    except Exception:\n        pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__update_progress_557": {
      "name": "_update_progress",
      "type": "function",
      "start_line": 557,
      "end_line": 579,
      "content_hash": "1254285be20513e5136b6964327411a9203b036e",
      "content": "def _update_progress(\n    workspace_path: str,\n    started_at: str,\n    processed: int,\n    total: int,\n    current_file: Path | None,\n) -> None:\n    try:\n        repo_name = _extract_repo_name_from_path(workspace_path)\n        update_indexing_status(\n            repo_name=repo_name,\n            status={\n                \"state\": \"indexing\",\n                \"started_at\": started_at,\n                \"progress\": {\n                    \"files_processed\": int(processed),\n                    \"total_files\": int(total),\n                    \"current_file\": str(current_file) if current_file else None,\n                },\n            },\n        )\n    except Exception:\n        pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__log_activity_582": {
      "name": "_log_activity",
      "type": "function",
      "start_line": 582,
      "end_line": 600,
      "content_hash": "4318946b7b3dd94c8769595b05fd72bcae8a14c8",
      "content": "def _log_activity(\n    workspace_path: str, action: str, file_path: Path, details: dict | None = None\n) -> None:\n    try:\n        repo_name = _extract_repo_name_from_path(workspace_path)\n        from scripts.workspace_state import log_activity\n\n        valid_actions = {\"indexed\", \"deleted\", \"skipped\", \"scan-completed\", \"initialized\", \"moved\"}\n        if action not in valid_actions:\n            action = \"indexed\"\n\n        log_activity(\n            repo_name=repo_name,\n            action=action,  # type: ignore[arg-type]\n            file_path=str(file_path),\n            details=details,\n        )\n    except Exception:\n        pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__rename_in_store_604": {
      "name": "_rename_in_store",
      "type": "function",
      "start_line": 604,
      "end_line": 736,
      "content_hash": "38cf6358948197af53a658275cbf284fee73d3b8",
      "content": "def _rename_in_store(\n    client: QdrantClient,\n    src_collection: str,\n    src: Path,\n    dest: Path,\n    dest_collection: Optional[str] = None,\n) -> tuple[int, str | None]:\n    \"\"\"Best-effort: if dest content hash matches previously indexed src hash,\n    update points in-place to the new path without re-embedding.\n\n    Returns number of points moved, or -1 if not applicable/failure.\n    \"\"\"\n    if dest_collection is None:\n        dest_collection = src_collection\n    try:\n        if not dest.exists() or dest.is_dir():\n            return -1\n        try:\n            text = dest.read_text(encoding=\"utf-8\", errors=\"ignore\")\n        except Exception:\n            return -1\n        dest_hash = hashlib.sha1(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n        prev = idx.get_indexed_file_hash(client, src_collection, str(src))\n        logger.debug(\n            \"rename fast-path candidate src=%s dest=%s prev_hash=%s dest_hash=%s\",\n            str(src),\n            str(dest),\n            prev,\n            dest_hash,\n        )\n        if not prev or prev != dest_hash:\n            return -1, prev if prev else None\n\n        moved = 0\n        next_offset = None\n        while True:\n            filt = models.Filter(\n                must=[\n                    models.FieldCondition(\n                        key=\"metadata.path\", match=models.MatchValue(value=str(src))\n                    )\n                ]\n            )\n            points, next_offset = client.scroll(\n                collection_name=src_collection,\n                scroll_filter=filt,\n                with_payload=True,\n                with_vectors=True,\n                limit=256,\n                offset=next_offset,\n            )\n            if not points:\n                break\n            new_points = []\n            for rec in points:\n                payload = rec.payload or {}\n                md = payload.get(\"metadata\") or {}\n                code = md.get(\"code\") or \"\"\n                try:\n                    start_line = int(md.get(\"start_line\") or 1)\n                    end_line = int(md.get(\"end_line\") or start_line)\n                except Exception:\n                    start_line, end_line = 1, 1\n                new_id = idx.hash_id(code, str(dest), start_line, end_line)\n\n                # Update metadata path fields\n                new_md = dict(md)\n                new_md[\"path\"] = str(dest)\n                new_md[\"path_prefix\"] = str(dest.parent)\n                # Recompute dual-path hints\n                cur_path = str(dest)\n                host_root = (\n                    str(os.environ.get(\"HOST_INDEX_PATH\") or \"\").strip().rstrip(\"/\")\n                )\n                if \":\" in host_root: # Windows drive letter (e.g., \"C:\")\n                    host_root = \"\"\n                host_path = None\n                container_path = None\n                try:\n                    if cur_path.startswith(\"/work/\") and host_root:\n                        rel = cur_path[len(\"/work/\") :]\n                        host_path = os.path.realpath(os.path.join(host_root, rel))\n                        container_path = cur_path\n                    else:\n                        host_path = cur_path\n                        if host_root and cur_path.startswith(host_root + \"/\"):\n                            rel = cur_path[len(host_root) + 1 :]\n                            container_path = \"/work/\" + rel\n                except Exception:\n                    host_path = cur_path\n                    container_path = cur_path if cur_path.startswith(\"/work/\") else None\n                new_md[\"host_path\"] = host_path\n                new_md[\"container_path\"] = container_path\n\n                new_payload = dict(payload)\n                new_payload[\"metadata\"] = new_md\n\n                vec = rec.vector  # Named or unnamed vector(s)\n                try:\n                    new_points.append(\n                        models.PointStruct(id=new_id, vector=vec, payload=new_payload)\n                    )\n                except Exception:\n                    continue\n            if new_points:\n                logger.debug(\n                    \"rename fast-path upserting %d chunk(s) %s -> %s into %s\",\n                    len(new_points),\n                    str(src),\n                    str(dest),\n                    dest_collection,\n                )\n                idx.upsert_points(client, dest_collection, new_points)\n                moved += len(new_points)\n            if next_offset is None:\n                break\n\n        try:\n            idx.delete_points_by_path(client, src_collection, str(src))\n        except Exception:\n            pass\n        return moved, dest_hash\n    except Exception as exc:\n        try:\n            logger.warning(\n                \"[rename_debug] rename failed for %s -> %s: %s\",\n                str(src),\n                str(dest),\n                exc,\n            )\n        except Exception:\n            pass\n        return -1, None",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__start_pseudo_backfill_worker_739": {
      "name": "_start_pseudo_backfill_worker",
      "type": "function",
      "start_line": 739,
      "end_line": 812,
      "content_hash": "27772c3c3d3e35eed7602504b5777893108b0396",
      "content": "def _start_pseudo_backfill_worker(\n    client: QdrantClient,\n    default_collection: str,\n    model_dim: int,\n    vector_name: str,\n) -> None:\n    flag = (os.environ.get(\"PSEUDO_BACKFILL_ENABLED\") or \"\").strip().lower()\n    if flag not in {\"1\", \"true\", \"yes\", \"on\"}:\n        return\n\n    try:\n        interval = float(os.environ.get(\"PSEUDO_BACKFILL_TICK_SECS\", \"60\") or 60.0)\n    except Exception:\n        interval = 60.0\n    if interval <= 0:\n        return\n    try:\n        max_points = int(os.environ.get(\"PSEUDO_BACKFILL_MAX_POINTS\", \"256\") or 256)\n    except Exception:\n        max_points = 256\n    if max_points <= 0:\n        max_points = 1\n\n    def _worker() -> None:\n        while True:\n            try:\n                try:\n                    mappings = get_collection_mappings(search_root=str(ROOT))\n                except Exception:\n                    mappings = []\n                if not mappings:\n                    mappings = [\n                        {\"repo_name\": None, \"collection_name\": default_collection},\n                    ]\n                for mapping in mappings:\n                    coll = mapping.get(\"collection_name\") or default_collection\n                    repo_name = mapping.get(\"repo_name\")\n                    if not coll:\n                        continue\n                    try:\n                        if is_multi_repo_mode() and repo_name:\n                            state_dir = _get_repo_state_dir(repo_name)\n                        else:\n                            state_dir = _get_global_state_dir(str(ROOT))\n                        lock_path = state_dir / \"pseudo.lock\"\n                        with _cross_process_lock(lock_path):\n                            processed = idx.pseudo_backfill_tick(\n                                client,\n                                coll,\n                                repo_name=repo_name,\n                                max_points=max_points,\n                                dim=model_dim,\n                                vector_name=vector_name,\n                            )\n                            if processed:\n                                try:\n                                    print(\n                                        f\"[pseudo_backfill] repo={repo_name or 'default'} collection={coll} processed={processed}\"\n                                    )\n                                except Exception:\n                                    pass\n                    except Exception as e:\n                        try:\n                            print(\n                                f\"[pseudo_backfill] error repo={repo_name or 'default'} collection={coll}: {e}\"\n                            )\n                        except Exception:\n                            pass\n            except Exception:\n                pass\n            time.sleep(interval)\n\n    thread = threading.Thread(target=_worker, name=\"pseudo-backfill\", daemon=True)\n    thread.start()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__worker_762": {
      "name": "_worker",
      "type": "function",
      "start_line": 762,
      "end_line": 809,
      "content_hash": "0834e33e25d4dbb4c11c1272790ff50534dc71e3",
      "content": "    def _worker() -> None:\n        while True:\n            try:\n                try:\n                    mappings = get_collection_mappings(search_root=str(ROOT))\n                except Exception:\n                    mappings = []\n                if not mappings:\n                    mappings = [\n                        {\"repo_name\": None, \"collection_name\": default_collection},\n                    ]\n                for mapping in mappings:\n                    coll = mapping.get(\"collection_name\") or default_collection\n                    repo_name = mapping.get(\"repo_name\")\n                    if not coll:\n                        continue\n                    try:\n                        if is_multi_repo_mode() and repo_name:\n                            state_dir = _get_repo_state_dir(repo_name)\n                        else:\n                            state_dir = _get_global_state_dir(str(ROOT))\n                        lock_path = state_dir / \"pseudo.lock\"\n                        with _cross_process_lock(lock_path):\n                            processed = idx.pseudo_backfill_tick(\n                                client,\n                                coll,\n                                repo_name=repo_name,\n                                max_points=max_points,\n                                dim=model_dim,\n                                vector_name=vector_name,\n                            )\n                            if processed:\n                                try:\n                                    print(\n                                        f\"[pseudo_backfill] repo={repo_name or 'default'} collection={coll} processed={processed}\"\n                                    )\n                                except Exception:\n                                    pass\n                    except Exception as e:\n                        try:\n                            print(\n                                f\"[pseudo_backfill] error repo={repo_name or 'default'} collection={coll}: {e}\"\n                            )\n                        except Exception:\n                            pass\n            except Exception:\n                pass\n            time.sleep(interval)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_main_815": {
      "name": "main",
      "type": "function",
      "start_line": 815,
      "end_line": 1016,
      "content_hash": "fe5e94ce590678c04d79fad49ffee548465b6342",
      "content": "def main():\n    # Resolve collection name from workspace state before any client/state ops\n    try:\n        from scripts.workspace_state import get_collection_name_with_staging as _get_coll\n    except Exception:\n        _get_coll = None\n\n    multi_repo_enabled = False\n    try:\n        multi_repo_enabled = bool(is_multi_repo_mode())\n    except Exception:\n        multi_repo_enabled = False\n\n    default_collection = os.environ.get(\"COLLECTION_NAME\", \"codebase\")\n    # In multi-repo mode, per-repo collections are resolved via _get_collection_for_file\n    # and workspace_state; avoid deriving a root-level collection like \"/work-<hash>\".\n    if _get_coll and not multi_repo_enabled:\n        try:\n            resolved = _get_coll(str(ROOT))\n            if resolved:\n                default_collection = resolved\n        except Exception:\n            pass\n    if multi_repo_enabled:\n        print(\"[multi_repo] Multi-repo mode enabled - per-repo collections in use\")\n    else:\n        print(\"[single_repo] Single-repo mode enabled - using single collection\")\n\n    global COLLECTION\n    COLLECTION = default_collection\n\n    print(\n        f\"Watch mode: root={ROOT} qdrant={QDRANT_URL} collection={default_collection} model={MODEL}\"\n    )\n\n    # Health check: detect and auto-heal cache/collection sync issues\n    try:\n        from scripts.collection_health import auto_heal_if_needed, auto_heal_multi_repo\n\n        print(\"[health_check] Checking collection health...\")\n        if multi_repo_enabled:\n            # Multi-repo: check each repo's collection\n            heal_result = auto_heal_multi_repo(str(ROOT), QDRANT_URL, dry_run=False)\n            if heal_result.get(\"repos_healed\", 0) > 0:\n                print(f\"[health_check] Cleared cache for {heal_result['repos_healed']} repos with empty collections\")\n            elif heal_result.get(\"repos_checked\", 0) > 0:\n                print(f\"[health_check] Checked {heal_result['repos_checked']} repos - all OK\")\n            else:\n                print(\"[health_check] No repos with cached state to check\")\n        else:\n            # Single-repo mode\n            heal_result = auto_heal_if_needed(\n                str(ROOT), default_collection, QDRANT_URL, dry_run=False\n            )\n            if heal_result.get(\"action_taken\") == \"cleared_cache\":\n                print(\"[health_check] Cache cleared due to sync issue - files will be reindexed\")\n            elif not heal_result.get(\"health_check\", {}).get(\"healthy\", True):\n                print(\n                    f\"[health_check] Issue detected: {heal_result['health_check'].get('issue', 'unknown')}\"\n                )\n            else:\n                print(\"[health_check] Collection health OK\")\n    except Exception as e:\n        print(f\"[health_check] Warning: health check failed: {e}\")\n\n    client = QdrantClient(\n        url=QDRANT_URL, timeout=int(os.environ.get(\"QDRANT_TIMEOUT\", \"20\") or 20)\n    )\n\n    # Use centralized embedder factory if available (supports Qwen3 feature flag)\n    try:\n        from scripts.embedder import get_embedding_model, get_model_dimension\n        model = get_embedding_model(MODEL)\n        model_dim = get_model_dimension(MODEL)\n    except ImportError:\n        # Fallback to direct fastembed initialization\n        from fastembed import TextEmbedding\n        model = TextEmbedding(model_name=MODEL)\n        model_dim = len(next(model.embed([\"dimension probe\"])))\n\n    try:\n        info = client.get_collection(default_collection)\n        cfg = info.config.params.vectors\n        if isinstance(cfg, dict) and cfg:\n            vector_name = None\n            for name, params in cfg.items():\n                psize = getattr(params, \"size\", None) or getattr(params, \"dim\", None)\n                if psize and int(psize) == int(model_dim):\n                    vector_name = name\n                    break\n            if vector_name is None and getattr(idx, \"LEX_VECTOR_NAME\", None) in cfg:\n                for name in cfg.keys():\n                    if name != idx.LEX_VECTOR_NAME:\n                        vector_name = name\n                        break\n            if vector_name is None:\n                vector_name = idx._sanitize_vector_name(MODEL)\n        else:\n            vector_name = idx._sanitize_vector_name(MODEL)\n    except Exception:\n        vector_name = idx._sanitize_vector_name(MODEL)\n\n    try:\n        idx.ensure_collection_and_indexes_once(\n            client, default_collection, model_dim, vector_name\n        )\n    except Exception:\n        pass\n\n    _start_pseudo_backfill_worker(client, default_collection, model_dim, vector_name)\n\n    try:\n        if multi_repo_enabled:\n            root_repo_name = _extract_repo_name_from_path(str(ROOT))\n            if root_repo_name:\n                root_collection = get_collection_name(root_repo_name)\n                try:\n                    if persist_indexing_config:\n                        persist_indexing_config(\n                            workspace_path=str(ROOT),\n                            repo_name=root_repo_name,\n                            pending=True,\n                        )\n                except Exception:\n                    pass\n                update_indexing_status(\n                    repo_name=root_repo_name,\n                    status={\"state\": \"watching\"},\n                )\n                print(\n                    f\"[workspace_state] Initialized repo state: {root_repo_name} -> {root_collection}\"\n                )\n            else:\n                print(\n                    \"[workspace_state] Multi-repo: root path is not a repo; skipping state initialization\"\n                )\n        else:\n            updates = {\"qdrant_collection\": default_collection}\n            try:\n                if get_indexing_config_snapshot and compute_indexing_config_hash:\n                    cfg = get_indexing_config_snapshot()\n                    updates[\"indexing_config\"] = cfg\n                    updates[\"indexing_config_hash\"] = compute_indexing_config_hash(cfg)\n            except Exception:\n                pass\n            update_workspace_state(workspace_path=str(ROOT), updates=updates)\n            try:\n                if persist_indexing_config:\n                    persist_indexing_config(\n                        workspace_path=str(ROOT),\n                        repo_name=None,\n                        pending=True,\n                    )\n            except Exception:\n                pass\n            update_indexing_status(status={\"state\": \"watching\"})\n    except Exception as e:\n        print(f\"[workspace_state] Error initializing workspace state: {e}\")\n\n    q = ChangeQueue(\n        lambda paths: _process_paths(\n            paths, client, model, vector_name, model_dim, str(ROOT)\n        )\n    )\n    handler = IndexHandler(ROOT, q, client, default_collection)\n\n    use_polling = (os.environ.get(\"WATCH_USE_POLLING\") or \"\").strip().lower() in (\n        \"1\",\n        \"true\",\n        \"yes\",\n        \"on\",\n    )\n    if use_polling:\n        try:\n            from watchdog.observers.polling import PollingObserver  # type: ignore\n\n            obs = PollingObserver()\n            try:\n                print(\"[watch_mode] Using polling observer for filesystem events\")\n            except Exception:\n                pass\n        except Exception:\n            obs = Observer()\n            try:\n                print(\n                    \"[watch_mode] Polling observer unavailable, falling back to default Observer\"\n                )\n            except Exception:\n                pass\n    else:\n        obs = Observer()\n    obs.schedule(handler, str(ROOT), recursive=True)\n    obs.start()\n\n    try:\n        while True:\n            time.sleep(1.0)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        obs.stop()\n        obs.join()",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__process_git_history_manifest_1019": {
      "name": "_process_git_history_manifest",
      "type": "function",
      "start_line": 1019,
      "end_line": 1055,
      "content_hash": "9871eec0c704a6a87dcb70f92e348f88ddefe486",
      "content": "def _process_git_history_manifest(\n    p: Path,\n    client,\n    model,\n    collection: str,\n    vector_name: str,\n    repo_name: Optional[str],\n    env_snapshot: Optional[Dict[str, str]] = None,\n):\n    try:\n        import sys\n\n        script = ROOT_DIR / \"scripts\" / \"ingest_history.py\"\n        if not script.exists():\n            return\n        cmd = [sys.executable or \"python3\", str(script), \"--manifest-json\", str(p)]\n        env = os.environ.copy()\n        try:\n            if env_snapshot:\n                env.update({str(k): str(v) for k, v in env_snapshot.items() if k})\n        except Exception:\n            pass\n        if collection:\n            env[\"COLLECTION_NAME\"] = collection\n        if QDRANT_URL:\n            env[\"QDRANT_URL\"] = QDRANT_URL\n        if repo_name:\n            env[\"REPO_NAME\"] = repo_name\n        try:\n            print(\n                f\"[git_history_manifest] launching ingest_history.py for {p} collection={collection} repo={repo_name}\"\n            )\n        except Exception:\n            pass\n        subprocess.Popen(cmd, env=env)\n    except Exception:\n        return",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__process_paths_1058": {
      "name": "_process_paths",
      "type": "function",
      "start_line": 1058,
      "end_line": 1444,
      "content_hash": "588e60f1478be2ab0850412433cda24eb2dbdb25",
      "content": "def _process_paths(paths, client, model, vector_name: str, model_dim: int, workspace_path: str):\n    unique_paths = sorted(set(Path(x) for x in paths))\n    if not unique_paths:\n        return\n\n    started_at = datetime.now().isoformat()\n\n    repo_groups: dict[str, list[Path]] = {}\n    for p in unique_paths:\n        repo_path = _detect_repo_for_file(p) or Path(workspace_path)\n        repo_groups.setdefault(str(repo_path), []).append(p)\n\n    for repo_path, repo_files in repo_groups.items():\n        try:\n            repo_name = _extract_repo_name_from_path(repo_path)\n            try:\n                if persist_indexing_config:\n                    persist_indexing_config(\n                        workspace_path=repo_path,\n                        repo_name=repo_name,\n                        pending=True,\n                    )\n            except Exception:\n                pass\n            update_indexing_status(\n                repo_name=repo_name,\n                status={\n                    \"state\": \"indexing\",\n                    \"started_at\": started_at,\n                    \"progress\": {\n                        \"files_processed\": 0,\n                        \"total_files\": len(repo_files),\n                    },\n                },\n            )\n        except Exception:\n            pass\n\n    repo_progress: dict[str, int] = {key: 0 for key in repo_groups.keys()}\n\n    for p in unique_paths:\n        repo_path = _detect_repo_for_file(p) or Path(workspace_path)\n        repo_key = str(repo_path)\n        repo_files = repo_groups.get(repo_key, [])\n        repo_name = _extract_repo_name_from_path(repo_key)\n        collection = _get_collection_for_file(p)\n        state_env: Optional[Dict[str, str]] = None\n        try:\n            st = get_workspace_state(repo_key, repo_name) if get_workspace_state else None\n            if isinstance(st, dict):\n                if is_staging_enabled():\n                    state_env = st.get(\"indexing_env\")\n        except Exception:\n            state_env = None\n\n        if \".remote-git\" in p.parts and p.suffix.lower() == \".json\":\n            try:\n                _process_git_history_manifest(\n                    p,\n                    client,\n                    model,\n                    collection,\n                    vector_name,\n                    repo_name,\n                    env_snapshot=(state_env if is_staging_enabled() else None),\n                )\n            except Exception as e:\n                try:\n                    print(f\"[commit_ingest_error] {p}: {e}\")\n                except Exception:\n                    pass\n            repo_progress[repo_key] = repo_progress.get(repo_key, 0) + 1\n            try:\n                _update_progress(\n                    repo_key,\n                    started_at,\n                    repo_progress[repo_key],\n                    len(repo_files),\n                    p,\n                )\n            except Exception:\n                pass\n            continue\n\n        if not p.exists():\n            if client is not None:\n                try:\n                    idx.delete_points_by_path(client, collection, str(p))\n                    print(f\"[deleted] {p} -> {collection}\")\n                except Exception:\n                    pass\n            try:\n                remove_cached_file(str(p), repo_name)\n            except Exception:\n                pass\n            _log_activity(repo_key, \"deleted\", p)\n            repo_progress[repo_key] = repo_progress.get(repo_key, 0) + 1\n            try:\n                _update_progress(\n                    repo_key,\n                    started_at,\n                    repo_progress[repo_key],\n                    len(repo_files),\n                    p,\n                )\n            except Exception:\n                pass\n            continue\n\n        # If a repo-specific indexing_env is present (staging), avoid mutating os.environ\n        # process-wide. Instead, run ingest_code in a subprocess with an explicit env dict.\n        if is_staging_enabled() and state_env and collection:\n            # Cheap pre-flight hash check so we can skip unchanged files without spawning a subprocess.\n            # TODO: Instead of launching one subprocess per file, queue changes and run a single ingest_code.py --root <repo> pass with --no-skip-unchanged. That reuses ingest\u2019s own skip logic, but requires more plumbing (collect paths, pass via manifest/CLI, etc.).\n            skip_via_hash = False\n            try:\n                text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n                try:\n                    file_hash = hashlib.sha1(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n                except Exception:\n                    file_hash = \"\"\n                if file_hash:\n                    try:\n                        cached_hash = get_cached_file_hash(str(p), repo_name) if repo_name else None\n                    except Exception:\n                        cached_hash = None\n                    if cached_hash and cached_hash == file_hash:\n                        try:\n                            print(f\"[skip_unchanged] {p} (hash match)\")\n                        except Exception:\n                            pass\n                        _log_activity(repo_key, \"skipped\", p, {\"reason\": \"hash_unchanged\"})\n                        repo_progress[repo_key] = repo_progress.get(repo_key, 0) + 1\n                        try:\n                            _update_progress(\n                                repo_key,\n                                started_at,\n                                repo_progress[repo_key],\n                                len(repo_files),\n                                p,\n                            )\n                        except Exception:\n                            pass\n                        skip_via_hash = True\n            except Exception:\n                pass\n            if skip_via_hash:\n                continue\n            try:\n                cmd = [\n                    sys.executable or \"python3\",\n                    str(ROOT_DIR / \"scripts\" / \"ingest_code.py\"),\n                    \"--root\",\n                    str(p),\n                    \"--no-skip-unchanged\",\n                ]\n                env = os.environ.copy()\n                try:\n                    env.update({str(k): str(v) for k, v in state_env.items() if k})\n                except Exception:\n                    pass\n                env[\"COLLECTION_NAME\"] = collection\n                if QDRANT_URL:\n                    env[\"QDRANT_URL\"] = QDRANT_URL\n                if repo_name:\n                    env[\"REPO_NAME\"] = repo_name\n                result = subprocess.run(cmd, env=env, capture_output=True, text=True)\n                if result.returncode != 0:\n                    try:\n                        logger.error(\n                            \"watch_index::subprocess_index_failed\",\n                            extra={\n                                \"repo_key\": repo_key,\n                                \"collection\": collection,\n                                \"file\": str(p),\n                                \"returncode\": result.returncode,\n                                \"stdout\": (result.stdout or \"\").strip(),\n                                \"stderr\": (result.stderr or \"\").strip(),\n                            },\n                        )\n                    except Exception:\n                        try:\n                            print(\n                                f\"[indexed_subprocess_error] {p} -> {collection} returncode={result.returncode}\"\n                            )\n                        except Exception:\n                            pass\n                else:\n                    try:\n                        print(f\"[indexed_subprocess] {p} -> {collection}\")\n                    except Exception:\n                        pass\n                repo_progress[repo_key] = repo_progress.get(repo_key, 0) + 1\n                try:\n                    _update_progress(\n                        repo_key,\n                        started_at,\n                        repo_progress[repo_key],\n                        len(repo_files),\n                        p,\n                    )\n                except Exception:\n                    pass\n                continue\n            except Exception:\n                pass\n        if client is not None and model is not None:\n            try:\n                try:\n                    idx.ensure_collection_and_indexes_once(\n                        client, collection, model_dim, vector_name\n                    )\n                except Exception:\n                    pass\n                ok = False\n                text: str | None = None\n                try:\n                    text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n                except Exception:\n                    text = None\n                if text is not None:\n                    try:\n                        language = idx.detect_language(p)\n                    except Exception:\n                        language = \"\"\n                    try:\n                        file_hash = hashlib.sha1(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n                    except Exception:\n                        file_hash = \"\"\n                    if file_hash:\n                        # Fast path: skip if content hash matches cached hash (file unchanged)\n                        # Safety: startup health check clears stale cache per-repo\n                        try:\n                            cached_hash = get_cached_file_hash(str(p), repo_name) if repo_name else None\n                            if cached_hash and cached_hash == file_hash:\n                                print(f\"[skip_unchanged] {p} (hash match)\")\n                                ok = True\n                                raise _SkipUnchanged()\n                        except _SkipUnchanged:\n                            raise\n                        except Exception:\n                            pass  # hash check failed, proceed with reindex\n\n                        try:\n                            use_smart, smart_reason = idx.should_use_smart_reindexing(str(p), file_hash)\n                        except Exception:\n                            use_smart, smart_reason = False, \"smart_check_failed\"\n\n                        # Bootstrap: if we have no symbol cache yet, still run smart path once\n                        bootstrap = smart_reason == \"no_cached_symbols\"\n                        if use_smart or bootstrap:\n                            msg_kind = \"smart reindexing\" if use_smart else \"bootstrap (no_cached_symbols) for smart reindex\"\n                            try:\n                                print(f\"[SMART_REINDEX][watcher] Using {msg_kind} for {p} ({smart_reason})\")\n                            except Exception:\n                                pass\n                            try:\n                                status = idx.process_file_with_smart_reindexing(\n                                    p,\n                                    text,\n                                    language,\n                                    client,\n                                    collection,\n                                    repo_name,\n                                    model,\n                                    vector_name,\n                                )\n                                ok = status in (\"success\", \"skipped\")\n                            except Exception as se:\n                                try:\n                                    print(f\"[SMART_REINDEX][watcher] Smart reindexing failed for {p}: {se}\")\n                                except Exception:\n                                    pass\n                                ok = False\n                        else:\n                            try:\n                                print(f\"[SMART_REINDEX][watcher] Using full reindexing for {p} ({smart_reason})\")\n                            except Exception:\n                                pass\n\n                    # Fallback: full single-file reindex. Pseudo/tags are inlined by default;\n                    # when PSEUDO_BACKFILL_ENABLED=1 we run base-only and rely on backfill.\n                    if not ok:\n                        flag = (os.environ.get(\"PSEUDO_BACKFILL_ENABLED\") or \"\").strip().lower()\n                        pseudo_mode = \"off\" if flag in {\"1\", \"true\", \"yes\", \"on\"} else \"full\"\n                        ok = idx.index_single_file(\n                            client,\n                            model,\n                            collection,\n                            vector_name,\n                            p,\n                            dedupe=True,\n                            skip_unchanged=False,\n                            pseudo_mode=pseudo_mode,\n                            repo_name_for_cache=repo_name,\n                        )\n            except _SkipUnchanged:\n                # File unchanged - skip without error but still count towards progress\n                status = \"skipped\"\n                print(f\"[{status}] {p} -> {collection}\")\n                _log_activity(repo_key, \"skipped\", p, {\"reason\": \"hash_unchanged\"})\n                repo_progress[repo_key] = repo_progress.get(repo_key, 0) + 1\n                try:\n                    _update_progress(\n                        repo_key,\n                        started_at,\n                        repo_progress[repo_key],\n                        len(repo_files),\n                        p,\n                    )\n                except Exception:\n                    pass\n                continue\n            except Exception as e:\n                repo_progress[repo_key] = repo_progress.get(repo_key, 0) + 1\n                try:\n                    logger.error(\n                        \"watch_index::_process_paths error\",\n                        extra={\n                            \"repo_key\": repo_key,\n                            \"collection\": collection,\n                            \"file\": str(p),\n                        },\n                        exc_info=True,\n                    )\n                except Exception:\n                    pass\n                try:\n                    _update_progress(\n                        repo_key,\n                        started_at,\n                        repo_progress[repo_key],\n                        len(repo_files),\n                        p,\n                    )\n                except Exception:\n                    pass\n                continue\n\n            status = \"indexed\" if ok else \"skipped\"\n            print(f\"[{status}] {p} -> {collection}\")\n            if ok:\n                try:\n                    size = int(p.stat().st_size)\n                except Exception:\n                    size = None\n                _log_activity(repo_key, \"indexed\", p, {\"file_size\": size})\n            else:\n                _log_activity(\n                    repo_key, \"skipped\", p, {\"reason\": \"no-change-or-error\"}\n                )\n            repo_progress[repo_key] = repo_progress.get(repo_key, 0) + 1\n            try:\n                _update_progress(\n                    repo_key,\n                    started_at,\n                    repo_progress[repo_key],\n                    len(repo_files),\n                    p,\n                )\n            except Exception:\n                pass\n        else:\n            print(f\"Not processing locally: {p}\")\n            _log_activity(repo_key, \"skipped\", p, {\"reason\": \"remote-mode\"})\n\n            repo_progress[repo_key] = repo_progress.get(repo_key, 0) + 1\n            try:\n                _update_progress(\n                    repo_key,\n                    started_at,\n                    repo_progress[repo_key],\n                    len(repo_files),\n                    p,\n                )\n            except Exception:\n                pass\n\n    for repo_path in repo_groups.keys():\n        try:\n            repo_name = _extract_repo_name_from_path(repo_path)\n            update_indexing_status(\n                repo_name=repo_name,\n                status={\"state\": \"watching\"},\n            )\n        except Exception:\n            pass",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}