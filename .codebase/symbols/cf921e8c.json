{
  "file_path": "/work/external-deps/Context-Engine/scripts/hybrid/ranking.py",
  "file_hash": "57ed37943857aca6512fd1ea6b0749ae0369bf3d",
  "updated_at": "2025-12-26T17:34:23.898324",
  "symbols": {
    "function__safe_int_30": {
      "name": "_safe_int",
      "type": "function",
      "start_line": 30,
      "end_line": 36,
      "content_hash": "421338f8f5a7d4ee0ad4a57abcf6c27eabc6780f",
      "content": "def _safe_int(val: Any, default: int) -> int:\n    try:\n        if val is None or (isinstance(val, str) and val.strip() == \"\"):\n            return default\n        return int(val)\n    except (ValueError, TypeError):\n        return default",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__safe_float_39": {
      "name": "_safe_float",
      "type": "function",
      "start_line": 39,
      "end_line": 45,
      "content_hash": "c6194722f9add608e9717824ee4532406090d6fb",
      "content": "def _safe_float(val: Any, default: float) -> float:\n    try:\n        if val is None or (isinstance(val, str) and val.strip() == \"\"):\n            return default\n        return float(val)\n    except (ValueError, TypeError):\n        return default",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_micro_defaults_71": {
      "name": "_get_micro_defaults",
      "type": "function",
      "start_line": 71,
      "end_line": 89,
      "content_hash": "927123a868afc7146d391c7a53fa1b1ae848b2ab",
      "content": "def _get_micro_defaults() -> Tuple[int, int, int, int]:\n    \"\"\"Return (max_spans, merge_lines, budget_tokens, tokens_per_line) based on runtime and micro chunk mode.\n\n    Budget tokens floor is 5000 to ensure context_answer has enough context for quality answers.\n    \"\"\"\n    micro_enabled = os.environ.get(\"INDEX_MICRO_CHUNKS\", \"1\").strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n    try:\n        from scripts.refrag_glm import detect_glm_runtime\n        is_glm = detect_glm_runtime()\n    except ImportError:\n        is_glm = False\n    if is_glm:\n        if micro_enabled:\n            return (24, 6, 8192, 32)\n        else:\n            return (12, 4, 6000, 32)\n    else:\n        # Non-GLM: still need reasonable budget for quality context_answer\n        return (8, 4, 5000, 32)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__get_collection_stats_115": {
      "name": "_get_collection_stats",
      "type": "function",
      "start_line": 115,
      "end_line": 128,
      "content_hash": "fb3ac303ccdbb246ab47e8d51b14952a9c3cdfed",
      "content": "def _get_collection_stats(client: Any, coll_name: str) -> Dict[str, Any]:\n    \"\"\"Get cached collection statistics for scaling decisions.\"\"\"\n    import time\n    now = time.time()\n    cached = _COLL_STATS_CACHE.get(coll_name)\n    if cached and (now - cached[0]) < _COLL_STATS_TTL:\n        return cached[1]\n    try:\n        info = client.get_collection(coll_name)\n        stats = {\"points_count\": info.points_count or 0}\n        _COLL_STATS_CACHE[coll_name] = (now, stats)\n        return stats\n    except Exception:\n        return {\"points_count\": 0}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_rrf_135": {
      "name": "rrf",
      "type": "function",
      "start_line": 135,
      "end_line": 137,
      "content_hash": "06fc9db1bd8baa02d619ca70fff3c152bc225154",
      "content": "def rrf(rank: int, k: int = RRF_K) -> float:\n    \"\"\"Reciprocal Rank Fusion score for a given rank.\"\"\"\n    return 1.0 / (k + rank)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__scale_rrf_k_140": {
      "name": "_scale_rrf_k",
      "type": "function",
      "start_line": 140,
      "end_line": 152,
      "content_hash": "1a0af3f63d486a1ef12e85ed910ee5c6551b5885",
      "content": "def _scale_rrf_k(base_k: int, collection_size: int) -> int:\n    \"\"\"Scale RRF k parameter based on collection size.\n\n    For large collections, increase k to spread score distribution.\n    Uses logarithmic scaling: k_scaled = k * (1 + log10(size/threshold))\n    Capped at MAX_RRF_K_SCALE * base_k.\n    \"\"\"\n    if collection_size < LARGE_COLLECTION_THRESHOLD:\n        return base_k\n    ratio = collection_size / LARGE_COLLECTION_THRESHOLD\n    scale = 1.0 + math.log10(max(1, ratio))\n    scale = min(scale, MAX_RRF_K_SCALE)\n    return int(base_k * scale)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__adaptive_per_query_155": {
      "name": "_adaptive_per_query",
      "type": "function",
      "start_line": 155,
      "end_line": 168,
      "content_hash": "9eeb348124631f234f74c959799c81aeb520b393",
      "content": "def _adaptive_per_query(base_limit: int, collection_size: int, has_filters: bool) -> int:\n    \"\"\"Increase candidate retrieval for larger collections.\n\n    Uses sublinear sqrt scaling to avoid excessive retrieval.\n    Filters reduce the need for extra candidates.\n    \"\"\"\n    if collection_size < LARGE_COLLECTION_THRESHOLD:\n        return base_limit\n    ratio = collection_size / LARGE_COLLECTION_THRESHOLD\n    scale = math.sqrt(ratio)\n    if has_filters:\n        scale = max(1.0, scale * 0.7)\n    scaled = int(base_limit * min(scale, 3.0))\n    return max(base_limit, min(scaled, 200))",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__normalize_scores_171": {
      "name": "_normalize_scores",
      "type": "function",
      "start_line": 171,
      "end_line": 195,
      "content_hash": "fb90647068ff4993e969918c944ed33b7be396eb",
      "content": "def _normalize_scores(score_map: Dict[str, Dict[str, Any]], collection_size: int) -> None:\n    \"\"\"Normalize scores using z-score + sigmoid for large collections.\n\n    This spreads compressed score distributions to improve discrimination.\n    Only applies when SCORE_NORMALIZE_ENABLED=true and collection is large.\n    \"\"\"\n    if not SCORE_NORMALIZE_ENABLED:\n        return\n    if collection_size < LARGE_COLLECTION_THRESHOLD:\n        return\n    if len(score_map) < 3:\n        return\n\n    scores = [rec[\"s\"] for rec in score_map.values()]\n    mean_s = sum(scores) / len(scores)\n    var_s = sum((s - mean_s) ** 2 for s in scores) / len(scores)\n    std_s = math.sqrt(var_s) if var_s > 0 else 1.0\n\n    if std_s < 1e-6:\n        return\n\n    for rec in score_map.values():\n        z = (rec[\"s\"] - mean_s) / std_s\n        normalized = 1.0 / (1.0 + math.exp(-z * 0.5))\n        rec[\"s\"] = normalized",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_sparse_lex_score_202": {
      "name": "sparse_lex_score",
      "type": "function",
      "start_line": 202,
      "end_line": 218,
      "content_hash": "114f537dfc89b301537ff74b4aa81ba3d8aafb60",
      "content": "def sparse_lex_score(raw_score: float, weight: float = LEX_VECTOR_WEIGHT) -> float:\n    \"\"\"Normalize sparse lexical vector score to RRF-equivalent range.\n\n    Maps sparse similarity scores to the same range as RRF(rank) scores,\n    preserving relative ordering while maintaining fusion balance.\n\n    Formula: weight * (RRF_MIN + (clamped_score / max_score) * (RRF_MAX - RRF_MIN))\n    - Sparse score 0 maps to RRF_MIN (like worst rank)\n    - Sparse score max maps to RRF_MAX (like rank 1)\n    - Quality ordering preserved, but doesn't dominate dense embedding scores\n    \"\"\"\n    if raw_score <= 0:\n        return 0.0\n    clamped = min(raw_score, SPARSE_LEX_MAX_SCORE)\n    ratio = clamped / SPARSE_LEX_MAX_SCORE\n    rrf_equiv = SPARSE_RRF_MIN + ratio * (SPARSE_RRF_MAX - SPARSE_RRF_MIN)\n    return weight * rrf_equiv",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__split_ident_231": {
      "name": "_split_ident",
      "type": "function",
      "start_line": 231,
      "end_line": 240,
      "content_hash": "cd3ede59fee983b2eb934d80feb673a5a1c2e169",
      "content": "def _split_ident(s: str) -> List[str]:\n    \"\"\"Split snake_case and camelCase identifiers into tokens.\"\"\"\n    parts = re.split(r\"[^A-Za-z0-9]+\", s)\n    out: List[str] = []\n    for p in parts:\n        if not p:\n            continue\n        segs = re.findall(r\"[A-Z]?[a-z]+|[A-Z]+(?![a-z])|\\d+\", p)\n        out.extend([x for x in segs if x])\n    return [x.lower() for x in out if x and x.lower() not in _STOP]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_tokenize_queries_243": {
      "name": "tokenize_queries",
      "type": "function",
      "start_line": 243,
      "end_line": 254,
      "content_hash": "5856fd898ada8f5b0fa593ffc197de33a38dfa01",
      "content": "def tokenize_queries(phrases: List[str]) -> List[str]:\n    \"\"\"Tokenize phrases into unique identifier-aware tokens.\"\"\"\n    toks: List[str] = []\n    for ph in phrases:\n        toks.extend(_split_ident(ph))\n    seen = set()\n    out: List[str] = []\n    for t in toks:\n        if t not in seen:\n            out.append(t)\n            seen.add(t)\n    return out",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_lexical_score_261": {
      "name": "lexical_score",
      "type": "function",
      "start_line": 261,
      "end_line": 308,
      "content_hash": "6be8c3b6690ae11fa34eb06f4999959ed65c4899",
      "content": "def lexical_score(\n    phrases: List[str],\n    md: Dict[str, Any],\n    token_weights: Dict[str, float] | None = None,\n    bm25_weight: float | None = None\n) -> float:\n    \"\"\"Smarter lexical: split identifiers, weight matches in symbol/path higher.\n    \n    If token_weights provided, apply a small BM25-style multiplicative factor per token:\n        factor = 1 + bm25_weight * (w - 1) where w are normalized around 1.0\n    \"\"\"\n    tokens = tokenize_queries(phrases)\n    if not tokens:\n        return 0.0\n    path = str(md.get(\"path\", \"\")).lower()\n    path_segs = re.split(r\"[/\\\\]\", path)\n    sym = str(md.get(\"symbol\", \"\")).lower()\n    symp = str(md.get(\"symbol_path\", \"\")).lower()\n    code = str(md.get(\"code\", \"\"))[:2000].lower()\n    pseudo = str(md.get(\"pseudo\") or \"\").lower()\n    tags_val = md.get(\"tags\") or []\n    if isinstance(tags_val, list):\n        tags_text = \" \".join(str(x) for x in tags_val).lower()\n    else:\n        tags_text = str(tags_val).lower()\n    s = 0.0\n    for t in tokens:\n        if not t:\n            continue\n        contrib = 0.0\n        if t in sym or t in symp:\n            contrib += 2.0\n        if any(t in seg for seg in path_segs):\n            contrib += 0.8\n            if path_segs and t in path_segs[-1]:\n                contrib += 0.3\n        if t in code:\n            contrib += 1.0\n        if PSEUDO_BOOST > 0.0:\n            if pseudo and t in pseudo:\n                contrib += PSEUDO_BOOST\n            if tags_text and t in tags_text:\n                contrib += 0.5 * PSEUDO_BOOST\n        if contrib > 0 and token_weights and bm25_weight:\n            w = float(token_weights.get(t, 1.0) or 1.0)\n            contrib *= (1.0 + float(bm25_weight) * (w - 1.0))\n        s += contrib\n    return s",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__compute_query_stats_315": {
      "name": "_compute_query_stats",
      "type": "function",
      "start_line": 315,
      "end_line": 345,
      "content_hash": "005953f764b80407d41e76b55d3f4ab8af1cbb69",
      "content": "def _compute_query_stats(queries: List[str]) -> Dict[str, Any]:\n    \"\"\"Compute statistics about query tokens for adaptive weighting.\"\"\"\n    toks = tokenize_queries(queries)\n    total = len(toks)\n\n    def _is_camel(t: str) -> bool:\n        try:\n            return any(c.isupper() for c in t[1:]) and any(c.islower() for c in t)\n        except Exception:\n            return False\n\n    def _is_identifier_like(t: str) -> bool:\n        try:\n            return (\"_\" in t) or t.isupper() or any(ch.isdigit() for ch in t) or _is_camel(t)\n        except Exception:\n            return False\n\n    id_like = sum(1 for t in toks if _is_identifier_like(t))\n    avg_tok_len = (sum(len(t) for t in toks) / max(1, total)) if total else 0.0\n    qchars = sum(len(q) for q in queries) if queries else 0\n    has_question = any((\"?\" in q) for q in (queries or []))\n    q0 = (queries[0].strip().lower() if queries else \"\")\n    wh_start = q0.startswith((\"how\", \"what\", \"why\", \"when\", \"where\", \"explain\", \"describe\"))\n    stats = {\n        \"total_tokens\": total,\n        \"identifier_density\": (id_like / max(1, total)),\n        \"avg_token_len\": avg_tok_len,\n        \"avg_query_chars\": (qchars / max(1, len(queries))) if queries else 0.0,\n        \"narrative_hint\": bool(has_question or wh_start),\n    }\n    return stats",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__is_camel_320": {
      "name": "_is_camel",
      "type": "function",
      "start_line": 320,
      "end_line": 324,
      "content_hash": "0ed86af8e767c4ce0b7b533ea75994134f6ae6b0",
      "content": "    def _is_camel(t: str) -> bool:\n        try:\n            return any(c.isupper() for c in t[1:]) and any(c.islower() for c in t)\n        except Exception:\n            return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__is_identifier_like_326": {
      "name": "_is_identifier_like",
      "type": "function",
      "start_line": 326,
      "end_line": 330,
      "content_hash": "d711c9493317205805986403efb9a68a17797130",
      "content": "    def _is_identifier_like(t: str) -> bool:\n        try:\n            return (\"_\" in t) or t.isupper() or any(ch.isdigit() for ch in t) or _is_camel(t)\n        except Exception:\n            return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__adaptive_weights_348": {
      "name": "_adaptive_weights",
      "type": "function",
      "start_line": 348,
      "end_line": 374,
      "content_hash": "de71f388f256c9bde15b91a7042634233fb8cdda",
      "content": "def _adaptive_weights(stats: Dict[str, Any]) -> Tuple[float, float, float]:\n    \"\"\"Return per-query weights (dense_w, lex_vec_w, lex_text_w) with gentle clamps.\n    \n    Dense/lex-vector vary within \u00b125%; lexical text component within \u00b120%.\n    \"\"\"\n    base_d = DENSE_WEIGHT\n    base_lv = LEX_VECTOR_WEIGHT\n    base_lx = LEXICAL_WEIGHT\n\n    id_density = float(stats.get(\"identifier_density\", 0.0) or 0.0)\n    total = int(stats.get(\"total_tokens\", 0) or 0)\n    narrative_hint = 1.0 if stats.get(\"narrative_hint\") else 0.0\n    longish = 1.0 if total >= 8 else 0.0\n\n    narrative_score = 0.6 * narrative_hint + 0.4 * longish\n    id_score = id_density\n    delta = max(-1.0, min(1.0, narrative_score - id_score))\n\n    dens_scale = 1.0 + 0.25 * delta\n    lv_scale = 1.0 - 0.25 * delta\n    lx_scale = 1.0 + 0.20 * (-delta)\n\n    dens_scale = max(0.75, min(1.25, dens_scale))\n    lv_scale = max(0.75, min(1.25, lv_scale))\n    lx_scale = max(0.80, min(1.20, lx_scale))\n\n    return base_d * dens_scale, base_lv * lv_scale, base_lx * lx_scale",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__bm25_token_weights_from_results_377": {
      "name": "_bm25_token_weights_from_results",
      "type": "function",
      "start_line": 377,
      "end_line": 410,
      "content_hash": "ce3fa1fa5cd77e5fee13b866138fcf13c81bc908",
      "content": "def _bm25_token_weights_from_results(phrases: List[str], results: List[Any]) -> Dict[str, float]:\n    \"\"\"Compute lightweight per-token IDF-like weights from a small sample of lex results.\n    \n    Returns weights normalized to mean 1.0 over tokens present in phrases.\n    \"\"\"\n    try:\n        tokens = [t for t in tokenize_queries(phrases) if t]\n        if not tokens or not results:\n            return {}\n        tok_set = set(tokens)\n        N = max(1, len(results))\n        df: Dict[str, int] = {t: 0 for t in tok_set}\n        for p in results:\n            try:\n                md = (p.payload or {}).get(\"metadata\") or {}\n            except Exception:\n                md = {}\n            text = \" \".join([\n                str(md.get(\"symbol\") or \"\"),\n                str(md.get(\"symbol_path\") or \"\"),\n                str(md.get(\"path\") or \"\"),\n                str((md.get(\"code\") or \"\"))[:2000],\n            ]).lower()\n            doc_toks = set(tokenize_queries([text]))\n            for t in tok_set:\n                if t in doc_toks:\n                    df[t] += 1\n        idf: Dict[str, float] = {t: math.log(1.0 + (N / float(df[t] + 1))) for t in tok_set}\n        mean = sum(idf.values()) / max(1, len(idf))\n        if mean <= 0:\n            return {t: 1.0 for t in tok_set}\n        return {t: (idf[t] / mean) for t in tok_set}\n    except Exception:\n        return {}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__mmr_diversify_417": {
      "name": "_mmr_diversify",
      "type": "function",
      "start_line": 417,
      "end_line": 475,
      "content_hash": "8d33f23d4c9cb69bd532817f2e9c034800776684",
      "content": "def _mmr_diversify(ranked: List[Dict[str, Any]], k: int = 60, lambda_: float = 0.7) -> List[Dict[str, Any]]:\n    \"\"\"Maximal Marginal Relevance over fused list.\n    \n    Preserves top-1 by relevance, then balances relevance vs. diversity by path/symbol.\n    Returns a reordered list (top-k diversified, remainder appended in original order).\n    \"\"\"\n    if not ranked:\n        return []\n    k = max(1, min(int(k or 1), len(ranked)))\n\n    def _path(md: Dict[str, Any]) -> str:\n        return str(md.get(\"path\") or \"\")\n\n    def _symp(md: Dict[str, Any]) -> str:\n        return str(md.get(\"symbol_path\") or md.get(\"symbol\") or \"\")\n\n    def _sim(a: Dict[str, Any], b: Dict[str, Any]) -> float:\n        mda = (a[\"pt\"].payload or {}).get(\"metadata\") or {}\n        mdb = (b[\"pt\"].payload or {}).get(\"metadata\") or {}\n        pa, pb = _path(mda), _path(mdb)\n        if pa and pb and pa == pb:\n            return 1.0\n        sa, sb = _symp(mda), _symp(mdb)\n        if sa and sb and sa == sb:\n            return 0.8\n        if pa and pb:\n            ta = set(re.split(r\"[/\\\\]+\", pa.lower()))\n            tb = set(re.split(r\"[/\\\\]+\", pb.lower()))\n            ta.discard(\"\")\n            tb.discard(\"\")\n            if ta and tb:\n                inter = len(ta & tb)\n                union = max(1, len(ta | tb))\n                return 0.5 * (inter / union)\n        return 0.0\n\n    rel = [float(m.get(\"s\", 0.0)) for m in ranked]\n    selected_idx = [0]\n    candidates = list(range(1, len(ranked)))\n    while len(selected_idx) < k and candidates:\n        best_idx = None\n        best_score = -1e18\n        for i in candidates:\n            r = rel[i]\n            if selected_idx:\n                max_sim = max(_sim(ranked[i], ranked[j]) for j in selected_idx)\n            else:\n                max_sim = 0.0\n            mmr = lambda_ * r - (1.0 - lambda_) * max_sim\n            if mmr > best_score:\n                best_score = mmr\n                best_idx = i\n        selected_idx.append(best_idx)\n        candidates.remove(best_idx)\n\n    sel_set = set(selected_idx)\n    diversified = [ranked[i] for i in selected_idx]\n    diversified.extend([ranked[i] for i in range(len(ranked)) if i not in sel_set])\n    return diversified",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__path_427": {
      "name": "_path",
      "type": "function",
      "start_line": 427,
      "end_line": 428,
      "content_hash": "1b2bf4f945e77f61bdeb2baff2f23e53e9b8a08c",
      "content": "    def _path(md: Dict[str, Any]) -> str:\n        return str(md.get(\"path\") or \"\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__symp_430": {
      "name": "_symp",
      "type": "function",
      "start_line": 430,
      "end_line": 431,
      "content_hash": "a800e6b5c6c938767c6909b71599f64ccae45e6a",
      "content": "    def _symp(md: Dict[str, Any]) -> str:\n        return str(md.get(\"symbol_path\") or md.get(\"symbol\") or \"\")",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__sim_433": {
      "name": "_sim",
      "type": "function",
      "start_line": 433,
      "end_line": 451,
      "content_hash": "6202300b6d2f373a931e5ca2478acf925dd0514f",
      "content": "    def _sim(a: Dict[str, Any], b: Dict[str, Any]) -> float:\n        mda = (a[\"pt\"].payload or {}).get(\"metadata\") or {}\n        mdb = (b[\"pt\"].payload or {}).get(\"metadata\") or {}\n        pa, pb = _path(mda), _path(mdb)\n        if pa and pb and pa == pb:\n            return 1.0\n        sa, sb = _symp(mda), _symp(mdb)\n        if sa and sb and sa == sb:\n            return 0.8\n        if pa and pb:\n            ta = set(re.split(r\"[/\\\\]+\", pa.lower()))\n            tb = set(re.split(r\"[/\\\\]+\", pb.lower()))\n            ta.discard(\"\")\n            tb.discard(\"\")\n            if ta and tb:\n                inter = len(ta & tb)\n                union = max(1, len(ta | tb))\n                return 0.5 * (inter / union)\n        return 0.0",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__merge_and_budget_spans_482": {
      "name": "_merge_and_budget_spans",
      "type": "function",
      "start_line": 482,
      "end_line": 596,
      "content_hash": "df6c1cca29c6e4aeff9f6eaaae17fc81a2589768",
      "content": "def _merge_and_budget_spans(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Given ranked items with metadata path/start_line/end_line, merge nearby spans\n    per path and enforce a token budget using a simple tokens-per-line estimate.\n    \n    Returns a filtered/merged list preserving score order as much as possible.\n    \"\"\"\n    try:\n        merge_lines = int(os.environ.get(\"MICRO_MERGE_LINES\", str(MICRO_MERGE_LINES)) or MICRO_MERGE_LINES)\n    except (ValueError, TypeError):\n        merge_lines = MICRO_MERGE_LINES\n    try:\n        budget_tokens = int(os.environ.get(\"MICRO_BUDGET_TOKENS\", str(MICRO_BUDGET_TOKENS)) or MICRO_BUDGET_TOKENS)\n    except (ValueError, TypeError):\n        budget_tokens = MICRO_BUDGET_TOKENS\n    try:\n        tokens_per_line = int(os.environ.get(\"MICRO_TOKENS_PER_LINE\", str(MICRO_TOKENS_PER_LINE)) or MICRO_TOKENS_PER_LINE)\n    except (ValueError, TypeError):\n        tokens_per_line = MICRO_TOKENS_PER_LINE\n    try:\n        out_max_spans = int(os.environ.get(\"MICRO_OUT_MAX_SPANS\", str(MICRO_OUT_MAX_SPANS)) or MICRO_OUT_MAX_SPANS)\n    except (ValueError, TypeError):\n        out_max_spans = MICRO_OUT_MAX_SPANS\n\n    clusters: Dict[str, List[Dict[str, Any]]] = {}\n    for m in items:\n        md = {}\n        try:\n            if isinstance(m, dict):\n                if m.get(\"path\") or m.get(\"start_line\") or m.get(\"end_line\"):\n                    md = {\"path\": m.get(\"path\"), \"start_line\": m.get(\"start_line\"), \"end_line\": m.get(\"end_line\")}\n                else:\n                    pt = m.get(\"pt\", {})\n                    if hasattr(pt, \"payload\") and getattr(pt, \"payload\"):\n                        md = (pt.payload or {}).get(\"metadata\") or {}\n        except Exception:\n            md = {}\n        path = str((md or {}).get(\"path\") or \"\")\n        start_line = int((md or {}).get(\"start_line\") or 0)\n        end_line = int((md or {}).get(\"end_line\") or 0)\n        if not path or start_line <= 0 or end_line <= 0:\n            continue\n        lst = clusters.setdefault(path, [])\n        merged = False\n        item_score = float(m.get(\"raw_score\") or m.get(\"score\") or m.get(\"s\") or 0.0)\n        for c in lst:\n            if (\n                start_line <= c[\"end\"] + merge_lines\n                and end_line >= c[\"start\"] - merge_lines\n            ):\n                cluster_score = float(c[\"m\"].get(\"raw_score\") or c[\"m\"].get(\"score\") or c[\"m\"].get(\"s\") or 0.0)\n                if item_score > cluster_score:\n                    c[\"m\"] = m\n                c[\"start\"] = min(c[\"start\"], start_line)\n                c[\"end\"] = max(c[\"end\"], end_line)\n                merged = True\n                break\n        if not merged:\n            lst.append({\"start\": start_line, \"end\": end_line, \"m\": m, \"p\": path})\n\n    budget = budget_tokens\n    out: List[Dict[str, Any]] = []\n    per_path_counts: Dict[str, int] = {}\n\n    def _line_tokens(s: int, e: int) -> int:\n        return max(1, (e - s + 1) * tokens_per_line)\n\n    flattened = []\n    for lst in clusters.values():\n        for c in lst:\n            flattened.append(c)\n\n    def _flat_key(c):\n        m = c.get(\"m\", {})\n        path = str(c.get(\"p\") or \"\")\n        start = int(c.get(\"start\") or 0)\n        score = float(m.get(\"raw_score\") or m.get(\"score\") or m.get(\"s\") or 0.0)\n        if score < 0:\n            return (score, path, start)\n        else:\n            return (-score, path, start)\n\n    flattened.sort(key=_flat_key)\n\n    for c in flattened:\n        m = c[\"m\"]\n        path = str(c.get(\"p\") or \"\")\n        if per_path_counts.get(path, 0) >= out_max_spans:\n            continue\n        need = _line_tokens(c[\"start\"], c[\"end\"])\n        if need <= budget:\n            budget -= need\n            per_path_counts[path] = per_path_counts.get(path, 0) + 1\n            out.append({\"m\": m, \"start\": c[\"start\"], \"end\": c[\"end\"], \"need_tokens\": need})\n        elif budget > 0 and per_path_counts.get(path, 0) < out_max_spans:\n            affordable_lines = max(1, budget // tokens_per_line)\n            trim_end = c[\"start\"] + affordable_lines - 1\n            if trim_end >= c[\"start\"]:\n                trimmed_need = _line_tokens(c[\"start\"], trim_end)\n                budget -= trimmed_need\n                per_path_counts[path] = per_path_counts.get(path, 0) + 1\n                out.append({\"m\": m, \"start\": c[\"start\"], \"end\": trim_end, \"need_tokens\": trimmed_need, \"_trimmed\": True})\n        if budget <= 0:\n            break\n\n    result: List[Dict[str, Any]] = []\n    for c in out:\n        m = c[\"m\"]\n        m[\"start_line\"] = c[\"start\"]\n        m[\"end_line\"] = c[\"end\"]\n        m[\"text\"] = \"\"\n        m[\"_merged_start\"] = c[\"start\"]\n        m[\"_merged_end\"] = c[\"end\"]\n        m[\"_budget_tokens\"] = c[\"need_tokens\"]\n        result.append(m)\n    return result",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__line_tokens_545": {
      "name": "_line_tokens",
      "type": "function",
      "start_line": 545,
      "end_line": 546,
      "content_hash": "c21c0588222a77eb8df428e77347e948c6acc1a0",
      "content": "    def _line_tokens(s: int, e: int) -> int:\n        return max(1, (e - s + 1) * tokens_per_line)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__flat_key_553": {
      "name": "_flat_key",
      "type": "function",
      "start_line": 553,
      "end_line": 561,
      "content_hash": "78a957630a9a42a677aa1fb45473e223c78cdfd8",
      "content": "    def _flat_key(c):\n        m = c.get(\"m\", {})\n        path = str(c.get(\"p\") or \"\")\n        start = int(c.get(\"start\") or 0)\n        score = float(m.get(\"raw_score\") or m.get(\"score\") or m.get(\"s\") or 0.0)\n        if score < 0:\n            return (score, path, start)\n        else:\n            return (-score, path, start)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function__detect_implementation_intent_603": {
      "name": "_detect_implementation_intent",
      "type": "function",
      "start_line": 603,
      "end_line": 611,
      "content_hash": "a3131f5fb80d91fde291bdddebf195f5ad9a45af",
      "content": "def _detect_implementation_intent(queries: List[str]) -> bool:\n    \"\"\"Detect if query signals user wants implementation code.\"\"\"\n    if not queries:\n        return False\n    joined = \" \".join(queries).lower()\n    for pattern in _IMPL_INTENT_PATTERNS:\n        if pattern in joined:\n            return True\n    return False",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}