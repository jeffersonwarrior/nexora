{
  "file_path": "/work/external-deps/helix-db/helix-db/benches/hnsw_benches.rs",
  "file_hash": "3c6b659a3a1ef40392e062975b881c20feab03cd",
  "updated_at": "2025-12-26T17:34:23.240808",
  "symbols": {
    "function_setup_temp_env_28": {
      "name": "setup_temp_env",
      "type": "function",
      "start_line": 28,
      "end_line": 41,
      "content_hash": "01cde0a6432ca554b776dce9214f6c789ed4adf7",
      "content": "    fn setup_temp_env() -> Env {\n        let temp_dir = tempfile::tempdir().unwrap();\n        let path = temp_dir.path().to_str().unwrap();\n\n        unsafe {\n            EnvOpenOptions::new()\n                .map_size(20 * 1024 * 1024 * 1024) // 20 GB\n                .max_dbs(10)\n                .open(path)\n                .unwrap()\n        }\n    }\n\n    #[allow(dead_code)]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_fetch_parquet_vectors_42": {
      "name": "fetch_parquet_vectors",
      "type": "function",
      "start_line": 42,
      "end_line": 60,
      "content_hash": "d40a08ffa115f8c1bf512a314761a160fa702f3d",
      "content": "    fn fetch_parquet_vectors() -> Result<(), Box<dyn std::error::Error>> {\n        let urls = [\n            \"https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M/resolve/main/data/train-00002-of-00026-b05ce48965853dad.parquet\",\n            \"https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M/resolve/main/data/train-00000-of-00026-3c7b99d1c7eda36e.parquet\",\n            \"https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M/resolve/main/data/train-00003-of-00026-d116c3c239aa7895.parquet\",\n        ];\n\n        for url in tqdm::new(urls.iter(), urls.len(), None, Some(\"fetching vectors\")) {\n            let res = reqwest::blocking::get(*url).unwrap();\n            //let mut file = File::create(\"output_file\")?;\n            let content = res.bytes()?;\n            println!(\"content: {:?}\", content);\n            //file.write_all(&content)?;\n        }\n\n        Ok(())\n    }\n\n    /// Returns query ids and their associated closest k vectors (by vec id)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_calc_ground_truths_61": {
      "name": "calc_ground_truths",
      "type": "function",
      "start_line": 61,
      "end_line": 118,
      "content_hash": "58051097a74f2948180c11994dcde957e36148be",
      "content": "    fn calc_ground_truths(\n        base_vectors: Vec<HVector>,\n        query_vectors: &Vec<(usize, Vec<f64>)>,\n        k: usize,\n    ) -> HashMap<usize, Vec<u128>> {\n        let base_vectors = Arc::new(base_vectors);\n        let results = Arc::new(Mutex::new(HashMap::new()));\n        let chunk_size = (query_vectors.len() + num_cpus::get() - 1) / num_cpus::get();\n\n        let handles: Vec<_> = query_vectors\n            .chunks(chunk_size)\n            .map(|chunk| {\n                let base_vectors = Arc::clone(&base_vectors);\n                let results = Arc::clone(&results);\n                let chunk = chunk.to_vec();\n\n                thread::spawn(move || {\n                    let local_results: HashMap<usize, Vec<u128>> = chunk\n                        .into_iter()\n                        .map(|(query_id, query_vec)| {\n                            let query_hvector = HVector::from_slice(0, query_vec);\n\n                            let mut distances: Vec<(u128, f64)> = base_vectors\n                                .iter()\n                                .filter_map(|base_vec| {\n                                    query_hvector\n                                        .distance_to(base_vec)\n                                        .map(|dist| (base_vec.id.clone(), dist))\n                                        .ok()\n                                })\n                            .collect();\n\n                            distances.sort_by(|a, b| {\n                                a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n                            });\n\n                            let top_k_ids: Vec<u128> = distances\n                                .into_iter()\n                                .take(k)\n                                .map(|(id, _)| id)\n                                .collect();\n\n                            (query_id, top_k_ids)\n                        })\n                    .collect();\n\n                    results.lock().unwrap().extend(local_results);\n                })\n            })\n        .collect();\n\n        for handle in handles {\n            handle.join().unwrap();\n        }\n\n        Arc::try_unwrap(results).unwrap().into_inner().unwrap()\n    }\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_load_dbpedia_vectors_119": {
      "name": "load_dbpedia_vectors",
      "type": "function",
      "start_line": 119,
      "end_line": 168,
      "content_hash": "01543dffa67a6477fc9e4582d6237a08a25bdfc4",
      "content": "    fn load_dbpedia_vectors(limit: usize) -> Result<Vec<Vec<f64>>, PolarsError> {\n        // https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M\n        if limit > 1_000_000 {\n            return Err(PolarsError::OutOfBounds(\n                \"can't load more than 1,000,000 vecs from this dataset\".into(),\n            ));\n        }\n\n        let data_dir = \"../data/dbpedia-openai-1m/\";\n        let mut all_vectors = Vec::new();\n        let mut total_loaded = 0;\n\n        for entry in fs::read_dir(data_dir)? {\n            let entry = entry?;\n            let path = entry.path();\n\n            if path.is_file() && path.extension().map_or(false, |ext| ext == \"parquet\") {\n                let df = ParquetReader::new(File::open(&path)?)\n                    .finish()?\n                    .lazy()\n                    .limit((limit - total_loaded) as u32)\n                    .collect()?;\n\n                let embeddings = df.column(\"openai\")?.list()?;\n\n                for embedding in embeddings.into_iter() {\n                    if total_loaded >= limit {\n                        break;\n                    }\n\n                    let embedding = embedding.unwrap();\n                    let f64_series = embedding.cast(&DataType::Float64).unwrap();\n                    let chunked = f64_series.f64().unwrap();\n                    let vector: Vec<f64> = chunked.into_no_null_iter().collect();\n\n                    all_vectors.push(vector);\n\n                    total_loaded += 1;\n                }\n\n                if total_loaded >= limit {\n                    break;\n                }\n            }\n        }\n        Ok(all_vectors)\n    }\n\n    /// Higher values of similarity make the vectors more similar\n    #[allow(dead_code)]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_gen_sim_vecs_169": {
      "name": "gen_sim_vecs",
      "type": "function",
      "start_line": 169,
      "end_line": 189,
      "content_hash": "35f81e667154d8fcc4566d8ed080461915776b08",
      "content": "    fn gen_sim_vecs(n: usize, dim: usize, similarity: f64) -> Vec<Vec<f64>> {\n        let mut rng = rand::rng();\n        let mut vectors = Vec::with_capacity(n);\n        let similarity = 1.0 - similarity;\n\n        let base: Vec<f64> = (0..dim).map(|_| rng.random_range(-1.0..1.0)).collect();\n\n        for _ in 0..n {\n            let mut vec = base.clone();\n            for v in vec.iter_mut() {\n                *v += rng.random_range(-similarity..similarity);\n                *v = v.clamp(-1.0, 1.0);\n            }\n            vectors.push(vec);\n        }\n\n        vectors\n    }\n\n    /*\n    #[test]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_bench_hnsw_search_short_190": {
      "name": "bench_hnsw_search_short",
      "type": "function",
      "start_line": 190,
      "end_line": 292,
      "content_hash": "713b1e1dc3c5ff0705c71c4162d24c1792c06625",
      "content": "    fn bench_hnsw_search_short() {\n        //fetch_parquet_vectors().unwrap();\n        let n_base = 4_000;\n        let dims = 950;\n        let vectors = gen_sim_vecs(n_base, dims, 0.8);\n\n        let n_query = 400;\n        let mut rng = rand::rng();\n        let mut shuffled_vectors = vectors.clone();\n        shuffled_vectors.shuffle(&mut rng);\n        let base_vectors = &shuffled_vectors[..n_base - n_query];\n        let query_vectors = &shuffled_vectors[n_base - n_query..];\n\n        println!(\"num of base vecs: {}\", base_vectors.len());\n        println!(\"num of query vecs: {}\", query_vectors.len());\n\n        let k = 10;\n\n        let env = setup_temp_env();\n        let mut txn = env.write_txn().unwrap();\n\n        let mut total_insertion_time = std::time::Duration::from_secs(0);\n        let index = VectorCore::new(&env, &mut txn, HNSWConfig::new(None, None, None)).unwrap();\n\n        let mut all_vectors: Vec<HVector> = Vec::new();\n        let over_all_time = Instant::now();\n        for (i, data) in vectors.iter().enumerate() {\n            let start_time = Instant::now();\n            let vec = index.insert::<Filter>(&mut txn, &data, None).unwrap();\n            let time = start_time.elapsed();\n            all_vectors.push(vec);\n            if i % 1000 == 0 {\n                println!(\"{} => inserting in {} ms\", i, time.as_millis());\n                println!(\"time taken so far: {:?}\", over_all_time.elapsed());\n            }\n            total_insertion_time += time;\n        }\n        txn.commit().unwrap();\n\n        let txn = env.read_txn().unwrap();\n        println!(\"{:?}\", index.config);\n\n        println!(\n            \"total insertion time: {:.2?} seconds\",\n            total_insertion_time.as_secs_f64()\n        );\n        println!(\n            \"average insertion time per vec: {:.2?} milliseconds\",\n            total_insertion_time.as_millis() as f64 / n_base as f64\n        );\n\n        println!(\"calculating ground truths\");\n        let ground_truths = calc_ground_truths(all_vectors, query_vectors.to_vec(), k);\n\n        println!(\"searching and comparing...\");\n        let test_id = format!(\"k = {} with {} queries\", k, n_query);\n\n        let mut total_recall = 0.0;\n        let mut total_precision = 0.0;\n        let mut total_search_time = std::time::Duration::from_secs(0);\n        for ((_, query), gt) in query_vectors.iter().zip(ground_truths.iter()) {\n            let start_time = Instant::now();\n            let results = index.search::<Filter>(&txn, query, k, None, false).unwrap();\n            let search_duration = start_time.elapsed();\n            total_search_time += search_duration;\n\n            let result_indices: HashSet<String> = results\n                .into_iter()\n                .map(|hvector| hvector.get_id().to_string())\n                .collect();\n\n            let gt_indices: HashSet<String> = gt.iter().cloned().collect();\n            //println!(\"gt: {:?}\\nresults: {:?}\\n\", gt_indices, result_indices);\n            let true_positives = result_indices.intersection(&gt_indices).count();\n\n            let recall: f64 = true_positives as f64 / gt_indices.len() as f64;\n            let precision: f64 = true_positives as f64 / result_indices.len() as f64;\n\n            total_recall += recall;\n            total_precision += precision;\n        }\n\n        println!(\n            \"total search time: {:.2?} seconds\",\n            total_search_time.as_secs_f64()\n        );\n        println!(\n            \"average search time per query: {:.2?} milliseconds\",\n            total_search_time.as_millis() as f64 / n_query as f64\n        );\n\n        total_recall = total_recall / n_query as f64;\n        total_precision = total_precision / n_query as f64;\n        println!(\n            \"{}: avg. recall: {:.4?}, avg. precision: {:.4?}\",\n            test_id, total_recall, total_precision\n        );\n        assert!(total_recall >= 0.8, \"recall not high enough!\");\n    }\n    */\n\n    /// Test the precision of the HNSW search algorithm\n    #[test]",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_bench_hnsw_search_long_293": {
      "name": "bench_hnsw_search_long",
      "type": "function",
      "start_line": 293,
      "end_line": 403,
      "content_hash": "960772f113005592f8e4620b89b93d7add5e5218",
      "content": "    fn bench_hnsw_search_long() {\n        let n_base = 5_000;\n        let n_query = 1000; // 10-20%\n        let k = 10;\n        let mut vectors = load_dbpedia_vectors(n_base).unwrap();\n\n        let mut rng = rand::rng();\n        vectors.shuffle(&mut rng);\n\n        let base_vectors = &vectors[..n_base - n_query];\n        let query_vectors = vectors[n_base - n_query..]\n            .to_vec()\n            .iter()\n            .enumerate()\n            .map(|(i, x)| (i + 1, x.clone()))\n            .collect::<Vec<(usize, Vec<f64>)>>();\n\n        println!(\"num of base vecs: {}\", base_vectors.len());\n        println!(\"num of query vecs: {}\", query_vectors.len());\n\n        let env = setup_temp_env();\n        let mut txn = env.write_txn().unwrap();\n        let index = VectorCore::new(&env, &mut txn, HNSWConfig::new(None, None, None)).unwrap();\n        let mut total_insertion_time = std::time::Duration::from_secs(0);\n\n        let mut base_all_vectors: Vec<HVector> = Vec::new();\n        let over_all_time = Instant::now();\n        for (i, data) in base_vectors.iter().enumerate() {\n            let start_time = Instant::now();\n            let vec = index.insert::<Filter>(&mut txn, &data, None).unwrap();\n            let time = start_time.elapsed();\n            base_all_vectors.push(vec);\n            //println!(\"{} => inserting in {} ms\", i, time.as_millis());\n            if i % 500 == 0 {\n                println!(\"{} => inserting in {} ms\", i, time.as_millis());\n                println!(\"time taken so far: {:?}\", over_all_time.elapsed());\n            }\n            total_insertion_time += time;\n        }\n        txn.commit().unwrap();\n\n        let txn = env.read_txn().unwrap();\n        println!(\"{:?}\", index.config);\n\n        println!(\n            \"total insertion time: {:.2?} seconds\",\n            total_insertion_time.as_secs_f64()\n        );\n        println!(\n            \"average insertion time per vec: {:.2?} milliseconds\",\n            total_insertion_time.as_millis() as f64 / n_base as f64\n        );\n\n        let ground_truths = calc_ground_truths(base_all_vectors, &query_vectors, k);\n        println!(\"calculating ground truths\");\n\n        println!(\"searching and comparing...\");\n        let test_id = format!(\"k = {} with {} queries\", k, n_query);\n\n        let mut total_recall = 0.0;\n        let mut total_precision = 0.0;\n        let mut total_search_time = std::time::Duration::from_secs(0);\n        for (qid, query) in query_vectors.iter() {\n            let start_time = Instant::now();\n            let results = index.search::<Filter>(&txn, query, k, \"vector\", None, false).unwrap();\n            let search_duration = start_time.elapsed();\n            total_search_time += search_duration;\n\n            let result_indices = results\n                .into_iter()\n                .map(|hvec| hvec.get_id())\n                .collect::<HashSet<u128>>();\n\n            let gt_indices = ground_truths\n                .get(&qid)\n                .unwrap()\n                .clone()\n                .into_iter()\n                .collect::<HashSet<u128>>();\n\n            println!(\"gt: {:?}\\nresults: {:?}\\n\", gt_indices, result_indices);\n            let true_positives = result_indices.intersection(&gt_indices).count();\n\n            let recall: f64 = true_positives as f64 / gt_indices.len() as f64;\n            let precision: f64 = true_positives as f64 / result_indices.len() as f64;\n\n            total_recall += recall;\n            total_precision += precision;\n        }\n\n        println!(\n            \"total search time: {:.2?} seconds\",\n            total_search_time.as_secs_f64()\n        );\n        println!(\n            \"average search time per query: {:.2?} milliseconds\",\n            total_search_time.as_millis() as f64 / n_query as f64\n        );\n\n        total_recall = total_recall / n_query as f64;\n        total_precision = total_precision / n_query as f64;\n        println!(\n            \"{}: avg. recall: {:.4?}, avg. precision: {:.4?}\",\n            test_id, total_recall, total_precision,\n        );\n        assert!(total_recall >= 0.8, \"recall not high enough!\");\n    }\n}\n\n// TODO: memory benchmark (only the hnsw index ofc)\n",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}