{
  "file_path": "/work/.local/tools/modelscan/sdk/stream/stream.go",
  "file_hash": "a8f2686c9a7bd2b94c45313faddde2dac226f368",
  "updated_at": "2025-12-26T17:34:20.691665",
  "symbols": {
    "struct_Chunk_33": {
      "name": "Chunk",
      "type": "struct",
      "start_line": 33,
      "end_line": 41,
      "content_hash": "88e24d9fba1a77a97df2de82898dd101c426eabb",
      "content": "type Chunk struct {\n\tType     ChunkType              // Type of chunk\n\tData     string                 // Content (for data chunks)\n\tMetadata map[string]interface{} // Additional metadata\n\tRaw      []byte                 // Raw bytes received\n\tError    error                  // Error if Type == ChunkTypeError\n}\n\n// Stream represents a unified streaming interface",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "struct_Stream_42": {
      "name": "Stream",
      "type": "struct",
      "start_line": 42,
      "end_line": 54,
      "content_hash": "b497257eef34348ba8765acc7ffc0dc3992a23c3",
      "content": "type Stream struct {\n\tstreamType StreamType\n\treader     io.Reader\n\tscanner    *bufio.Scanner\n\tchunks     chan *Chunk\n\tdone       chan struct{}\n\terr        error\n\tmu         sync.RWMutex\n\tctx        context.Context\n\tcancel     context.CancelFunc\n}\n\n// NewStream creates a new stream from a reader",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "function_NewStream_55": {
      "name": "NewStream",
      "type": "function",
      "start_line": 55,
      "end_line": 72,
      "content_hash": "a6818599f060a37139d547d90043091bac00463d",
      "content": "func NewStream(ctx context.Context, reader io.Reader, streamType StreamType) *Stream {\n\tctx, cancel := context.WithCancel(ctx)\n\n\ts := &Stream{\n\t\tstreamType: streamType,\n\t\treader:     reader,\n\t\tscanner:    bufio.NewScanner(reader),\n\t\tchunks:     make(chan *Chunk, 10),\n\t\tdone:       make(chan struct{}),\n\t\tctx:        ctx,\n\t\tcancel:     cancel,\n\t}\n\n\tgo s.processStream()\n\treturn s\n}\n\n// Chunks returns a channel that receives chunks as they arrive",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_Chunks_73": {
      "name": "Chunks",
      "type": "method",
      "start_line": 73,
      "end_line": 77,
      "content_hash": "7e7105b338fd47839f140473ab5b470cb2da1095",
      "content": "func (s *Stream) Chunks() <-chan *Chunk {\n\treturn s.chunks\n}\n\n// Close closes the stream and releases resources",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_Close_78": {
      "name": "Close",
      "type": "method",
      "start_line": 78,
      "end_line": 84,
      "content_hash": "9bd48f5da015cfc6fb7435731d21cf31e3fe5f30",
      "content": "func (s *Stream) Close() error {\n\ts.cancel()\n\t<-s.done\n\treturn s.err\n}\n\n// Err returns any error that occurred during streaming",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_Err_85": {
      "name": "Err",
      "type": "method",
      "start_line": 85,
      "end_line": 91,
      "content_hash": "8d9cba10d13ac990b7a50ca5ba5d86608d707adf",
      "content": "func (s *Stream) Err() error {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.err\n}\n\n// processStream reads from the stream and sends chunks",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_processStream_92": {
      "name": "processStream",
      "type": "method",
      "start_line": 92,
      "end_line": 108,
      "content_hash": "1fbc8be0bcf057176dc5b8b2d721afc39c233b1c",
      "content": "func (s *Stream) processStream() {\n\tdefer close(s.chunks)\n\tdefer close(s.done)\n\n\tswitch s.streamType {\n\tcase StreamTypeSSE:\n\t\ts.processSSE()\n\tcase StreamTypeHTTP:\n\t\ts.processHTTP()\n\tcase StreamTypeWebSocket:\n\t\ts.processWebSocket()\n\tdefault:\n\t\ts.setError(fmt.Errorf(\"unsupported stream type: %s\", s.streamType))\n\t}\n}\n\n// processSSE handles Server-Sent Events format",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_processSSE_109": {
      "name": "processSSE",
      "type": "method",
      "start_line": 109,
      "end_line": 143,
      "content_hash": "b3ab97ff77f055cf29e5b9831fef832f53d5ba9c",
      "content": "func (s *Stream) processSSE() {\n\tvar currentEvent strings.Builder\n\n\tfor s.scanner.Scan() {\n\t\tselect {\n\t\tcase <-s.ctx.Done():\n\t\t\ts.setError(s.ctx.Err())\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\tline := s.scanner.Text()\n\n\t\t// Empty line signals end of event\n\t\tif line == \"\" {\n\t\t\tif currentEvent.Len() > 0 {\n\t\t\t\ts.parseSSEEvent(currentEvent.String())\n\t\t\t\tcurrentEvent.Reset()\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// Accumulate event lines\n\t\tif currentEvent.Len() > 0 {\n\t\t\tcurrentEvent.WriteString(\"\\n\")\n\t\t}\n\t\tcurrentEvent.WriteString(line)\n\t}\n\n\tif err := s.scanner.Err(); err != nil {\n\t\ts.setError(err)\n\t}\n}\n\n// parseSSEEvent parses a complete SSE event",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_parseSSEEvent_144": {
      "name": "parseSSEEvent",
      "type": "method",
      "start_line": 144,
      "end_line": 196,
      "content_hash": "aef38970ad806cb6a5cdec4be112ec4cc85cb53f",
      "content": "func (s *Stream) parseSSEEvent(event string) {\n\tlines := strings.Split(event, \"\\n\")\n\tchunk := &Chunk{\n\t\tType:     ChunkTypeData,\n\t\tMetadata: make(map[string]interface{}),\n\t}\n\n\tvar hasData bool\n\n\tfor _, line := range lines {\n\t\tif strings.HasPrefix(line, \"data: \") {\n\t\t\tdata := strings.TrimPrefix(line, \"data: \")\n\t\t\thasData = true\n\n\t\t\t// Check for [DONE] marker (OpenAI convention)\n\t\t\tif data == \"[DONE]\" {\n\t\t\t\tchunk.Type = ChunkTypeDone\n\t\t\t\ts.sendChunk(chunk)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Try to parse as JSON\n\t\t\tvar jsonData map[string]interface{}\n\t\t\tif err := json.Unmarshal([]byte(data), &jsonData); err == nil {\n\t\t\t\t// Extract content from various provider formats\n\t\t\t\tif content := s.extractContent(jsonData); content != \"\" {\n\t\t\t\t\tchunk.Data = content\n\t\t\t\t}\n\t\t\t\t// Merge JSON data into metadata\n\t\t\t\tfor k, v := range jsonData {\n\t\t\t\t\tchunk.Metadata[k] = v\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Plain text data\n\t\t\t\tchunk.Data = data\n\t\t\t}\n\t\t\tchunk.Raw = []byte(data)\n\t\t} else if strings.HasPrefix(line, \"event: \") {\n\t\t\tchunk.Metadata[\"event\"] = strings.TrimPrefix(line, \"event: \")\n\t\t} else if strings.HasPrefix(line, \"id: \") {\n\t\t\tchunk.Metadata[\"id\"] = strings.TrimPrefix(line, \"id: \")\n\t\t} else if strings.HasPrefix(line, \"retry: \") {\n\t\t\tchunk.Metadata[\"retry\"] = strings.TrimPrefix(line, \"retry: \")\n\t\t}\n\t}\n\n\t// Only send if we got data or meaningful metadata\n\tif hasData || len(chunk.Metadata) > 0 {\n\t\ts.sendChunk(chunk)\n\t}\n}\n\n// processHTTP handles plain HTTP chunked responses",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_processHTTP_197": {
      "name": "processHTTP",
      "type": "method",
      "start_line": 197,
      "end_line": 226,
      "content_hash": "e7b9240dcc93b4a01030d03df7562043f158e61c",
      "content": "func (s *Stream) processHTTP() {\n\tbuf := make([]byte, 4096)\n\tfor {\n\t\tselect {\n\t\tcase <-s.ctx.Done():\n\t\t\ts.setError(s.ctx.Err())\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\tn, err := s.reader.Read(buf)\n\t\tif n > 0 {\n\t\t\tchunk := &Chunk{\n\t\t\t\tType: ChunkTypeData,\n\t\t\t\tData: string(buf[:n]),\n\t\t\t\tRaw:  buf[:n],\n\t\t\t}\n\t\t\ts.sendChunk(chunk)\n\t\t}\n\n\t\tif err != nil {\n\t\t\tif err != io.EOF {\n\t\t\t\ts.setError(err)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// processWebSocket handles WebSocket frames (placeholder)",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_processWebSocket_227": {
      "name": "processWebSocket",
      "type": "method",
      "start_line": 227,
      "end_line": 233,
      "content_hash": "4e658c52910d69e2ccd84683c261ca1753ae2316",
      "content": "func (s *Stream) processWebSocket() {\n\t// WebSocket implementation would go here\n\t// For now, treat like HTTP chunks\n\ts.processHTTP()\n}\n\n// extractContent extracts content from various provider response formats",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_extractContent_234": {
      "name": "extractContent",
      "type": "method",
      "start_line": 234,
      "end_line": 287,
      "content_hash": "f10d3e70d29076a2854e6506033651592dfbe97d",
      "content": "func (s *Stream) extractContent(data map[string]interface{}) string {\n\t// OpenAI format: choices[0].delta.content\n\tif choices, ok := data[\"choices\"].([]interface{}); ok && len(choices) > 0 {\n\t\tif choice, ok := choices[0].(map[string]interface{}); ok {\n\t\t\tif delta, ok := choice[\"delta\"].(map[string]interface{}); ok {\n\t\t\t\tif content, ok := delta[\"content\"].(string); ok {\n\t\t\t\t\treturn content\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Anthropic format: delta.text or content_block.text\n\tif delta, ok := data[\"delta\"].(map[string]interface{}); ok {\n\t\tif text, ok := delta[\"text\"].(string); ok {\n\t\t\treturn text\n\t\t}\n\t}\n\tif contentBlock, ok := data[\"content_block\"].(map[string]interface{}); ok {\n\t\tif text, ok := contentBlock[\"text\"].(string); ok {\n\t\t\treturn text\n\t\t}\n\t}\n\n\t// Google format: candidates[0].content.parts[0].text\n\tif candidates, ok := data[\"candidates\"].([]interface{}); ok && len(candidates) > 0 {\n\t\tif candidate, ok := candidates[0].(map[string]interface{}); ok {\n\t\t\tif content, ok := candidate[\"content\"].(map[string]interface{}); ok {\n\t\t\t\tif parts, ok := content[\"parts\"].([]interface{}); ok && len(parts) > 0 {\n\t\t\t\t\tif part, ok := parts[0].(map[string]interface{}); ok {\n\t\t\t\t\t\tif text, ok := part[\"text\"].(string); ok {\n\t\t\t\t\t\t\treturn text\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Generic fallback: look for \"text\", \"content\", or \"message\" keys\n\tif text, ok := data[\"text\"].(string); ok {\n\t\treturn text\n\t}\n\tif content, ok := data[\"content\"].(string); ok {\n\t\treturn content\n\t}\n\tif message, ok := data[\"message\"].(string); ok {\n\t\treturn message\n\t}\n\n\treturn \"\"\n}\n\n// sendChunk sends a chunk to the channel",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_sendChunk_288": {
      "name": "sendChunk",
      "type": "method",
      "start_line": 288,
      "end_line": 296,
      "content_hash": "9f807eff631eb51dd01f546e5ce438c484b07ce8",
      "content": "func (s *Stream) sendChunk(chunk *Chunk) {\n\tselect {\n\tcase s.chunks <- chunk:\n\tcase <-s.ctx.Done():\n\t\treturn\n\t}\n}\n\n// setError sets the error state",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_setError_297": {
      "name": "setError",
      "type": "method",
      "start_line": 297,
      "end_line": 305,
      "content_hash": "9840bc21f26d444c4adfd2461f448d576d1d0504",
      "content": "func (s *Stream) setError(err error) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif s.err == nil {\n\t\ts.err = err\n\t}\n}\n\n// Collect accumulates all chunks into a single string",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_Collect_306": {
      "name": "Collect",
      "type": "method",
      "start_line": 306,
      "end_line": 322,
      "content_hash": "621aebb9535f20f710801faf8f199159b41fbd12",
      "content": "func (s *Stream) Collect() (string, error) {\n\tvar builder strings.Builder\n\tfor chunk := range s.chunks {\n\t\tif chunk.Type == ChunkTypeError {\n\t\t\treturn \"\", chunk.Error\n\t\t}\n\t\tif chunk.Type == ChunkTypeDone {\n\t\t\tbreak\n\t\t}\n\t\tif chunk.Data != \"\" {\n\t\t\tbuilder.WriteString(chunk.Data)\n\t\t}\n\t}\n\treturn builder.String(), s.Err()\n}\n\n// Filter creates a new stream with only chunks matching the predicate",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_Filter_323": {
      "name": "Filter",
      "type": "method",
      "start_line": 323,
      "end_line": 349,
      "content_hash": "75ca60e6035daae8db7fb2713d347cac6c698ac0",
      "content": "func (s *Stream) Filter(predicate func(*Chunk) bool) *Stream {\n\tfiltered := &Stream{\n\t\tstreamType: s.streamType,\n\t\tchunks:     make(chan *Chunk, 10),\n\t\tdone:       make(chan struct{}),\n\t\tctx:        s.ctx,\n\t}\n\n\tgo func() {\n\t\tdefer close(filtered.chunks)\n\t\tdefer close(filtered.done)\n\n\t\tfor chunk := range s.chunks {\n\t\t\tif predicate(chunk) {\n\t\t\t\tselect {\n\t\t\t\tcase filtered.chunks <- chunk:\n\t\t\t\tcase <-filtered.ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn filtered\n}\n\n// Map transforms chunks using the provided function",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_Map_350": {
      "name": "Map",
      "type": "method",
      "start_line": 350,
      "end_line": 377,
      "content_hash": "568d911df6b341161a37b303a3f39b7f4235cc7a",
      "content": "func (s *Stream) Map(transform func(*Chunk) *Chunk) *Stream {\n\tmapped := &Stream{\n\t\tstreamType: s.streamType,\n\t\tchunks:     make(chan *Chunk, 10),\n\t\tdone:       make(chan struct{}),\n\t\tctx:        s.ctx,\n\t}\n\n\tgo func() {\n\t\tdefer close(mapped.chunks)\n\t\tdefer close(mapped.done)\n\n\t\tfor chunk := range s.chunks {\n\t\t\ttransformed := transform(chunk)\n\t\t\tif transformed != nil {\n\t\t\t\tselect {\n\t\t\t\tcase mapped.chunks <- transformed:\n\t\t\t\tcase <-mapped.ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn mapped\n}\n\n// Tap allows observing chunks without modifying the stream",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    },
    "method_Tap_378": {
      "name": "Tap",
      "type": "method",
      "start_line": 378,
      "end_line": 383,
      "content_hash": "18acae6edb1053e1a7340c2459c7400eb8164b82",
      "content": "func (s *Stream) Tap(observer func(*Chunk)) *Stream {\n\treturn s.Map(func(chunk *Chunk) *Chunk {\n\t\tobserver(chunk)\n\t\treturn chunk\n\t})\n}",
      "pseudo": "",
      "tags": [],
      "qdrant_ids": []
    }
  }
}